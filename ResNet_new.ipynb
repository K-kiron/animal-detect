{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d60b6b",
   "metadata": {},
   "source": [
    "Lien de téléchargement des données: https://cvml.ista.ac.at/AwA2/        \n",
    "\n",
    "13GB file : https://cvml.ista.ac.at/AwA2/AwA2-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e2f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e50c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"ResNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf76634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copytree('/content/drive/MyDrive/Animals_with_Attributes2/','AWA4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d23f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 # Pour utiliser open_cv, il faut la version de python est 3.7\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision \n",
    "from torchvision.io import read_image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant. Should be the path to the folder named JPEGImages, containing the 33K images in its subfolders.\n",
    "# DATA_FOLDER_PATH = \"C:\\\\Users\\othma\\\\OneDrive - Universite de Montreal\\\\IFT3710-Data\\\\Animals_with_Attributes2\\\\\"\n",
    "# JPEGIMAGES_FOLDER_PATH = \"C:\\\\Users\\othma\\\\OneDrive - Universite de Montreal\\\\IFT3710-Data\\\\Animals_with_Attributes2\\\\JPEGImages\\\\\"\n",
    "DATA_FOLDER_PATH = '/content/AWA4/'\n",
    "JPEGIMAGES_FOLDER_PATH = '/content/AWA4/JPEGImages/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf49382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73113c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(764, 918, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "test = JPEGIMAGES_FOLDER_PATH+\"fox/fox_10001.jpg\"\n",
    "img = cv2.imread(test) \n",
    "print(img.shape) #ndarray\n",
    "print(type(img))\n",
    "cv2_imshow(img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e0165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 768, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "test = JPEGIMAGES_FOLDER_PATH+\"antelope/antelope_10015.jpg\"\n",
    "img = cv2.imread(test) \n",
    "print(img.shape) #ndarray\n",
    "print(type(img))\n",
    "cv2.imshow(img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f192da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antelope', 'bat', 'beaver', 'blue+whale', 'bobcat', 'buffalo', 'chihuahua', 'chimpanzee', 'collie', 'cow', 'dalmatian', 'deer', 'dolphin', 'elephant', 'fox', 'german+shepherd', 'giant+panda', 'giraffe', 'gorilla', 'grizzly+bear', 'hamster', 'hippopotamus', 'horse', 'humpback+whale', 'killer+whale', 'leopard', 'lion', 'mole', 'moose', 'mouse', 'otter', 'ox', 'persian+cat', 'pig', 'polar+bear', 'rabbit', 'raccoon', 'rat', 'rhinoceros', 'seal', 'sheep', 'siamese+cat', 'skunk', 'spider+monkey', 'squirrel', 'tiger', 'walrus', 'weasel', 'wolf', 'zebra']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dirs = os.listdir(JPEGIMAGES_FOLDER_PATH)\n",
    "labels_dirs = [f for f in labels_dirs if not f.startswith('.')] # remove hidden files\n",
    "print(labels_dirs)\n",
    "len(labels_dirs) # 50 labels / subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60066a3e",
   "metadata": {},
   "source": [
    "# Note : Some labels have a low number of images. \n",
    "\n",
    "## Possible solutions to explore : \n",
    "    Data augmentation : creating new training data by applying random transformations to existing images, such as rotating, cropping, or flipping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade44228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antelope': 1046, 'bat': 383, 'beaver': 193, 'blue+whale': 174, 'bobcat': 630, 'buffalo': 895, 'chihuahua': 567, 'chimpanzee': 728, 'collie': 1028, 'cow': 1338, 'dalmatian': 549, 'deer': 1344, 'dolphin': 946, 'elephant': 1038, 'fox': 664, 'german+shepherd': 1033, 'giant+panda': 874, 'giraffe': 1202, 'gorilla': 872, 'grizzly+bear': 852, 'hamster': 779, 'hippopotamus': 684, 'horse': 1645, 'humpback+whale': 709, 'killer+whale': 291, 'leopard': 720, 'lion': 1019, 'mole': 100, 'moose': 704, 'mouse': 185, 'otter': 758, 'ox': 728, 'persian+cat': 747, 'pig': 713, 'polar+bear': 868, 'rabbit': 1088, 'raccoon': 512, 'rat': 310, 'rhinoceros': 696, 'seal': 988, 'sheep': 1420, 'siamese+cat': 500, 'skunk': 188, 'spider+monkey': 291, 'squirrel': 1200, 'tiger': 877, 'walrus': 215, 'weasel': 272, 'wolf': 589, 'zebra': 1170}\n",
      "{'antelope': 0.028, 'bat': 0.0103, 'beaver': 0.0052, 'blue+whale': 0.0047, 'bobcat': 0.0169, 'buffalo': 0.024, 'chihuahua': 0.0152, 'chimpanzee': 0.0195, 'collie': 0.0275, 'cow': 0.0359, 'dalmatian': 0.0147, 'deer': 0.036, 'dolphin': 0.0253, 'elephant': 0.0278, 'fox': 0.0178, 'german+shepherd': 0.0277, 'giant+panda': 0.0234, 'giraffe': 0.0322, 'gorilla': 0.0234, 'grizzly+bear': 0.0228, 'hamster': 0.0209, 'hippopotamus': 0.0183, 'horse': 0.0441, 'humpback+whale': 0.019, 'killer+whale': 0.0078, 'leopard': 0.0193, 'lion': 0.0273, 'mole': 0.0027, 'moose': 0.0189, 'mouse': 0.005, 'otter': 0.0203, 'ox': 0.0195, 'persian+cat': 0.02, 'pig': 0.0191, 'polar+bear': 0.0233, 'rabbit': 0.0292, 'raccoon': 0.0137, 'rat': 0.0083, 'rhinoceros': 0.0186, 'seal': 0.0265, 'sheep': 0.038, 'siamese+cat': 0.0134, 'skunk': 0.005, 'spider+monkey': 0.0078, 'squirrel': 0.0322, 'tiger': 0.0235, 'walrus': 0.0058, 'weasel': 0.0073, 'wolf': 0.0158, 'zebra': 0.0313}\n"
     ]
    }
   ],
   "source": [
    "def find_num_images_per_label(img_dir = JPEGIMAGES_FOLDER_PATH) -> tuple[dict,dict]: \n",
    "    \"\"\" \n",
    "    USEFUL FOR SAMPLING.\n",
    "    Return a dict with keys as the 50 labels, and values being the number of images in each subdirectory corresponding to label\n",
    "    and a second dict with the relative numbers (proportion) for every label compared to the total number of images (useful for sampling)\"\"\"\n",
    "    labels_dirs = os.listdir(img_dir)\n",
    "    num_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    proportions_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    total_num_images = 0\n",
    "\n",
    "    # Update absolute number of images per label\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        specific_label_path = os.path.join(img_dir, labels_dirs[i])\n",
    "        num_images_label = len(os.listdir(specific_label_path))\n",
    "        total_num_images += num_images_label\n",
    "        num_images_per_label[label] = num_images_label\n",
    "\n",
    "    # Update relative number of images per label (proportion)\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        num_images_label = num_images_per_label[label]\n",
    "        proportion_label = round(num_images_label / total_num_images, 4)\n",
    "        proportions_images_per_label[label] = proportion_label\n",
    "\n",
    "    return num_images_per_label, proportions_images_per_label\n",
    "\n",
    "num_images_per_label, proportions_images_per_label = find_num_images_per_label()\n",
    "print(num_images_per_label)\n",
    "print(proportions_images_per_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existent annotations.csv file.\n",
      " ---------------------------\n",
      "Sucessfully created annotations.csv file.\n"
     ]
    }
   ],
   "source": [
    "ANNOTATIONS_FILENAME = 'annotations.csv'\n",
    "\n",
    "def create_annotations_csv_file(annotations_filename = ANNOTATIONS_FILENAME, img_dir = JPEGIMAGES_FOLDER_PATH) : \n",
    "    \"\"\" \n",
    "    Create a csv annotations_file, annotations.csv, with two columns, in the format : \n",
    "                        path/to/image, label\n",
    "    \n",
    "    The annotation csv is necessary for DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    labels_dirs:list = os.listdir(img_dir)\n",
    "   \n",
    "    if os.path.exists(annotations_filename):\n",
    "        os.remove(annotations_filename)\n",
    "        print(f'Deleted existent {ANNOTATIONS_FILENAME} file.\\n ---------------------------')\n",
    "    \n",
    "    with open(annotations_filename, 'w', newline='') as file :\n",
    "        writer = csv.writer(file, dialect='excel', delimiter=',')\n",
    "\n",
    "        for i, label in enumerate(labels_dirs) : \n",
    "\n",
    "            specific_label_path = os.path.join(img_dir, label)\n",
    "            images_names = os.listdir(specific_label_path)\n",
    "\n",
    "            for j, image_name in enumerate(images_names):\n",
    "                full_path_to_img= os.path.join(specific_label_path, image_name)\n",
    "                full_path_to_img= os.path.join(label, image_name)\n",
    "\n",
    "                row = [full_path_to_img, label]\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f'Sucessfully created {ANNOTATIONS_FILENAME} file.')\n",
    "\n",
    "#\n",
    "create_annotations_csv_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antelope': 0, 'grizzly+bear': 1, 'killer+whale': 2, 'beaver': 3, 'dalmatian': 4, 'persian+cat': 5, 'horse': 6, 'german+shepherd': 7, 'blue+whale': 8, 'siamese+cat': 9, 'skunk': 10, 'mole': 11, 'tiger': 12, 'hippopotamus': 13, 'leopard': 14, 'moose': 15, 'spider+monkey': 16, 'humpback+whale': 17, 'elephant': 18, 'gorilla': 19, 'ox': 20, 'fox': 21, 'sheep': 22, 'seal': 23, 'chimpanzee': 24, 'hamster': 25, 'squirrel': 26, 'rhinoceros': 27, 'rabbit': 28, 'bat': 29, 'giraffe': 30, 'wolf': 31, 'chihuahua': 32, 'rat': 33, 'weasel': 34, 'otter': 35, 'buffalo': 36, 'zebra': 37, 'giant+panda': 38, 'deer': 39, 'bobcat': 40, 'pig': 41, 'lion': 42, 'mouse': 43, 'polar+bear': 44, 'collie': 45, 'walrus': 46, 'raccoon': 47, 'cow': 48, 'dolphin': 49}\n"
     ]
    }
   ],
   "source": [
    "# labels_in_number = pd.read_csv(DATA_FOLDER_PATH+\"classes.txt\", delim_whitespace=True,header=None)\n",
    "labels_dict = {}\n",
    "with open(DATA_FOLDER_PATH+\"classes.txt\") as f:\n",
    "    for line in f:\n",
    "        # print(line.split())\n",
    "        (key,val) = line.split()\n",
    "        labels_dict[val] = int(key)-1\n",
    "print(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08979242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image, ImageReadMode\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class AWA2Dataset(Dataset): # Dataset class to serve as input for the DataLoader.\n",
    "    \"\"\" \n",
    "    Dataset class to serve as input for the DataLoader.\n",
    "    Implements all the required methods and more. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotations_file=ANNOTATIONS_FILENAME, img_dir=JPEGIMAGES_FOLDER_PATH, \n",
    "                transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        numbers_infos_dicts: tuple[dict,dict] = find_num_images_per_label(img_dir=JPEGIMAGES_FOLDER_PATH)\n",
    "        self.num_images_per_label = numbers_infos_dicts[0]\n",
    "        self.proportions_images_per_label = numbers_infos_dicts[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # img_path = self.img_labels.iloc[idx, 0]\n",
    "        key = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        # Mapping the labels from string to tensor\n",
    "        label = labels_dict[key]\n",
    "\n",
    "        image = read_image(path = img_path, mode = ImageReadMode.RGB)\n",
    "        # with open(img_path, 'rb') as f:\n",
    "        #     image = Image.open(f)\n",
    "        #     image = image.convert('RGB')  # convert to RGB\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class Subset_(AWA2Dataset) : \n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        original_index_in_AWA2Dataset = self.indices[index]\n",
    "        image, label = self.dataset[original_index_in_AWA2Dataset]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a923daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_pipeline_train = transforms.Compose([\n",
    "                    ## Input size\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize((256,256)),\n",
    "                    # transforms.Grayscale(num_output_channels=3),\n",
    "                    \n",
    "                    ## Data augmentation \n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.RandomHorizontalFlip(p=0.4),\n",
    "                    # transforms.RandomApply(transforms.RandAugment(), p=0.4), # 40% of the time, apply a random additional combo of transformations #https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html\n",
    "                    transforms.ColorJitter(brightness=0.2,\n",
    "                                            contrast=0.2,\n",
    "                                            saturation=0.2,\n",
    "                                            hue=0.1),\n",
    "                    transforms.RandomCrop((224,224)),  # transforms.RandomResizedCrop(size=(224,224), scale=(0.6, 0.9), ratio=(0.5, 1.08,))\n",
    "                    ## Normalize\n",
    "                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the \n",
    "                    transforms.Normalize(mean = [0.4643, 0.4640, 0.3985] , std=[0.2521, 0.2425, 0.2538]) # real mean and std of AwA2\n",
    "                ])\n",
    "\n",
    "\n",
    "transforms_pipeline_test = transforms.Compose([\n",
    "                    ## Input size\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize((256,256)),\n",
    "                    # transforms.Grayscale(num_output_channels=3),\n",
    "                    transforms.CenterCrop((224,224)),   \n",
    "                    ## Normalize\n",
    "                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the \n",
    "                    transforms.Normalize(mean = [0.4643, 0.4640, 0.3985] , std=[0.2521, 0.2425, 0.2538]) # real mean and std of AwA2\n",
    "                ])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize dataset and train/valid/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = AWA2Dataset()\n",
    "n_images = len(dataset)\n",
    "# Split all indices into training/testing sets\n",
    "train_indices, test_indices = train_test_split(range(n_images), test_size=0.2, random_state=1)\n",
    "# Split training indices into training/validation sets.\n",
    "train_indices, valid_indices = train_test_split(train_indices, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "# Initialize the 3 DataSet objects (as Subset_) and apply the relevant Transforms to each subset (train/test/valid)\n",
    "train_data = Subset_(dataset, train_indices, transform = transforms_pipeline_train)\n",
    "valid_data = Subset_(dataset, valid_indices, transform = transforms_pipeline_test)\n",
    "test_data  = Subset_(dataset, test_indices, transform = transforms_pipeline_test) \n",
    "\n",
    "# Initalize DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True)\n",
    "test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64d291b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('horse', 'giraffe', 'hippopotamus', 'tiger') tensor([[[[226, 227, 226,  ..., 225, 224, 224],\n",
      "          [227, 224, 228,  ..., 226, 223, 223],\n",
      "          [226, 229, 225,  ..., 225, 223, 221],\n",
      "          ...,\n",
      "          [170, 137, 167,  ..., 171, 169, 187],\n",
      "          [220, 227, 220,  ..., 160, 229, 228],\n",
      "          [208, 208, 217,  ..., 180, 163, 188]],\n",
      "\n",
      "         [[232, 232, 231,  ..., 225, 225, 225],\n",
      "          [231, 228, 232,  ..., 225, 227, 227],\n",
      "          [232, 230, 229,  ..., 224, 226, 224],\n",
      "          ...,\n",
      "          [165, 131, 166,  ..., 169, 168, 184],\n",
      "          [219, 227, 217,  ..., 156, 229, 220],\n",
      "          [207, 207, 218,  ..., 176, 158, 185]],\n",
      "\n",
      "         [[233, 232, 232,  ..., 229, 228, 229],\n",
      "          [233, 229, 233,  ..., 230, 230, 230],\n",
      "          [233, 234, 230,  ..., 230, 229, 229],\n",
      "          ...,\n",
      "          [165, 141, 172,  ..., 164, 160, 178],\n",
      "          [214, 221, 211,  ..., 145, 221, 217],\n",
      "          [202, 204, 213,  ..., 173, 153, 179]]],\n",
      "\n",
      "\n",
      "        [[[111, 115, 102,  ...,  70,  73,  77],\n",
      "          [112,  99,  82,  ...,  68,  71,  85],\n",
      "          [ 64,  72,  97,  ...,  70,  66,  58],\n",
      "          ...,\n",
      "          [136, 184, 130,  ..., 132, 103, 122],\n",
      "          [150, 124, 123,  ..., 111, 123, 114],\n",
      "          [138, 124, 119,  ..., 129, 132, 143]],\n",
      "\n",
      "         [[ 99,  92,  85,  ...,  56,  58,  58],\n",
      "          [ 93,  86,  75,  ...,  51,  61,  67],\n",
      "          [ 56,  68, 100,  ...,  55,  52,  48],\n",
      "          ...,\n",
      "          [120, 129,  90,  ..., 115,  98, 112],\n",
      "          [128, 104,  98,  ..., 112, 109, 107],\n",
      "          [117, 111, 107,  ..., 121, 109, 114]],\n",
      "\n",
      "         [[ 49,  53,  54,  ...,  36,  33,  33],\n",
      "          [ 54,  53,  43,  ...,  36,  36,  38],\n",
      "          [ 34,  40,  52,  ...,  35,  31,  29],\n",
      "          ...,\n",
      "          [ 73,  72,  42,  ...,  76,  52,  66],\n",
      "          [ 88,  63,  50,  ...,  62,  63,  61],\n",
      "          [ 78,  69,  64,  ...,  67,  62,  72]]],\n",
      "\n",
      "\n",
      "        [[[149, 152, 153,  ..., 137, 136, 136],\n",
      "          [146, 147, 149,  ..., 136, 135, 140],\n",
      "          [137, 142, 151,  ..., 135, 136, 140],\n",
      "          ...,\n",
      "          [ 96,  99,  95,  ..., 148, 144, 152],\n",
      "          [108, 106,  97,  ..., 158, 163, 176],\n",
      "          [101, 104, 107,  ..., 223, 213, 205]],\n",
      "\n",
      "         [[135, 135, 134,  ..., 118, 117, 117],\n",
      "          [130, 131, 132,  ..., 117, 116, 120],\n",
      "          [125, 126, 133,  ..., 118, 117, 122],\n",
      "          ...,\n",
      "          [ 89,  91,  88,  ..., 122, 121, 127],\n",
      "          [100,  98,  89,  ..., 127, 132, 149],\n",
      "          [ 95,  97, 100,  ..., 216, 198, 190]],\n",
      "\n",
      "         [[ 62,  60,  59,  ...,  50,  49,  50],\n",
      "          [ 58,  56,  54,  ...,  49,  50,  51],\n",
      "          [ 53,  51,  55,  ...,  49,  48,  50],\n",
      "          ...,\n",
      "          [ 44,  48,  44,  ...,  53,  54,  45],\n",
      "          [ 49,  49,  40,  ...,  75,  77, 107],\n",
      "          [ 43,  46,  45,  ..., 178, 158, 153]]],\n",
      "\n",
      "\n",
      "        [[[ 88,  90, 103,  ...,   0,   2,   5],\n",
      "          [122, 127, 118,  ...,  17,   2,   6],\n",
      "          [120, 101,  96,  ...,  18,  20,  19],\n",
      "          ...,\n",
      "          [206, 243, 251,  ..., 162, 149, 142],\n",
      "          [189, 179, 215,  ..., 161, 155, 161],\n",
      "          [240, 221, 162,  ..., 138, 150, 178]],\n",
      "\n",
      "         [[ 61,  66,  84,  ...,  19,  23,  28],\n",
      "          [ 95, 101,  99,  ...,  37,  22,  31],\n",
      "          [ 92,  78,  78,  ...,  39,  43,  42],\n",
      "          ...,\n",
      "          [199, 236, 244,  ..., 108, 101, 101],\n",
      "          [184, 174, 210,  ..., 110, 105, 111],\n",
      "          [236, 219, 159,  ...,  93, 104, 128]],\n",
      "\n",
      "         [[ 32,  38,  57,  ...,  10,  16,  20],\n",
      "          [ 67,  74,  70,  ...,  28,  15,  23],\n",
      "          [ 60,  44,  41,  ...,  30,  35,  34],\n",
      "          ...,\n",
      "          [181, 218, 226,  ...,  48,  47,  49],\n",
      "          [162, 154, 191,  ...,  53,  53,  58],\n",
      "          [214, 198, 140,  ...,  34,  48,  66]]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# Experiment with DataLoader. Everything works good\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=4, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "data = next(dataiter)\n",
    "\n",
    "images, labels = data \n",
    "print(labels, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8756690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37321 9331\n",
      "epoch 1 / 2, step, 5/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 1 / 2, step, 10/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 1 / 2, step, 15/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 1 / 2, step, 20/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "Completed\n",
      "epoch 2 / 2, step, 5/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 2 / 2, step, 10/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 2 / 2, step, 15/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 2 / 2, step, 20/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Training loop example\n",
    "num_epochs = 2 \n",
    "batch_size = 4\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs) : \n",
    "    # loop over trainloader \n",
    "    for i, (inputs, labels) in enumerate(dataloader) : \n",
    "        \n",
    "        # Do forward and backward pass, update the weights \n",
    "        if(i+1) % 5 == 0 :\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step, {i+1}/{n_iterations}, inputs {inputs.shape}')\n",
    "\n",
    "        if i==20 : \n",
    "            print('Completed')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7e548da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37321 9331\n",
      "epoch 1 / 2, step, 5/9331, inputs torch.Size([4, 3, 256, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\othma\\OneDrive - Universite de Montreal\\IFT3710-Data\\AWA2_Dataloader.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=8'>9</a>\u001b[0m i_global \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs) : \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=10'>11</a>\u001b[0m     \u001b[39m# loop over trainloader \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader) : \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=12'>13</a>\u001b[0m         \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=13'>14</a>\u001b[0m         \u001b[39m# Do forward and backward pass, update the weights \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=14'>15</a>\u001b[0m         i_global \u001b[39m=\u001b[39m i\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=15'>16</a>\u001b[0m         \u001b[39mif\u001b[39;00m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m :\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\othma\\OneDrive - Universite de Montreal\\IFT3710-Data\\AWA2_Dataloader.ipynb Cell 14\u001b[0m in \u001b[0;36mAWA2Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=22'>23</a>\u001b[0m \u001b[39m# img_path = self.img_labels.iloc[idx, 0]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=23'>24</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_labels\u001b[39m.\u001b[39miloc[idx, \u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=25'>26</a>\u001b[0m image \u001b[39m=\u001b[39m read_image(img_path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/othma/OneDrive%20-%20Universite%20de%20Montreal/IFT3710-Data/AWA2_Dataloader.ipynb#ch0000014?line=27'>28</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\io\\image.py:253\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m    252\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m--> 253\u001b[0m data \u001b[39m=\u001b[39m read_file(path)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\io\\image.py:47\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m     46\u001b[0m     _log_api_usage_once(read_file)\n\u001b[1;32m---> 47\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mread_file(path)\n\u001b[0;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    438\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    439\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    440\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    441\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_op(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs \u001b[39mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop example\n",
    "num_epochs = 2 \n",
    "batch_size = 4\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=batch_size, shuffle=True)\n",
    "i_global = 0\n",
    "for epoch in range(num_epochs) : \n",
    "    # loop over trainloader \n",
    "    for i, (inputs, labels) in enumerate(dataloader) : \n",
    "        \n",
    "        # Do forward and backward pass, update the weights \n",
    "        i_global = i\n",
    "        if(i+1) % 5 == 0 :\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step, {i+1}/{n_iterations}, inputs {inputs.shape}')\n",
    "\n",
    "        # if i==20 : \n",
    "        #     print('Completed')\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7183ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "# resnet = models.resnet18(pretrained=True)\n",
    "resnet = models.resnet34(pretrained=True)\n",
    "print(resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0839c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle= True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle= True)\n",
    "\n",
    "model = resnet\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training statistics\n",
    "        if (i+0) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44898873",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval()  # switch to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "\n",
    "        outputs = resnet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the model on the test images: {:.2f}%'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
