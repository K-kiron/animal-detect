{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d60b6b",
   "metadata": {},
   "source": [
    "Lien de téléchargement des données: https://cvml.ista.ac.at/AwA2/        \n",
    "\n",
    "13GB file : https://cvml.ista.ac.at/AwA2/AwA2-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d23f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 # Pour utiliser open_cv, il faut la version de python est 3.7\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision \n",
    "from torchvision.io import read_image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8e4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant. Should be the path to the folder named JPEGImages, containing the 33K images in its subfolders.\n",
    "JPEGIMAGES_FOLDER_PATH = \"C:/Users/1/Desktop/data_ift3710/Animals_with_Attributes2/JPEGImages/\"\n",
    "path_project = \"C:/Users/1/Desktop/data_ift3710/Animals_with_Attributes2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f192da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antelope', 'bat', 'beaver', 'blue+whale', 'bobcat', 'buffalo', 'chihuahua', 'chimpanzee', 'collie', 'cow', 'dalmatian', 'deer', 'dolphin', 'elephant', 'fox', 'german+shepherd', 'giant+panda', 'giraffe', 'gorilla', 'grizzly+bear', 'hamster', 'hippopotamus', 'horse', 'humpback+whale', 'killer+whale', 'leopard', 'lion', 'mole', 'moose', 'mouse', 'otter', 'ox', 'persian+cat', 'pig', 'polar+bear', 'rabbit', 'raccoon', 'rat', 'rhinoceros', 'seal', 'sheep', 'siamese+cat', 'skunk', 'spider+monkey', 'squirrel', 'tiger', 'walrus', 'weasel', 'wolf', 'zebra']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dirs = os.listdir(JPEGIMAGES_FOLDER_PATH)\n",
    "print(labels_dirs)\n",
    "len(labels_dirs) # 50 labels / subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c46b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_label(label):\n",
    "    return labels_dirs.index(labels_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60066a3e",
   "metadata": {},
   "source": [
    "# Note : Some labels have a low number of images. \n",
    "\n",
    "## Possible solutions to explore : \n",
    "    Data augmentation : creating new training data by applying random transformations to existing images, such as rotating, cropping, or flipping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade44228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_images_per_label(img_dir = JPEGIMAGES_FOLDER_PATH) -> tuple[dict,dict]: \n",
    "    \"\"\" \n",
    "    USEFUL FOR SAMPLING.\n",
    "    Return a dict with keys as the 50 labels, and values being the number of images in each subdirectory corresponding to label\n",
    "    and a second dict with the relative numbers (proportion) for every label compared to the total number of images (useful for sampling)\"\"\"\n",
    "    labels_dirs = os.listdir(img_dir)\n",
    "    num_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    proportions_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    total_num_images = 0\n",
    "\n",
    "    # Update absolute number of images per label\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        specific_label_path = os.path.join(img_dir, labels_dirs[i])\n",
    "        num_images_label = len(os.listdir(specific_label_path))\n",
    "        total_num_images += num_images_label\n",
    "        num_images_per_label[label] = num_images_label\n",
    "\n",
    "    # Update relative number of images per label (proportion)\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        num_images_label = num_images_per_label[label]\n",
    "        proportion_label = round(num_images_label / total_num_images, 4)\n",
    "        proportions_images_per_label[label] = proportion_label\n",
    "\n",
    "    return num_images_per_label, proportions_images_per_label\n",
    "\n",
    "num_images_per_label, proportions_images_per_label = find_num_images_per_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3b38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existent annotations.csv file.\n",
      " ---------------------------\n",
      "Sucessfully created annotations.csv file.\n"
     ]
    }
   ],
   "source": [
    "ANNOTATIONS_FILENAME = 'annotations.csv'\n",
    "\n",
    "def create_annotations_csv_file(annotations_filename = ANNOTATIONS_FILENAME, img_dir = JPEGIMAGES_FOLDER_PATH) : \n",
    "    \"\"\" \n",
    "    Create a csv annotations_file, annotations.csv, with two columns, in the format : \n",
    "                        path/to/image, label\n",
    "    \n",
    "    The annotation csv is necessary for DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    labels_dirs:list = os.listdir(img_dir)\n",
    "   \n",
    "    if os.path.exists(annotations_filename):\n",
    "        os.remove(annotations_filename)\n",
    "        print(f'Deleted existent {ANNOTATIONS_FILENAME} file.\\n ---------------------------')\n",
    "    \n",
    "    with open(annotations_filename, 'w', newline='') as file :\n",
    "        writer = csv.writer(file, dialect='excel', delimiter=',')\n",
    "\n",
    "        for i, label in enumerate(labels_dirs) : \n",
    "\n",
    "            specific_label_path = os.path.join(img_dir, label)\n",
    "            images_names = os.listdir(specific_label_path)\n",
    "\n",
    "            for j, image_name in enumerate(images_names):\n",
    "                full_path_to_img= os.path.join(specific_label_path, image_name)\n",
    "                full_path_to_img= os.path.join(label, image_name)\n",
    "\n",
    "                row = [full_path_to_img, label]\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f'Sucessfully created {ANNOTATIONS_FILENAME} file.')\n",
    "\n",
    "#\n",
    "create_annotations_csv_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08979242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWA2Dataset(Dataset): # Dataset class to serve as input for the DataLoader.\n",
    "    \"\"\" \n",
    "    Dataset class to serve as input for the DataLoader.\n",
    "    Implements all the required methods and more. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotations_file=ANNOTATIONS_FILENAME, img_dir=JPEGIMAGES_FOLDER_PATH, \n",
    "                transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        numbers_infos_dicts: tuple[dict,dict] = find_num_images_per_label(img_dir=JPEGIMAGES_FOLDER_PATH)\n",
    "        self.num_images_per_label = numbers_infos_dicts[0]\n",
    "        self.proportions_images_per_label = numbers_infos_dicts[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # img_path = self.img_labels.iloc[idx, 0]\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        image = read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a923daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AWA2Dataset()\n",
    "\n",
    "## TODO : Change transforms. Currently this is not useful.\n",
    "dataset.transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize((256,256)),\n",
    "                    transforms.CenterCrop((224,224)),\n",
    "                    transforms.Grayscale(num_output_channels=3),\n",
    "                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the reaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "# Testing. All good\n",
    "random_index = np.random.randint(0, len(dataset))\n",
    "image, label = dataset[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d291b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with DataLoader. Everything works good\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=4, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "data = next(dataiter)\n",
    "\n",
    "images, labels = data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c87352",
   "metadata": {},
   "source": [
    "###   AlexNet  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0660d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\1/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "C:\\Users\\1\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\1\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "\n",
    "#Model description\n",
    "AlexNet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1257cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model.classifier[4] = torch.nn.Linear(4096,1024)\n",
    "AlexNet_model.classifier[6] = torch.nn.Linear(1024,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a30c33",
   "metadata": {},
   "source": [
    "###  CUDA  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bd80101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7061430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=1024, out_features=50, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexNet_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0df16",
   "metadata": {},
   "source": [
    "### Question: check dimension de dataloader   ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102a925",
   "metadata": {},
   "source": [
    "###  Split training data and test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb3dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset = dataset, batch_size=4, shuffle=True)\n",
    "train_size = int(0.8*len(dataset))\n",
    "test_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e40acdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset,[train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e26603ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 4, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e671c",
   "metadata": {},
   "source": [
    "###   transfomer labels  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40ae5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_class = path_project +\"classes.txt\"\n",
    "class_animal = pd.read_table(path_class,header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "765b9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = class_animal[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d2ae060",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label_animal = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "034096dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "for i in range(0,len(animals)):\n",
    "    dict_label_animal[animals[i]] = n\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a12cece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(tuple_labels):\n",
    "    list_labels =[]\n",
    "    for tuple_label in tuple_labels:\n",
    "        list_labels.append(dict_label_animal[tuple_label])\n",
    "    return torch.tensor(list_labels)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1dea18",
   "metadata": {},
   "source": [
    "###   Loss function  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e29d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(AlexNet_model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e823e5d0",
   "metadata": {},
   "source": [
    "###  Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceb3d9c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9168\\1020713833.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_to_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        inputs, labels = data[0].to(device), label_to_num(data[1]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = AlexNet_model(inputs)       \n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "\n",
    "print('Finished Training of AlexNet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8756690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop example\n",
    "num_epochs = 2 \n",
    "batch_size = 1\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs) : \n",
    "    running_loss = 0.0\n",
    "    # loop over trainloader \n",
    "    for i, (inputs, labels) in enumerate(dataloader) : \n",
    "        l = l+1\n",
    "        ww.append(l)\n",
    "        print(l)\n",
    "        break\n",
    "    break    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = AlexNet_model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        # Do forward and backward pass, update the weights \n",
    "        if(i+1) % 5 == 0 :\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step, {i+1}/{n_iterations}, inputs {inputs.shape}')\n",
    "\n",
    "        if i==20 : \n",
    "            print('Completed')\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
