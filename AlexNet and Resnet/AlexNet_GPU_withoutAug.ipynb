{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d60b6b",
   "metadata": {},
   "source": [
    "Lien de téléchargement des données: https://cvml.ista.ac.at/AwA2/        \n",
    "\n",
    "13GB file : https://cvml.ista.ac.at/AwA2/AwA2-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b124fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed06051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myanzhang7271\u001b[0m (\u001b[33m-yanzhang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to your W&B account\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3784f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb: Currently logged in as: yanzhang7271 (-yanzhang). Use `wandb login --relogin` to force relogin\n",
    "# pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4d23f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 # Pour utiliser open_cv, il faut la version de python est 3.7\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision \n",
    "from torchvision.io import read_image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cf6e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e8e4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant. Should be the path to the folder named JPEGImages, containing the 33K images in its subfolders.\n",
    "JPEGIMAGES_FOLDER_PATH = \"C:/Users/1/Desktop/data_ift3710/Animals_with_Attributes2/JPEGImages/\"\n",
    "path_project = \"C:/Users/1/Desktop/data_ift3710/Animals_with_Attributes2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f192da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antelope', 'bat', 'beaver', 'blue+whale', 'bobcat', 'buffalo', 'chihuahua', 'chimpanzee', 'collie', 'cow', 'dalmatian', 'deer', 'dolphin', 'elephant', 'fox', 'german+shepherd', 'giant+panda', 'giraffe', 'gorilla', 'grizzly+bear', 'hamster', 'hippopotamus', 'horse', 'humpback+whale', 'killer+whale', 'leopard', 'lion', 'mole', 'moose', 'mouse', 'otter', 'ox', 'persian+cat', 'pig', 'polar+bear', 'rabbit', 'raccoon', 'rat', 'rhinoceros', 'seal', 'sheep', 'siamese+cat', 'skunk', 'spider+monkey', 'squirrel', 'tiger', 'walrus', 'weasel', 'wolf', 'zebra']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dirs = os.listdir(JPEGIMAGES_FOLDER_PATH)\n",
    "print(labels_dirs)\n",
    "len(labels_dirs) # 50 labels / subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c46b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_label(label):\n",
    "    return labels_dirs.index(labels_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60066a3e",
   "metadata": {},
   "source": [
    "# Note : Some labels have a low number of images. \n",
    "\n",
    "## Possible solutions to explore : \n",
    "    Data augmentation : creating new training data by applying random transformations to existing images, such as rotating, cropping, or flipping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade44228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_images_per_label(img_dir = JPEGIMAGES_FOLDER_PATH): \n",
    "    \"\"\" \n",
    "    USEFUL FOR SAMPLING.\n",
    "    Return a dict with keys as the 50 labels, and values being the number of images in each subdirectory corresponding to label\n",
    "    and a second dict with the relative numbers (proportion) for every label compared to the total number of images (useful for sampling)\"\"\"\n",
    "    labels_dirs = os.listdir(img_dir)\n",
    "    num_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    proportions_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    total_num_images = 0\n",
    "\n",
    "    # Update absolute number of images per label\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        specific_label_path = os.path.join(img_dir, labels_dirs[i])\n",
    "        num_images_label = len(os.listdir(specific_label_path))\n",
    "        total_num_images += num_images_label\n",
    "        num_images_per_label[label] = num_images_label\n",
    "\n",
    "    # Update relative number of images per label (proportion)\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        num_images_label = num_images_per_label[label]\n",
    "        proportion_label = round(num_images_label / total_num_images, 4)\n",
    "        proportions_images_per_label[label] = proportion_label\n",
    "\n",
    "    return num_images_per_label, proportions_images_per_label\n",
    "\n",
    "num_images_per_label, proportions_images_per_label = find_num_images_per_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3b38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existent annotations.csv file.\n",
      " ---------------------------\n",
      "Sucessfully created annotations.csv file.\n"
     ]
    }
   ],
   "source": [
    "ANNOTATIONS_FILENAME = 'annotations.csv'\n",
    "\n",
    "def create_annotations_csv_file(annotations_filename = ANNOTATIONS_FILENAME, img_dir = JPEGIMAGES_FOLDER_PATH) : \n",
    "    \"\"\" \n",
    "    Create a csv annotations_file, annotations.csv, with two columns, in the format : \n",
    "                        path/to/image, label\n",
    "    \n",
    "    The annotation csv is necessary for DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    labels_dirs:list = os.listdir(img_dir)\n",
    "   \n",
    "    if os.path.exists(annotations_filename):\n",
    "        os.remove(annotations_filename)\n",
    "        print(f'Deleted existent {ANNOTATIONS_FILENAME} file.\\n ---------------------------')\n",
    "    \n",
    "    with open(annotations_filename, 'w', newline='') as file :\n",
    "        writer = csv.writer(file, dialect='excel', delimiter=',')\n",
    "\n",
    "        for i, label in enumerate(labels_dirs) : \n",
    "\n",
    "            specific_label_path = os.path.join(img_dir, label)\n",
    "            images_names = os.listdir(specific_label_path)\n",
    "\n",
    "            for j, image_name in enumerate(images_names):\n",
    "                full_path_to_img= os.path.join(specific_label_path, image_name)\n",
    "                full_path_to_img= os.path.join(label, image_name)\n",
    "\n",
    "                row = [full_path_to_img, label]\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f'Sucessfully created {ANNOTATIONS_FILENAME} file.')\n",
    "\n",
    "#\n",
    "create_annotations_csv_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08979242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWA2Dataset(Dataset): # Dataset class to serve as input for the DataLoader.\n",
    "    \"\"\" \n",
    "    Dataset class to serve as input for the DataLoader.\n",
    "    Implements all the required methods and more. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotations_file=ANNOTATIONS_FILENAME, img_dir=JPEGIMAGES_FOLDER_PATH, \n",
    "                transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        numbers_infos_dicts: tuple[dict,dict] = find_num_images_per_label(img_dir=JPEGIMAGES_FOLDER_PATH)\n",
    "        self.num_images_per_label = numbers_infos_dicts[0]\n",
    "        self.proportions_images_per_label = numbers_infos_dicts[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # img_path = self.img_labels.iloc[idx, 0]\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        image = read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a923daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AWA2Dataset()\n",
    "\n",
    "## TODO : Change transforms. Currently this is not useful.\n",
    "dataset.transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize((256,256)),\n",
    "                    transforms.CenterCrop((224,224)),\n",
    "                    transforms.Grayscale(num_output_channels=3),\n",
    "                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the reaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "# Testing. All good\n",
    "random_index = np.random.randint(0, len(dataset))\n",
    "image, label = dataset[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64d291b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with DataLoader. Everything works good\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=16, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "data = next(dataiter)\n",
    "\n",
    "images, labels = data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c87352",
   "metadata": {},
   "source": [
    "###   AlexNet  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0660d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\1/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=False)\n",
    "\n",
    "#Model description\n",
    "AlexNet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f0f14",
   "metadata": {},
   "source": [
    "### CUDA ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1dc164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded1c951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f56c8",
   "metadata": {},
   "source": [
    "### Question: nombre de noeud ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7bd16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_noeud = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1257cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model.classifier[4] = torch.nn.Linear(4096,nb_noeud)\n",
    "AlexNet_model.classifier[6] = torch.nn.Linear(nb_noeud,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96ce8e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.16 (default, Mar  2 2023, 03:18:16) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b0b01ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=50, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AlexNet_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0df16",
   "metadata": {},
   "source": [
    "### Question: check dimension de dataloader   ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102a925",
   "metadata": {},
   "source": [
    "###  Split training data and test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ee2b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebb3dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset = dataset, batch_size=batch_size, shuffle=True)\n",
    "train_size = int(0.8*len(dataset))\n",
    "test_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40acdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset,[train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e26603ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e671c",
   "metadata": {},
   "source": [
    "###   transfomer labels  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40ae5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_class = path_project +\"classes.txt\"\n",
    "class_animal = pd.read_table(path_class,header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "765b9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = class_animal[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d2ae060",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label_animal = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "034096dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for i in range(0,len(animals)):\n",
    "    dict_label_animal[animals[i]] = n\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a12cece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(tuple_labels):\n",
    "    list_labels =[]\n",
    "    for tuple_label in tuple_labels:\n",
    "        list_labels.append(dict_label_animal[tuple_label])\n",
    "    return torch.tensor(list_labels)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1dea18",
   "metadata": {},
   "source": [
    "###   Loss function  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24d92177",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e29d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(AlexNet_model.parameters(), lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f15e66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff86ae",
   "metadata": {},
   "source": [
    "###  Training  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33e15d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\1\\Desktop\\data_ift3710\\wandb\\run-20230501_044351-w4egdmus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-yanzhang/AlexNet/runs/w4egdmus' target=\"_blank\">hopeful-night-23</a></strong> to <a href='https://wandb.ai/-yanzhang/AlexNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-yanzhang/AlexNet' target=\"_blank\">https://wandb.ai/-yanzhang/AlexNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-yanzhang/AlexNet/runs/w4egdmus' target=\"_blank\">https://wandb.ai/-yanzhang/AlexNet/runs/w4egdmus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "epoch: 1\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "Finished Training of AlexNet\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"AlexNet\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"architecture\": \"AlexNet\",\n",
    "    \"dataset\": \"AWA2\",\n",
    "    \"epochs\": 50,\n",
    "    }\n",
    ")\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = data[0].to(device)\n",
    "        labels = label_to_num(data[1]).to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        output = AlexNet_model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "\n",
    "        # log metrics to wandb\n",
    "        wandb.log({\"loss\": running_loss, \"acc\": (labels == output.argmax(dim=1)[0]).int().sum() / labels.size(0)})\n",
    "        \n",
    "        running_loss = 0.0\n",
    "    print(\"epoch: \"+str(epoch))    \n",
    "\n",
    "print('Finished Training of AlexNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b741a",
   "metadata": {},
   "source": [
    "### Test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8756690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 36 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "wrong_pred =[]\n",
    "right_label = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images = data[0].to(device)\n",
    "        labels = label_to_num(data[1]).to(device)\n",
    "        outputs = AlexNet_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        for i in range(0,len(predicted)):\n",
    "              if predicted[i].item() != labels[i].item():\n",
    "                    wrong_pred.append(predicted[i].item())\n",
    "                    right_label.append(labels[i].item())\n",
    "        \n",
    "\n",
    "        \n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f947ac",
   "metadata": {},
   "source": [
    "###   Analyse  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9c8b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_label_wrongpred = []\n",
    "for i in range(0,50):\n",
    "    nb_label_wrongpred.append(right_label.count(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e078fa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_animal = list(dict_label_animal.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9416adea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(50),nb_label_wrongpred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e717515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_classification = []\n",
    "bad_classification = []\n",
    "for i in range(50):\n",
    "    if nb_label_wrongpred[i]<=5:\n",
    "        good_classification.append(i)\n",
    "    if    nb_label_wrongpred[i]>=60:\n",
    "        bad_classification.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adcf24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_right_animal(m):\n",
    "    wrong_pred_m =[]\n",
    "    for j in [i for i,x in enumerate(right_label) if x == m]:\n",
    "        wrong_pred_m.append(wrong_pred[j])\n",
    "    return list_animal[max(wrong_pred_m,key = wrong_pred_m.count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "042105c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in good_classification :\n",
    "    print('Alexnet a bien classifie '+animals[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfa502a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexnet a mal classifie antelope , melange souvent avec deer\n",
      "Alexnet a mal classifie grizzly+bear , melange souvent avec gorilla\n",
      "Alexnet a mal classifie persian+cat , melange souvent avec chihuahua\n",
      "Alexnet a mal classifie horse , melange souvent avec cow\n",
      "Alexnet a mal classifie german+shepherd , melange souvent avec chihuahua\n",
      "Alexnet a mal classifie siamese+cat , melange souvent avec chihuahua\n",
      "Alexnet a mal classifie tiger , melange souvent avec giraffe\n",
      "Alexnet a mal classifie hippopotamus , melange souvent avec rhinoceros\n",
      "Alexnet a mal classifie leopard , melange souvent avec giraffe\n",
      "Alexnet a mal classifie moose , melange souvent avec ox\n",
      "Alexnet a mal classifie humpback+whale , melange souvent avec blue+whale\n",
      "Alexnet a mal classifie elephant , melange souvent avec ox\n",
      "Alexnet a mal classifie gorilla , melange souvent avec chimpanzee\n",
      "Alexnet a mal classifie ox , melange souvent avec cow\n",
      "Alexnet a mal classifie fox , melange souvent avec squirrel\n",
      "Alexnet a mal classifie sheep , melange souvent avec cow\n",
      "Alexnet a mal classifie seal , melange souvent avec otter\n",
      "Alexnet a mal classifie chimpanzee , melange souvent avec gorilla\n",
      "Alexnet a mal classifie hamster , melange souvent avec persian+cat\n",
      "Alexnet a mal classifie squirrel , melange souvent avec lion\n",
      "Alexnet a mal classifie rhinoceros , melange souvent avec elephant\n",
      "Alexnet a mal classifie rabbit , melange souvent avec squirrel\n",
      "Alexnet a mal classifie bat , melange souvent avec horse\n",
      "Alexnet a mal classifie giraffe , melange souvent avec zebra\n",
      "Alexnet a mal classifie wolf , melange souvent avec lion\n",
      "Alexnet a mal classifie chihuahua , melange souvent avec horse\n",
      "Alexnet a mal classifie rat , melange souvent avec hamster\n",
      "Alexnet a mal classifie otter , melange souvent avec hippopotamus\n",
      "Alexnet a mal classifie buffalo , melange souvent avec cow\n",
      "Alexnet a mal classifie giant+panda , melange souvent avec chihuahua\n",
      "Alexnet a mal classifie deer , melange souvent avec antelope\n",
      "Alexnet a mal classifie bobcat , melange souvent avec squirrel\n",
      "Alexnet a mal classifie pig , melange souvent avec sheep\n",
      "Alexnet a mal classifie lion , melange souvent avec fox\n",
      "Alexnet a mal classifie collie , melange souvent avec chihuahua\n",
      "Alexnet a mal classifie raccoon , melange souvent avec otter\n",
      "Alexnet a mal classifie cow , melange souvent avec sheep\n",
      "Alexnet a mal classifie dolphin , melange souvent avec humpback+whale\n"
     ]
    }
   ],
   "source": [
    "for i in bad_classification:\n",
    "    print('Alexnet a mal classifie '+animals[i]+' , melange souvent avec '+find_right_animal(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edd90245",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = pd.read_csv(\"annotations.csv\",header=None)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e943417",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_list =list(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19def7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 1046 antelope\n",
      "Il y a 852 grizzly+bear\n",
      "Il y a 291 killer+whale\n",
      "Il y a 193 beaver\n",
      "Il y a 549 dalmatian\n",
      "Il y a 747 persian+cat\n",
      "Il y a 1645 horse\n",
      "Il y a 1033 german+shepherd\n",
      "Il y a 174 blue+whale\n",
      "Il y a 500 siamese+cat\n",
      "Il y a 188 skunk\n",
      "Il y a 100 mole\n",
      "Il y a 877 tiger\n",
      "Il y a 684 hippopotamus\n",
      "Il y a 720 leopard\n",
      "Il y a 704 moose\n",
      "Il y a 291 spider+monkey\n",
      "Il y a 709 humpback+whale\n",
      "Il y a 1038 elephant\n",
      "Il y a 872 gorilla\n",
      "Il y a 728 ox\n",
      "Il y a 664 fox\n",
      "Il y a 1420 sheep\n",
      "Il y a 988 seal\n",
      "Il y a 728 chimpanzee\n",
      "Il y a 779 hamster\n",
      "Il y a 1200 squirrel\n",
      "Il y a 696 rhinoceros\n",
      "Il y a 1088 rabbit\n",
      "Il y a 383 bat\n",
      "Il y a 1202 giraffe\n",
      "Il y a 589 wolf\n",
      "Il y a 567 chihuahua\n",
      "Il y a 310 rat\n",
      "Il y a 272 weasel\n",
      "Il y a 758 otter\n",
      "Il y a 895 buffalo\n",
      "Il y a 1170 zebra\n",
      "Il y a 874 giant+panda\n",
      "Il y a 1344 deer\n",
      "Il y a 630 bobcat\n",
      "Il y a 713 pig\n",
      "Il y a 1019 lion\n",
      "Il y a 185 mouse\n",
      "Il y a 868 polar+bear\n",
      "Il y a 1028 collie\n",
      "Il y a 215 walrus\n",
      "Il y a 512 raccoon\n",
      "Il y a 1338 cow\n",
      "Il y a 946 dolphin\n"
     ]
    }
   ],
   "source": [
    "for animal in animals:\n",
    "    print(\"Il y a \" + str(anno_list.count(animal)) +\" \"+ animal  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a996c",
   "metadata": {},
   "source": [
    "### Conclusion ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd42c74",
   "metadata": {},
   "source": [
    "Alexnet a une très bien performance en classification des animaux. Mais il a même la difficulté à classifer deux animaux similaires.\n",
    "À faire:\n",
    "1.Étudier si VIT a une meilleure performance à classifer deux animaux similaire.\n",
    "2.Essayer de comprendre la logique de la construction de AlexNet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
