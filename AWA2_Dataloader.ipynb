{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d60b6b",
   "metadata": {},
   "source": [
    "Lien de téléchargement des données: https://cvml.ista.ac.at/AwA2/        \n",
    "\n",
    "13GB file : https://cvml.ista.ac.at/AwA2/AwA2-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d23f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 # Pour utiliser open_cv, il faut la version de python est 3.7\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision \n",
    "from torchvision.io import read_image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8e4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant. Should be the path to the folder named JPEGImages, containing the 33K images in its subfolders.\n",
    "DATA_FOLDER_PATH = \"E:\\\\3710datas\\\\Animals_with_Attributes2\\\\\"\n",
    "JPEGIMAGES_FOLDER_PATH = \"E:\\\\3710datas\\\\Animals_with_Attributes2\\\\JPEGImages\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60066a3e",
   "metadata": {},
   "source": [
    "# Note : Some labels have a low number of images. \n",
    "\n",
    "## Possible solutions to explore : \n",
    "    Data augmentation : creating new training data by applying random transformations to existing images, such as rotating, cropping, or flipping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade44228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antelope': 1046, 'bat': 383, 'beaver': 193, 'blue+whale': 174, 'bobcat': 630, 'buffalo': 895, 'chihuahua': 567, 'chimpanzee': 728, 'collie': 1028, 'cow': 1338, 'dalmatian': 549, 'deer': 1344, 'dolphin': 946, 'elephant': 1038, 'fox': 664, 'german+shepherd': 1033, 'giant+panda': 874, 'giraffe': 1202, 'gorilla': 872, 'grizzly+bear': 852, 'hamster': 779, 'hippopotamus': 684, 'horse': 1645, 'humpback+whale': 709, 'killer+whale': 291, 'leopard': 720, 'lion': 1019, 'mole': 100, 'moose': 704, 'mouse': 185, 'otter': 758, 'ox': 728, 'persian+cat': 747, 'pig': 713, 'polar+bear': 868, 'rabbit': 1088, 'raccoon': 512, 'rat': 310, 'rhinoceros': 696, 'seal': 988, 'sheep': 1420, 'siamese+cat': 500, 'skunk': 188, 'spider+monkey': 291, 'squirrel': 1200, 'tiger': 877, 'walrus': 215, 'weasel': 272, 'wolf': 589, 'zebra': 1170}\n",
      "{'antelope': 0.028, 'bat': 0.0103, 'beaver': 0.0052, 'blue+whale': 0.0047, 'bobcat': 0.0169, 'buffalo': 0.024, 'chihuahua': 0.0152, 'chimpanzee': 0.0195, 'collie': 0.0275, 'cow': 0.0359, 'dalmatian': 0.0147, 'deer': 0.036, 'dolphin': 0.0253, 'elephant': 0.0278, 'fox': 0.0178, 'german+shepherd': 0.0277, 'giant+panda': 0.0234, 'giraffe': 0.0322, 'gorilla': 0.0234, 'grizzly+bear': 0.0228, 'hamster': 0.0209, 'hippopotamus': 0.0183, 'horse': 0.0441, 'humpback+whale': 0.019, 'killer+whale': 0.0078, 'leopard': 0.0193, 'lion': 0.0273, 'mole': 0.0027, 'moose': 0.0189, 'mouse': 0.005, 'otter': 0.0203, 'ox': 0.0195, 'persian+cat': 0.02, 'pig': 0.0191, 'polar+bear': 0.0233, 'rabbit': 0.0292, 'raccoon': 0.0137, 'rat': 0.0083, 'rhinoceros': 0.0186, 'seal': 0.0265, 'sheep': 0.038, 'siamese+cat': 0.0134, 'skunk': 0.005, 'spider+monkey': 0.0078, 'squirrel': 0.0322, 'tiger': 0.0235, 'walrus': 0.0058, 'weasel': 0.0073, 'wolf': 0.0158, 'zebra': 0.0313}\n"
     ]
    }
   ],
   "source": [
    "def find_num_images_per_label(img_dir = JPEGIMAGES_FOLDER_PATH) -> tuple[dict,dict]: \n",
    "    \"\"\" \n",
    "    USEFUL FOR SAMPLING.\n",
    "    Return a dict with keys as the 50 labels, and values being the number of images in each subdirectory corresponding to label\n",
    "    and a second dict with the relative numbers (proportion) for every label compared to the total number of images (useful for sampling)\"\"\"\n",
    "    labels_dirs = os.listdir(img_dir)\n",
    "    num_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    proportions_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    total_num_images = 0\n",
    "\n",
    "    # Update absolute number of images per label\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        specific_label_path = os.path.join(img_dir, labels_dirs[i])\n",
    "        num_images_label = len(os.listdir(specific_label_path))\n",
    "        total_num_images += num_images_label\n",
    "        num_images_per_label[label] = num_images_label\n",
    "\n",
    "    # Update relative number of images per label (proportion)\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        num_images_label = num_images_per_label[label]\n",
    "        proportion_label = round(num_images_label / total_num_images, 4)\n",
    "        proportions_images_per_label[label] = proportion_label\n",
    "\n",
    "    return num_images_per_label, proportions_images_per_label\n",
    "\n",
    "num_images_per_label, proportions_images_per_label = find_num_images_per_label()\n",
    "print(num_images_per_label)\n",
    "print(proportions_images_per_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3b38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existent annotations.csv file.\n",
      " ---------------------------\n",
      "Sucessfully created annotations.csv file.\n"
     ]
    }
   ],
   "source": [
    "ANNOTATIONS_FILENAME = 'annotations.csv'\n",
    "\n",
    "def create_annotations_csv_file(annotations_filename = ANNOTATIONS_FILENAME, img_dir = JPEGIMAGES_FOLDER_PATH): \n",
    "    \"\"\" \n",
    "    Create a csv annotations_file, annotations.csv, with two columns, in the format : \n",
    "                        path/to/image, label\n",
    "    \n",
    "    The annotation csv is necessary for DataLoader.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels_dirs:list = os.listdir(img_dir)\n",
    "   \n",
    "    if os.path.exists(annotations_filename):\n",
    "        os.remove(annotations_filename)\n",
    "        print(f'Deleted existent {ANNOTATIONS_FILENAME} file.\\n ---------------------------')\n",
    "    \n",
    "    with open(annotations_filename, 'w', newline='') as file :\n",
    "        writer = csv.writer(file, dialect='excel', delimiter=',')\n",
    "\n",
    "        for i, label in enumerate(labels_dirs) : \n",
    "\n",
    "            specific_label_path = os.path.join(img_dir, label)\n",
    "            images_names = os.listdir(specific_label_path)\n",
    "\n",
    "            for j, image_name in enumerate(images_names):\n",
    "                full_path_to_img= os.path.join(specific_label_path, image_name)\n",
    "                full_path_to_img= os.path.join(label, image_name)\n",
    "\n",
    "                row = [full_path_to_img, label]\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f'Sucessfully created {ANNOTATIONS_FILENAME} file.')\n",
    "\n",
    "#\n",
    "create_annotations_csv_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24dc7a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antelope': 0, 'grizzly+bear': 1, 'killer+whale': 2, 'beaver': 3, 'dalmatian': 4, 'persian+cat': 5, 'horse': 6, 'german+shepherd': 7, 'blue+whale': 8, 'siamese+cat': 9, 'skunk': 10, 'mole': 11, 'tiger': 12, 'hippopotamus': 13, 'leopard': 14, 'moose': 15, 'spider+monkey': 16, 'humpback+whale': 17, 'elephant': 18, 'gorilla': 19, 'ox': 20, 'fox': 21, 'sheep': 22, 'seal': 23, 'chimpanzee': 24, 'hamster': 25, 'squirrel': 26, 'rhinoceros': 27, 'rabbit': 28, 'bat': 29, 'giraffe': 30, 'wolf': 31, 'chihuahua': 32, 'rat': 33, 'weasel': 34, 'otter': 35, 'buffalo': 36, 'zebra': 37, 'giant+panda': 38, 'deer': 39, 'bobcat': 40, 'pig': 41, 'lion': 42, 'mouse': 43, 'polar+bear': 44, 'collie': 45, 'walrus': 46, 'raccoon': 47, 'cow': 48, 'dolphin': 49}\n"
     ]
    }
   ],
   "source": [
    "# labels_in_number = pd.read_csv(DATA_FOLDER_PATH+\"classes.txt\", delim_whitespace=True,header=None)\n",
    "labels_dict = {}\n",
    "with open(DATA_FOLDER_PATH+\"classes.txt\") as f:\n",
    "    for line in f:\n",
    "        # print(line.split())\n",
    "        (key,val) = line.split()\n",
    "        labels_dict[val] = int(key)-1\n",
    "print(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08979242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWA2Dataset(Dataset): # Dataset class to serve as input for the DataLoader.\n",
    "    \"\"\" \n",
    "    Dataset class to serve as input for the DataLoader.\n",
    "    Implements all the required methods and more. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotations_file=ANNOTATIONS_FILENAME, img_dir=JPEGIMAGES_FOLDER_PATH, \n",
    "                transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        numbers_infos_dicts: tuple[dict,dict] = find_num_images_per_label(img_dir=JPEGIMAGES_FOLDER_PATH)\n",
    "        self.num_images_per_label = numbers_infos_dicts[0]\n",
    "        self.proportions_images_per_label = numbers_infos_dicts[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # img_path = self.img_labels.iloc[idx, 0]\n",
    "        key = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        # Mapping the labels from string to tensor\n",
    "        label = labels_dict[key]\n",
    "\n",
    "        image = read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a923daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AWA2Dataset()\n",
    "## TODO : Change transforms. Currently this is not useful.\n",
    "dataset.transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.Grayscale(num_output_channels=3),\n",
    "                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the reaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
    "                    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "train_size =  int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d291b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset = dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle= True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "802f9705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1/3732], Loss: 3.9076, accuracy: 0.0000%\n",
      "Epoch [1/2], Step [2/3732], Loss: 3.9077, accuracy: 0.0000%\n",
      "Epoch [1/2], Step [3/3732], Loss: 3.8974, accuracy: 0.0000%\n",
      "Epoch [1/2], Step [4/3732], Loss: 3.9033, accuracy: 0.0000%\n",
      "Epoch [1/2], Step [5/3732], Loss: 3.9312, accuracy: 0.0000%\n",
      "Epoch [1/2], Step [6/3732], Loss: 3.8958, accuracy: 2.0833%\n",
      "Epoch [1/2], Step [7/3732], Loss: 3.8873, accuracy: 1.7857%\n",
      "Epoch [1/2], Step [8/3732], Loss: 3.8776, accuracy: 1.5625%\n",
      "Epoch [1/2], Step [9/3732], Loss: 3.8705, accuracy: 1.3889%\n",
      "Epoch [1/2], Step [10/3732], Loss: 3.8555, accuracy: 1.2500%\n",
      "Epoch [1/2], Step [11/3732], Loss: 3.9025, accuracy: 1.1364%\n",
      "Epoch [1/2], Step [12/3732], Loss: 3.9054, accuracy: 1.0417%\n",
      "Epoch [1/2], Step [13/3732], Loss: 3.9100, accuracy: 0.9615%\n",
      "Epoch [1/2], Step [14/3732], Loss: 3.8004, accuracy: 0.8929%\n",
      "Epoch [1/2], Step [15/3732], Loss: 3.7327, accuracy: 0.8333%\n",
      "Epoch [1/2], Step [16/3732], Loss: 3.9761, accuracy: 0.7812%\n",
      "Epoch [1/2], Step [17/3732], Loss: 3.9816, accuracy: 0.7353%\n",
      "Epoch [1/2], Step [18/3732], Loss: 4.0927, accuracy: 0.6944%\n",
      "Epoch [1/2], Step [19/3732], Loss: 4.0195, accuracy: 0.6579%\n",
      "Epoch [1/2], Step [20/3732], Loss: 3.9050, accuracy: 1.2500%\n",
      "Epoch [1/2], Step [21/3732], Loss: 3.9030, accuracy: 1.1905%\n",
      "Epoch [1/2], Step [22/3732], Loss: 3.8600, accuracy: 1.7045%\n",
      "Epoch [1/2], Step [23/3732], Loss: 3.9416, accuracy: 2.1739%\n",
      "Epoch [1/2], Step [24/3732], Loss: 4.0270, accuracy: 2.0833%\n",
      "Epoch [1/2], Step [25/3732], Loss: 3.8449, accuracy: 2.0000%\n",
      "Epoch [1/2], Step [26/3732], Loss: 3.9192, accuracy: 1.9231%\n",
      "Epoch [1/2], Step [27/3732], Loss: 3.8885, accuracy: 1.8519%\n",
      "Epoch [1/2], Step [28/3732], Loss: 3.9030, accuracy: 1.7857%\n",
      "Epoch [1/2], Step [29/3732], Loss: 3.9364, accuracy: 1.7241%\n",
      "Epoch [1/2], Step [30/3732], Loss: 3.8045, accuracy: 1.6667%\n",
      "Epoch [1/2], Step [31/3732], Loss: 3.9050, accuracy: 1.6129%\n",
      "Epoch [1/2], Step [32/3732], Loss: 3.8723, accuracy: 1.5625%\n",
      "Epoch [1/2], Step [33/3732], Loss: 3.9920, accuracy: 1.5152%\n",
      "Epoch [1/2], Step [34/3732], Loss: 3.7534, accuracy: 1.4706%\n",
      "Epoch [1/2], Step [35/3732], Loss: 3.9703, accuracy: 1.4286%\n",
      "Epoch [1/2], Step [36/3732], Loss: 3.7400, accuracy: 1.7361%\n",
      "Epoch [1/2], Step [37/3732], Loss: 3.7586, accuracy: 1.6892%\n",
      "Epoch [1/2], Step [38/3732], Loss: 3.8695, accuracy: 1.6447%\n",
      "Epoch [1/2], Step [39/3732], Loss: 3.6480, accuracy: 1.9231%\n",
      "Epoch [1/2], Step [40/3732], Loss: 3.7086, accuracy: 1.8750%\n",
      "Epoch [1/2], Step [41/3732], Loss: 3.9728, accuracy: 1.8293%\n",
      "Epoch [1/2], Step [42/3732], Loss: 3.8911, accuracy: 1.7857%\n",
      "Epoch [1/2], Step [43/3732], Loss: 3.6359, accuracy: 2.3256%\n",
      "Epoch [1/2], Step [44/3732], Loss: 3.7871, accuracy: 2.5568%\n",
      "Epoch [1/2], Step [45/3732], Loss: 3.8207, accuracy: 2.5000%\n",
      "Epoch [1/2], Step [46/3732], Loss: 3.6779, accuracy: 2.4457%\n",
      "Epoch [1/2], Step [47/3732], Loss: 3.9201, accuracy: 2.3936%\n",
      "Epoch [1/2], Step [48/3732], Loss: 3.7407, accuracy: 2.3438%\n",
      "Epoch [1/2], Step [49/3732], Loss: 3.8556, accuracy: 2.2959%\n",
      "Epoch [1/2], Step [50/3732], Loss: 3.5708, accuracy: 2.2500%\n",
      "Epoch [1/2], Step [51/3732], Loss: 3.7964, accuracy: 2.2059%\n",
      "Epoch [1/2], Step [52/3732], Loss: 3.8356, accuracy: 2.1635%\n",
      "Epoch [1/2], Step [53/3732], Loss: 3.7847, accuracy: 2.1226%\n",
      "Epoch [1/2], Step [54/3732], Loss: 4.0509, accuracy: 2.0833%\n",
      "Epoch [1/2], Step [55/3732], Loss: 3.8872, accuracy: 2.2727%\n",
      "Epoch [1/2], Step [56/3732], Loss: 3.4678, accuracy: 2.2321%\n",
      "Epoch [1/2], Step [57/3732], Loss: 4.0936, accuracy: 2.1930%\n",
      "Epoch [1/2], Step [58/3732], Loss: 3.6889, accuracy: 2.1552%\n",
      "Epoch [1/2], Step [59/3732], Loss: 3.8032, accuracy: 2.1186%\n",
      "Epoch [1/2], Step [60/3732], Loss: 3.8457, accuracy: 2.0833%\n",
      "Epoch [1/2], Step [61/3732], Loss: 3.9112, accuracy: 2.0492%\n",
      "Epoch [1/2], Step [62/3732], Loss: 3.6412, accuracy: 2.0161%\n",
      "Epoch [1/2], Step [63/3732], Loss: 3.7567, accuracy: 2.1825%\n",
      "Epoch [1/2], Step [64/3732], Loss: 3.9948, accuracy: 2.1484%\n",
      "Epoch [1/2], Step [65/3732], Loss: 3.6792, accuracy: 2.3077%\n",
      "Epoch [1/2], Step [66/3732], Loss: 3.7986, accuracy: 2.4621%\n",
      "Epoch [1/2], Step [67/3732], Loss: 3.7068, accuracy: 2.6119%\n",
      "Epoch [1/2], Step [68/3732], Loss: 3.8014, accuracy: 2.7574%\n",
      "Epoch [1/2], Step [69/3732], Loss: 3.6689, accuracy: 2.7174%\n",
      "Epoch [1/2], Step [70/3732], Loss: 4.1082, accuracy: 2.6786%\n",
      "Epoch [1/2], Step [71/3732], Loss: 3.9547, accuracy: 2.8169%\n",
      "Epoch [1/2], Step [72/3732], Loss: 3.6725, accuracy: 2.7778%\n",
      "Epoch [1/2], Step [73/3732], Loss: 3.9410, accuracy: 2.7397%\n",
      "Epoch [1/2], Step [74/3732], Loss: 3.5407, accuracy: 2.7027%\n",
      "Epoch [1/2], Step [75/3732], Loss: 4.1624, accuracy: 2.6667%\n",
      "Epoch [1/2], Step [76/3732], Loss: 4.1086, accuracy: 2.6316%\n",
      "Epoch [1/2], Step [77/3732], Loss: 3.8090, accuracy: 2.5974%\n",
      "Epoch [1/2], Step [78/3732], Loss: 3.9290, accuracy: 2.5641%\n",
      "Epoch [1/2], Step [79/3732], Loss: 3.8069, accuracy: 2.5316%\n",
      "Epoch [1/2], Step [80/3732], Loss: 3.9326, accuracy: 2.5000%\n",
      "Epoch [1/2], Step [81/3732], Loss: 3.8743, accuracy: 2.4691%\n",
      "Epoch [1/2], Step [82/3732], Loss: 3.7208, accuracy: 2.4390%\n",
      "Epoch [1/2], Step [83/3732], Loss: 3.8764, accuracy: 2.4096%\n",
      "Epoch [1/2], Step [84/3732], Loss: 3.6058, accuracy: 2.3810%\n",
      "Epoch [1/2], Step [85/3732], Loss: 3.7126, accuracy: 2.5000%\n",
      "Epoch [1/2], Step [86/3732], Loss: 3.8867, accuracy: 2.6163%\n",
      "Epoch [1/2], Step [87/3732], Loss: 3.8605, accuracy: 2.5862%\n",
      "Epoch [1/2], Step [88/3732], Loss: 3.8320, accuracy: 2.5568%\n",
      "Epoch [1/2], Step [89/3732], Loss: 3.8486, accuracy: 2.5281%\n",
      "Epoch [1/2], Step [90/3732], Loss: 3.8448, accuracy: 2.5000%\n",
      "Epoch [1/2], Step [91/3732], Loss: 3.9131, accuracy: 2.4725%\n",
      "Epoch [1/2], Step [92/3732], Loss: 3.8038, accuracy: 2.4457%\n",
      "Epoch [1/2], Step [93/3732], Loss: 3.8521, accuracy: 2.4194%\n",
      "Epoch [1/2], Step [94/3732], Loss: 3.6932, accuracy: 2.3936%\n",
      "Epoch [1/2], Step [95/3732], Loss: 3.5681, accuracy: 2.6316%\n",
      "Epoch [1/2], Step [96/3732], Loss: 3.7651, accuracy: 2.6042%\n",
      "Epoch [1/2], Step [97/3732], Loss: 3.9161, accuracy: 2.5773%\n",
      "Epoch [1/2], Step [98/3732], Loss: 3.8013, accuracy: 2.5510%\n",
      "Epoch [1/2], Step [99/3732], Loss: 3.7944, accuracy: 2.5253%\n",
      "Epoch [1/2], Step [100/3732], Loss: 3.6495, accuracy: 2.6250%\n",
      "Epoch [1/2], Step [101/3732], Loss: 3.7661, accuracy: 2.5990%\n",
      "Epoch [1/2], Step [102/3732], Loss: 3.9178, accuracy: 2.5735%\n",
      "Epoch [1/2], Step [103/3732], Loss: 4.0337, accuracy: 2.5485%\n",
      "Epoch [1/2], Step [104/3732], Loss: 3.8924, accuracy: 2.6442%\n",
      "Epoch [1/2], Step [105/3732], Loss: 4.0582, accuracy: 2.6190%\n",
      "Epoch [1/2], Step [106/3732], Loss: 3.8670, accuracy: 2.5943%\n",
      "Epoch [1/2], Step [107/3732], Loss: 3.7488, accuracy: 2.5701%\n",
      "Epoch [1/2], Step [108/3732], Loss: 3.9966, accuracy: 2.5463%\n",
      "Epoch [1/2], Step [109/3732], Loss: 4.0518, accuracy: 2.5229%\n",
      "Epoch [1/2], Step [110/3732], Loss: 3.6218, accuracy: 2.5000%\n",
      "Epoch [1/2], Step [111/3732], Loss: 3.7402, accuracy: 2.4775%\n",
      "Epoch [1/2], Step [112/3732], Loss: 3.8593, accuracy: 2.4554%\n",
      "Epoch [1/2], Step [113/3732], Loss: 3.9797, accuracy: 2.4336%\n",
      "Epoch [1/2], Step [114/3732], Loss: 3.7140, accuracy: 2.4123%\n",
      "Epoch [1/2], Step [115/3732], Loss: 3.8606, accuracy: 2.5000%\n",
      "Epoch [1/2], Step [116/3732], Loss: 3.8442, accuracy: 2.4784%\n",
      "Epoch [1/2], Step [117/3732], Loss: 3.9222, accuracy: 2.4573%\n",
      "Epoch [1/2], Step [118/3732], Loss: 3.8100, accuracy: 2.4364%\n",
      "Epoch [1/2], Step [119/3732], Loss: 3.7571, accuracy: 2.4160%\n",
      "Epoch [1/2], Step [120/3732], Loss: 3.7395, accuracy: 2.5000%\n",
      "Epoch [1/2], Step [121/3732], Loss: 3.9020, accuracy: 2.4793%\n",
      "Epoch [1/2], Step [122/3732], Loss: 3.7130, accuracy: 2.6639%\n",
      "Epoch [1/2], Step [123/3732], Loss: 3.9089, accuracy: 2.6423%\n",
      "Epoch [1/2], Step [124/3732], Loss: 3.9456, accuracy: 2.6210%\n",
      "Epoch [1/2], Step [125/3732], Loss: 3.6915, accuracy: 2.7000%\n",
      "Epoch [1/2], Step [126/3732], Loss: 3.7519, accuracy: 2.6786%\n",
      "Epoch [1/2], Step [127/3732], Loss: 3.9418, accuracy: 2.6575%\n",
      "Epoch [1/2], Step [128/3732], Loss: 3.8957, accuracy: 2.6367%\n",
      "Epoch [1/2], Step [129/3732], Loss: 3.8061, accuracy: 2.6163%\n",
      "Epoch [1/2], Step [130/3732], Loss: 3.9371, accuracy: 2.5962%\n",
      "Epoch [1/2], Step [131/3732], Loss: 3.8979, accuracy: 2.5763%\n",
      "Epoch [1/2], Step [132/3732], Loss: 3.7228, accuracy: 2.6515%\n",
      "Epoch [1/2], Step [133/3732], Loss: 3.7323, accuracy: 2.7256%\n",
      "Epoch [1/2], Step [134/3732], Loss: 3.7842, accuracy: 2.7052%\n",
      "Epoch [1/2], Step [135/3732], Loss: 3.8662, accuracy: 2.6852%\n",
      "Epoch [1/2], Step [136/3732], Loss: 3.9323, accuracy: 2.7574%\n",
      "Epoch [1/2], Step [137/3732], Loss: 3.7534, accuracy: 2.7372%\n",
      "Epoch [1/2], Step [138/3732], Loss: 4.0771, accuracy: 2.7174%\n",
      "Epoch [1/2], Step [139/3732], Loss: 3.9809, accuracy: 2.7878%\n",
      "Epoch [1/2], Step [140/3732], Loss: 3.6095, accuracy: 2.7679%\n",
      "Epoch [1/2], Step [141/3732], Loss: 3.9415, accuracy: 2.7482%\n",
      "Epoch [1/2], Step [142/3732], Loss: 3.8041, accuracy: 2.7289%\n",
      "Epoch [1/2], Step [143/3732], Loss: 3.6023, accuracy: 2.7972%\n",
      "Epoch [1/2], Step [144/3732], Loss: 3.9363, accuracy: 2.7778%\n",
      "Epoch [1/2], Step [145/3732], Loss: 3.9253, accuracy: 2.7586%\n",
      "Epoch [1/2], Step [146/3732], Loss: 3.8281, accuracy: 2.8253%\n",
      "Epoch [1/2], Step [147/3732], Loss: 3.8441, accuracy: 2.8061%\n",
      "Epoch [1/2], Step [148/3732], Loss: 3.6890, accuracy: 2.8716%\n",
      "Epoch [1/2], Step [149/3732], Loss: 3.6294, accuracy: 2.8523%\n",
      "Epoch [1/2], Step [150/3732], Loss: 3.7439, accuracy: 2.9167%\n",
      "Epoch [1/2], Step [151/3732], Loss: 3.7370, accuracy: 2.8974%\n",
      "Epoch [1/2], Step [152/3732], Loss: 3.8481, accuracy: 2.8783%\n",
      "Epoch [1/2], Step [153/3732], Loss: 3.8544, accuracy: 2.8595%\n",
      "Epoch [1/2], Step [154/3732], Loss: 3.7811, accuracy: 2.8409%\n",
      "Epoch [1/2], Step [155/3732], Loss: 3.5233, accuracy: 2.9839%\n",
      "Epoch [1/2], Step [156/3732], Loss: 3.6981, accuracy: 2.9647%\n",
      "Epoch [1/2], Step [157/3732], Loss: 3.7405, accuracy: 2.9459%\n",
      "Epoch [1/2], Step [158/3732], Loss: 3.7307, accuracy: 3.0063%\n",
      "Epoch [1/2], Step [159/3732], Loss: 3.5876, accuracy: 2.9874%\n",
      "Epoch [1/2], Step [160/3732], Loss: 3.6558, accuracy: 3.0469%\n",
      "Epoch [1/2], Step [161/3732], Loss: 3.6957, accuracy: 3.0280%\n",
      "Epoch [1/2], Step [162/3732], Loss: 3.9627, accuracy: 3.0093%\n",
      "Epoch [1/2], Step [163/3732], Loss: 3.8038, accuracy: 2.9908%\n",
      "Epoch [1/2], Step [164/3732], Loss: 3.8383, accuracy: 2.9726%\n",
      "Epoch [1/2], Step [165/3732], Loss: 3.9998, accuracy: 2.9545%\n",
      "Epoch [1/2], Step [166/3732], Loss: 3.8693, accuracy: 3.0120%\n",
      "Epoch [1/2], Step [167/3732], Loss: 3.8547, accuracy: 2.9940%\n",
      "Epoch [1/2], Step [168/3732], Loss: 3.8380, accuracy: 3.1250%\n",
      "Epoch [1/2], Step [169/3732], Loss: 3.8097, accuracy: 3.1065%\n",
      "Epoch [1/2], Step [170/3732], Loss: 3.8003, accuracy: 3.1618%\n",
      "Epoch [1/2], Step [171/3732], Loss: 3.9966, accuracy: 3.1433%\n",
      "Epoch [1/2], Step [172/3732], Loss: 3.7620, accuracy: 3.1250%\n",
      "Epoch [1/2], Step [173/3732], Loss: 4.2334, accuracy: 3.1069%\n",
      "Epoch [1/2], Step [174/3732], Loss: 3.8840, accuracy: 3.0891%\n",
      "Epoch [1/2], Step [175/3732], Loss: 3.9662, accuracy: 3.0714%\n",
      "Epoch [1/2], Step [176/3732], Loss: 3.7343, accuracy: 3.0540%\n",
      "Epoch [1/2], Step [177/3732], Loss: 3.8397, accuracy: 3.0367%\n",
      "Epoch [1/2], Step [178/3732], Loss: 3.7542, accuracy: 3.0197%\n",
      "Epoch [1/2], Step [179/3732], Loss: 3.8130, accuracy: 3.0726%\n",
      "Epoch [1/2], Step [180/3732], Loss: 3.7647, accuracy: 3.0556%\n",
      "Epoch [1/2], Step [181/3732], Loss: 3.8194, accuracy: 3.0387%\n",
      "Epoch [1/2], Step [182/3732], Loss: 3.7264, accuracy: 3.0220%\n",
      "Epoch [1/2], Step [183/3732], Loss: 3.8922, accuracy: 3.0055%\n",
      "Epoch [1/2], Step [184/3732], Loss: 3.8918, accuracy: 2.9891%\n",
      "Epoch [1/2], Step [185/3732], Loss: 3.8283, accuracy: 2.9730%\n",
      "Epoch [1/2], Step [186/3732], Loss: 3.8436, accuracy: 3.0242%\n",
      "Epoch [1/2], Step [187/3732], Loss: 3.7593, accuracy: 3.0080%\n",
      "Epoch [1/2], Step [188/3732], Loss: 3.6827, accuracy: 3.1250%\n",
      "Epoch [1/2], Step [189/3732], Loss: 3.7322, accuracy: 3.1746%\n",
      "Epoch [1/2], Step [190/3732], Loss: 3.7733, accuracy: 3.1579%\n",
      "Epoch [1/2], Step [191/3732], Loss: 3.8825, accuracy: 3.2068%\n",
      "Epoch [1/2], Step [192/3732], Loss: 3.7504, accuracy: 3.1901%\n",
      "Epoch [1/2], Step [193/3732], Loss: 3.7306, accuracy: 3.2383%\n",
      "Epoch [1/2], Step [194/3732], Loss: 3.5460, accuracy: 3.2216%\n",
      "Epoch [1/2], Step [195/3732], Loss: 3.9802, accuracy: 3.2051%\n",
      "Epoch [1/2], Step [196/3732], Loss: 3.7561, accuracy: 3.2526%\n",
      "Epoch [1/2], Step [197/3732], Loss: 4.0943, accuracy: 3.2360%\n",
      "Epoch [1/2], Step [198/3732], Loss: 3.4831, accuracy: 3.3460%\n",
      "Epoch [1/2], Step [199/3732], Loss: 3.8662, accuracy: 3.3291%\n",
      "Epoch [1/2], Step [200/3732], Loss: 3.6453, accuracy: 3.3125%\n",
      "Epoch [1/2], Step [201/3732], Loss: 3.6902, accuracy: 3.2960%\n",
      "Epoch [1/2], Step [202/3732], Loss: 3.7557, accuracy: 3.2797%\n",
      "Epoch [1/2], Step [203/3732], Loss: 4.0582, accuracy: 3.2635%\n",
      "Epoch [1/2], Step [204/3732], Loss: 3.9343, accuracy: 3.3088%\n",
      "Epoch [1/2], Step [205/3732], Loss: 3.9166, accuracy: 3.2927%\n",
      "Epoch [1/2], Step [206/3732], Loss: 3.9054, accuracy: 3.2767%\n",
      "Epoch [1/2], Step [207/3732], Loss: 3.6523, accuracy: 3.3213%\n",
      "Epoch [1/2], Step [208/3732], Loss: 3.6474, accuracy: 3.3654%\n",
      "Epoch [1/2], Step [209/3732], Loss: 3.7093, accuracy: 3.3493%\n",
      "Epoch [1/2], Step [210/3732], Loss: 3.9982, accuracy: 3.3333%\n",
      "Epoch [1/2], Step [211/3732], Loss: 3.8511, accuracy: 3.3768%\n",
      "Epoch [1/2], Step [212/3732], Loss: 3.5706, accuracy: 3.4198%\n",
      "Epoch [1/2], Step [213/3732], Loss: 3.8213, accuracy: 3.4624%\n",
      "Epoch [1/2], Step [214/3732], Loss: 3.8187, accuracy: 3.4463%\n",
      "Epoch [1/2], Step [215/3732], Loss: 3.9789, accuracy: 3.4302%\n",
      "Epoch [1/2], Step [216/3732], Loss: 3.5434, accuracy: 3.5301%\n",
      "Epoch [1/2], Step [217/3732], Loss: 3.8482, accuracy: 3.5138%\n",
      "Epoch [1/2], Step [218/3732], Loss: 3.8872, accuracy: 3.4977%\n",
      "Epoch [1/2], Step [219/3732], Loss: 3.9330, accuracy: 3.4817%\n",
      "Epoch [1/2], Step [220/3732], Loss: 3.8459, accuracy: 3.4659%\n",
      "Epoch [1/2], Step [221/3732], Loss: 3.5780, accuracy: 3.5068%\n",
      "Epoch [1/2], Step [222/3732], Loss: 3.7337, accuracy: 3.4910%\n",
      "Epoch [1/2], Step [223/3732], Loss: 4.0912, accuracy: 3.5314%\n",
      "Epoch [1/2], Step [224/3732], Loss: 3.9409, accuracy: 3.5156%\n",
      "Epoch [1/2], Step [225/3732], Loss: 3.7821, accuracy: 3.5000%\n",
      "Epoch [1/2], Step [226/3732], Loss: 3.8858, accuracy: 3.4845%\n",
      "Epoch [1/2], Step [227/3732], Loss: 3.6514, accuracy: 3.4692%\n",
      "Epoch [1/2], Step [228/3732], Loss: 3.6661, accuracy: 3.4539%\n",
      "Epoch [1/2], Step [229/3732], Loss: 3.7898, accuracy: 3.4389%\n",
      "Epoch [1/2], Step [230/3732], Loss: 3.9161, accuracy: 3.4239%\n",
      "Epoch [1/2], Step [231/3732], Loss: 3.6771, accuracy: 3.4091%\n",
      "Epoch [1/2], Step [232/3732], Loss: 3.8509, accuracy: 3.3944%\n",
      "Epoch [1/2], Step [233/3732], Loss: 4.0714, accuracy: 3.3798%\n",
      "Epoch [1/2], Step [234/3732], Loss: 3.5794, accuracy: 3.4188%\n",
      "Epoch [1/2], Step [235/3732], Loss: 3.9407, accuracy: 3.4043%\n",
      "Epoch [1/2], Step [236/3732], Loss: 3.8346, accuracy: 3.3898%\n",
      "Epoch [1/2], Step [237/3732], Loss: 3.7665, accuracy: 3.4283%\n",
      "Epoch [1/2], Step [238/3732], Loss: 3.7530, accuracy: 3.4139%\n",
      "Epoch [1/2], Step [239/3732], Loss: 3.9068, accuracy: 3.4519%\n",
      "Epoch [1/2], Step [240/3732], Loss: 3.8591, accuracy: 3.4375%\n",
      "Epoch [1/2], Step [241/3732], Loss: 3.7157, accuracy: 3.4232%\n",
      "Epoch [1/2], Step [242/3732], Loss: 4.0160, accuracy: 3.4091%\n",
      "Epoch [1/2], Step [243/3732], Loss: 3.8829, accuracy: 3.3951%\n",
      "Epoch [1/2], Step [244/3732], Loss: 3.8395, accuracy: 3.3811%\n",
      "Epoch [1/2], Step [245/3732], Loss: 3.8511, accuracy: 3.4184%\n",
      "Epoch [1/2], Step [246/3732], Loss: 3.6363, accuracy: 3.4553%\n",
      "Epoch [1/2], Step [247/3732], Loss: 3.5979, accuracy: 3.4413%\n",
      "Epoch [1/2], Step [248/3732], Loss: 3.7335, accuracy: 3.4274%\n",
      "Epoch [1/2], Step [249/3732], Loss: 3.8514, accuracy: 3.4639%\n",
      "Epoch [1/2], Step [250/3732], Loss: 3.6428, accuracy: 3.5500%\n",
      "Epoch [1/2], Step [251/3732], Loss: 3.9780, accuracy: 3.5359%\n",
      "Epoch [1/2], Step [252/3732], Loss: 3.8104, accuracy: 3.5714%\n",
      "Epoch [1/2], Step [253/3732], Loss: 3.8174, accuracy: 3.6067%\n",
      "Epoch [1/2], Step [254/3732], Loss: 3.7938, accuracy: 3.6417%\n",
      "Epoch [1/2], Step [255/3732], Loss: 3.7321, accuracy: 3.6275%\n",
      "Epoch [1/2], Step [256/3732], Loss: 3.8228, accuracy: 3.6133%\n",
      "Epoch [1/2], Step [257/3732], Loss: 3.8323, accuracy: 3.5992%\n",
      "Epoch [1/2], Step [258/3732], Loss: 4.1230, accuracy: 3.5853%\n",
      "Epoch [1/2], Step [259/3732], Loss: 3.5992, accuracy: 3.6197%\n",
      "Epoch [1/2], Step [260/3732], Loss: 3.9375, accuracy: 3.6538%\n",
      "Epoch [1/2], Step [261/3732], Loss: 3.9761, accuracy: 3.6398%\n",
      "Epoch [1/2], Step [262/3732], Loss: 3.7259, accuracy: 3.6737%\n",
      "Epoch [1/2], Step [263/3732], Loss: 3.8178, accuracy: 3.6597%\n",
      "Epoch [1/2], Step [264/3732], Loss: 3.9314, accuracy: 3.6458%\n",
      "Epoch [1/2], Step [265/3732], Loss: 3.7593, accuracy: 3.6321%\n",
      "Epoch [1/2], Step [266/3732], Loss: 3.7503, accuracy: 3.6184%\n",
      "Epoch [1/2], Step [267/3732], Loss: 3.8090, accuracy: 3.6049%\n",
      "Epoch [1/2], Step [268/3732], Loss: 3.7756, accuracy: 3.6381%\n",
      "Epoch [1/2], Step [269/3732], Loss: 3.9868, accuracy: 3.6245%\n",
      "Epoch [1/2], Step [270/3732], Loss: 3.9538, accuracy: 3.6111%\n",
      "Epoch [1/2], Step [271/3732], Loss: 3.8751, accuracy: 3.5978%\n",
      "Epoch [1/2], Step [272/3732], Loss: 3.8160, accuracy: 3.5846%\n",
      "Epoch [1/2], Step [273/3732], Loss: 3.7377, accuracy: 3.5714%\n",
      "Epoch [1/2], Step [274/3732], Loss: 3.8446, accuracy: 3.5584%\n",
      "Epoch [1/2], Step [275/3732], Loss: 3.7083, accuracy: 3.5455%\n",
      "Epoch [1/2], Step [276/3732], Loss: 3.9412, accuracy: 3.5326%\n",
      "Epoch [1/2], Step [277/3732], Loss: 3.9202, accuracy: 3.5199%\n",
      "Epoch [1/2], Step [278/3732], Loss: 3.9308, accuracy: 3.5522%\n",
      "Epoch [1/2], Step [279/3732], Loss: 3.7474, accuracy: 3.5394%\n",
      "Epoch [1/2], Step [280/3732], Loss: 3.8000, accuracy: 3.5268%\n",
      "Epoch [1/2], Step [281/3732], Loss: 3.9329, accuracy: 3.5587%\n",
      "Epoch [1/2], Step [282/3732], Loss: 3.7566, accuracy: 3.5461%\n",
      "Epoch [1/2], Step [283/3732], Loss: 3.8003, accuracy: 3.5336%\n",
      "Epoch [1/2], Step [284/3732], Loss: 3.8082, accuracy: 3.5211%\n",
      "Epoch [1/2], Step [285/3732], Loss: 4.0322, accuracy: 3.5088%\n",
      "Epoch [1/2], Step [286/3732], Loss: 3.8686, accuracy: 3.4965%\n",
      "Epoch [1/2], Step [287/3732], Loss: 3.7737, accuracy: 3.5279%\n",
      "Epoch [1/2], Step [288/3732], Loss: 3.8607, accuracy: 3.5156%\n",
      "Epoch [1/2], Step [289/3732], Loss: 3.8734, accuracy: 3.5035%\n",
      "Epoch [1/2], Step [290/3732], Loss: 3.9298, accuracy: 3.4914%\n",
      "Epoch [1/2], Step [291/3732], Loss: 3.6712, accuracy: 3.4794%\n",
      "Epoch [1/2], Step [292/3732], Loss: 3.6891, accuracy: 3.4675%\n",
      "Epoch [1/2], Step [293/3732], Loss: 3.7993, accuracy: 3.4556%\n",
      "Epoch [1/2], Step [294/3732], Loss: 3.6680, accuracy: 3.4439%\n",
      "Epoch [1/2], Step [295/3732], Loss: 3.6579, accuracy: 3.4322%\n",
      "Epoch [1/2], Step [296/3732], Loss: 3.9087, accuracy: 3.4206%\n",
      "Epoch [1/2], Step [297/3732], Loss: 3.6461, accuracy: 3.4091%\n",
      "Epoch [1/2], Step [298/3732], Loss: 3.6104, accuracy: 3.3977%\n",
      "Epoch [1/2], Step [299/3732], Loss: 3.6935, accuracy: 3.4281%\n",
      "Epoch [1/2], Step [300/3732], Loss: 3.6569, accuracy: 3.4167%\n",
      "Epoch [1/2], Step [301/3732], Loss: 3.7251, accuracy: 3.4468%\n",
      "Epoch [1/2], Step [302/3732], Loss: 3.9553, accuracy: 3.4354%\n",
      "Epoch [1/2], Step [303/3732], Loss: 3.9150, accuracy: 3.4241%\n",
      "Epoch [1/2], Step [304/3732], Loss: 3.5512, accuracy: 3.4128%\n",
      "Epoch [1/2], Step [305/3732], Loss: 3.7143, accuracy: 3.4016%\n",
      "Epoch [1/2], Step [306/3732], Loss: 3.7494, accuracy: 3.4314%\n",
      "Epoch [1/2], Step [307/3732], Loss: 3.6750, accuracy: 3.4202%\n",
      "Epoch [1/2], Step [308/3732], Loss: 3.6532, accuracy: 3.4497%\n",
      "Epoch [1/2], Step [309/3732], Loss: 3.6527, accuracy: 3.4385%\n",
      "Epoch [1/2], Step [310/3732], Loss: 3.7966, accuracy: 3.4274%\n",
      "Epoch [1/2], Step [311/3732], Loss: 4.3803, accuracy: 3.4566%\n",
      "Epoch [1/2], Step [312/3732], Loss: 3.8192, accuracy: 3.4455%\n",
      "Epoch [1/2], Step [313/3732], Loss: 3.7781, accuracy: 3.4345%\n",
      "Epoch [1/2], Step [314/3732], Loss: 3.6820, accuracy: 3.4236%\n",
      "Epoch [1/2], Step [315/3732], Loss: 3.7106, accuracy: 3.4127%\n",
      "Epoch [1/2], Step [316/3732], Loss: 3.4236, accuracy: 3.4019%\n",
      "Epoch [1/2], Step [317/3732], Loss: 3.9465, accuracy: 3.3912%\n",
      "Epoch [1/2], Step [318/3732], Loss: 4.3507, accuracy: 3.3805%\n",
      "Epoch [1/2], Step [319/3732], Loss: 3.9411, accuracy: 3.3699%\n",
      "Epoch [1/2], Step [320/3732], Loss: 3.9397, accuracy: 3.3594%\n",
      "Epoch [1/2], Step [321/3732], Loss: 3.9038, accuracy: 3.3489%\n",
      "Epoch [1/2], Step [322/3732], Loss: 3.6197, accuracy: 3.3385%\n",
      "Epoch [1/2], Step [323/3732], Loss: 3.8001, accuracy: 3.3282%\n",
      "Epoch [1/2], Step [324/3732], Loss: 3.8684, accuracy: 3.3179%\n",
      "Epoch [1/2], Step [325/3732], Loss: 3.9734, accuracy: 3.3077%\n",
      "Epoch [1/2], Step [326/3732], Loss: 3.8232, accuracy: 3.3742%\n",
      "Epoch [1/2], Step [327/3732], Loss: 3.8544, accuracy: 3.3639%\n",
      "Epoch [1/2], Step [328/3732], Loss: 3.6608, accuracy: 3.3537%\n",
      "Epoch [1/2], Step [329/3732], Loss: 3.8597, accuracy: 3.3435%\n",
      "Epoch [1/2], Step [330/3732], Loss: 3.7298, accuracy: 3.3333%\n",
      "Epoch [1/2], Step [331/3732], Loss: 3.8117, accuracy: 3.3988%\n",
      "Epoch [1/2], Step [332/3732], Loss: 3.6546, accuracy: 3.4639%\n",
      "Epoch [1/2], Step [333/3732], Loss: 3.7686, accuracy: 3.4910%\n",
      "Epoch [1/2], Step [334/3732], Loss: 3.9012, accuracy: 3.4805%\n",
      "Epoch [1/2], Step [335/3732], Loss: 3.8789, accuracy: 3.4701%\n",
      "Epoch [1/2], Step [336/3732], Loss: 4.0351, accuracy: 3.4598%\n",
      "Epoch [1/2], Step [337/3732], Loss: 3.8397, accuracy: 3.4496%\n",
      "Epoch [1/2], Step [338/3732], Loss: 3.7167, accuracy: 3.4393%\n",
      "Epoch [1/2], Step [339/3732], Loss: 3.8574, accuracy: 3.4292%\n",
      "Epoch [1/2], Step [340/3732], Loss: 3.6983, accuracy: 3.4191%\n",
      "Epoch [1/2], Step [341/3732], Loss: 3.7868, accuracy: 3.4091%\n",
      "Epoch [1/2], Step [342/3732], Loss: 3.9009, accuracy: 3.4357%\n",
      "Epoch [1/2], Step [343/3732], Loss: 3.8389, accuracy: 3.4257%\n",
      "Epoch [1/2], Step [344/3732], Loss: 3.6770, accuracy: 3.4157%\n",
      "Epoch [1/2], Step [345/3732], Loss: 3.8399, accuracy: 3.4058%\n",
      "Epoch [1/2], Step [346/3732], Loss: 3.8618, accuracy: 3.3960%\n",
      "Epoch [1/2], Step [347/3732], Loss: 3.8969, accuracy: 3.3862%\n",
      "Epoch [1/2], Step [348/3732], Loss: 3.8504, accuracy: 3.3764%\n",
      "Epoch [1/2], Step [349/3732], Loss: 3.5394, accuracy: 3.4384%\n",
      "Epoch [1/2], Step [350/3732], Loss: 3.7183, accuracy: 3.4643%\n",
      "Epoch [1/2], Step [351/3732], Loss: 3.8099, accuracy: 3.4900%\n",
      "Epoch [1/2], Step [352/3732], Loss: 3.5416, accuracy: 3.5156%\n",
      "Epoch [1/2], Step [353/3732], Loss: 3.8920, accuracy: 3.5057%\n",
      "Epoch [1/2], Step [354/3732], Loss: 3.5549, accuracy: 3.4958%\n",
      "Epoch [1/2], Step [355/3732], Loss: 3.9409, accuracy: 3.4859%\n",
      "Epoch [1/2], Step [356/3732], Loss: 3.6855, accuracy: 3.5112%\n",
      "Epoch [1/2], Step [357/3732], Loss: 3.7485, accuracy: 3.5014%\n",
      "Epoch [1/2], Step [358/3732], Loss: 3.4792, accuracy: 3.5265%\n",
      "Epoch [1/2], Step [359/3732], Loss: 3.9755, accuracy: 3.5167%\n",
      "Epoch [1/2], Step [360/3732], Loss: 3.7827, accuracy: 3.5417%\n",
      "Epoch [1/2], Step [361/3732], Loss: 3.4853, accuracy: 3.6011%\n",
      "Epoch [1/2], Step [362/3732], Loss: 3.9040, accuracy: 3.5912%\n",
      "Epoch [1/2], Step [363/3732], Loss: 3.6447, accuracy: 3.5813%\n",
      "Epoch [1/2], Step [364/3732], Loss: 3.9151, accuracy: 3.5714%\n",
      "Epoch [1/2], Step [365/3732], Loss: 3.5600, accuracy: 3.6301%\n",
      "Epoch [1/2], Step [366/3732], Loss: 3.7847, accuracy: 3.6202%\n",
      "Epoch [1/2], Step [367/3732], Loss: 3.4319, accuracy: 3.6444%\n",
      "Epoch [1/2], Step [368/3732], Loss: 3.9801, accuracy: 3.6685%\n",
      "Epoch [1/2], Step [369/3732], Loss: 3.4334, accuracy: 3.6585%\n",
      "Epoch [1/2], Step [370/3732], Loss: 3.8042, accuracy: 3.6486%\n",
      "Epoch [1/2], Step [371/3732], Loss: 3.8472, accuracy: 3.6388%\n",
      "Epoch [1/2], Step [372/3732], Loss: 3.7033, accuracy: 3.6290%\n",
      "Epoch [1/2], Step [373/3732], Loss: 3.4708, accuracy: 3.6528%\n",
      "Epoch [1/2], Step [374/3732], Loss: 4.0033, accuracy: 3.6430%\n",
      "Epoch [1/2], Step [375/3732], Loss: 3.7220, accuracy: 3.6333%\n",
      "Epoch [1/2], Step [376/3732], Loss: 3.3325, accuracy: 3.6902%\n",
      "Epoch [1/2], Step [377/3732], Loss: 3.8658, accuracy: 3.6804%\n",
      "Epoch [1/2], Step [378/3732], Loss: 4.3541, accuracy: 3.7037%\n",
      "Epoch [1/2], Step [379/3732], Loss: 3.7469, accuracy: 3.7269%\n",
      "Epoch [1/2], Step [380/3732], Loss: 3.8632, accuracy: 3.7500%\n",
      "Epoch [1/2], Step [381/3732], Loss: 3.6079, accuracy: 3.7402%\n",
      "Epoch [1/2], Step [382/3732], Loss: 3.7357, accuracy: 3.7304%\n",
      "Epoch [1/2], Step [383/3732], Loss: 3.8146, accuracy: 3.7533%\n",
      "Epoch [1/2], Step [384/3732], Loss: 3.8813, accuracy: 3.7435%\n",
      "Epoch [1/2], Step [385/3732], Loss: 3.8930, accuracy: 3.7338%\n",
      "Epoch [1/2], Step [386/3732], Loss: 3.9027, accuracy: 3.7241%\n",
      "Epoch [1/2], Step [387/3732], Loss: 3.7922, accuracy: 3.7468%\n",
      "Epoch [1/2], Step [388/3732], Loss: 3.6322, accuracy: 3.7371%\n",
      "Epoch [1/2], Step [389/3732], Loss: 3.8141, accuracy: 3.7275%\n",
      "Epoch [1/2], Step [390/3732], Loss: 3.8637, accuracy: 3.7500%\n",
      "Epoch [1/2], Step [391/3732], Loss: 3.5519, accuracy: 3.7724%\n",
      "Epoch [1/2], Step [392/3732], Loss: 3.5916, accuracy: 3.7628%\n",
      "Epoch [1/2], Step [393/3732], Loss: 3.9327, accuracy: 3.7850%\n",
      "Epoch [1/2], Step [394/3732], Loss: 3.7326, accuracy: 3.7754%\n",
      "Epoch [1/2], Step [395/3732], Loss: 3.7384, accuracy: 3.7658%\n",
      "Epoch [1/2], Step [396/3732], Loss: 3.8004, accuracy: 3.7563%\n",
      "Epoch [1/2], Step [397/3732], Loss: 3.7324, accuracy: 3.7783%\n",
      "Epoch [1/2], Step [398/3732], Loss: 3.7858, accuracy: 3.8003%\n",
      "Epoch [1/2], Step [399/3732], Loss: 3.9031, accuracy: 3.7907%\n",
      "Epoch [1/2], Step [400/3732], Loss: 3.8175, accuracy: 3.8125%\n",
      "Epoch [1/2], Step [401/3732], Loss: 3.9004, accuracy: 3.8030%\n",
      "Epoch [1/2], Step [402/3732], Loss: 3.6378, accuracy: 3.8557%\n",
      "Epoch [1/2], Step [403/3732], Loss: 3.5620, accuracy: 3.8462%\n",
      "Epoch [1/2], Step [404/3732], Loss: 3.4203, accuracy: 3.8985%\n",
      "Epoch [1/2], Step [405/3732], Loss: 4.0245, accuracy: 3.8889%\n",
      "Epoch [1/2], Step [406/3732], Loss: 3.9468, accuracy: 3.8793%\n",
      "Epoch [1/2], Step [407/3732], Loss: 3.9541, accuracy: 3.8698%\n",
      "Epoch [1/2], Step [408/3732], Loss: 3.5800, accuracy: 3.8909%\n",
      "Epoch [1/2], Step [409/3732], Loss: 3.5258, accuracy: 3.9120%\n",
      "Epoch [1/2], Step [410/3732], Loss: 4.1350, accuracy: 3.9024%\n",
      "Epoch [1/2], Step [411/3732], Loss: 3.5800, accuracy: 3.9538%\n",
      "Epoch [1/2], Step [412/3732], Loss: 3.6337, accuracy: 3.9442%\n",
      "Epoch [1/2], Step [413/3732], Loss: 3.4692, accuracy: 3.9649%\n",
      "Epoch [1/2], Step [414/3732], Loss: 3.6800, accuracy: 4.0157%\n",
      "Epoch [1/2], Step [415/3732], Loss: 3.6881, accuracy: 4.0060%\n",
      "Epoch [1/2], Step [416/3732], Loss: 3.6550, accuracy: 3.9964%\n",
      "Epoch [1/2], Step [417/3732], Loss: 4.0309, accuracy: 3.9868%\n",
      "Epoch [1/2], Step [418/3732], Loss: 3.5284, accuracy: 4.0371%\n",
      "Epoch [1/2], Step [419/3732], Loss: 3.6647, accuracy: 4.0274%\n",
      "Epoch [1/2], Step [420/3732], Loss: 3.6170, accuracy: 4.0476%\n",
      "Epoch [1/2], Step [421/3732], Loss: 3.8572, accuracy: 4.0380%\n",
      "Epoch [1/2], Step [422/3732], Loss: 3.9531, accuracy: 4.0284%\n",
      "Epoch [1/2], Step [423/3732], Loss: 3.6463, accuracy: 4.0485%\n",
      "Epoch [1/2], Step [424/3732], Loss: 3.7344, accuracy: 4.0389%\n",
      "Epoch [1/2], Step [425/3732], Loss: 3.5549, accuracy: 4.0294%\n",
      "Epoch [1/2], Step [426/3732], Loss: 3.7632, accuracy: 4.0200%\n",
      "Epoch [1/2], Step [427/3732], Loss: 4.3636, accuracy: 4.0105%\n",
      "Epoch [1/2], Step [428/3732], Loss: 3.8003, accuracy: 4.0304%\n",
      "Epoch [1/2], Step [429/3732], Loss: 3.4144, accuracy: 4.0793%\n",
      "Epoch [1/2], Step [430/3732], Loss: 3.7095, accuracy: 4.0698%\n",
      "Epoch [1/2], Step [431/3732], Loss: 3.6208, accuracy: 4.0603%\n",
      "Epoch [1/2], Step [432/3732], Loss: 3.3352, accuracy: 4.1088%\n",
      "Epoch [1/2], Step [433/3732], Loss: 3.8116, accuracy: 4.0993%\n",
      "Epoch [1/2], Step [434/3732], Loss: 3.5957, accuracy: 4.1187%\n",
      "Epoch [1/2], Step [435/3732], Loss: 3.4813, accuracy: 4.1667%\n",
      "Epoch [1/2], Step [436/3732], Loss: 3.8683, accuracy: 4.1858%\n",
      "Epoch [1/2], Step [437/3732], Loss: 3.6869, accuracy: 4.1762%\n",
      "Epoch [1/2], Step [438/3732], Loss: 3.4858, accuracy: 4.1952%\n",
      "Epoch [1/2], Step [439/3732], Loss: 4.3564, accuracy: 4.1856%\n",
      "Epoch [1/2], Step [440/3732], Loss: 3.6994, accuracy: 4.2045%\n",
      "Epoch [1/2], Step [441/3732], Loss: 3.6900, accuracy: 4.1950%\n",
      "Epoch [1/2], Step [442/3732], Loss: 3.7553, accuracy: 4.1855%\n",
      "Epoch [1/2], Step [443/3732], Loss: 4.1532, accuracy: 4.1761%\n",
      "Epoch [1/2], Step [444/3732], Loss: 4.1395, accuracy: 4.1667%\n",
      "Epoch [1/2], Step [445/3732], Loss: 4.0196, accuracy: 4.1573%\n",
      "Epoch [1/2], Step [446/3732], Loss: 3.8340, accuracy: 4.1480%\n",
      "Epoch [1/2], Step [447/3732], Loss: 3.9130, accuracy: 4.1387%\n",
      "Epoch [1/2], Step [448/3732], Loss: 3.7176, accuracy: 4.1574%\n",
      "Epoch [1/2], Step [449/3732], Loss: 4.0137, accuracy: 4.1481%\n",
      "Epoch [1/2], Step [450/3732], Loss: 3.9329, accuracy: 4.1389%\n",
      "Epoch [1/2], Step [451/3732], Loss: 3.8583, accuracy: 4.1574%\n",
      "Epoch [1/2], Step [452/3732], Loss: 3.7292, accuracy: 4.1759%\n",
      "Epoch [1/2], Step [453/3732], Loss: 3.7754, accuracy: 4.2219%\n",
      "Epoch [1/2], Step [454/3732], Loss: 3.7854, accuracy: 4.2126%\n",
      "Epoch [1/2], Step [455/3732], Loss: 3.8161, accuracy: 4.2033%\n",
      "Epoch [1/2], Step [456/3732], Loss: 3.8557, accuracy: 4.1941%\n",
      "Epoch [1/2], Step [457/3732], Loss: 3.8840, accuracy: 4.1849%\n",
      "Epoch [1/2], Step [458/3732], Loss: 3.6596, accuracy: 4.1758%\n",
      "Epoch [1/2], Step [459/3732], Loss: 3.8954, accuracy: 4.1667%\n",
      "Epoch [1/2], Step [460/3732], Loss: 3.7535, accuracy: 4.1848%\n",
      "Epoch [1/2], Step [461/3732], Loss: 3.6892, accuracy: 4.2028%\n",
      "Epoch [1/2], Step [462/3732], Loss: 3.7750, accuracy: 4.2208%\n",
      "Epoch [1/2], Step [463/3732], Loss: 3.8823, accuracy: 4.2117%\n",
      "Epoch [1/2], Step [464/3732], Loss: 3.8074, accuracy: 4.2026%\n",
      "Epoch [1/2], Step [465/3732], Loss: 3.8498, accuracy: 4.1935%\n",
      "Epoch [1/2], Step [466/3732], Loss: 4.0216, accuracy: 4.1845%\n",
      "Epoch [1/2], Step [467/3732], Loss: 3.6589, accuracy: 4.2024%\n",
      "Epoch [1/2], Step [468/3732], Loss: 3.6282, accuracy: 4.1934%\n",
      "Epoch [1/2], Step [469/3732], Loss: 3.7978, accuracy: 4.1844%\n",
      "Epoch [1/2], Step [470/3732], Loss: 3.7029, accuracy: 4.2021%\n",
      "Epoch [1/2], Step [471/3732], Loss: 3.5918, accuracy: 4.1932%\n",
      "Epoch [1/2], Step [472/3732], Loss: 3.7749, accuracy: 4.2108%\n",
      "Epoch [1/2], Step [473/3732], Loss: 3.5224, accuracy: 4.2019%\n",
      "Epoch [1/2], Step [474/3732], Loss: 3.9453, accuracy: 4.1930%\n",
      "Epoch [1/2], Step [475/3732], Loss: 3.9950, accuracy: 4.1842%\n",
      "Epoch [1/2], Step [476/3732], Loss: 3.7165, accuracy: 4.1754%\n",
      "Epoch [1/2], Step [477/3732], Loss: 3.6800, accuracy: 4.2191%\n",
      "Epoch [1/2], Step [478/3732], Loss: 3.4410, accuracy: 4.2887%\n",
      "Epoch [1/2], Step [479/3732], Loss: 3.6747, accuracy: 4.3058%\n",
      "Epoch [1/2], Step [480/3732], Loss: 3.8005, accuracy: 4.2969%\n",
      "Epoch [1/2], Step [481/3732], Loss: 3.9340, accuracy: 4.2879%\n",
      "Epoch [1/2], Step [482/3732], Loss: 3.7987, accuracy: 4.2790%\n",
      "Epoch [1/2], Step [483/3732], Loss: 3.6977, accuracy: 4.2702%\n",
      "Epoch [1/2], Step [484/3732], Loss: 3.5136, accuracy: 4.3130%\n",
      "Epoch [1/2], Step [485/3732], Loss: 3.8662, accuracy: 4.3557%\n",
      "Epoch [1/2], Step [486/3732], Loss: 3.5690, accuracy: 4.3467%\n",
      "Epoch [1/2], Step [487/3732], Loss: 3.6857, accuracy: 4.3634%\n",
      "Epoch [1/2], Step [488/3732], Loss: 3.7190, accuracy: 4.3545%\n",
      "Epoch [1/2], Step [489/3732], Loss: 3.6908, accuracy: 4.3712%\n",
      "Epoch [1/2], Step [490/3732], Loss: 3.9650, accuracy: 4.3622%\n",
      "Epoch [1/2], Step [491/3732], Loss: 3.5660, accuracy: 4.4043%\n",
      "Epoch [1/2], Step [492/3732], Loss: 3.6637, accuracy: 4.4207%\n",
      "Epoch [1/2], Step [493/3732], Loss: 3.7161, accuracy: 4.4118%\n",
      "Epoch [1/2], Step [494/3732], Loss: 3.5653, accuracy: 4.4028%\n",
      "Epoch [1/2], Step [495/3732], Loss: 3.7778, accuracy: 4.3939%\n",
      "Epoch [1/2], Step [496/3732], Loss: 4.0064, accuracy: 4.3851%\n",
      "Epoch [1/2], Step [497/3732], Loss: 3.5062, accuracy: 4.4014%\n",
      "Epoch [1/2], Step [498/3732], Loss: 3.7870, accuracy: 4.3926%\n",
      "Epoch [1/2], Step [499/3732], Loss: 3.9233, accuracy: 4.3838%\n",
      "Epoch [1/2], Step [500/3732], Loss: 3.6589, accuracy: 4.4000%\n",
      "Epoch [1/2], Step [501/3732], Loss: 3.6345, accuracy: 4.3912%\n",
      "Epoch [1/2], Step [502/3732], Loss: 3.8521, accuracy: 4.3825%\n",
      "Epoch [1/2], Step [503/3732], Loss: 3.8091, accuracy: 4.3738%\n",
      "Epoch [1/2], Step [504/3732], Loss: 4.0496, accuracy: 4.3899%\n",
      "Epoch [1/2], Step [505/3732], Loss: 3.8613, accuracy: 4.3812%\n",
      "Epoch [1/2], Step [506/3732], Loss: 3.8924, accuracy: 4.3725%\n",
      "Epoch [1/2], Step [507/3732], Loss: 4.0449, accuracy: 4.3886%\n",
      "Epoch [1/2], Step [508/3732], Loss: 3.6436, accuracy: 4.3799%\n",
      "Epoch [1/2], Step [509/3732], Loss: 3.8551, accuracy: 4.3713%\n",
      "Epoch [1/2], Step [510/3732], Loss: 3.5939, accuracy: 4.3873%\n",
      "Epoch [1/2], Step [511/3732], Loss: 3.7940, accuracy: 4.3787%\n",
      "Epoch [1/2], Step [512/3732], Loss: 3.7334, accuracy: 4.3945%\n",
      "Epoch [1/2], Step [513/3732], Loss: 3.7901, accuracy: 4.3860%\n",
      "Epoch [1/2], Step [514/3732], Loss: 3.6021, accuracy: 4.3774%\n",
      "Epoch [1/2], Step [515/3732], Loss: 4.0230, accuracy: 4.3932%\n",
      "Epoch [1/2], Step [516/3732], Loss: 3.6198, accuracy: 4.4089%\n",
      "Epoch [1/2], Step [517/3732], Loss: 3.6224, accuracy: 4.4729%\n",
      "Epoch [1/2], Step [518/3732], Loss: 3.6101, accuracy: 4.4643%\n",
      "Epoch [1/2], Step [519/3732], Loss: 3.7853, accuracy: 4.5039%\n",
      "Epoch [1/2], Step [520/3732], Loss: 3.7754, accuracy: 4.4952%\n",
      "Epoch [1/2], Step [521/3732], Loss: 3.5328, accuracy: 4.4866%\n",
      "Epoch [1/2], Step [522/3732], Loss: 4.2351, accuracy: 4.5019%\n",
      "Epoch [1/2], Step [523/3732], Loss: 3.8928, accuracy: 4.4933%\n",
      "Epoch [1/2], Step [524/3732], Loss: 3.6776, accuracy: 4.4847%\n",
      "Epoch [1/2], Step [525/3732], Loss: 3.8944, accuracy: 4.5000%\n",
      "Epoch [1/2], Step [526/3732], Loss: 3.8102, accuracy: 4.4914%\n",
      "Epoch [1/2], Step [527/3732], Loss: 3.3839, accuracy: 4.5541%\n",
      "Epoch [1/2], Step [528/3732], Loss: 3.6744, accuracy: 4.5455%\n",
      "Epoch [1/2], Step [529/3732], Loss: 3.5272, accuracy: 4.5605%\n",
      "Epoch [1/2], Step [530/3732], Loss: 4.0762, accuracy: 4.5519%\n",
      "Epoch [1/2], Step [531/3732], Loss: 3.7230, accuracy: 4.5433%\n",
      "Epoch [1/2], Step [532/3732], Loss: 3.8776, accuracy: 4.5348%\n",
      "Epoch [1/2], Step [533/3732], Loss: 3.6326, accuracy: 4.5732%\n",
      "Epoch [1/2], Step [534/3732], Loss: 3.8458, accuracy: 4.5646%\n",
      "Epoch [1/2], Step [535/3732], Loss: 3.7810, accuracy: 4.5561%\n",
      "Epoch [1/2], Step [536/3732], Loss: 3.6540, accuracy: 4.5709%\n",
      "Epoch [1/2], Step [537/3732], Loss: 3.8532, accuracy: 4.5624%\n",
      "Epoch [1/2], Step [538/3732], Loss: 3.5786, accuracy: 4.5771%\n",
      "Epoch [1/2], Step [539/3732], Loss: 3.5695, accuracy: 4.5686%\n",
      "Epoch [1/2], Step [540/3732], Loss: 3.6759, accuracy: 4.5602%\n",
      "Epoch [1/2], Step [541/3732], Loss: 3.6670, accuracy: 4.5518%\n",
      "Epoch [1/2], Step [542/3732], Loss: 3.4857, accuracy: 4.5664%\n",
      "Epoch [1/2], Step [543/3732], Loss: 3.9513, accuracy: 4.5580%\n",
      "Epoch [1/2], Step [544/3732], Loss: 3.6054, accuracy: 4.5496%\n",
      "Epoch [1/2], Step [545/3732], Loss: 3.2893, accuracy: 4.5642%\n",
      "Epoch [1/2], Step [546/3732], Loss: 3.7988, accuracy: 4.5788%\n",
      "Epoch [1/2], Step [547/3732], Loss: 3.6838, accuracy: 4.6161%\n",
      "Epoch [1/2], Step [548/3732], Loss: 3.6927, accuracy: 4.6077%\n",
      "Epoch [1/2], Step [549/3732], Loss: 3.9344, accuracy: 4.6220%\n",
      "Epoch [1/2], Step [550/3732], Loss: 3.8695, accuracy: 4.6136%\n",
      "Epoch [1/2], Step [551/3732], Loss: 3.5178, accuracy: 4.6053%\n",
      "Epoch [1/2], Step [552/3732], Loss: 3.3713, accuracy: 4.6422%\n",
      "Epoch [1/2], Step [553/3732], Loss: 3.5085, accuracy: 4.6564%\n",
      "Epoch [1/2], Step [554/3732], Loss: 3.6447, accuracy: 4.6706%\n",
      "Epoch [1/2], Step [555/3732], Loss: 3.8702, accuracy: 4.6622%\n",
      "Epoch [1/2], Step [556/3732], Loss: 3.7500, accuracy: 4.6763%\n",
      "Epoch [1/2], Step [557/3732], Loss: 4.2222, accuracy: 4.6679%\n",
      "Epoch [1/2], Step [558/3732], Loss: 3.6443, accuracy: 4.6595%\n",
      "Epoch [1/2], Step [559/3732], Loss: 3.8800, accuracy: 4.6512%\n",
      "Epoch [1/2], Step [560/3732], Loss: 4.0634, accuracy: 4.6652%\n",
      "Epoch [1/2], Step [561/3732], Loss: 3.3245, accuracy: 4.6791%\n",
      "Epoch [1/2], Step [562/3732], Loss: 3.6651, accuracy: 4.6708%\n",
      "Epoch [1/2], Step [563/3732], Loss: 3.9878, accuracy: 4.6625%\n",
      "Epoch [1/2], Step [564/3732], Loss: 3.7866, accuracy: 4.6543%\n",
      "Epoch [1/2], Step [565/3732], Loss: 3.3525, accuracy: 4.6681%\n",
      "Epoch [1/2], Step [566/3732], Loss: 3.9694, accuracy: 4.6599%\n",
      "Epoch [1/2], Step [567/3732], Loss: 3.4641, accuracy: 4.6737%\n",
      "Epoch [1/2], Step [568/3732], Loss: 3.4415, accuracy: 4.7095%\n",
      "Epoch [1/2], Step [569/3732], Loss: 3.8469, accuracy: 4.7232%\n",
      "Epoch [1/2], Step [570/3732], Loss: 3.7326, accuracy: 4.7149%\n",
      "Epoch [1/2], Step [571/3732], Loss: 3.7583, accuracy: 4.7067%\n",
      "Epoch [1/2], Step [572/3732], Loss: 3.9324, accuracy: 4.7203%\n",
      "Epoch [1/2], Step [573/3732], Loss: 3.5596, accuracy: 4.7557%\n",
      "Epoch [1/2], Step [574/3732], Loss: 3.4847, accuracy: 4.7692%\n",
      "Epoch [1/2], Step [575/3732], Loss: 3.5932, accuracy: 4.7609%\n",
      "Epoch [1/2], Step [576/3732], Loss: 3.5244, accuracy: 4.7743%\n",
      "Epoch [1/2], Step [577/3732], Loss: 3.5468, accuracy: 4.8094%\n",
      "Epoch [1/2], Step [578/3732], Loss: 4.0410, accuracy: 4.8010%\n",
      "Epoch [1/2], Step [579/3732], Loss: 3.4793, accuracy: 4.8143%\n",
      "Epoch [1/2], Step [580/3732], Loss: 3.5288, accuracy: 4.8060%\n",
      "Epoch [1/2], Step [581/3732], Loss: 3.7706, accuracy: 4.7978%\n",
      "Epoch [1/2], Step [582/3732], Loss: 3.6967, accuracy: 4.7895%\n",
      "Epoch [1/2], Step [583/3732], Loss: 3.9417, accuracy: 4.7813%\n",
      "Epoch [1/2], Step [584/3732], Loss: 3.8637, accuracy: 4.7731%\n",
      "Epoch [1/2], Step [585/3732], Loss: 3.6644, accuracy: 4.7863%\n",
      "Epoch [1/2], Step [586/3732], Loss: 3.6376, accuracy: 4.7995%\n",
      "Epoch [1/2], Step [587/3732], Loss: 3.4030, accuracy: 4.7913%\n",
      "Epoch [1/2], Step [588/3732], Loss: 3.4995, accuracy: 4.8257%\n",
      "Epoch [1/2], Step [589/3732], Loss: 3.8559, accuracy: 4.8175%\n",
      "Epoch [1/2], Step [590/3732], Loss: 3.5088, accuracy: 4.8093%\n",
      "Epoch [1/2], Step [591/3732], Loss: 3.7878, accuracy: 4.8012%\n",
      "Epoch [1/2], Step [592/3732], Loss: 3.2832, accuracy: 4.8142%\n",
      "Epoch [1/2], Step [593/3732], Loss: 3.7344, accuracy: 4.8061%\n",
      "Epoch [1/2], Step [594/3732], Loss: 3.5356, accuracy: 4.8190%\n",
      "Epoch [1/2], Step [595/3732], Loss: 3.6029, accuracy: 4.8319%\n",
      "Epoch [1/2], Step [596/3732], Loss: 3.9662, accuracy: 4.8238%\n",
      "Epoch [1/2], Step [597/3732], Loss: 3.7634, accuracy: 4.8157%\n",
      "Epoch [1/2], Step [598/3732], Loss: 3.4158, accuracy: 4.8077%\n",
      "Epoch [1/2], Step [599/3732], Loss: 3.6934, accuracy: 4.8205%\n",
      "Epoch [1/2], Step [600/3732], Loss: 3.8746, accuracy: 4.8125%\n",
      "Epoch [1/2], Step [601/3732], Loss: 3.6057, accuracy: 4.8045%\n",
      "Epoch [1/2], Step [602/3732], Loss: 3.3723, accuracy: 4.8173%\n",
      "Epoch [1/2], Step [603/3732], Loss: 3.7549, accuracy: 4.8300%\n",
      "Epoch [1/2], Step [604/3732], Loss: 3.8124, accuracy: 4.8220%\n",
      "Epoch [1/2], Step [605/3732], Loss: 4.0647, accuracy: 4.8347%\n",
      "Epoch [1/2], Step [606/3732], Loss: 3.8849, accuracy: 4.8474%\n",
      "Epoch [1/2], Step [607/3732], Loss: 3.4692, accuracy: 4.8600%\n",
      "Epoch [1/2], Step [608/3732], Loss: 3.7587, accuracy: 4.8725%\n",
      "Epoch [1/2], Step [609/3732], Loss: 3.6882, accuracy: 4.8645%\n",
      "Epoch [1/2], Step [610/3732], Loss: 4.0566, accuracy: 4.8566%\n",
      "Epoch [1/2], Step [611/3732], Loss: 3.6104, accuracy: 4.8691%\n",
      "Epoch [1/2], Step [612/3732], Loss: 3.9343, accuracy: 4.8611%\n",
      "Epoch [1/2], Step [613/3732], Loss: 3.5196, accuracy: 4.8532%\n",
      "Epoch [1/2], Step [614/3732], Loss: 3.4745, accuracy: 4.8860%\n",
      "Epoch [1/2], Step [615/3732], Loss: 3.6657, accuracy: 4.8984%\n",
      "Epoch [1/2], Step [616/3732], Loss: 3.5298, accuracy: 4.8904%\n",
      "Epoch [1/2], Step [617/3732], Loss: 3.9963, accuracy: 4.8825%\n",
      "Epoch [1/2], Step [618/3732], Loss: 3.5560, accuracy: 4.8746%\n",
      "Epoch [1/2], Step [619/3732], Loss: 3.6745, accuracy: 4.8667%\n",
      "Epoch [1/2], Step [620/3732], Loss: 3.5079, accuracy: 4.8992%\n",
      "Epoch [1/2], Step [621/3732], Loss: 3.3863, accuracy: 4.9114%\n",
      "Epoch [1/2], Step [622/3732], Loss: 3.4784, accuracy: 4.9035%\n",
      "Epoch [1/2], Step [623/3732], Loss: 3.5702, accuracy: 4.8957%\n",
      "Epoch [1/2], Step [624/3732], Loss: 3.4397, accuracy: 4.9079%\n",
      "Epoch [1/2], Step [625/3732], Loss: 3.7144, accuracy: 4.9200%\n",
      "Epoch [1/2], Step [626/3732], Loss: 3.5142, accuracy: 4.9521%\n",
      "Epoch [1/2], Step [627/3732], Loss: 3.9645, accuracy: 4.9442%\n",
      "Epoch [1/2], Step [628/3732], Loss: 3.5706, accuracy: 4.9562%\n",
      "Epoch [1/2], Step [629/3732], Loss: 3.2600, accuracy: 4.9682%\n",
      "Epoch [1/2], Step [630/3732], Loss: 3.4856, accuracy: 5.0000%\n",
      "Epoch [1/2], Step [631/3732], Loss: 4.3118, accuracy: 4.9921%\n",
      "Epoch [1/2], Step [632/3732], Loss: 3.6405, accuracy: 5.0040%\n",
      "Epoch [1/2], Step [633/3732], Loss: 3.4402, accuracy: 4.9961%\n",
      "Epoch [1/2], Step [634/3732], Loss: 3.7618, accuracy: 5.0079%\n",
      "Epoch [1/2], Step [635/3732], Loss: 3.3379, accuracy: 5.0197%\n",
      "Epoch [1/2], Step [636/3732], Loss: 3.6849, accuracy: 5.0118%\n",
      "Epoch [1/2], Step [637/3732], Loss: 3.8349, accuracy: 5.0235%\n",
      "Epoch [1/2], Step [638/3732], Loss: 3.4774, accuracy: 5.0157%\n",
      "Epoch [1/2], Step [639/3732], Loss: 3.6576, accuracy: 5.0274%\n",
      "Epoch [1/2], Step [640/3732], Loss: 3.4435, accuracy: 5.0586%\n",
      "Epoch [1/2], Step [641/3732], Loss: 3.9680, accuracy: 5.0507%\n",
      "Epoch [1/2], Step [642/3732], Loss: 3.6450, accuracy: 5.0623%\n",
      "Epoch [1/2], Step [643/3732], Loss: 4.3281, accuracy: 5.0544%\n",
      "Epoch [1/2], Step [644/3732], Loss: 3.8701, accuracy: 5.0660%\n",
      "Epoch [1/2], Step [645/3732], Loss: 3.8479, accuracy: 5.0775%\n",
      "Epoch [1/2], Step [646/3732], Loss: 3.8924, accuracy: 5.0697%\n",
      "Epoch [1/2], Step [647/3732], Loss: 3.7202, accuracy: 5.0618%\n",
      "Epoch [1/2], Step [648/3732], Loss: 3.7840, accuracy: 5.0540%\n",
      "Epoch [1/2], Step [649/3732], Loss: 3.7205, accuracy: 5.0655%\n",
      "Epoch [1/2], Step [650/3732], Loss: 3.5845, accuracy: 5.0577%\n",
      "Epoch [1/2], Step [651/3732], Loss: 3.7391, accuracy: 5.0499%\n",
      "Epoch [1/2], Step [652/3732], Loss: 3.9614, accuracy: 5.0422%\n",
      "Epoch [1/2], Step [653/3732], Loss: 3.6393, accuracy: 5.0536%\n",
      "Epoch [1/2], Step [654/3732], Loss: 3.5539, accuracy: 5.0841%\n",
      "Epoch [1/2], Step [655/3732], Loss: 3.5947, accuracy: 5.0954%\n",
      "Epoch [1/2], Step [656/3732], Loss: 3.3227, accuracy: 5.1067%\n",
      "Epoch [1/2], Step [657/3732], Loss: 3.6773, accuracy: 5.0989%\n",
      "Epoch [1/2], Step [658/3732], Loss: 3.6196, accuracy: 5.0912%\n",
      "Epoch [1/2], Step [659/3732], Loss: 3.8331, accuracy: 5.0835%\n",
      "Epoch [1/2], Step [660/3732], Loss: 3.5598, accuracy: 5.0947%\n",
      "Epoch [1/2], Step [661/3732], Loss: 3.9930, accuracy: 5.0870%\n",
      "Epoch [1/2], Step [662/3732], Loss: 3.7577, accuracy: 5.0793%\n",
      "Epoch [1/2], Step [663/3732], Loss: 3.3909, accuracy: 5.0905%\n",
      "Epoch [1/2], Step [664/3732], Loss: 3.6025, accuracy: 5.1017%\n",
      "Epoch [1/2], Step [665/3732], Loss: 3.7904, accuracy: 5.1128%\n",
      "Epoch [1/2], Step [666/3732], Loss: 3.7938, accuracy: 5.1239%\n",
      "Epoch [1/2], Step [667/3732], Loss: 3.7436, accuracy: 5.1162%\n",
      "Epoch [1/2], Step [668/3732], Loss: 3.7909, accuracy: 5.1085%\n",
      "Epoch [1/2], Step [669/3732], Loss: 3.5911, accuracy: 5.1009%\n",
      "Epoch [1/2], Step [670/3732], Loss: 3.7263, accuracy: 5.1119%\n",
      "Epoch [1/2], Step [671/3732], Loss: 3.7284, accuracy: 5.1230%\n",
      "Epoch [1/2], Step [672/3732], Loss: 3.7504, accuracy: 5.1153%\n",
      "Epoch [1/2], Step [673/3732], Loss: 3.6717, accuracy: 5.1077%\n",
      "Epoch [1/2], Step [674/3732], Loss: 3.7602, accuracy: 5.1001%\n",
      "Epoch [1/2], Step [675/3732], Loss: 3.4960, accuracy: 5.1111%\n",
      "Epoch [1/2], Step [676/3732], Loss: 3.8869, accuracy: 5.1036%\n",
      "Epoch [1/2], Step [677/3732], Loss: 3.7029, accuracy: 5.0960%\n",
      "Epoch [1/2], Step [678/3732], Loss: 3.5908, accuracy: 5.0885%\n",
      "Epoch [1/2], Step [679/3732], Loss: 4.2325, accuracy: 5.0810%\n",
      "Epoch [1/2], Step [680/3732], Loss: 3.6652, accuracy: 5.0919%\n",
      "Epoch [1/2], Step [681/3732], Loss: 4.0637, accuracy: 5.0844%\n",
      "Epoch [1/2], Step [682/3732], Loss: 3.6992, accuracy: 5.0953%\n",
      "Epoch [1/2], Step [683/3732], Loss: 3.7984, accuracy: 5.0878%\n",
      "Epoch [1/2], Step [684/3732], Loss: 3.9287, accuracy: 5.0987%\n",
      "Epoch [1/2], Step [685/3732], Loss: 3.8389, accuracy: 5.0912%\n",
      "Epoch [1/2], Step [686/3732], Loss: 3.5593, accuracy: 5.1020%\n",
      "Epoch [1/2], Step [687/3732], Loss: 3.7083, accuracy: 5.1128%\n",
      "Epoch [1/2], Step [688/3732], Loss: 3.5411, accuracy: 5.1417%\n",
      "Epoch [1/2], Step [689/3732], Loss: 3.6432, accuracy: 5.1524%\n",
      "Epoch [1/2], Step [690/3732], Loss: 3.5081, accuracy: 5.1812%\n",
      "Epoch [1/2], Step [691/3732], Loss: 3.7571, accuracy: 5.1918%\n",
      "Epoch [1/2], Step [692/3732], Loss: 4.1254, accuracy: 5.1842%\n",
      "Epoch [1/2], Step [693/3732], Loss: 3.7053, accuracy: 5.1948%\n",
      "Epoch [1/2], Step [694/3732], Loss: 3.8840, accuracy: 5.1873%\n",
      "Epoch [1/2], Step [695/3732], Loss: 3.4138, accuracy: 5.1799%\n",
      "Epoch [1/2], Step [696/3732], Loss: 3.9256, accuracy: 5.1724%\n",
      "Epoch [1/2], Step [697/3732], Loss: 3.5890, accuracy: 5.1829%\n",
      "Epoch [1/2], Step [698/3732], Loss: 3.6997, accuracy: 5.2292%\n",
      "Epoch [1/2], Step [699/3732], Loss: 3.4434, accuracy: 5.2575%\n",
      "Epoch [1/2], Step [700/3732], Loss: 3.9615, accuracy: 5.2500%\n",
      "Epoch [1/2], Step [701/3732], Loss: 3.8425, accuracy: 5.2425%\n",
      "Epoch [1/2], Step [702/3732], Loss: 3.5962, accuracy: 5.3063%\n",
      "Epoch [1/2], Step [703/3732], Loss: 3.8138, accuracy: 5.2987%\n",
      "Epoch [1/2], Step [704/3732], Loss: 3.9233, accuracy: 5.2912%\n",
      "Epoch [1/2], Step [705/3732], Loss: 3.9470, accuracy: 5.2837%\n",
      "Epoch [1/2], Step [706/3732], Loss: 3.7748, accuracy: 5.2939%\n",
      "Epoch [1/2], Step [707/3732], Loss: 3.5042, accuracy: 5.3395%\n",
      "Epoch [1/2], Step [708/3732], Loss: 3.9721, accuracy: 5.3672%\n",
      "Epoch [1/2], Step [709/3732], Loss: 3.5244, accuracy: 5.3773%\n",
      "Epoch [1/2], Step [710/3732], Loss: 3.6020, accuracy: 5.3697%\n",
      "Epoch [1/2], Step [711/3732], Loss: 3.8503, accuracy: 5.3622%\n",
      "Epoch [1/2], Step [712/3732], Loss: 3.5695, accuracy: 5.3546%\n",
      "Epoch [1/2], Step [713/3732], Loss: 3.7048, accuracy: 5.3471%\n",
      "Epoch [1/2], Step [714/3732], Loss: 3.5864, accuracy: 5.3396%\n",
      "Epoch [1/2], Step [715/3732], Loss: 3.6366, accuracy: 5.3322%\n",
      "Epoch [1/2], Step [716/3732], Loss: 3.7536, accuracy: 5.3247%\n",
      "Epoch [1/2], Step [717/3732], Loss: 3.4171, accuracy: 5.3173%\n",
      "Epoch [1/2], Step [718/3732], Loss: 3.4171, accuracy: 5.3273%\n",
      "Epoch [1/2], Step [719/3732], Loss: 3.9399, accuracy: 5.3373%\n",
      "Epoch [1/2], Step [720/3732], Loss: 3.9487, accuracy: 5.3299%\n",
      "Epoch [1/2], Step [721/3732], Loss: 3.6600, accuracy: 5.3225%\n",
      "Epoch [1/2], Step [722/3732], Loss: 3.9497, accuracy: 5.3324%\n",
      "Epoch [1/2], Step [723/3732], Loss: 3.6457, accuracy: 5.3596%\n",
      "Epoch [1/2], Step [724/3732], Loss: 3.7559, accuracy: 5.3522%\n",
      "Epoch [1/2], Step [725/3732], Loss: 3.6659, accuracy: 5.3448%\n",
      "Epoch [1/2], Step [726/3732], Loss: 3.7646, accuracy: 5.3547%\n",
      "Epoch [1/2], Step [727/3732], Loss: 3.3821, accuracy: 5.3817%\n",
      "Epoch [1/2], Step [728/3732], Loss: 3.5068, accuracy: 5.3915%\n",
      "Epoch [1/2], Step [729/3732], Loss: 3.4918, accuracy: 5.3841%\n",
      "Epoch [1/2], Step [730/3732], Loss: 3.6744, accuracy: 5.3938%\n",
      "Epoch [1/2], Step [731/3732], Loss: 3.2948, accuracy: 5.3865%\n",
      "Epoch [1/2], Step [732/3732], Loss: 3.8317, accuracy: 5.3791%\n",
      "Epoch [1/2], Step [733/3732], Loss: 3.5272, accuracy: 5.3888%\n",
      "Epoch [1/2], Step [734/3732], Loss: 3.2638, accuracy: 5.4155%\n",
      "Epoch [1/2], Step [735/3732], Loss: 4.2014, accuracy: 5.4082%\n",
      "Epoch [1/2], Step [736/3732], Loss: 3.6200, accuracy: 5.4008%\n",
      "Epoch [1/2], Step [737/3732], Loss: 3.5120, accuracy: 5.3935%\n",
      "Epoch [1/2], Step [738/3732], Loss: 3.9671, accuracy: 5.3862%\n",
      "Epoch [1/2], Step [739/3732], Loss: 3.9236, accuracy: 5.3789%\n",
      "Epoch [1/2], Step [740/3732], Loss: 3.6171, accuracy: 5.3716%\n",
      "Epoch [1/2], Step [741/3732], Loss: 3.6296, accuracy: 5.3812%\n",
      "Epoch [1/2], Step [742/3732], Loss: 3.8273, accuracy: 5.3908%\n",
      "Epoch [1/2], Step [743/3732], Loss: 3.5176, accuracy: 5.4004%\n",
      "Epoch [1/2], Step [744/3732], Loss: 3.4155, accuracy: 5.4099%\n",
      "Epoch [1/2], Step [745/3732], Loss: 3.2735, accuracy: 5.4362%\n",
      "Epoch [1/2], Step [746/3732], Loss: 3.3610, accuracy: 5.4457%\n",
      "Epoch [1/2], Step [747/3732], Loss: 3.8227, accuracy: 5.4384%\n",
      "Epoch [1/2], Step [748/3732], Loss: 3.7864, accuracy: 5.4311%\n",
      "Epoch [1/2], Step [749/3732], Loss: 3.9845, accuracy: 5.4239%\n",
      "Epoch [1/2], Step [750/3732], Loss: 3.7761, accuracy: 5.4167%\n",
      "Epoch [1/2], Step [751/3732], Loss: 3.6799, accuracy: 5.4261%\n",
      "Epoch [1/2], Step [752/3732], Loss: 3.0871, accuracy: 5.4688%\n",
      "Epoch [1/2], Step [753/3732], Loss: 3.4996, accuracy: 5.4781%\n",
      "Epoch [1/2], Step [754/3732], Loss: 4.1512, accuracy: 5.4708%\n",
      "Epoch [1/2], Step [755/3732], Loss: 3.9019, accuracy: 5.4636%\n",
      "Epoch [1/2], Step [756/3732], Loss: 3.9524, accuracy: 5.4563%\n",
      "Epoch [1/2], Step [757/3732], Loss: 3.5427, accuracy: 5.4491%\n",
      "Epoch [1/2], Step [758/3732], Loss: 3.7504, accuracy: 5.4420%\n",
      "Epoch [1/2], Step [759/3732], Loss: 3.8786, accuracy: 5.4348%\n",
      "Epoch [1/2], Step [760/3732], Loss: 3.9691, accuracy: 5.4276%\n",
      "Epoch [1/2], Step [761/3732], Loss: 3.8179, accuracy: 5.4369%\n",
      "Epoch [1/2], Step [762/3732], Loss: 3.6834, accuracy: 5.4462%\n",
      "Epoch [1/2], Step [763/3732], Loss: 3.9431, accuracy: 5.4391%\n",
      "Epoch [1/2], Step [764/3732], Loss: 3.7630, accuracy: 5.4319%\n",
      "Epoch [1/2], Step [765/3732], Loss: 3.7289, accuracy: 5.4248%\n",
      "Epoch [1/2], Step [766/3732], Loss: 4.0250, accuracy: 5.4178%\n",
      "Epoch [1/2], Step [767/3732], Loss: 3.7053, accuracy: 5.4107%\n",
      "Epoch [1/2], Step [768/3732], Loss: 3.8948, accuracy: 5.4036%\n",
      "Epoch [1/2], Step [769/3732], Loss: 3.6179, accuracy: 5.4291%\n",
      "Epoch [1/2], Step [770/3732], Loss: 3.7241, accuracy: 5.4383%\n",
      "Epoch [1/2], Step [771/3732], Loss: 3.7697, accuracy: 5.4475%\n",
      "Epoch [1/2], Step [772/3732], Loss: 3.4690, accuracy: 5.4566%\n",
      "Epoch [1/2], Step [773/3732], Loss: 3.8204, accuracy: 5.4495%\n",
      "Epoch [1/2], Step [774/3732], Loss: 3.5260, accuracy: 5.4748%\n",
      "Epoch [1/2], Step [775/3732], Loss: 3.9173, accuracy: 5.4677%\n",
      "Epoch [1/2], Step [776/3732], Loss: 3.7626, accuracy: 5.4607%\n",
      "Epoch [1/2], Step [777/3732], Loss: 4.0244, accuracy: 5.4537%\n",
      "Epoch [1/2], Step [778/3732], Loss: 3.7245, accuracy: 5.4467%\n",
      "Epoch [1/2], Step [779/3732], Loss: 3.7334, accuracy: 5.4397%\n",
      "Epoch [1/2], Step [780/3732], Loss: 3.7497, accuracy: 5.4327%\n",
      "Epoch [1/2], Step [781/3732], Loss: 3.7549, accuracy: 5.4417%\n",
      "Epoch [1/2], Step [782/3732], Loss: 3.5155, accuracy: 5.4508%\n",
      "Epoch [1/2], Step [783/3732], Loss: 3.6096, accuracy: 5.4438%\n",
      "Epoch [1/2], Step [784/3732], Loss: 3.7617, accuracy: 5.4369%\n",
      "Epoch [1/2], Step [785/3732], Loss: 3.6443, accuracy: 5.4299%\n",
      "Epoch [1/2], Step [786/3732], Loss: 3.5980, accuracy: 5.4389%\n",
      "Epoch [1/2], Step [787/3732], Loss: 3.5184, accuracy: 5.4638%\n",
      "Epoch [1/2], Step [788/3732], Loss: 4.0749, accuracy: 5.4727%\n",
      "Epoch [1/2], Step [789/3732], Loss: 3.8625, accuracy: 5.4658%\n",
      "Epoch [1/2], Step [790/3732], Loss: 3.5318, accuracy: 5.4747%\n",
      "Epoch [1/2], Step [791/3732], Loss: 3.6992, accuracy: 5.4678%\n",
      "Epoch [1/2], Step [792/3732], Loss: 3.8469, accuracy: 5.4609%\n",
      "Epoch [1/2], Step [793/3732], Loss: 3.7947, accuracy: 5.4697%\n",
      "Epoch [1/2], Step [794/3732], Loss: 3.5207, accuracy: 5.4628%\n",
      "Epoch [1/2], Step [795/3732], Loss: 3.5933, accuracy: 5.4717%\n",
      "Epoch [1/2], Step [796/3732], Loss: 3.6148, accuracy: 5.4962%\n",
      "Epoch [1/2], Step [797/3732], Loss: 3.8212, accuracy: 5.5050%\n",
      "Epoch [1/2], Step [798/3732], Loss: 3.7926, accuracy: 5.4981%\n",
      "Epoch [1/2], Step [799/3732], Loss: 3.4224, accuracy: 5.4912%\n",
      "Epoch [1/2], Step [800/3732], Loss: 3.6376, accuracy: 5.5156%\n",
      "Epoch [1/2], Step [801/3732], Loss: 3.5727, accuracy: 5.5087%\n",
      "Epoch [1/2], Step [802/3732], Loss: 3.6915, accuracy: 5.5019%\n",
      "Epoch [1/2], Step [803/3732], Loss: 3.6337, accuracy: 5.4950%\n",
      "Epoch [1/2], Step [804/3732], Loss: 3.4353, accuracy: 5.4882%\n",
      "Epoch [1/2], Step [805/3732], Loss: 4.1019, accuracy: 5.4814%\n",
      "Epoch [1/2], Step [806/3732], Loss: 3.6062, accuracy: 5.4746%\n",
      "Epoch [1/2], Step [807/3732], Loss: 3.4351, accuracy: 5.4833%\n",
      "Epoch [1/2], Step [808/3732], Loss: 3.5474, accuracy: 5.5074%\n",
      "Epoch [1/2], Step [809/3732], Loss: 3.7867, accuracy: 5.5161%\n",
      "Epoch [1/2], Step [810/3732], Loss: 3.8847, accuracy: 5.5093%\n",
      "Epoch [1/2], Step [811/3732], Loss: 3.8469, accuracy: 5.5025%\n",
      "Epoch [1/2], Step [812/3732], Loss: 3.6346, accuracy: 5.4957%\n",
      "Epoch [1/2], Step [813/3732], Loss: 3.7270, accuracy: 5.4889%\n",
      "Epoch [1/2], Step [814/3732], Loss: 3.6442, accuracy: 5.4975%\n",
      "Epoch [1/2], Step [815/3732], Loss: 3.6224, accuracy: 5.5061%\n",
      "Epoch [1/2], Step [816/3732], Loss: 3.9747, accuracy: 5.4994%\n",
      "Epoch [1/2], Step [817/3732], Loss: 3.7647, accuracy: 5.4927%\n",
      "Epoch [1/2], Step [818/3732], Loss: 3.9313, accuracy: 5.4859%\n",
      "Epoch [1/2], Step [819/3732], Loss: 4.2016, accuracy: 5.4792%\n",
      "Epoch [1/2], Step [820/3732], Loss: 3.5616, accuracy: 5.4726%\n",
      "Epoch [1/2], Step [821/3732], Loss: 3.4205, accuracy: 5.4811%\n",
      "Epoch [1/2], Step [822/3732], Loss: 3.3746, accuracy: 5.4745%\n",
      "Epoch [1/2], Step [823/3732], Loss: 3.6290, accuracy: 5.4678%\n",
      "Epoch [1/2], Step [824/3732], Loss: 3.6809, accuracy: 5.4612%\n",
      "Epoch [1/2], Step [825/3732], Loss: 3.4958, accuracy: 5.4697%\n",
      "Epoch [1/2], Step [826/3732], Loss: 3.8263, accuracy: 5.4631%\n",
      "Epoch [1/2], Step [827/3732], Loss: 3.5994, accuracy: 5.4565%\n",
      "Epoch [1/2], Step [828/3732], Loss: 3.6346, accuracy: 5.4650%\n",
      "Epoch [1/2], Step [829/3732], Loss: 3.8289, accuracy: 5.4735%\n",
      "Epoch [1/2], Step [830/3732], Loss: 3.4234, accuracy: 5.4970%\n",
      "Epoch [1/2], Step [831/3732], Loss: 3.6700, accuracy: 5.5054%\n",
      "Epoch [1/2], Step [832/3732], Loss: 3.9564, accuracy: 5.4988%\n",
      "Epoch [1/2], Step [833/3732], Loss: 3.8594, accuracy: 5.4922%\n",
      "Epoch [1/2], Step [834/3732], Loss: 3.4559, accuracy: 5.5156%\n",
      "Epoch [1/2], Step [835/3732], Loss: 3.8120, accuracy: 5.5240%\n",
      "Epoch [1/2], Step [836/3732], Loss: 3.4107, accuracy: 5.5173%\n",
      "Epoch [1/2], Step [837/3732], Loss: 3.7045, accuracy: 5.5108%\n",
      "Epoch [1/2], Step [838/3732], Loss: 3.8070, accuracy: 5.5042%\n",
      "Epoch [1/2], Step [839/3732], Loss: 3.3911, accuracy: 5.5125%\n",
      "Epoch [1/2], Step [840/3732], Loss: 3.6978, accuracy: 5.5060%\n",
      "Epoch [1/2], Step [841/3732], Loss: 4.3607, accuracy: 5.4994%\n",
      "Epoch [1/2], Step [842/3732], Loss: 3.1876, accuracy: 5.5077%\n",
      "Epoch [1/2], Step [843/3732], Loss: 3.6795, accuracy: 5.5012%\n",
      "Epoch [1/2], Step [844/3732], Loss: 3.5151, accuracy: 5.4947%\n",
      "Epoch [1/2], Step [845/3732], Loss: 3.6140, accuracy: 5.5030%\n",
      "Epoch [1/2], Step [846/3732], Loss: 3.5912, accuracy: 5.5112%\n",
      "Epoch [1/2], Step [847/3732], Loss: 3.6454, accuracy: 5.5195%\n",
      "Epoch [1/2], Step [848/3732], Loss: 3.6204, accuracy: 5.5130%\n",
      "Epoch [1/2], Step [849/3732], Loss: 3.6104, accuracy: 5.5212%\n",
      "Epoch [1/2], Step [850/3732], Loss: 3.5653, accuracy: 5.5294%\n",
      "Epoch [1/2], Step [851/3732], Loss: 3.5955, accuracy: 5.5229%\n",
      "Epoch [1/2], Step [852/3732], Loss: 3.8757, accuracy: 5.5458%\n",
      "Epoch [1/2], Step [853/3732], Loss: 3.9596, accuracy: 5.5393%\n",
      "Epoch [1/2], Step [854/3732], Loss: 3.6671, accuracy: 5.5474%\n",
      "Epoch [1/2], Step [855/3732], Loss: 3.9668, accuracy: 5.5409%\n",
      "Epoch [1/2], Step [856/3732], Loss: 4.0373, accuracy: 5.5491%\n",
      "Epoch [1/2], Step [857/3732], Loss: 3.4105, accuracy: 5.5572%\n",
      "Epoch [1/2], Step [858/3732], Loss: 3.9204, accuracy: 5.5507%\n",
      "Epoch [1/2], Step [859/3732], Loss: 3.4699, accuracy: 5.5588%\n",
      "Epoch [1/2], Step [860/3732], Loss: 3.5047, accuracy: 5.5814%\n",
      "Epoch [1/2], Step [861/3732], Loss: 4.0067, accuracy: 5.5749%\n",
      "Epoch [1/2], Step [862/3732], Loss: 3.8698, accuracy: 5.5684%\n",
      "Epoch [1/2], Step [863/3732], Loss: 4.0591, accuracy: 5.5620%\n",
      "Epoch [1/2], Step [864/3732], Loss: 3.3052, accuracy: 5.5556%\n",
      "Epoch [1/2], Step [865/3732], Loss: 3.9230, accuracy: 5.5491%\n",
      "Epoch [1/2], Step [866/3732], Loss: 3.5775, accuracy: 5.5572%\n",
      "Epoch [1/2], Step [867/3732], Loss: 3.6904, accuracy: 5.5507%\n",
      "Epoch [1/2], Step [868/3732], Loss: 3.8226, accuracy: 5.5732%\n",
      "Epoch [1/2], Step [869/3732], Loss: 3.8364, accuracy: 5.5667%\n",
      "Epoch [1/2], Step [870/3732], Loss: 3.9717, accuracy: 5.5603%\n",
      "Epoch [1/2], Step [871/3732], Loss: 3.5665, accuracy: 5.5540%\n",
      "Epoch [1/2], Step [872/3732], Loss: 4.2201, accuracy: 5.5476%\n",
      "Epoch [1/2], Step [873/3732], Loss: 3.7208, accuracy: 5.5412%\n",
      "Epoch [1/2], Step [874/3732], Loss: 3.4624, accuracy: 5.5492%\n",
      "Epoch [1/2], Step [875/3732], Loss: 3.8849, accuracy: 5.5429%\n",
      "Epoch [1/2], Step [876/3732], Loss: 3.8805, accuracy: 5.5651%\n",
      "Epoch [1/2], Step [877/3732], Loss: 3.7941, accuracy: 5.5587%\n",
      "Epoch [1/2], Step [878/3732], Loss: 3.4115, accuracy: 5.5666%\n",
      "Epoch [1/2], Step [879/3732], Loss: 3.7290, accuracy: 5.5745%\n",
      "Epoch [1/2], Step [880/3732], Loss: 3.5261, accuracy: 5.5824%\n",
      "Epoch [1/2], Step [881/3732], Loss: 3.5474, accuracy: 5.5760%\n",
      "Epoch [1/2], Step [882/3732], Loss: 3.7250, accuracy: 5.5697%\n",
      "Epoch [1/2], Step [883/3732], Loss: 4.0537, accuracy: 5.5776%\n",
      "Epoch [1/2], Step [884/3732], Loss: 3.6146, accuracy: 5.5854%\n",
      "Epoch [1/2], Step [885/3732], Loss: 3.4846, accuracy: 5.5791%\n",
      "Epoch [1/2], Step [886/3732], Loss: 3.4625, accuracy: 5.6151%\n",
      "Epoch [1/2], Step [887/3732], Loss: 3.8394, accuracy: 5.6088%\n",
      "Epoch [1/2], Step [888/3732], Loss: 3.9249, accuracy: 5.6025%\n",
      "Epoch [1/2], Step [889/3732], Loss: 3.7054, accuracy: 5.5962%\n",
      "Epoch [1/2], Step [890/3732], Loss: 3.8839, accuracy: 5.5899%\n",
      "Epoch [1/2], Step [891/3732], Loss: 3.4891, accuracy: 5.5976%\n",
      "Epoch [1/2], Step [892/3732], Loss: 3.4387, accuracy: 5.6054%\n",
      "Epoch [1/2], Step [893/3732], Loss: 3.6875, accuracy: 5.5991%\n",
      "Epoch [1/2], Step [894/3732], Loss: 3.2831, accuracy: 5.5928%\n",
      "Epoch [1/2], Step [895/3732], Loss: 3.4183, accuracy: 5.6006%\n",
      "Epoch [1/2], Step [896/3732], Loss: 3.3844, accuracy: 5.6083%\n",
      "Epoch [1/2], Step [897/3732], Loss: 3.5811, accuracy: 5.6020%\n",
      "Epoch [1/2], Step [898/3732], Loss: 3.3260, accuracy: 5.6097%\n",
      "Epoch [1/2], Step [899/3732], Loss: 3.6962, accuracy: 5.6034%\n",
      "Epoch [1/2], Step [900/3732], Loss: 3.3346, accuracy: 5.6250%\n",
      "Epoch [1/2], Step [901/3732], Loss: 3.4499, accuracy: 5.6188%\n",
      "Epoch [1/2], Step [902/3732], Loss: 3.8089, accuracy: 5.6125%\n",
      "Epoch [1/2], Step [903/3732], Loss: 4.2798, accuracy: 5.6063%\n",
      "Epoch [1/2], Step [904/3732], Loss: 3.5920, accuracy: 5.6139%\n",
      "Epoch [1/2], Step [905/3732], Loss: 3.2571, accuracy: 5.6215%\n",
      "Epoch [1/2], Step [906/3732], Loss: 3.4228, accuracy: 5.6153%\n",
      "Epoch [1/2], Step [907/3732], Loss: 3.8441, accuracy: 5.6092%\n",
      "Epoch [1/2], Step [908/3732], Loss: 3.6102, accuracy: 5.6167%\n",
      "Epoch [1/2], Step [909/3732], Loss: 3.4366, accuracy: 5.6106%\n",
      "Epoch [1/2], Step [910/3732], Loss: 3.4907, accuracy: 5.6181%\n",
      "Epoch [1/2], Step [911/3732], Loss: 2.9822, accuracy: 5.6257%\n",
      "Epoch [1/2], Step [912/3732], Loss: 3.6527, accuracy: 5.6332%\n",
      "Epoch [1/2], Step [913/3732], Loss: 3.9385, accuracy: 5.6271%\n",
      "Epoch [1/2], Step [914/3732], Loss: 3.6371, accuracy: 5.6346%\n",
      "Epoch [1/2], Step [915/3732], Loss: 3.3814, accuracy: 5.6421%\n",
      "Epoch [1/2], Step [916/3732], Loss: 3.5768, accuracy: 5.6359%\n",
      "Epoch [1/2], Step [917/3732], Loss: 3.7797, accuracy: 5.6298%\n",
      "Epoch [1/2], Step [918/3732], Loss: 3.2792, accuracy: 5.6509%\n",
      "Epoch [1/2], Step [919/3732], Loss: 3.4594, accuracy: 5.6447%\n",
      "Epoch [1/2], Step [920/3732], Loss: 3.4662, accuracy: 5.6522%\n",
      "Epoch [1/2], Step [921/3732], Loss: 3.5962, accuracy: 5.6732%\n",
      "Epoch [1/2], Step [922/3732], Loss: 3.6819, accuracy: 5.6670%\n",
      "Epoch [1/2], Step [923/3732], Loss: 3.4623, accuracy: 5.6744%\n",
      "Epoch [1/2], Step [924/3732], Loss: 3.3297, accuracy: 5.6953%\n",
      "Epoch [1/2], Step [925/3732], Loss: 3.8352, accuracy: 5.6892%\n",
      "Epoch [1/2], Step [926/3732], Loss: 3.3854, accuracy: 5.6965%\n",
      "Epoch [1/2], Step [927/3732], Loss: 3.7757, accuracy: 5.7174%\n",
      "Epoch [1/2], Step [928/3732], Loss: 3.3536, accuracy: 5.7247%\n",
      "Epoch [1/2], Step [929/3732], Loss: 3.5634, accuracy: 5.7320%\n",
      "Epoch [1/2], Step [930/3732], Loss: 3.8094, accuracy: 5.7258%\n",
      "Epoch [1/2], Step [931/3732], Loss: 4.0051, accuracy: 5.7197%\n",
      "Epoch [1/2], Step [932/3732], Loss: 3.5260, accuracy: 5.7269%\n",
      "Epoch [1/2], Step [933/3732], Loss: 3.5452, accuracy: 5.7342%\n",
      "Epoch [1/2], Step [934/3732], Loss: 3.8804, accuracy: 5.7281%\n",
      "Epoch [1/2], Step [935/3732], Loss: 3.5553, accuracy: 5.7487%\n",
      "Epoch [1/2], Step [936/3732], Loss: 3.6092, accuracy: 5.7425%\n",
      "Epoch [1/2], Step [937/3732], Loss: 3.5344, accuracy: 5.7364%\n",
      "Epoch [1/2], Step [938/3732], Loss: 3.6097, accuracy: 5.7569%\n",
      "Epoch [1/2], Step [939/3732], Loss: 3.5987, accuracy: 5.7508%\n",
      "Epoch [1/2], Step [940/3732], Loss: 3.9094, accuracy: 5.7447%\n",
      "Epoch [1/2], Step [941/3732], Loss: 4.1634, accuracy: 5.7386%\n",
      "Epoch [1/2], Step [942/3732], Loss: 3.6906, accuracy: 5.7325%\n",
      "Epoch [1/2], Step [943/3732], Loss: 3.6561, accuracy: 5.7264%\n",
      "Epoch [1/2], Step [944/3732], Loss: 3.4533, accuracy: 5.7468%\n",
      "Epoch [1/2], Step [945/3732], Loss: 3.9814, accuracy: 5.7407%\n",
      "Epoch [1/2], Step [946/3732], Loss: 3.6371, accuracy: 5.7479%\n",
      "Epoch [1/2], Step [947/3732], Loss: 3.2758, accuracy: 5.7682%\n",
      "Epoch [1/2], Step [948/3732], Loss: 3.7026, accuracy: 5.7621%\n",
      "Epoch [1/2], Step [949/3732], Loss: 3.2675, accuracy: 5.7956%\n",
      "Epoch [1/2], Step [950/3732], Loss: 3.7228, accuracy: 5.8026%\n",
      "Epoch [1/2], Step [951/3732], Loss: 3.4799, accuracy: 5.8228%\n",
      "Epoch [1/2], Step [952/3732], Loss: 3.3258, accuracy: 5.8298%\n",
      "Epoch [1/2], Step [953/3732], Loss: 3.3221, accuracy: 5.8368%\n",
      "Epoch [1/2], Step [954/3732], Loss: 3.6646, accuracy: 5.8307%\n",
      "Epoch [1/2], Step [955/3732], Loss: 3.7752, accuracy: 5.8246%\n",
      "Epoch [1/2], Step [956/3732], Loss: 3.8717, accuracy: 5.8316%\n",
      "Epoch [1/2], Step [957/3732], Loss: 3.6726, accuracy: 5.8255%\n",
      "Epoch [1/2], Step [958/3732], Loss: 4.1127, accuracy: 5.8194%\n",
      "Epoch [1/2], Step [959/3732], Loss: 4.1277, accuracy: 5.8133%\n",
      "Epoch [1/2], Step [960/3732], Loss: 3.7310, accuracy: 5.8203%\n",
      "Epoch [1/2], Step [961/3732], Loss: 3.5574, accuracy: 5.8143%\n",
      "Epoch [1/2], Step [962/3732], Loss: 3.6577, accuracy: 5.8082%\n",
      "Epoch [1/2], Step [963/3732], Loss: 3.8357, accuracy: 5.8022%\n",
      "Epoch [1/2], Step [964/3732], Loss: 3.9493, accuracy: 5.7962%\n",
      "Epoch [1/2], Step [965/3732], Loss: 3.7869, accuracy: 5.8161%\n",
      "Epoch [1/2], Step [966/3732], Loss: 3.6990, accuracy: 5.8100%\n",
      "Epoch [1/2], Step [967/3732], Loss: 3.9317, accuracy: 5.8040%\n",
      "Epoch [1/2], Step [968/3732], Loss: 3.4537, accuracy: 5.8110%\n",
      "Epoch [1/2], Step [969/3732], Loss: 3.4058, accuracy: 5.8308%\n",
      "Epoch [1/2], Step [970/3732], Loss: 3.7781, accuracy: 5.8247%\n",
      "Epoch [1/2], Step [971/3732], Loss: 3.5039, accuracy: 5.8445%\n",
      "Epoch [1/2], Step [972/3732], Loss: 3.7774, accuracy: 5.8385%\n",
      "Epoch [1/2], Step [973/3732], Loss: 3.9009, accuracy: 5.8325%\n",
      "Epoch [1/2], Step [974/3732], Loss: 3.7982, accuracy: 5.8265%\n",
      "Epoch [1/2], Step [975/3732], Loss: 3.7536, accuracy: 5.8205%\n",
      "Epoch [1/2], Step [976/3732], Loss: 3.3571, accuracy: 5.8145%\n",
      "Epoch [1/2], Step [977/3732], Loss: 3.5700, accuracy: 5.8214%\n",
      "Epoch [1/2], Step [978/3732], Loss: 3.7199, accuracy: 5.8282%\n",
      "Epoch [1/2], Step [979/3732], Loss: 3.5231, accuracy: 5.8478%\n",
      "Epoch [1/2], Step [980/3732], Loss: 3.9068, accuracy: 5.8418%\n",
      "Epoch [1/2], Step [981/3732], Loss: 3.4809, accuracy: 5.8486%\n",
      "Epoch [1/2], Step [982/3732], Loss: 3.7703, accuracy: 5.8427%\n",
      "Epoch [1/2], Step [983/3732], Loss: 3.6066, accuracy: 5.8494%\n",
      "Epoch [1/2], Step [984/3732], Loss: 3.3943, accuracy: 5.8562%\n",
      "Epoch [1/2], Step [985/3732], Loss: 3.4434, accuracy: 5.8629%\n",
      "Epoch [1/2], Step [986/3732], Loss: 3.8756, accuracy: 5.8570%\n",
      "Epoch [1/2], Step [987/3732], Loss: 3.6724, accuracy: 5.8637%\n",
      "Epoch [1/2], Step [988/3732], Loss: 3.6655, accuracy: 5.8578%\n",
      "Epoch [1/2], Step [989/3732], Loss: 3.4702, accuracy: 5.8771%\n",
      "Epoch [1/2], Step [990/3732], Loss: 3.2870, accuracy: 5.8838%\n",
      "Epoch [1/2], Step [991/3732], Loss: 3.8883, accuracy: 5.8779%\n",
      "Epoch [1/2], Step [992/3732], Loss: 3.3729, accuracy: 5.8846%\n",
      "Epoch [1/2], Step [993/3732], Loss: 3.4007, accuracy: 5.8912%\n",
      "Epoch [1/2], Step [994/3732], Loss: 3.2866, accuracy: 5.8979%\n",
      "Epoch [1/2], Step [995/3732], Loss: 3.7546, accuracy: 5.9171%\n",
      "Epoch [1/2], Step [996/3732], Loss: 3.4042, accuracy: 5.9237%\n",
      "Epoch [1/2], Step [997/3732], Loss: 3.5399, accuracy: 5.9178%\n",
      "Epoch [1/2], Step [998/3732], Loss: 3.9179, accuracy: 5.9118%\n",
      "Epoch [1/2], Step [999/3732], Loss: 3.9253, accuracy: 5.9059%\n",
      "Epoch [1/2], Step [1000/3732], Loss: 3.5356, accuracy: 5.9250%\n",
      "Epoch [1/2], Step [1001/3732], Loss: 3.9754, accuracy: 5.9191%\n",
      "Epoch [1/2], Step [1002/3732], Loss: 3.9556, accuracy: 5.9132%\n",
      "Epoch [1/2], Step [1003/3732], Loss: 3.9017, accuracy: 5.9073%\n",
      "Epoch [1/2], Step [1004/3732], Loss: 2.9426, accuracy: 5.9138%\n",
      "Epoch [1/2], Step [1005/3732], Loss: 3.4514, accuracy: 5.9080%\n",
      "Epoch [1/2], Step [1006/3732], Loss: 3.6066, accuracy: 5.9145%\n",
      "Epoch [1/2], Step [1007/3732], Loss: 3.4777, accuracy: 5.9086%\n",
      "Epoch [1/2], Step [1008/3732], Loss: 3.6009, accuracy: 5.9028%\n",
      "Epoch [1/2], Step [1009/3732], Loss: 3.7540, accuracy: 5.8969%\n",
      "Epoch [1/2], Step [1010/3732], Loss: 3.9991, accuracy: 5.9035%\n",
      "Epoch [1/2], Step [1011/3732], Loss: 3.6130, accuracy: 5.9100%\n",
      "Epoch [1/2], Step [1012/3732], Loss: 3.9005, accuracy: 5.9165%\n",
      "Epoch [1/2], Step [1013/3732], Loss: 3.7515, accuracy: 5.9107%\n",
      "Epoch [1/2], Step [1014/3732], Loss: 3.8462, accuracy: 5.9048%\n",
      "Epoch [1/2], Step [1015/3732], Loss: 3.8005, accuracy: 5.8990%\n",
      "Epoch [1/2], Step [1016/3732], Loss: 3.8032, accuracy: 5.9055%\n",
      "Epoch [1/2], Step [1017/3732], Loss: 3.8634, accuracy: 5.8997%\n",
      "Epoch [1/2], Step [1018/3732], Loss: 4.0803, accuracy: 5.8939%\n",
      "Epoch [1/2], Step [1019/3732], Loss: 3.5932, accuracy: 5.9004%\n",
      "Epoch [1/2], Step [1020/3732], Loss: 3.8911, accuracy: 5.9069%\n",
      "Epoch [1/2], Step [1021/3732], Loss: 3.4921, accuracy: 5.9011%\n",
      "Epoch [1/2], Step [1022/3732], Loss: 3.7076, accuracy: 5.8953%\n",
      "Epoch [1/2], Step [1023/3732], Loss: 3.2716, accuracy: 5.9018%\n",
      "Epoch [1/2], Step [1024/3732], Loss: 3.5699, accuracy: 5.9082%\n",
      "Epoch [1/2], Step [1025/3732], Loss: 3.6204, accuracy: 5.9146%\n",
      "Epoch [1/2], Step [1026/3732], Loss: 4.0186, accuracy: 5.9089%\n",
      "Epoch [1/2], Step [1027/3732], Loss: 3.2241, accuracy: 5.9153%\n",
      "Epoch [1/2], Step [1028/3732], Loss: 4.1459, accuracy: 5.9095%\n",
      "Epoch [1/2], Step [1029/3732], Loss: 3.9537, accuracy: 5.9038%\n",
      "Epoch [1/2], Step [1030/3732], Loss: 3.5906, accuracy: 5.8981%\n",
      "Epoch [1/2], Step [1031/3732], Loss: 3.5504, accuracy: 5.8923%\n",
      "Epoch [1/2], Step [1032/3732], Loss: 2.9757, accuracy: 5.9109%\n",
      "Epoch [1/2], Step [1033/3732], Loss: 3.6412, accuracy: 5.9293%\n",
      "Epoch [1/2], Step [1034/3732], Loss: 3.8227, accuracy: 5.9357%\n",
      "Epoch [1/2], Step [1035/3732], Loss: 3.4939, accuracy: 5.9541%\n",
      "Epoch [1/2], Step [1036/3732], Loss: 3.5650, accuracy: 5.9604%\n",
      "Epoch [1/2], Step [1037/3732], Loss: 3.5951, accuracy: 5.9667%\n",
      "Epoch [1/2], Step [1038/3732], Loss: 3.6754, accuracy: 5.9851%\n",
      "Epoch [1/2], Step [1039/3732], Loss: 3.6507, accuracy: 5.9913%\n",
      "Epoch [1/2], Step [1040/3732], Loss: 3.4721, accuracy: 5.9856%\n",
      "Epoch [1/2], Step [1041/3732], Loss: 3.7296, accuracy: 5.9798%\n",
      "Epoch [1/2], Step [1042/3732], Loss: 3.4996, accuracy: 5.9741%\n",
      "Epoch [1/2], Step [1043/3732], Loss: 3.4615, accuracy: 5.9803%\n",
      "Epoch [1/2], Step [1044/3732], Loss: 3.7784, accuracy: 5.9866%\n",
      "Epoch [1/2], Step [1045/3732], Loss: 4.0550, accuracy: 5.9809%\n",
      "Epoch [1/2], Step [1046/3732], Loss: 3.6432, accuracy: 5.9871%\n",
      "Epoch [1/2], Step [1047/3732], Loss: 3.9698, accuracy: 5.9814%\n",
      "Epoch [1/2], Step [1048/3732], Loss: 3.5780, accuracy: 5.9876%\n",
      "Epoch [1/2], Step [1049/3732], Loss: 3.7800, accuracy: 5.9819%\n",
      "Epoch [1/2], Step [1050/3732], Loss: 3.9596, accuracy: 5.9762%\n",
      "Epoch [1/2], Step [1051/3732], Loss: 3.3458, accuracy: 5.9943%\n",
      "Epoch [1/2], Step [1052/3732], Loss: 3.4952, accuracy: 6.0005%\n",
      "Epoch [1/2], Step [1053/3732], Loss: 3.7250, accuracy: 5.9948%\n",
      "Epoch [1/2], Step [1054/3732], Loss: 3.2981, accuracy: 6.0009%\n",
      "Epoch [1/2], Step [1055/3732], Loss: 3.7446, accuracy: 5.9953%\n",
      "Epoch [1/2], Step [1056/3732], Loss: 3.2065, accuracy: 6.0014%\n",
      "Epoch [1/2], Step [1057/3732], Loss: 3.7966, accuracy: 5.9957%\n",
      "Epoch [1/2], Step [1058/3732], Loss: 3.5682, accuracy: 6.0019%\n",
      "Epoch [1/2], Step [1059/3732], Loss: 3.6410, accuracy: 6.0080%\n",
      "Epoch [1/2], Step [1060/3732], Loss: 3.8134, accuracy: 6.0142%\n",
      "Epoch [1/2], Step [1061/3732], Loss: 3.4559, accuracy: 6.0085%\n",
      "Epoch [1/2], Step [1062/3732], Loss: 3.6814, accuracy: 6.0146%\n",
      "Epoch [1/2], Step [1063/3732], Loss: 3.4205, accuracy: 6.0207%\n",
      "Epoch [1/2], Step [1064/3732], Loss: 3.9487, accuracy: 6.0268%\n",
      "Epoch [1/2], Step [1065/3732], Loss: 3.3320, accuracy: 6.0329%\n",
      "Epoch [1/2], Step [1066/3732], Loss: 3.5885, accuracy: 6.0507%\n",
      "Epoch [1/2], Step [1067/3732], Loss: 3.6049, accuracy: 6.0450%\n",
      "Epoch [1/2], Step [1068/3732], Loss: 3.8842, accuracy: 6.0393%\n",
      "Epoch [1/2], Step [1069/3732], Loss: 3.6430, accuracy: 6.0337%\n",
      "Epoch [1/2], Step [1070/3732], Loss: 3.4450, accuracy: 6.0631%\n",
      "Epoch [1/2], Step [1071/3732], Loss: 3.8264, accuracy: 6.0808%\n",
      "Epoch [1/2], Step [1072/3732], Loss: 3.6168, accuracy: 6.0868%\n",
      "Epoch [1/2], Step [1073/3732], Loss: 3.3217, accuracy: 6.1044%\n",
      "Epoch [1/2], Step [1074/3732], Loss: 3.7371, accuracy: 6.1103%\n",
      "Epoch [1/2], Step [1075/3732], Loss: 3.3393, accuracy: 6.1163%\n",
      "Epoch [1/2], Step [1076/3732], Loss: 3.7655, accuracy: 6.1222%\n",
      "Epoch [1/2], Step [1077/3732], Loss: 3.6778, accuracy: 6.1165%\n",
      "Epoch [1/2], Step [1078/3732], Loss: 3.6754, accuracy: 6.1109%\n",
      "Epoch [1/2], Step [1079/3732], Loss: 3.8195, accuracy: 6.1052%\n",
      "Epoch [1/2], Step [1080/3732], Loss: 3.3100, accuracy: 6.1227%\n",
      "Epoch [1/2], Step [1081/3732], Loss: 3.4967, accuracy: 6.1286%\n",
      "Epoch [1/2], Step [1082/3732], Loss: 3.2166, accuracy: 6.1576%\n",
      "Epoch [1/2], Step [1083/3732], Loss: 3.4983, accuracy: 6.1519%\n",
      "Epoch [1/2], Step [1084/3732], Loss: 3.5454, accuracy: 6.1462%\n",
      "Epoch [1/2], Step [1085/3732], Loss: 4.1583, accuracy: 6.1406%\n",
      "Epoch [1/2], Step [1086/3732], Loss: 3.4524, accuracy: 6.1349%\n",
      "Epoch [1/2], Step [1087/3732], Loss: 3.3078, accuracy: 6.1408%\n",
      "Epoch [1/2], Step [1088/3732], Loss: 3.6874, accuracy: 6.1466%\n",
      "Epoch [1/2], Step [1089/3732], Loss: 3.8209, accuracy: 6.1410%\n",
      "Epoch [1/2], Step [1090/3732], Loss: 4.1154, accuracy: 6.1353%\n",
      "Epoch [1/2], Step [1091/3732], Loss: 3.8254, accuracy: 6.1412%\n",
      "Epoch [1/2], Step [1092/3732], Loss: 3.4984, accuracy: 6.1470%\n",
      "Epoch [1/2], Step [1093/3732], Loss: 3.7091, accuracy: 6.1414%\n",
      "Epoch [1/2], Step [1094/3732], Loss: 3.4889, accuracy: 6.1586%\n",
      "Epoch [1/2], Step [1095/3732], Loss: 3.6780, accuracy: 6.1530%\n",
      "Epoch [1/2], Step [1096/3732], Loss: 3.4516, accuracy: 6.1588%\n",
      "Epoch [1/2], Step [1097/3732], Loss: 3.5668, accuracy: 6.1531%\n",
      "Epoch [1/2], Step [1098/3732], Loss: 3.9092, accuracy: 6.1589%\n",
      "Epoch [1/2], Step [1099/3732], Loss: 3.2173, accuracy: 6.1647%\n",
      "Epoch [1/2], Step [1100/3732], Loss: 3.4473, accuracy: 6.1591%\n",
      "Epoch [1/2], Step [1101/3732], Loss: 3.7762, accuracy: 6.1535%\n",
      "Epoch [1/2], Step [1102/3732], Loss: 3.6266, accuracy: 6.1593%\n",
      "Epoch [1/2], Step [1103/3732], Loss: 3.2252, accuracy: 6.1763%\n",
      "Epoch [1/2], Step [1104/3732], Loss: 3.2615, accuracy: 6.1934%\n",
      "Epoch [1/2], Step [1105/3732], Loss: 4.1564, accuracy: 6.1878%\n",
      "Epoch [1/2], Step [1106/3732], Loss: 3.3169, accuracy: 6.1935%\n",
      "Epoch [1/2], Step [1107/3732], Loss: 3.8054, accuracy: 6.1992%\n",
      "Epoch [1/2], Step [1108/3732], Loss: 3.4955, accuracy: 6.2049%\n",
      "Epoch [1/2], Step [1109/3732], Loss: 3.8334, accuracy: 6.1993%\n",
      "Epoch [1/2], Step [1110/3732], Loss: 4.1139, accuracy: 6.1937%\n",
      "Epoch [1/2], Step [1111/3732], Loss: 3.7783, accuracy: 6.1881%\n",
      "Epoch [1/2], Step [1112/3732], Loss: 3.1733, accuracy: 6.2050%\n",
      "Epoch [1/2], Step [1113/3732], Loss: 3.6798, accuracy: 6.2107%\n",
      "Epoch [1/2], Step [1114/3732], Loss: 3.6677, accuracy: 6.2163%\n",
      "Epoch [1/2], Step [1115/3732], Loss: 3.2091, accuracy: 6.2220%\n",
      "Epoch [1/2], Step [1116/3732], Loss: 3.7845, accuracy: 6.2164%\n",
      "Epoch [1/2], Step [1117/3732], Loss: 3.4719, accuracy: 6.2220%\n",
      "Epoch [1/2], Step [1118/3732], Loss: 3.5696, accuracy: 6.2165%\n",
      "Epoch [1/2], Step [1119/3732], Loss: 3.7151, accuracy: 6.2332%\n",
      "Epoch [1/2], Step [1120/3732], Loss: 3.8769, accuracy: 6.2277%\n",
      "Epoch [1/2], Step [1121/3732], Loss: 3.3921, accuracy: 6.2333%\n",
      "Epoch [1/2], Step [1122/3732], Loss: 3.4793, accuracy: 6.2277%\n",
      "Epoch [1/2], Step [1123/3732], Loss: 3.5549, accuracy: 6.2222%\n",
      "Epoch [1/2], Step [1124/3732], Loss: 3.3735, accuracy: 6.2389%\n",
      "Epoch [1/2], Step [1125/3732], Loss: 3.7403, accuracy: 6.2333%\n",
      "Epoch [1/2], Step [1126/3732], Loss: 3.4409, accuracy: 6.2389%\n",
      "Epoch [1/2], Step [1127/3732], Loss: 3.5163, accuracy: 6.2555%\n",
      "Epoch [1/2], Step [1128/3732], Loss: 3.6485, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1129/3732], Loss: 3.7273, accuracy: 6.2555%\n",
      "Epoch [1/2], Step [1130/3732], Loss: 3.7216, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1131/3732], Loss: 4.1261, accuracy: 6.2445%\n",
      "Epoch [1/2], Step [1132/3732], Loss: 3.6198, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1133/3732], Loss: 3.5400, accuracy: 6.2555%\n",
      "Epoch [1/2], Step [1134/3732], Loss: 4.0698, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1135/3732], Loss: 3.3784, accuracy: 6.2445%\n",
      "Epoch [1/2], Step [1136/3732], Loss: 3.8484, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1137/3732], Loss: 3.7323, accuracy: 6.2445%\n",
      "Epoch [1/2], Step [1138/3732], Loss: 3.6015, accuracy: 6.2390%\n",
      "Epoch [1/2], Step [1139/3732], Loss: 3.5984, accuracy: 6.2335%\n",
      "Epoch [1/2], Step [1140/3732], Loss: 3.9149, accuracy: 6.2281%\n",
      "Epoch [1/2], Step [1141/3732], Loss: 3.3925, accuracy: 6.2336%\n",
      "Epoch [1/2], Step [1142/3732], Loss: 3.3003, accuracy: 6.2391%\n",
      "Epoch [1/2], Step [1143/3732], Loss: 3.2287, accuracy: 6.2336%\n",
      "Epoch [1/2], Step [1144/3732], Loss: 3.5703, accuracy: 6.2281%\n",
      "Epoch [1/2], Step [1145/3732], Loss: 4.0070, accuracy: 6.2227%\n",
      "Epoch [1/2], Step [1146/3732], Loss: 3.7174, accuracy: 6.2173%\n",
      "Epoch [1/2], Step [1147/3732], Loss: 3.3174, accuracy: 6.2337%\n",
      "Epoch [1/2], Step [1148/3732], Loss: 3.4144, accuracy: 6.2391%\n",
      "Epoch [1/2], Step [1149/3732], Loss: 3.5364, accuracy: 6.2337%\n",
      "Epoch [1/2], Step [1150/3732], Loss: 3.4721, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1151/3732], Loss: 3.6509, accuracy: 6.2446%\n",
      "Epoch [1/2], Step [1152/3732], Loss: 3.8470, accuracy: 6.2391%\n",
      "Epoch [1/2], Step [1153/3732], Loss: 3.6961, accuracy: 6.2337%\n",
      "Epoch [1/2], Step [1154/3732], Loss: 3.7718, accuracy: 6.2392%\n",
      "Epoch [1/2], Step [1155/3732], Loss: 3.5413, accuracy: 6.2554%\n",
      "Epoch [1/2], Step [1156/3732], Loss: 3.7810, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1157/3732], Loss: 4.1733, accuracy: 6.2446%\n",
      "Epoch [1/2], Step [1158/3732], Loss: 4.1033, accuracy: 6.2392%\n",
      "Epoch [1/2], Step [1159/3732], Loss: 3.8958, accuracy: 6.2446%\n",
      "Epoch [1/2], Step [1160/3732], Loss: 3.6456, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1161/3732], Loss: 3.5949, accuracy: 6.2554%\n",
      "Epoch [1/2], Step [1162/3732], Loss: 3.5688, accuracy: 6.2608%\n",
      "Epoch [1/2], Step [1163/3732], Loss: 3.6284, accuracy: 6.2661%\n",
      "Epoch [1/2], Step [1164/3732], Loss: 3.8181, accuracy: 6.2607%\n",
      "Epoch [1/2], Step [1165/3732], Loss: 3.5320, accuracy: 6.2661%\n",
      "Epoch [1/2], Step [1166/3732], Loss: 3.5206, accuracy: 6.2607%\n",
      "Epoch [1/2], Step [1167/3732], Loss: 3.7925, accuracy: 6.2661%\n",
      "Epoch [1/2], Step [1168/3732], Loss: 3.9659, accuracy: 6.2607%\n",
      "Epoch [1/2], Step [1169/3732], Loss: 3.5232, accuracy: 6.2553%\n",
      "Epoch [1/2], Step [1170/3732], Loss: 3.8747, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1171/3732], Loss: 3.5963, accuracy: 6.2447%\n",
      "Epoch [1/2], Step [1172/3732], Loss: 3.7186, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1173/3732], Loss: 3.9311, accuracy: 6.2447%\n",
      "Epoch [1/2], Step [1174/3732], Loss: 3.7485, accuracy: 6.2394%\n",
      "Epoch [1/2], Step [1175/3732], Loss: 3.4131, accuracy: 6.2340%\n",
      "Epoch [1/2], Step [1176/3732], Loss: 4.0195, accuracy: 6.2287%\n",
      "Epoch [1/2], Step [1177/3732], Loss: 3.5895, accuracy: 6.2234%\n",
      "Epoch [1/2], Step [1178/3732], Loss: 3.6628, accuracy: 6.2182%\n",
      "Epoch [1/2], Step [1179/3732], Loss: 3.1565, accuracy: 6.2341%\n",
      "Epoch [1/2], Step [1180/3732], Loss: 3.5830, accuracy: 6.2288%\n",
      "Epoch [1/2], Step [1181/3732], Loss: 3.3104, accuracy: 6.2447%\n",
      "Epoch [1/2], Step [1182/3732], Loss: 3.7678, accuracy: 6.2394%\n",
      "Epoch [1/2], Step [1183/3732], Loss: 3.7727, accuracy: 6.2447%\n",
      "Epoch [1/2], Step [1184/3732], Loss: 3.5793, accuracy: 6.2500%\n",
      "Epoch [1/2], Step [1185/3732], Loss: 3.3763, accuracy: 6.2447%\n",
      "Epoch [1/2], Step [1186/3732], Loss: 3.7708, accuracy: 6.2395%\n",
      "Epoch [1/2], Step [1187/3732], Loss: 3.5297, accuracy: 6.2553%\n",
      "Epoch [1/2], Step [1188/3732], Loss: 3.4400, accuracy: 6.2605%\n",
      "Epoch [1/2], Step [1189/3732], Loss: 3.7894, accuracy: 6.2658%\n",
      "Epoch [1/2], Step [1190/3732], Loss: 3.6523, accuracy: 6.2605%\n",
      "Epoch [1/2], Step [1191/3732], Loss: 3.5520, accuracy: 6.2762%\n",
      "Epoch [1/2], Step [1192/3732], Loss: 3.6283, accuracy: 6.2919%\n",
      "Epoch [1/2], Step [1193/3732], Loss: 3.5701, accuracy: 6.2867%\n",
      "Epoch [1/2], Step [1194/3732], Loss: 3.8813, accuracy: 6.2814%\n",
      "Epoch [1/2], Step [1195/3732], Loss: 3.3422, accuracy: 6.2971%\n",
      "Epoch [1/2], Step [1196/3732], Loss: 3.3785, accuracy: 6.3023%\n",
      "Epoch [1/2], Step [1197/3732], Loss: 3.9237, accuracy: 6.2970%\n",
      "Epoch [1/2], Step [1198/3732], Loss: 3.8690, accuracy: 6.2917%\n",
      "Epoch [1/2], Step [1199/3732], Loss: 3.5597, accuracy: 6.2865%\n",
      "Epoch [1/2], Step [1200/3732], Loss: 3.5932, accuracy: 6.2812%\n",
      "Epoch [1/2], Step [1201/3732], Loss: 3.3125, accuracy: 6.2760%\n",
      "Epoch [1/2], Step [1202/3732], Loss: 3.5717, accuracy: 6.2708%\n",
      "Epoch [1/2], Step [1203/3732], Loss: 3.5060, accuracy: 6.2760%\n",
      "Epoch [1/2], Step [1204/3732], Loss: 3.4564, accuracy: 6.2811%\n",
      "Epoch [1/2], Step [1205/3732], Loss: 3.5791, accuracy: 6.2967%\n",
      "Epoch [1/2], Step [1206/3732], Loss: 3.1920, accuracy: 6.3226%\n",
      "Epoch [1/2], Step [1207/3732], Loss: 3.8764, accuracy: 6.3173%\n",
      "Epoch [1/2], Step [1208/3732], Loss: 3.2158, accuracy: 6.3224%\n",
      "Epoch [1/2], Step [1209/3732], Loss: 3.7674, accuracy: 6.3172%\n",
      "Epoch [1/2], Step [1210/3732], Loss: 3.0568, accuracy: 6.3223%\n",
      "Epoch [1/2], Step [1211/3732], Loss: 3.4951, accuracy: 6.3171%\n",
      "Epoch [1/2], Step [1212/3732], Loss: 4.0375, accuracy: 6.3119%\n",
      "Epoch [1/2], Step [1213/3732], Loss: 4.0397, accuracy: 6.3067%\n",
      "Epoch [1/2], Step [1214/3732], Loss: 3.9704, accuracy: 6.3015%\n",
      "Epoch [1/2], Step [1215/3732], Loss: 3.6165, accuracy: 6.2963%\n",
      "Epoch [1/2], Step [1216/3732], Loss: 3.6131, accuracy: 6.3014%\n",
      "Epoch [1/2], Step [1217/3732], Loss: 3.8139, accuracy: 6.3168%\n",
      "Epoch [1/2], Step [1218/3732], Loss: 3.7812, accuracy: 6.3218%\n",
      "Epoch [1/2], Step [1219/3732], Loss: 3.3939, accuracy: 6.3269%\n",
      "Epoch [1/2], Step [1220/3732], Loss: 3.7499, accuracy: 6.3217%\n",
      "Epoch [1/2], Step [1221/3732], Loss: 3.4307, accuracy: 6.3473%\n",
      "Epoch [1/2], Step [1222/3732], Loss: 3.6299, accuracy: 6.3421%\n",
      "Epoch [1/2], Step [1223/3732], Loss: 3.4834, accuracy: 6.3369%\n",
      "Epoch [1/2], Step [1224/3732], Loss: 4.2866, accuracy: 6.3419%\n",
      "Epoch [1/2], Step [1225/3732], Loss: 3.7130, accuracy: 6.3469%\n",
      "Epoch [1/2], Step [1226/3732], Loss: 3.6488, accuracy: 6.3418%\n",
      "Epoch [1/2], Step [1227/3732], Loss: 3.2713, accuracy: 6.3468%\n",
      "Epoch [1/2], Step [1228/3732], Loss: 3.4499, accuracy: 6.3721%\n",
      "Epoch [1/2], Step [1229/3732], Loss: 3.7638, accuracy: 6.3670%\n",
      "Epoch [1/2], Step [1230/3732], Loss: 3.8733, accuracy: 6.3618%\n",
      "Epoch [1/2], Step [1231/3732], Loss: 3.5717, accuracy: 6.3668%\n",
      "Epoch [1/2], Step [1232/3732], Loss: 3.5518, accuracy: 6.3616%\n",
      "Epoch [1/2], Step [1233/3732], Loss: 3.8483, accuracy: 6.3564%\n",
      "Epoch [1/2], Step [1234/3732], Loss: 3.7849, accuracy: 6.3614%\n",
      "Epoch [1/2], Step [1235/3732], Loss: 3.4998, accuracy: 6.3664%\n",
      "Epoch [1/2], Step [1236/3732], Loss: 3.3944, accuracy: 6.3815%\n",
      "Epoch [1/2], Step [1237/3732], Loss: 3.4411, accuracy: 6.3763%\n",
      "Epoch [1/2], Step [1238/3732], Loss: 3.5202, accuracy: 6.3813%\n",
      "Epoch [1/2], Step [1239/3732], Loss: 3.4094, accuracy: 6.3761%\n",
      "Epoch [1/2], Step [1240/3732], Loss: 3.5036, accuracy: 6.3710%\n",
      "Epoch [1/2], Step [1241/3732], Loss: 3.5372, accuracy: 6.3759%\n",
      "Epoch [1/2], Step [1242/3732], Loss: 3.6755, accuracy: 6.3808%\n",
      "Epoch [1/2], Step [1243/3732], Loss: 3.3628, accuracy: 6.3858%\n",
      "Epoch [1/2], Step [1244/3732], Loss: 3.2702, accuracy: 6.3907%\n",
      "Epoch [1/2], Step [1245/3732], Loss: 3.8040, accuracy: 6.3855%\n",
      "Epoch [1/2], Step [1246/3732], Loss: 3.4482, accuracy: 6.3904%\n",
      "Epoch [1/2], Step [1247/3732], Loss: 3.8313, accuracy: 6.3853%\n",
      "Epoch [1/2], Step [1248/3732], Loss: 3.8230, accuracy: 6.3802%\n",
      "Epoch [1/2], Step [1249/3732], Loss: 3.8891, accuracy: 6.3751%\n",
      "Epoch [1/2], Step [1250/3732], Loss: 3.6516, accuracy: 6.3800%\n",
      "Epoch [1/2], Step [1251/3732], Loss: 3.6017, accuracy: 6.3849%\n",
      "Epoch [1/2], Step [1252/3732], Loss: 3.5427, accuracy: 6.3898%\n",
      "Epoch [1/2], Step [1253/3732], Loss: 3.3994, accuracy: 6.4046%\n",
      "Epoch [1/2], Step [1254/3732], Loss: 3.7701, accuracy: 6.3995%\n",
      "Epoch [1/2], Step [1255/3732], Loss: 3.4370, accuracy: 6.3944%\n",
      "Epoch [1/2], Step [1256/3732], Loss: 3.3210, accuracy: 6.3993%\n",
      "Epoch [1/2], Step [1257/3732], Loss: 3.5754, accuracy: 6.4041%\n",
      "Epoch [1/2], Step [1258/3732], Loss: 3.6607, accuracy: 6.4090%\n",
      "Epoch [1/2], Step [1259/3732], Loss: 3.5240, accuracy: 6.4039%\n",
      "Epoch [1/2], Step [1260/3732], Loss: 3.0562, accuracy: 6.4187%\n",
      "Epoch [1/2], Step [1261/3732], Loss: 3.8100, accuracy: 6.4136%\n",
      "Epoch [1/2], Step [1262/3732], Loss: 3.7850, accuracy: 6.4184%\n",
      "Epoch [1/2], Step [1263/3732], Loss: 4.0846, accuracy: 6.4133%\n",
      "Epoch [1/2], Step [1264/3732], Loss: 3.4748, accuracy: 6.4181%\n",
      "Epoch [1/2], Step [1265/3732], Loss: 3.0699, accuracy: 6.4328%\n",
      "Epoch [1/2], Step [1266/3732], Loss: 3.9825, accuracy: 6.4277%\n",
      "Epoch [1/2], Step [1267/3732], Loss: 3.8191, accuracy: 6.4227%\n",
      "Epoch [1/2], Step [1268/3732], Loss: 3.5551, accuracy: 6.4176%\n",
      "Epoch [1/2], Step [1269/3732], Loss: 3.7361, accuracy: 6.4224%\n",
      "Epoch [1/2], Step [1270/3732], Loss: 3.7977, accuracy: 6.4173%\n",
      "Epoch [1/2], Step [1271/3732], Loss: 3.5133, accuracy: 6.4123%\n",
      "Epoch [1/2], Step [1272/3732], Loss: 3.8844, accuracy: 6.4072%\n",
      "Epoch [1/2], Step [1273/3732], Loss: 3.9342, accuracy: 6.4022%\n",
      "Epoch [1/2], Step [1274/3732], Loss: 3.6452, accuracy: 6.3972%\n",
      "Epoch [1/2], Step [1275/3732], Loss: 3.4367, accuracy: 6.4020%\n",
      "Epoch [1/2], Step [1276/3732], Loss: 3.8452, accuracy: 6.3969%\n",
      "Epoch [1/2], Step [1277/3732], Loss: 3.7663, accuracy: 6.4017%\n",
      "Epoch [1/2], Step [1278/3732], Loss: 3.2947, accuracy: 6.4065%\n",
      "Epoch [1/2], Step [1279/3732], Loss: 3.3699, accuracy: 6.4210%\n",
      "Epoch [1/2], Step [1280/3732], Loss: 3.7208, accuracy: 6.4258%\n",
      "Epoch [1/2], Step [1281/3732], Loss: 3.6321, accuracy: 6.4305%\n",
      "Epoch [1/2], Step [1282/3732], Loss: 3.7465, accuracy: 6.4255%\n",
      "Epoch [1/2], Step [1283/3732], Loss: 3.6915, accuracy: 6.4400%\n",
      "Epoch [1/2], Step [1284/3732], Loss: 3.6314, accuracy: 6.4350%\n",
      "Epoch [1/2], Step [1285/3732], Loss: 3.8989, accuracy: 6.4300%\n",
      "Epoch [1/2], Step [1286/3732], Loss: 3.2991, accuracy: 6.4250%\n",
      "Epoch [1/2], Step [1287/3732], Loss: 3.4359, accuracy: 6.4200%\n",
      "Epoch [1/2], Step [1288/3732], Loss: 3.4410, accuracy: 6.4247%\n",
      "Epoch [1/2], Step [1289/3732], Loss: 3.1921, accuracy: 6.4294%\n",
      "Epoch [1/2], Step [1290/3732], Loss: 3.6299, accuracy: 6.4341%\n",
      "Epoch [1/2], Step [1291/3732], Loss: 3.6234, accuracy: 6.4291%\n",
      "Epoch [1/2], Step [1292/3732], Loss: 3.2894, accuracy: 6.4435%\n",
      "Epoch [1/2], Step [1293/3732], Loss: 3.7431, accuracy: 6.4385%\n",
      "Epoch [1/2], Step [1294/3732], Loss: 3.9399, accuracy: 6.4335%\n",
      "Epoch [1/2], Step [1295/3732], Loss: 3.1916, accuracy: 6.4382%\n",
      "Epoch [1/2], Step [1296/3732], Loss: 3.5655, accuracy: 6.4333%\n",
      "Epoch [1/2], Step [1297/3732], Loss: 3.4950, accuracy: 6.4283%\n",
      "Epoch [1/2], Step [1298/3732], Loss: 3.0654, accuracy: 6.4330%\n",
      "Epoch [1/2], Step [1299/3732], Loss: 4.0838, accuracy: 6.4473%\n",
      "Epoch [1/2], Step [1300/3732], Loss: 3.5727, accuracy: 6.4519%\n",
      "Epoch [1/2], Step [1301/3732], Loss: 3.9647, accuracy: 6.4566%\n",
      "Epoch [1/2], Step [1302/3732], Loss: 3.6317, accuracy: 6.4708%\n",
      "Epoch [1/2], Step [1303/3732], Loss: 3.6020, accuracy: 6.4754%\n",
      "Epoch [1/2], Step [1304/3732], Loss: 3.5305, accuracy: 6.4801%\n",
      "Epoch [1/2], Step [1305/3732], Loss: 3.4707, accuracy: 6.4847%\n",
      "Epoch [1/2], Step [1306/3732], Loss: 3.6008, accuracy: 6.4797%\n",
      "Epoch [1/2], Step [1307/3732], Loss: 4.0312, accuracy: 6.4843%\n",
      "Epoch [1/2], Step [1308/3732], Loss: 3.7217, accuracy: 6.4794%\n",
      "Epoch [1/2], Step [1309/3732], Loss: 3.7746, accuracy: 6.4840%\n",
      "Epoch [1/2], Step [1310/3732], Loss: 3.3376, accuracy: 6.5076%\n",
      "Epoch [1/2], Step [1311/3732], Loss: 3.9616, accuracy: 6.5027%\n",
      "Epoch [1/2], Step [1312/3732], Loss: 3.5542, accuracy: 6.5072%\n",
      "Epoch [1/2], Step [1313/3732], Loss: 3.4356, accuracy: 6.5023%\n",
      "Epoch [1/2], Step [1314/3732], Loss: 4.1918, accuracy: 6.5068%\n",
      "Epoch [1/2], Step [1315/3732], Loss: 3.6855, accuracy: 6.5114%\n",
      "Epoch [1/2], Step [1316/3732], Loss: 3.5585, accuracy: 6.5160%\n",
      "Epoch [1/2], Step [1317/3732], Loss: 4.0483, accuracy: 6.5110%\n",
      "Epoch [1/2], Step [1318/3732], Loss: 3.2448, accuracy: 6.5250%\n",
      "Epoch [1/2], Step [1319/3732], Loss: 3.4541, accuracy: 6.5296%\n",
      "Epoch [1/2], Step [1320/3732], Loss: 3.5055, accuracy: 6.5341%\n",
      "Epoch [1/2], Step [1321/3732], Loss: 3.4528, accuracy: 6.5481%\n",
      "Epoch [1/2], Step [1322/3732], Loss: 3.9545, accuracy: 6.5431%\n",
      "Epoch [1/2], Step [1323/3732], Loss: 4.0309, accuracy: 6.5476%\n",
      "Epoch [1/2], Step [1324/3732], Loss: 3.4332, accuracy: 6.5521%\n",
      "Epoch [1/2], Step [1325/3732], Loss: 3.8877, accuracy: 6.5472%\n",
      "Epoch [1/2], Step [1326/3732], Loss: 3.4845, accuracy: 6.5517%\n",
      "Epoch [1/2], Step [1327/3732], Loss: 3.8898, accuracy: 6.5561%\n",
      "Epoch [1/2], Step [1328/3732], Loss: 3.8370, accuracy: 6.5512%\n",
      "Epoch [1/2], Step [1329/3732], Loss: 3.6799, accuracy: 6.5463%\n",
      "Epoch [1/2], Step [1330/3732], Loss: 3.6021, accuracy: 6.5414%\n",
      "Epoch [1/2], Step [1331/3732], Loss: 3.4907, accuracy: 6.5364%\n",
      "Epoch [1/2], Step [1332/3732], Loss: 3.6412, accuracy: 6.5315%\n",
      "Epoch [1/2], Step [1333/3732], Loss: 3.3528, accuracy: 6.5266%\n",
      "Epoch [1/2], Step [1334/3732], Loss: 3.8425, accuracy: 6.5217%\n",
      "Epoch [1/2], Step [1335/3732], Loss: 3.6126, accuracy: 6.5169%\n",
      "Epoch [1/2], Step [1336/3732], Loss: 3.4041, accuracy: 6.5213%\n",
      "Epoch [1/2], Step [1337/3732], Loss: 3.4583, accuracy: 6.5539%\n",
      "Epoch [1/2], Step [1338/3732], Loss: 3.6001, accuracy: 6.5583%\n",
      "Epoch [1/2], Step [1339/3732], Loss: 3.3624, accuracy: 6.5721%\n",
      "Epoch [1/2], Step [1340/3732], Loss: 3.6221, accuracy: 6.5672%\n",
      "Epoch [1/2], Step [1341/3732], Loss: 3.6357, accuracy: 6.5716%\n",
      "Epoch [1/2], Step [1342/3732], Loss: 4.4327, accuracy: 6.5667%\n",
      "Epoch [1/2], Step [1343/3732], Loss: 3.1491, accuracy: 6.5711%\n",
      "Epoch [1/2], Step [1344/3732], Loss: 3.4805, accuracy: 6.5848%\n",
      "Epoch [1/2], Step [1345/3732], Loss: 3.6597, accuracy: 6.5799%\n",
      "Epoch [1/2], Step [1346/3732], Loss: 3.6024, accuracy: 6.5843%\n",
      "Epoch [1/2], Step [1347/3732], Loss: 3.3155, accuracy: 6.5794%\n",
      "Epoch [1/2], Step [1348/3732], Loss: 4.0777, accuracy: 6.5746%\n",
      "Epoch [1/2], Step [1349/3732], Loss: 3.6958, accuracy: 6.5697%\n",
      "Epoch [1/2], Step [1350/3732], Loss: 3.3354, accuracy: 6.5741%\n",
      "Epoch [1/2], Step [1351/3732], Loss: 3.9757, accuracy: 6.5692%\n",
      "Epoch [1/2], Step [1352/3732], Loss: 3.8663, accuracy: 6.5643%\n",
      "Epoch [1/2], Step [1353/3732], Loss: 3.7837, accuracy: 6.5595%\n",
      "Epoch [1/2], Step [1354/3732], Loss: 3.6406, accuracy: 6.5639%\n",
      "Epoch [1/2], Step [1355/3732], Loss: 3.5293, accuracy: 6.5590%\n",
      "Epoch [1/2], Step [1356/3732], Loss: 3.4088, accuracy: 6.5819%\n",
      "Epoch [1/2], Step [1357/3732], Loss: 2.9753, accuracy: 6.6046%\n",
      "Epoch [1/2], Step [1358/3732], Loss: 3.3814, accuracy: 6.6090%\n",
      "Epoch [1/2], Step [1359/3732], Loss: 3.5543, accuracy: 6.6225%\n",
      "Epoch [1/2], Step [1360/3732], Loss: 3.6869, accuracy: 6.6176%\n",
      "Epoch [1/2], Step [1361/3732], Loss: 3.3351, accuracy: 6.6220%\n",
      "Epoch [1/2], Step [1362/3732], Loss: 3.8188, accuracy: 6.6263%\n",
      "Epoch [1/2], Step [1363/3732], Loss: 3.8267, accuracy: 6.6214%\n",
      "Epoch [1/2], Step [1364/3732], Loss: 3.7696, accuracy: 6.6166%\n",
      "Epoch [1/2], Step [1365/3732], Loss: 3.4312, accuracy: 6.6209%\n",
      "Epoch [1/2], Step [1366/3732], Loss: 3.6393, accuracy: 6.6160%\n",
      "Epoch [1/2], Step [1367/3732], Loss: 3.5959, accuracy: 6.6203%\n",
      "Epoch [1/2], Step [1368/3732], Loss: 3.5966, accuracy: 6.6246%\n",
      "Epoch [1/2], Step [1369/3732], Loss: 3.6293, accuracy: 6.6289%\n",
      "Epoch [1/2], Step [1370/3732], Loss: 3.7903, accuracy: 6.6241%\n",
      "Epoch [1/2], Step [1371/3732], Loss: 3.4227, accuracy: 6.6284%\n",
      "Epoch [1/2], Step [1372/3732], Loss: 3.5138, accuracy: 6.6235%\n",
      "Epoch [1/2], Step [1373/3732], Loss: 3.6864, accuracy: 6.6187%\n",
      "Epoch [1/2], Step [1374/3732], Loss: 3.9672, accuracy: 6.6139%\n",
      "Epoch [1/2], Step [1375/3732], Loss: 3.5134, accuracy: 6.6091%\n",
      "Epoch [1/2], Step [1376/3732], Loss: 3.5194, accuracy: 6.6134%\n",
      "Epoch [1/2], Step [1377/3732], Loss: 3.3929, accuracy: 6.6176%\n",
      "Epoch [1/2], Step [1378/3732], Loss: 3.8454, accuracy: 6.6128%\n",
      "Epoch [1/2], Step [1379/3732], Loss: 3.9679, accuracy: 6.6080%\n",
      "Epoch [1/2], Step [1380/3732], Loss: 3.4914, accuracy: 6.6123%\n",
      "Epoch [1/2], Step [1381/3732], Loss: 3.7839, accuracy: 6.6075%\n",
      "Epoch [1/2], Step [1382/3732], Loss: 3.7758, accuracy: 6.6027%\n",
      "Epoch [1/2], Step [1383/3732], Loss: 3.3290, accuracy: 6.6161%\n",
      "Epoch [1/2], Step [1384/3732], Loss: 3.7564, accuracy: 6.6113%\n",
      "Epoch [1/2], Step [1385/3732], Loss: 3.3045, accuracy: 6.6155%\n",
      "Epoch [1/2], Step [1386/3732], Loss: 3.9196, accuracy: 6.6108%\n",
      "Epoch [1/2], Step [1387/3732], Loss: 3.5674, accuracy: 6.6150%\n",
      "Epoch [1/2], Step [1388/3732], Loss: 3.9252, accuracy: 6.6102%\n",
      "Epoch [1/2], Step [1389/3732], Loss: 3.8811, accuracy: 6.6055%\n",
      "Epoch [1/2], Step [1390/3732], Loss: 4.1969, accuracy: 6.6007%\n",
      "Epoch [1/2], Step [1391/3732], Loss: 3.5006, accuracy: 6.6050%\n",
      "Epoch [1/2], Step [1392/3732], Loss: 3.4829, accuracy: 6.6092%\n",
      "Epoch [1/2], Step [1393/3732], Loss: 3.9129, accuracy: 6.6045%\n",
      "Epoch [1/2], Step [1394/3732], Loss: 3.5847, accuracy: 6.5997%\n",
      "Epoch [1/2], Step [1395/3732], Loss: 3.5355, accuracy: 6.6039%\n",
      "Epoch [1/2], Step [1396/3732], Loss: 3.9144, accuracy: 6.5992%\n",
      "Epoch [1/2], Step [1397/3732], Loss: 3.5671, accuracy: 6.6034%\n",
      "Epoch [1/2], Step [1398/3732], Loss: 3.2250, accuracy: 6.6077%\n",
      "Epoch [1/2], Step [1399/3732], Loss: 3.7625, accuracy: 6.6119%\n",
      "Epoch [1/2], Step [1400/3732], Loss: 3.7895, accuracy: 6.6071%\n",
      "Epoch [1/2], Step [1401/3732], Loss: 3.8843, accuracy: 6.6024%\n",
      "Epoch [1/2], Step [1402/3732], Loss: 3.5018, accuracy: 6.6066%\n",
      "Epoch [1/2], Step [1403/3732], Loss: 3.4891, accuracy: 6.6108%\n",
      "Epoch [1/2], Step [1404/3732], Loss: 3.5302, accuracy: 6.6239%\n",
      "Epoch [1/2], Step [1405/3732], Loss: 3.4778, accuracy: 6.6281%\n",
      "Epoch [1/2], Step [1406/3732], Loss: 3.7894, accuracy: 6.6234%\n",
      "Epoch [1/2], Step [1407/3732], Loss: 3.5471, accuracy: 6.6365%\n",
      "Epoch [1/2], Step [1408/3732], Loss: 3.8674, accuracy: 6.6406%\n",
      "Epoch [1/2], Step [1409/3732], Loss: 3.6587, accuracy: 6.6448%\n",
      "Epoch [1/2], Step [1410/3732], Loss: 3.3839, accuracy: 6.6578%\n",
      "Epoch [1/2], Step [1411/3732], Loss: 3.7317, accuracy: 6.6531%\n",
      "Epoch [1/2], Step [1412/3732], Loss: 3.5799, accuracy: 6.6484%\n",
      "Epoch [1/2], Step [1413/3732], Loss: 3.9187, accuracy: 6.6437%\n",
      "Epoch [1/2], Step [1414/3732], Loss: 3.6469, accuracy: 6.6390%\n",
      "Epoch [1/2], Step [1415/3732], Loss: 3.8554, accuracy: 6.6343%\n",
      "Epoch [1/2], Step [1416/3732], Loss: 3.1900, accuracy: 6.6384%\n",
      "Epoch [1/2], Step [1417/3732], Loss: 3.3681, accuracy: 6.6602%\n",
      "Epoch [1/2], Step [1418/3732], Loss: 3.6846, accuracy: 6.6555%\n",
      "Epoch [1/2], Step [1419/3732], Loss: 3.4156, accuracy: 6.6508%\n",
      "Epoch [1/2], Step [1420/3732], Loss: 3.5352, accuracy: 6.6461%\n",
      "Epoch [1/2], Step [1421/3732], Loss: 3.5977, accuracy: 6.6414%\n",
      "Epoch [1/2], Step [1422/3732], Loss: 4.1111, accuracy: 6.6368%\n",
      "Epoch [1/2], Step [1423/3732], Loss: 3.0042, accuracy: 6.6585%\n",
      "Epoch [1/2], Step [1424/3732], Loss: 3.4616, accuracy: 6.6626%\n",
      "Epoch [1/2], Step [1425/3732], Loss: 4.4246, accuracy: 6.6579%\n",
      "Epoch [1/2], Step [1426/3732], Loss: 3.0828, accuracy: 6.6708%\n",
      "Epoch [1/2], Step [1427/3732], Loss: 3.2434, accuracy: 6.6748%\n",
      "Epoch [1/2], Step [1428/3732], Loss: 3.7174, accuracy: 6.6702%\n",
      "Epoch [1/2], Step [1429/3732], Loss: 3.8149, accuracy: 6.6655%\n",
      "Epoch [1/2], Step [1430/3732], Loss: 3.6193, accuracy: 6.6608%\n",
      "Epoch [1/2], Step [1431/3732], Loss: 3.4242, accuracy: 6.6649%\n",
      "Epoch [1/2], Step [1432/3732], Loss: 3.4802, accuracy: 6.6777%\n",
      "Epoch [1/2], Step [1433/3732], Loss: 3.7501, accuracy: 6.6731%\n",
      "Epoch [1/2], Step [1434/3732], Loss: 4.1613, accuracy: 6.6684%\n",
      "Epoch [1/2], Step [1435/3732], Loss: 3.9735, accuracy: 6.6638%\n",
      "Epoch [1/2], Step [1436/3732], Loss: 3.4387, accuracy: 6.6591%\n",
      "Epoch [1/2], Step [1437/3732], Loss: 2.9889, accuracy: 6.6806%\n",
      "Epoch [1/2], Step [1438/3732], Loss: 3.4028, accuracy: 6.6759%\n",
      "Epoch [1/2], Step [1439/3732], Loss: 3.3479, accuracy: 6.6713%\n",
      "Epoch [1/2], Step [1440/3732], Loss: 3.8058, accuracy: 6.6667%\n",
      "Epoch [1/2], Step [1441/3732], Loss: 3.8056, accuracy: 6.6707%\n",
      "Epoch [1/2], Step [1442/3732], Loss: 3.5816, accuracy: 6.6661%\n",
      "Epoch [1/2], Step [1443/3732], Loss: 3.5896, accuracy: 6.6701%\n",
      "Epoch [1/2], Step [1444/3732], Loss: 3.7811, accuracy: 6.6655%\n",
      "Epoch [1/2], Step [1445/3732], Loss: 3.3810, accuracy: 6.6696%\n",
      "Epoch [1/2], Step [1446/3732], Loss: 3.7372, accuracy: 6.6736%\n",
      "Epoch [1/2], Step [1447/3732], Loss: 3.5060, accuracy: 6.6776%\n",
      "Epoch [1/2], Step [1448/3732], Loss: 3.9660, accuracy: 6.6730%\n",
      "Epoch [1/2], Step [1449/3732], Loss: 3.6107, accuracy: 6.6684%\n",
      "Epoch [1/2], Step [1450/3732], Loss: 3.7631, accuracy: 6.6724%\n",
      "Epoch [1/2], Step [1451/3732], Loss: 3.7811, accuracy: 6.6764%\n",
      "Epoch [1/2], Step [1452/3732], Loss: 3.6119, accuracy: 6.6804%\n",
      "Epoch [1/2], Step [1453/3732], Loss: 4.1162, accuracy: 6.6758%\n",
      "Epoch [1/2], Step [1454/3732], Loss: 3.7582, accuracy: 6.6713%\n",
      "Epoch [1/2], Step [1455/3732], Loss: 3.5411, accuracy: 6.6753%\n",
      "Epoch [1/2], Step [1456/3732], Loss: 3.1211, accuracy: 6.6964%\n",
      "Epoch [1/2], Step [1457/3732], Loss: 3.6182, accuracy: 6.6918%\n",
      "Epoch [1/2], Step [1458/3732], Loss: 3.3418, accuracy: 6.7044%\n",
      "Epoch [1/2], Step [1459/3732], Loss: 3.3510, accuracy: 6.7084%\n",
      "Epoch [1/2], Step [1460/3732], Loss: 3.7458, accuracy: 6.7123%\n",
      "Epoch [1/2], Step [1461/3732], Loss: 3.3936, accuracy: 6.7163%\n",
      "Epoch [1/2], Step [1462/3732], Loss: 3.8924, accuracy: 6.7202%\n",
      "Epoch [1/2], Step [1463/3732], Loss: 4.0703, accuracy: 6.7157%\n",
      "Epoch [1/2], Step [1464/3732], Loss: 3.4941, accuracy: 6.7281%\n",
      "Epoch [1/2], Step [1465/3732], Loss: 4.0836, accuracy: 6.7321%\n",
      "Epoch [1/2], Step [1466/3732], Loss: 3.5418, accuracy: 6.7275%\n",
      "Epoch [1/2], Step [1467/3732], Loss: 3.8599, accuracy: 6.7229%\n",
      "Epoch [1/2], Step [1468/3732], Loss: 3.8076, accuracy: 6.7268%\n",
      "Epoch [1/2], Step [1469/3732], Loss: 3.3294, accuracy: 6.7393%\n",
      "Epoch [1/2], Step [1470/3732], Loss: 3.4602, accuracy: 6.7517%\n",
      "Epoch [1/2], Step [1471/3732], Loss: 3.5238, accuracy: 6.7556%\n",
      "Epoch [1/2], Step [1472/3732], Loss: 3.8270, accuracy: 6.7510%\n",
      "Epoch [1/2], Step [1473/3732], Loss: 3.7700, accuracy: 6.7464%\n",
      "Epoch [1/2], Step [1474/3732], Loss: 3.6280, accuracy: 6.7419%\n",
      "Epoch [1/2], Step [1475/3732], Loss: 3.3873, accuracy: 6.7458%\n",
      "Epoch [1/2], Step [1476/3732], Loss: 3.3595, accuracy: 6.7581%\n",
      "Epoch [1/2], Step [1477/3732], Loss: 3.6671, accuracy: 6.7620%\n",
      "Epoch [1/2], Step [1478/3732], Loss: 4.3275, accuracy: 6.7574%\n",
      "Epoch [1/2], Step [1479/3732], Loss: 3.8140, accuracy: 6.7529%\n",
      "Epoch [1/2], Step [1480/3732], Loss: 3.4634, accuracy: 6.7483%\n",
      "Epoch [1/2], Step [1481/3732], Loss: 3.8933, accuracy: 6.7438%\n",
      "Epoch [1/2], Step [1482/3732], Loss: 3.6162, accuracy: 6.7476%\n",
      "Epoch [1/2], Step [1483/3732], Loss: 3.6775, accuracy: 6.7515%\n",
      "Epoch [1/2], Step [1484/3732], Loss: 3.7397, accuracy: 6.7554%\n",
      "Epoch [1/2], Step [1485/3732], Loss: 3.7523, accuracy: 6.7508%\n",
      "Epoch [1/2], Step [1486/3732], Loss: 3.5520, accuracy: 6.7463%\n",
      "Epoch [1/2], Step [1487/3732], Loss: 3.7622, accuracy: 6.7418%\n",
      "Epoch [1/2], Step [1488/3732], Loss: 3.4025, accuracy: 6.7456%\n",
      "Epoch [1/2], Step [1489/3732], Loss: 3.6918, accuracy: 6.7495%\n",
      "Epoch [1/2], Step [1490/3732], Loss: 3.2532, accuracy: 6.7450%\n",
      "Epoch [1/2], Step [1491/3732], Loss: 3.3025, accuracy: 6.7572%\n",
      "Epoch [1/2], Step [1492/3732], Loss: 3.7232, accuracy: 6.7694%\n",
      "Epoch [1/2], Step [1493/3732], Loss: 3.7690, accuracy: 6.7649%\n",
      "Epoch [1/2], Step [1494/3732], Loss: 3.6605, accuracy: 6.7687%\n",
      "Epoch [1/2], Step [1495/3732], Loss: 3.7635, accuracy: 6.7642%\n",
      "Epoch [1/2], Step [1496/3732], Loss: 3.7366, accuracy: 6.7680%\n",
      "Epoch [1/2], Step [1497/3732], Loss: 3.3772, accuracy: 6.7719%\n",
      "Epoch [1/2], Step [1498/3732], Loss: 3.2522, accuracy: 6.7757%\n",
      "Epoch [1/2], Step [1499/3732], Loss: 3.4668, accuracy: 6.7712%\n",
      "Epoch [1/2], Step [1500/3732], Loss: 3.8820, accuracy: 6.7667%\n",
      "Epoch [1/2], Step [1501/3732], Loss: 3.4003, accuracy: 6.7622%\n",
      "Epoch [1/2], Step [1502/3732], Loss: 4.1207, accuracy: 6.7577%\n",
      "Epoch [1/2], Step [1503/3732], Loss: 3.9733, accuracy: 6.7532%\n",
      "Epoch [1/2], Step [1504/3732], Loss: 3.7342, accuracy: 6.7570%\n",
      "Epoch [1/2], Step [1505/3732], Loss: 3.7350, accuracy: 6.7525%\n",
      "Epoch [1/2], Step [1506/3732], Loss: 3.5621, accuracy: 6.7480%\n",
      "Epoch [1/2], Step [1507/3732], Loss: 3.7414, accuracy: 6.7435%\n",
      "Epoch [1/2], Step [1508/3732], Loss: 3.9844, accuracy: 6.7391%\n",
      "Epoch [1/2], Step [1509/3732], Loss: 3.7014, accuracy: 6.7429%\n",
      "Epoch [1/2], Step [1510/3732], Loss: 3.2730, accuracy: 6.7467%\n",
      "Epoch [1/2], Step [1511/3732], Loss: 3.8549, accuracy: 6.7422%\n",
      "Epoch [1/2], Step [1512/3732], Loss: 3.7661, accuracy: 6.7460%\n",
      "Epoch [1/2], Step [1513/3732], Loss: 3.6806, accuracy: 6.7416%\n",
      "Epoch [1/2], Step [1514/3732], Loss: 3.8072, accuracy: 6.7371%\n",
      "Epoch [1/2], Step [1515/3732], Loss: 3.3505, accuracy: 6.7409%\n",
      "Epoch [1/2], Step [1516/3732], Loss: 3.4916, accuracy: 6.7447%\n",
      "Epoch [1/2], Step [1517/3732], Loss: 3.5138, accuracy: 6.7485%\n",
      "Epoch [1/2], Step [1518/3732], Loss: 3.7019, accuracy: 6.7441%\n",
      "Epoch [1/2], Step [1519/3732], Loss: 4.1942, accuracy: 6.7396%\n",
      "Epoch [1/2], Step [1520/3732], Loss: 3.3020, accuracy: 6.7516%\n",
      "Epoch [1/2], Step [1521/3732], Loss: 3.4726, accuracy: 6.7636%\n",
      "Epoch [1/2], Step [1522/3732], Loss: 3.6241, accuracy: 6.7592%\n",
      "Epoch [1/2], Step [1523/3732], Loss: 3.7251, accuracy: 6.7548%\n",
      "Epoch [1/2], Step [1524/3732], Loss: 3.6301, accuracy: 6.7503%\n",
      "Epoch [1/2], Step [1525/3732], Loss: 3.9351, accuracy: 6.7459%\n",
      "Epoch [1/2], Step [1526/3732], Loss: 3.2499, accuracy: 6.7497%\n",
      "Epoch [1/2], Step [1527/3732], Loss: 3.7494, accuracy: 6.7453%\n",
      "Epoch [1/2], Step [1528/3732], Loss: 3.2797, accuracy: 6.7490%\n",
      "Epoch [1/2], Step [1529/3732], Loss: 3.9371, accuracy: 6.7446%\n",
      "Epoch [1/2], Step [1530/3732], Loss: 3.5571, accuracy: 6.7484%\n",
      "Epoch [1/2], Step [1531/3732], Loss: 3.5792, accuracy: 6.7521%\n",
      "Epoch [1/2], Step [1532/3732], Loss: 3.5040, accuracy: 6.7559%\n",
      "Epoch [1/2], Step [1533/3732], Loss: 3.5373, accuracy: 6.7596%\n",
      "Epoch [1/2], Step [1534/3732], Loss: 3.2194, accuracy: 6.7634%\n",
      "Epoch [1/2], Step [1535/3732], Loss: 3.3679, accuracy: 6.7590%\n",
      "Epoch [1/2], Step [1536/3732], Loss: 3.5635, accuracy: 6.7546%\n",
      "Epoch [1/2], Step [1537/3732], Loss: 3.3432, accuracy: 6.7502%\n",
      "Epoch [1/2], Step [1538/3732], Loss: 3.3867, accuracy: 6.7539%\n",
      "Epoch [1/2], Step [1539/3732], Loss: 3.9071, accuracy: 6.7495%\n",
      "Epoch [1/2], Step [1540/3732], Loss: 3.2570, accuracy: 6.7614%\n",
      "Epoch [1/2], Step [1541/3732], Loss: 3.5892, accuracy: 6.7651%\n",
      "Epoch [1/2], Step [1542/3732], Loss: 4.3545, accuracy: 6.7607%\n",
      "Epoch [1/2], Step [1543/3732], Loss: 3.7838, accuracy: 6.7563%\n",
      "Epoch [1/2], Step [1544/3732], Loss: 3.5275, accuracy: 6.7600%\n",
      "Epoch [1/2], Step [1545/3732], Loss: 3.2866, accuracy: 6.7718%\n",
      "Epoch [1/2], Step [1546/3732], Loss: 3.2331, accuracy: 6.7755%\n",
      "Epoch [1/2], Step [1547/3732], Loss: 3.1939, accuracy: 6.7873%\n",
      "Epoch [1/2], Step [1548/3732], Loss: 3.2779, accuracy: 6.7910%\n",
      "Epoch [1/2], Step [1549/3732], Loss: 3.3148, accuracy: 6.8028%\n",
      "Epoch [1/2], Step [1550/3732], Loss: 3.5945, accuracy: 6.8065%\n",
      "Epoch [1/2], Step [1551/3732], Loss: 3.7457, accuracy: 6.8021%\n",
      "Epoch [1/2], Step [1552/3732], Loss: 3.5200, accuracy: 6.8138%\n",
      "Epoch [1/2], Step [1553/3732], Loss: 3.9808, accuracy: 6.8175%\n",
      "Epoch [1/2], Step [1554/3732], Loss: 3.4059, accuracy: 6.8131%\n",
      "Epoch [1/2], Step [1555/3732], Loss: 3.8222, accuracy: 6.8087%\n",
      "Epoch [1/2], Step [1556/3732], Loss: 3.3668, accuracy: 6.8123%\n",
      "Epoch [1/2], Step [1557/3732], Loss: 3.2720, accuracy: 6.8240%\n",
      "Epoch [1/2], Step [1558/3732], Loss: 3.8924, accuracy: 6.8196%\n",
      "Epoch [1/2], Step [1559/3732], Loss: 3.8341, accuracy: 6.8153%\n",
      "Epoch [1/2], Step [1560/3732], Loss: 3.5191, accuracy: 6.8189%\n",
      "Epoch [1/2], Step [1561/3732], Loss: 3.6230, accuracy: 6.8225%\n",
      "Epoch [1/2], Step [1562/3732], Loss: 3.8234, accuracy: 6.8342%\n",
      "Epoch [1/2], Step [1563/3732], Loss: 3.1991, accuracy: 6.8538%\n",
      "Epoch [1/2], Step [1564/3732], Loss: 3.5863, accuracy: 6.8494%\n",
      "Epoch [1/2], Step [1565/3732], Loss: 3.4596, accuracy: 6.8530%\n",
      "Epoch [1/2], Step [1566/3732], Loss: 3.5108, accuracy: 6.8487%\n",
      "Epoch [1/2], Step [1567/3732], Loss: 3.8555, accuracy: 6.8443%\n",
      "Epoch [1/2], Step [1568/3732], Loss: 3.5179, accuracy: 6.8399%\n",
      "Epoch [1/2], Step [1569/3732], Loss: 3.9578, accuracy: 6.8356%\n",
      "Epoch [1/2], Step [1570/3732], Loss: 3.4894, accuracy: 6.8392%\n",
      "Epoch [1/2], Step [1571/3732], Loss: 3.5026, accuracy: 6.8507%\n",
      "Epoch [1/2], Step [1572/3732], Loss: 3.5494, accuracy: 6.8543%\n",
      "Epoch [1/2], Step [1573/3732], Loss: 3.5419, accuracy: 6.8579%\n",
      "Epoch [1/2], Step [1574/3732], Loss: 3.5124, accuracy: 6.8615%\n",
      "Epoch [1/2], Step [1575/3732], Loss: 3.3101, accuracy: 6.8730%\n",
      "Epoch [1/2], Step [1576/3732], Loss: 3.3064, accuracy: 6.8766%\n",
      "Epoch [1/2], Step [1577/3732], Loss: 3.1099, accuracy: 6.8802%\n",
      "Epoch [1/2], Step [1578/3732], Loss: 4.0207, accuracy: 6.8758%\n",
      "Epoch [1/2], Step [1579/3732], Loss: 3.4624, accuracy: 6.8873%\n",
      "Epoch [1/2], Step [1580/3732], Loss: 3.2544, accuracy: 6.8908%\n",
      "Epoch [1/2], Step [1581/3732], Loss: 3.7375, accuracy: 6.8944%\n",
      "Epoch [1/2], Step [1582/3732], Loss: 3.6898, accuracy: 6.8900%\n",
      "Epoch [1/2], Step [1583/3732], Loss: 4.0279, accuracy: 6.8857%\n",
      "Epoch [1/2], Step [1584/3732], Loss: 3.9380, accuracy: 6.8813%\n",
      "Epoch [1/2], Step [1585/3732], Loss: 3.9325, accuracy: 6.8770%\n",
      "Epoch [1/2], Step [1586/3732], Loss: 3.6022, accuracy: 6.8805%\n",
      "Epoch [1/2], Step [1587/3732], Loss: 3.6593, accuracy: 6.8919%\n",
      "Epoch [1/2], Step [1588/3732], Loss: 3.2683, accuracy: 6.8876%\n",
      "Epoch [1/2], Step [1589/3732], Loss: 3.9187, accuracy: 6.8833%\n",
      "Epoch [1/2], Step [1590/3732], Loss: 3.6259, accuracy: 6.8789%\n",
      "Epoch [1/2], Step [1591/3732], Loss: 3.2533, accuracy: 6.8825%\n",
      "Epoch [1/2], Step [1592/3732], Loss: 3.4757, accuracy: 6.8938%\n",
      "Epoch [1/2], Step [1593/3732], Loss: 3.7285, accuracy: 6.8895%\n",
      "Epoch [1/2], Step [1594/3732], Loss: 3.0360, accuracy: 6.9009%\n",
      "Epoch [1/2], Step [1595/3732], Loss: 3.7550, accuracy: 6.9044%\n",
      "Epoch [1/2], Step [1596/3732], Loss: 3.8152, accuracy: 6.9079%\n",
      "Epoch [1/2], Step [1597/3732], Loss: 3.3385, accuracy: 6.9114%\n",
      "Epoch [1/2], Step [1598/3732], Loss: 3.5097, accuracy: 6.9071%\n",
      "Epoch [1/2], Step [1599/3732], Loss: 3.8404, accuracy: 6.9106%\n",
      "Epoch [1/2], Step [1600/3732], Loss: 3.4807, accuracy: 6.9141%\n",
      "Epoch [1/2], Step [1601/3732], Loss: 3.4537, accuracy: 6.9176%\n",
      "Epoch [1/2], Step [1602/3732], Loss: 3.2645, accuracy: 6.9210%\n",
      "Epoch [1/2], Step [1603/3732], Loss: 3.6726, accuracy: 6.9245%\n",
      "Epoch [1/2], Step [1604/3732], Loss: 3.7475, accuracy: 6.9280%\n",
      "Epoch [1/2], Step [1605/3732], Loss: 3.8837, accuracy: 6.9237%\n",
      "Epoch [1/2], Step [1606/3732], Loss: 3.5324, accuracy: 6.9194%\n",
      "Epoch [1/2], Step [1607/3732], Loss: 3.2489, accuracy: 6.9228%\n",
      "Epoch [1/2], Step [1608/3732], Loss: 3.6746, accuracy: 6.9263%\n",
      "Epoch [1/2], Step [1609/3732], Loss: 3.7166, accuracy: 6.9375%\n",
      "Epoch [1/2], Step [1610/3732], Loss: 3.5638, accuracy: 6.9410%\n",
      "Epoch [1/2], Step [1611/3732], Loss: 3.7929, accuracy: 6.9444%\n",
      "Epoch [1/2], Step [1612/3732], Loss: 2.9935, accuracy: 6.9556%\n",
      "Epoch [1/2], Step [1613/3732], Loss: 3.5891, accuracy: 6.9591%\n",
      "Epoch [1/2], Step [1614/3732], Loss: 2.6851, accuracy: 6.9780%\n",
      "Epoch [1/2], Step [1615/3732], Loss: 3.3001, accuracy: 6.9737%\n",
      "Epoch [1/2], Step [1616/3732], Loss: 3.7062, accuracy: 6.9771%\n",
      "Epoch [1/2], Step [1617/3732], Loss: 4.1530, accuracy: 6.9728%\n",
      "Epoch [1/2], Step [1618/3732], Loss: 3.7969, accuracy: 6.9762%\n",
      "Epoch [1/2], Step [1619/3732], Loss: 3.7375, accuracy: 6.9719%\n",
      "Epoch [1/2], Step [1620/3732], Loss: 3.2969, accuracy: 6.9753%\n",
      "Epoch [1/2], Step [1621/3732], Loss: 3.7752, accuracy: 6.9787%\n",
      "Epoch [1/2], Step [1622/3732], Loss: 3.2582, accuracy: 6.9898%\n",
      "Epoch [1/2], Step [1623/3732], Loss: 3.1414, accuracy: 6.9932%\n",
      "Epoch [1/2], Step [1624/3732], Loss: 3.2778, accuracy: 7.0043%\n",
      "Epoch [1/2], Step [1625/3732], Loss: 3.2552, accuracy: 7.0000%\n",
      "Epoch [1/2], Step [1626/3732], Loss: 3.7426, accuracy: 7.0034%\n",
      "Epoch [1/2], Step [1627/3732], Loss: 3.7088, accuracy: 7.0068%\n",
      "Epoch [1/2], Step [1628/3732], Loss: 3.5657, accuracy: 7.0178%\n",
      "Epoch [1/2], Step [1629/3732], Loss: 3.9768, accuracy: 7.0212%\n",
      "Epoch [1/2], Step [1630/3732], Loss: 3.2602, accuracy: 7.0245%\n",
      "Epoch [1/2], Step [1631/3732], Loss: 3.5067, accuracy: 7.0356%\n",
      "Epoch [1/2], Step [1632/3732], Loss: 3.7319, accuracy: 7.0312%\n",
      "Epoch [1/2], Step [1633/3732], Loss: 3.2764, accuracy: 7.0423%\n",
      "Epoch [1/2], Step [1634/3732], Loss: 2.9354, accuracy: 7.0456%\n",
      "Epoch [1/2], Step [1635/3732], Loss: 3.5900, accuracy: 7.0489%\n",
      "Epoch [1/2], Step [1636/3732], Loss: 3.6493, accuracy: 7.0446%\n",
      "Epoch [1/2], Step [1637/3732], Loss: 3.6630, accuracy: 7.0403%\n",
      "Epoch [1/2], Step [1638/3732], Loss: 3.6660, accuracy: 7.0437%\n",
      "Epoch [1/2], Step [1639/3732], Loss: 3.6057, accuracy: 7.0470%\n",
      "Epoch [1/2], Step [1640/3732], Loss: 3.5159, accuracy: 7.0427%\n",
      "Epoch [1/2], Step [1641/3732], Loss: 3.8208, accuracy: 7.0460%\n",
      "Epoch [1/2], Step [1642/3732], Loss: 4.2875, accuracy: 7.0417%\n",
      "Epoch [1/2], Step [1643/3732], Loss: 3.3596, accuracy: 7.0603%\n",
      "Epoch [1/2], Step [1644/3732], Loss: 3.5845, accuracy: 7.0636%\n",
      "Epoch [1/2], Step [1645/3732], Loss: 3.8845, accuracy: 7.0593%\n",
      "Epoch [1/2], Step [1646/3732], Loss: 3.6841, accuracy: 7.0550%\n",
      "Epoch [1/2], Step [1647/3732], Loss: 3.5679, accuracy: 7.0507%\n",
      "Epoch [1/2], Step [1648/3732], Loss: 3.7956, accuracy: 7.0540%\n",
      "Epoch [1/2], Step [1649/3732], Loss: 3.3943, accuracy: 7.0573%\n",
      "Epoch [1/2], Step [1650/3732], Loss: 3.9321, accuracy: 7.0606%\n",
      "Epoch [1/2], Step [1651/3732], Loss: 3.1229, accuracy: 7.0715%\n",
      "Epoch [1/2], Step [1652/3732], Loss: 3.3937, accuracy: 7.0823%\n",
      "Epoch [1/2], Step [1653/3732], Loss: 3.1408, accuracy: 7.0856%\n",
      "Epoch [1/2], Step [1654/3732], Loss: 3.5736, accuracy: 7.0889%\n",
      "Epoch [1/2], Step [1655/3732], Loss: 3.6127, accuracy: 7.0846%\n",
      "Epoch [1/2], Step [1656/3732], Loss: 3.7560, accuracy: 7.0803%\n",
      "Epoch [1/2], Step [1657/3732], Loss: 3.6091, accuracy: 7.0836%\n",
      "Epoch [1/2], Step [1658/3732], Loss: 3.6806, accuracy: 7.0793%\n",
      "Epoch [1/2], Step [1659/3732], Loss: 3.3430, accuracy: 7.0901%\n",
      "Epoch [1/2], Step [1660/3732], Loss: 3.6543, accuracy: 7.0934%\n",
      "Epoch [1/2], Step [1661/3732], Loss: 3.2971, accuracy: 7.0966%\n",
      "Epoch [1/2], Step [1662/3732], Loss: 3.1764, accuracy: 7.1074%\n",
      "Epoch [1/2], Step [1663/3732], Loss: 3.1106, accuracy: 7.1257%\n",
      "Epoch [1/2], Step [1664/3732], Loss: 3.0363, accuracy: 7.1364%\n",
      "Epoch [1/2], Step [1665/3732], Loss: 3.8160, accuracy: 7.1396%\n",
      "Epoch [1/2], Step [1666/3732], Loss: 3.8732, accuracy: 7.1354%\n",
      "Epoch [1/2], Step [1667/3732], Loss: 3.2520, accuracy: 7.1386%\n",
      "Epoch [1/2], Step [1668/3732], Loss: 3.4244, accuracy: 7.1418%\n",
      "Epoch [1/2], Step [1669/3732], Loss: 3.4732, accuracy: 7.1525%\n",
      "Epoch [1/2], Step [1670/3732], Loss: 3.2473, accuracy: 7.1557%\n",
      "Epoch [1/2], Step [1671/3732], Loss: 3.7638, accuracy: 7.1589%\n",
      "Epoch [1/2], Step [1672/3732], Loss: 3.4678, accuracy: 7.1696%\n",
      "Epoch [1/2], Step [1673/3732], Loss: 3.6622, accuracy: 7.1653%\n",
      "Epoch [1/2], Step [1674/3732], Loss: 3.7395, accuracy: 7.1610%\n",
      "Epoch [1/2], Step [1675/3732], Loss: 3.5639, accuracy: 7.1567%\n",
      "Epoch [1/2], Step [1676/3732], Loss: 3.5336, accuracy: 7.1599%\n",
      "Epoch [1/2], Step [1677/3732], Loss: 4.0973, accuracy: 7.1556%\n",
      "Epoch [1/2], Step [1678/3732], Loss: 3.2779, accuracy: 7.1588%\n",
      "Epoch [1/2], Step [1679/3732], Loss: 3.9046, accuracy: 7.1546%\n",
      "Epoch [1/2], Step [1680/3732], Loss: 3.7044, accuracy: 7.1577%\n",
      "Epoch [1/2], Step [1681/3732], Loss: 3.1215, accuracy: 7.1684%\n",
      "Epoch [1/2], Step [1682/3732], Loss: 3.4715, accuracy: 7.1641%\n",
      "Epoch [1/2], Step [1683/3732], Loss: 3.7141, accuracy: 7.1673%\n",
      "Epoch [1/2], Step [1684/3732], Loss: 3.3632, accuracy: 7.1704%\n",
      "Epoch [1/2], Step [1685/3732], Loss: 3.9062, accuracy: 7.1662%\n",
      "Epoch [1/2], Step [1686/3732], Loss: 3.5525, accuracy: 7.1619%\n",
      "Epoch [1/2], Step [1687/3732], Loss: 3.6547, accuracy: 7.1577%\n",
      "Epoch [1/2], Step [1688/3732], Loss: 3.9398, accuracy: 7.1534%\n",
      "Epoch [1/2], Step [1689/3732], Loss: 3.4786, accuracy: 7.1492%\n",
      "Epoch [1/2], Step [1690/3732], Loss: 3.3951, accuracy: 7.1524%\n",
      "Epoch [1/2], Step [1691/3732], Loss: 3.7857, accuracy: 7.1555%\n",
      "Epoch [1/2], Step [1692/3732], Loss: 3.8709, accuracy: 7.1513%\n",
      "Epoch [1/2], Step [1693/3732], Loss: 3.5340, accuracy: 7.1545%\n",
      "Epoch [1/2], Step [1694/3732], Loss: 3.3421, accuracy: 7.1724%\n",
      "Epoch [1/2], Step [1695/3732], Loss: 3.5357, accuracy: 7.1755%\n",
      "Epoch [1/2], Step [1696/3732], Loss: 3.5756, accuracy: 7.1713%\n",
      "Epoch [1/2], Step [1697/3732], Loss: 3.6701, accuracy: 7.1671%\n",
      "Epoch [1/2], Step [1698/3732], Loss: 3.6774, accuracy: 7.1702%\n",
      "Epoch [1/2], Step [1699/3732], Loss: 3.5221, accuracy: 7.1733%\n",
      "Epoch [1/2], Step [1700/3732], Loss: 3.6119, accuracy: 7.1691%\n",
      "Epoch [1/2], Step [1701/3732], Loss: 3.5200, accuracy: 7.1723%\n",
      "Epoch [1/2], Step [1702/3732], Loss: 3.7504, accuracy: 7.1680%\n",
      "Epoch [1/2], Step [1703/3732], Loss: 3.6890, accuracy: 7.1712%\n",
      "Epoch [1/2], Step [1704/3732], Loss: 3.8083, accuracy: 7.1743%\n",
      "Epoch [1/2], Step [1705/3732], Loss: 3.7970, accuracy: 7.1774%\n",
      "Epoch [1/2], Step [1706/3732], Loss: 3.7276, accuracy: 7.1879%\n",
      "Epoch [1/2], Step [1707/3732], Loss: 3.7010, accuracy: 7.1837%\n",
      "Epoch [1/2], Step [1708/3732], Loss: 3.6062, accuracy: 7.1868%\n",
      "Epoch [1/2], Step [1709/3732], Loss: 3.4630, accuracy: 7.2045%\n",
      "Epoch [1/2], Step [1710/3732], Loss: 3.5147, accuracy: 7.2003%\n",
      "Epoch [1/2], Step [1711/3732], Loss: 3.5322, accuracy: 7.2034%\n",
      "Epoch [1/2], Step [1712/3732], Loss: 3.6679, accuracy: 7.2065%\n",
      "Epoch [1/2], Step [1713/3732], Loss: 4.1037, accuracy: 7.2023%\n",
      "Epoch [1/2], Step [1714/3732], Loss: 4.2149, accuracy: 7.2054%\n",
      "Epoch [1/2], Step [1715/3732], Loss: 3.4193, accuracy: 7.2085%\n",
      "Epoch [1/2], Step [1716/3732], Loss: 3.6819, accuracy: 7.2115%\n",
      "Epoch [1/2], Step [1717/3732], Loss: 3.4177, accuracy: 7.2146%\n",
      "Epoch [1/2], Step [1718/3732], Loss: 3.5081, accuracy: 7.2104%\n",
      "Epoch [1/2], Step [1719/3732], Loss: 3.7541, accuracy: 7.2062%\n",
      "Epoch [1/2], Step [1720/3732], Loss: 3.5217, accuracy: 7.2093%\n",
      "Epoch [1/2], Step [1721/3732], Loss: 3.9311, accuracy: 7.2051%\n",
      "Epoch [1/2], Step [1722/3732], Loss: 3.5397, accuracy: 7.2009%\n",
      "Epoch [1/2], Step [1723/3732], Loss: 3.3622, accuracy: 7.1967%\n",
      "Epoch [1/2], Step [1724/3732], Loss: 3.5023, accuracy: 7.1926%\n",
      "Epoch [1/2], Step [1725/3732], Loss: 3.3168, accuracy: 7.1957%\n",
      "Epoch [1/2], Step [1726/3732], Loss: 3.5475, accuracy: 7.1987%\n",
      "Epoch [1/2], Step [1727/3732], Loss: 3.5052, accuracy: 7.2090%\n",
      "Epoch [1/2], Step [1728/3732], Loss: 3.0630, accuracy: 7.2193%\n",
      "Epoch [1/2], Step [1729/3732], Loss: 3.8536, accuracy: 7.2224%\n",
      "Epoch [1/2], Step [1730/3732], Loss: 3.3018, accuracy: 7.2254%\n",
      "Epoch [1/2], Step [1731/3732], Loss: 3.6625, accuracy: 7.2213%\n",
      "Epoch [1/2], Step [1732/3732], Loss: 4.0452, accuracy: 7.2315%\n",
      "Epoch [1/2], Step [1733/3732], Loss: 3.9185, accuracy: 7.2274%\n",
      "Epoch [1/2], Step [1734/3732], Loss: 3.5619, accuracy: 7.2232%\n",
      "Epoch [1/2], Step [1735/3732], Loss: 3.0415, accuracy: 7.2262%\n",
      "Epoch [1/2], Step [1736/3732], Loss: 3.5727, accuracy: 7.2293%\n",
      "Epoch [1/2], Step [1737/3732], Loss: 3.9082, accuracy: 7.2251%\n",
      "Epoch [1/2], Step [1738/3732], Loss: 4.0190, accuracy: 7.2281%\n",
      "Epoch [1/2], Step [1739/3732], Loss: 3.4683, accuracy: 7.2240%\n",
      "Epoch [1/2], Step [1740/3732], Loss: 3.6011, accuracy: 7.2198%\n",
      "Epoch [1/2], Step [1741/3732], Loss: 3.6115, accuracy: 7.2229%\n",
      "Epoch [1/2], Step [1742/3732], Loss: 4.1519, accuracy: 7.2187%\n",
      "Epoch [1/2], Step [1743/3732], Loss: 2.9895, accuracy: 7.2361%\n",
      "Epoch [1/2], Step [1744/3732], Loss: 3.7150, accuracy: 7.2319%\n",
      "Epoch [1/2], Step [1745/3732], Loss: 3.6205, accuracy: 7.2350%\n",
      "Epoch [1/2], Step [1746/3732], Loss: 2.6912, accuracy: 7.2523%\n",
      "Epoch [1/2], Step [1747/3732], Loss: 3.4656, accuracy: 7.2481%\n",
      "Epoch [1/2], Step [1748/3732], Loss: 3.5274, accuracy: 7.2511%\n",
      "Epoch [1/2], Step [1749/3732], Loss: 3.6788, accuracy: 7.2541%\n",
      "Epoch [1/2], Step [1750/3732], Loss: 3.3126, accuracy: 7.2643%\n",
      "Epoch [1/2], Step [1751/3732], Loss: 3.7705, accuracy: 7.2601%\n",
      "Epoch [1/2], Step [1752/3732], Loss: 3.6268, accuracy: 7.2560%\n",
      "Epoch [1/2], Step [1753/3732], Loss: 3.2219, accuracy: 7.2519%\n",
      "Epoch [1/2], Step [1754/3732], Loss: 3.7127, accuracy: 7.2477%\n",
      "Epoch [1/2], Step [1755/3732], Loss: 4.0277, accuracy: 7.2436%\n",
      "Epoch [1/2], Step [1756/3732], Loss: 3.3583, accuracy: 7.2537%\n",
      "Epoch [1/2], Step [1757/3732], Loss: 3.5879, accuracy: 7.2567%\n",
      "Epoch [1/2], Step [1758/3732], Loss: 3.6890, accuracy: 7.2526%\n",
      "Epoch [1/2], Step [1759/3732], Loss: 3.7436, accuracy: 7.2555%\n",
      "Epoch [1/2], Step [1760/3732], Loss: 3.2115, accuracy: 7.2656%\n",
      "Epoch [1/2], Step [1761/3732], Loss: 3.4751, accuracy: 7.2686%\n",
      "Epoch [1/2], Step [1762/3732], Loss: 3.4171, accuracy: 7.2716%\n",
      "Epoch [1/2], Step [1763/3732], Loss: 3.6795, accuracy: 7.2674%\n",
      "Epoch [1/2], Step [1764/3732], Loss: 3.5185, accuracy: 7.2633%\n",
      "Epoch [1/2], Step [1765/3732], Loss: 3.7817, accuracy: 7.2663%\n",
      "Epoch [1/2], Step [1766/3732], Loss: 3.4295, accuracy: 7.2622%\n",
      "Epoch [1/2], Step [1767/3732], Loss: 3.8194, accuracy: 7.2651%\n",
      "Epoch [1/2], Step [1768/3732], Loss: 3.0375, accuracy: 7.2681%\n",
      "Epoch [1/2], Step [1769/3732], Loss: 3.5901, accuracy: 7.2711%\n",
      "Epoch [1/2], Step [1770/3732], Loss: 3.3346, accuracy: 7.2740%\n",
      "Epoch [1/2], Step [1771/3732], Loss: 3.3665, accuracy: 7.2770%\n",
      "Epoch [1/2], Step [1772/3732], Loss: 3.4195, accuracy: 7.2870%\n",
      "Epoch [1/2], Step [1773/3732], Loss: 3.6368, accuracy: 7.2829%\n",
      "Epoch [1/2], Step [1774/3732], Loss: 3.5131, accuracy: 7.2858%\n",
      "Epoch [1/2], Step [1775/3732], Loss: 3.3840, accuracy: 7.2958%\n",
      "Epoch [1/2], Step [1776/3732], Loss: 3.3333, accuracy: 7.2987%\n",
      "Epoch [1/2], Step [1777/3732], Loss: 3.6347, accuracy: 7.3087%\n",
      "Epoch [1/2], Step [1778/3732], Loss: 3.5476, accuracy: 7.3046%\n",
      "Epoch [1/2], Step [1779/3732], Loss: 3.7898, accuracy: 7.3004%\n",
      "Epoch [1/2], Step [1780/3732], Loss: 3.5336, accuracy: 7.3034%\n",
      "Epoch [1/2], Step [1781/3732], Loss: 3.8632, accuracy: 7.3063%\n",
      "Epoch [1/2], Step [1782/3732], Loss: 3.5095, accuracy: 7.3092%\n",
      "Epoch [1/2], Step [1783/3732], Loss: 3.3745, accuracy: 7.3191%\n",
      "Epoch [1/2], Step [1784/3732], Loss: 3.5571, accuracy: 7.3150%\n",
      "Epoch [1/2], Step [1785/3732], Loss: 3.9793, accuracy: 7.3109%\n",
      "Epoch [1/2], Step [1786/3732], Loss: 3.3249, accuracy: 7.3208%\n",
      "Epoch [1/2], Step [1787/3732], Loss: 3.3708, accuracy: 7.3307%\n",
      "Epoch [1/2], Step [1788/3732], Loss: 3.9022, accuracy: 7.3266%\n",
      "Epoch [1/2], Step [1789/3732], Loss: 3.5893, accuracy: 7.3295%\n",
      "Epoch [1/2], Step [1790/3732], Loss: 3.5377, accuracy: 7.3254%\n",
      "Epoch [1/2], Step [1791/3732], Loss: 3.6085, accuracy: 7.3283%\n",
      "Epoch [1/2], Step [1792/3732], Loss: 3.7295, accuracy: 7.3382%\n",
      "Epoch [1/2], Step [1793/3732], Loss: 3.7192, accuracy: 7.3341%\n",
      "Epoch [1/2], Step [1794/3732], Loss: 3.5592, accuracy: 7.3439%\n",
      "Epoch [1/2], Step [1795/3732], Loss: 3.3676, accuracy: 7.3398%\n",
      "Epoch [1/2], Step [1796/3732], Loss: 3.4314, accuracy: 7.3427%\n",
      "Epoch [1/2], Step [1797/3732], Loss: 3.5548, accuracy: 7.3456%\n",
      "Epoch [1/2], Step [1798/3732], Loss: 2.9925, accuracy: 7.3623%\n",
      "Epoch [1/2], Step [1799/3732], Loss: 3.6823, accuracy: 7.3722%\n",
      "Epoch [1/2], Step [1800/3732], Loss: 4.1170, accuracy: 7.3681%\n",
      "Epoch [1/2], Step [1801/3732], Loss: 3.3732, accuracy: 7.3640%\n",
      "Epoch [1/2], Step [1802/3732], Loss: 3.4533, accuracy: 7.3738%\n",
      "Epoch [1/2], Step [1803/3732], Loss: 3.3507, accuracy: 7.3835%\n",
      "Epoch [1/2], Step [1804/3732], Loss: 2.8025, accuracy: 7.3864%\n",
      "Epoch [1/2], Step [1805/3732], Loss: 3.7177, accuracy: 7.3892%\n",
      "Epoch [1/2], Step [1806/3732], Loss: 3.2730, accuracy: 7.3920%\n",
      "Epoch [1/2], Step [1807/3732], Loss: 3.8745, accuracy: 7.3879%\n",
      "Epoch [1/2], Step [1808/3732], Loss: 3.5394, accuracy: 7.3908%\n",
      "Epoch [1/2], Step [1809/3732], Loss: 3.6153, accuracy: 7.3867%\n",
      "Epoch [1/2], Step [1810/3732], Loss: 4.0039, accuracy: 7.3895%\n",
      "Epoch [1/2], Step [1811/3732], Loss: 3.4178, accuracy: 7.3923%\n",
      "Epoch [1/2], Step [1812/3732], Loss: 3.3691, accuracy: 7.3951%\n",
      "Epoch [1/2], Step [1813/3732], Loss: 3.8867, accuracy: 7.3911%\n",
      "Epoch [1/2], Step [1814/3732], Loss: 3.6742, accuracy: 7.3870%\n",
      "Epoch [1/2], Step [1815/3732], Loss: 4.1267, accuracy: 7.3829%\n",
      "Epoch [1/2], Step [1816/3732], Loss: 4.2178, accuracy: 7.3789%\n",
      "Epoch [1/2], Step [1817/3732], Loss: 3.5518, accuracy: 7.3817%\n",
      "Epoch [1/2], Step [1818/3732], Loss: 3.2542, accuracy: 7.3845%\n",
      "Epoch [1/2], Step [1819/3732], Loss: 4.1430, accuracy: 7.3804%\n",
      "Epoch [1/2], Step [1820/3732], Loss: 3.5229, accuracy: 7.3764%\n",
      "Epoch [1/2], Step [1821/3732], Loss: 3.5038, accuracy: 7.3792%\n",
      "Epoch [1/2], Step [1822/3732], Loss: 3.8114, accuracy: 7.3820%\n",
      "Epoch [1/2], Step [1823/3732], Loss: 3.6412, accuracy: 7.3779%\n",
      "Epoch [1/2], Step [1824/3732], Loss: 3.8535, accuracy: 7.3739%\n",
      "Epoch [1/2], Step [1825/3732], Loss: 3.6067, accuracy: 7.3767%\n",
      "Epoch [1/2], Step [1826/3732], Loss: 3.2104, accuracy: 7.3727%\n",
      "Epoch [1/2], Step [1827/3732], Loss: 3.2433, accuracy: 7.3823%\n",
      "Epoch [1/2], Step [1828/3732], Loss: 3.3218, accuracy: 7.3920%\n",
      "Epoch [1/2], Step [1829/3732], Loss: 3.6450, accuracy: 7.3948%\n",
      "Epoch [1/2], Step [1830/3732], Loss: 3.8210, accuracy: 7.3907%\n",
      "Epoch [1/2], Step [1831/3732], Loss: 3.3556, accuracy: 7.3935%\n",
      "Epoch [1/2], Step [1832/3732], Loss: 3.3109, accuracy: 7.4031%\n",
      "Epoch [1/2], Step [1833/3732], Loss: 3.7127, accuracy: 7.4127%\n",
      "Epoch [1/2], Step [1834/3732], Loss: 3.7731, accuracy: 7.4155%\n",
      "Epoch [1/2], Step [1835/3732], Loss: 3.4584, accuracy: 7.4251%\n",
      "Epoch [1/2], Step [1836/3732], Loss: 3.3697, accuracy: 7.4346%\n",
      "Epoch [1/2], Step [1837/3732], Loss: 3.6578, accuracy: 7.4510%\n",
      "Epoch [1/2], Step [1838/3732], Loss: 3.9411, accuracy: 7.4470%\n",
      "Epoch [1/2], Step [1839/3732], Loss: 3.0105, accuracy: 7.4565%\n",
      "Epoch [1/2], Step [1840/3732], Loss: 3.3577, accuracy: 7.4592%\n",
      "Epoch [1/2], Step [1841/3732], Loss: 3.2526, accuracy: 7.4620%\n",
      "Epoch [1/2], Step [1842/3732], Loss: 3.6983, accuracy: 7.4579%\n",
      "Epoch [1/2], Step [1843/3732], Loss: 3.1361, accuracy: 7.4607%\n",
      "Epoch [1/2], Step [1844/3732], Loss: 3.2148, accuracy: 7.4634%\n",
      "Epoch [1/2], Step [1845/3732], Loss: 2.8618, accuracy: 7.4729%\n",
      "Epoch [1/2], Step [1846/3732], Loss: 3.7886, accuracy: 7.4756%\n",
      "Epoch [1/2], Step [1847/3732], Loss: 3.4550, accuracy: 7.4716%\n",
      "Epoch [1/2], Step [1848/3732], Loss: 3.4160, accuracy: 7.4675%\n",
      "Epoch [1/2], Step [1849/3732], Loss: 3.7530, accuracy: 7.4703%\n",
      "Epoch [1/2], Step [1850/3732], Loss: 3.6775, accuracy: 7.4662%\n",
      "Epoch [1/2], Step [1851/3732], Loss: 3.4974, accuracy: 7.4689%\n",
      "Epoch [1/2], Step [1852/3732], Loss: 3.7546, accuracy: 7.4649%\n",
      "Epoch [1/2], Step [1853/3732], Loss: 3.1397, accuracy: 7.4744%\n",
      "Epoch [1/2], Step [1854/3732], Loss: 3.2744, accuracy: 7.4838%\n",
      "Epoch [1/2], Step [1855/3732], Loss: 3.6825, accuracy: 7.4798%\n",
      "Epoch [1/2], Step [1856/3732], Loss: 3.2071, accuracy: 7.4892%\n",
      "Epoch [1/2], Step [1857/3732], Loss: 3.0518, accuracy: 7.4987%\n",
      "Epoch [1/2], Step [1858/3732], Loss: 3.9395, accuracy: 7.4946%\n",
      "Epoch [1/2], Step [1859/3732], Loss: 3.6814, accuracy: 7.4906%\n",
      "Epoch [1/2], Step [1860/3732], Loss: 4.0036, accuracy: 7.4866%\n",
      "Epoch [1/2], Step [1861/3732], Loss: 2.7715, accuracy: 7.5094%\n",
      "Epoch [1/2], Step [1862/3732], Loss: 3.6479, accuracy: 7.5054%\n",
      "Epoch [1/2], Step [1863/3732], Loss: 3.8217, accuracy: 7.5081%\n",
      "Epoch [1/2], Step [1864/3732], Loss: 3.8989, accuracy: 7.5040%\n",
      "Epoch [1/2], Step [1865/3732], Loss: 3.7860, accuracy: 7.5067%\n",
      "Epoch [1/2], Step [1866/3732], Loss: 3.3310, accuracy: 7.5161%\n",
      "Epoch [1/2], Step [1867/3732], Loss: 3.8072, accuracy: 7.5121%\n",
      "Epoch [1/2], Step [1868/3732], Loss: 3.2115, accuracy: 7.5214%\n",
      "Epoch [1/2], Step [1869/3732], Loss: 3.3807, accuracy: 7.5308%\n",
      "Epoch [1/2], Step [1870/3732], Loss: 3.5353, accuracy: 7.5334%\n",
      "Epoch [1/2], Step [1871/3732], Loss: 3.4522, accuracy: 7.5294%\n",
      "Epoch [1/2], Step [1872/3732], Loss: 3.9185, accuracy: 7.5254%\n",
      "Epoch [1/2], Step [1873/3732], Loss: 3.3315, accuracy: 7.5214%\n",
      "Epoch [1/2], Step [1874/3732], Loss: 3.7121, accuracy: 7.5173%\n",
      "Epoch [1/2], Step [1875/3732], Loss: 3.3515, accuracy: 7.5133%\n",
      "Epoch [1/2], Step [1876/3732], Loss: 3.7625, accuracy: 7.5093%\n",
      "Epoch [1/2], Step [1877/3732], Loss: 3.4761, accuracy: 7.5120%\n",
      "Epoch [1/2], Step [1878/3732], Loss: 3.3822, accuracy: 7.5213%\n",
      "Epoch [1/2], Step [1879/3732], Loss: 3.7888, accuracy: 7.5173%\n",
      "Epoch [1/2], Step [1880/3732], Loss: 3.4298, accuracy: 7.5266%\n",
      "Epoch [1/2], Step [1881/3732], Loss: 3.9892, accuracy: 7.5226%\n",
      "Epoch [1/2], Step [1882/3732], Loss: 3.4230, accuracy: 7.5186%\n",
      "Epoch [1/2], Step [1883/3732], Loss: 3.7848, accuracy: 7.5146%\n",
      "Epoch [1/2], Step [1884/3732], Loss: 3.3672, accuracy: 7.5173%\n",
      "Epoch [1/2], Step [1885/3732], Loss: 3.7337, accuracy: 7.5133%\n",
      "Epoch [1/2], Step [1886/3732], Loss: 3.1595, accuracy: 7.5159%\n",
      "Epoch [1/2], Step [1887/3732], Loss: 3.4967, accuracy: 7.5119%\n",
      "Epoch [1/2], Step [1888/3732], Loss: 3.2200, accuracy: 7.5146%\n",
      "Epoch [1/2], Step [1889/3732], Loss: 2.7803, accuracy: 7.5172%\n",
      "Epoch [1/2], Step [1890/3732], Loss: 3.6119, accuracy: 7.5198%\n",
      "Epoch [1/2], Step [1891/3732], Loss: 3.4266, accuracy: 7.5225%\n",
      "Epoch [1/2], Step [1892/3732], Loss: 2.7280, accuracy: 7.5383%\n",
      "Epoch [1/2], Step [1893/3732], Loss: 3.7071, accuracy: 7.5409%\n",
      "Epoch [1/2], Step [1894/3732], Loss: 3.5661, accuracy: 7.5370%\n",
      "Epoch [1/2], Step [1895/3732], Loss: 3.7615, accuracy: 7.5330%\n",
      "Epoch [1/2], Step [1896/3732], Loss: 3.4153, accuracy: 7.5422%\n",
      "Epoch [1/2], Step [1897/3732], Loss: 3.5101, accuracy: 7.5514%\n",
      "Epoch [1/2], Step [1898/3732], Loss: 3.7806, accuracy: 7.5474%\n",
      "Epoch [1/2], Step [1899/3732], Loss: 3.3296, accuracy: 7.5434%\n",
      "Epoch [1/2], Step [1900/3732], Loss: 3.5224, accuracy: 7.5461%\n",
      "Epoch [1/2], Step [1901/3732], Loss: 3.9779, accuracy: 7.5421%\n",
      "Epoch [1/2], Step [1902/3732], Loss: 3.5625, accuracy: 7.5447%\n",
      "Epoch [1/2], Step [1903/3732], Loss: 2.9963, accuracy: 7.5539%\n",
      "Epoch [1/2], Step [1904/3732], Loss: 3.3183, accuracy: 7.5565%\n",
      "Epoch [1/2], Step [1905/3732], Loss: 3.4730, accuracy: 7.5591%\n",
      "Epoch [1/2], Step [1906/3732], Loss: 3.6674, accuracy: 7.5616%\n",
      "Epoch [1/2], Step [1907/3732], Loss: 3.5230, accuracy: 7.5642%\n",
      "Epoch [1/2], Step [1908/3732], Loss: 3.9637, accuracy: 7.5603%\n",
      "Epoch [1/2], Step [1909/3732], Loss: 3.3222, accuracy: 7.5760%\n",
      "Epoch [1/2], Step [1910/3732], Loss: 3.9774, accuracy: 7.5720%\n",
      "Epoch [1/2], Step [1911/3732], Loss: 3.1665, accuracy: 7.5811%\n",
      "Epoch [1/2], Step [1912/3732], Loss: 4.0793, accuracy: 7.5837%\n",
      "Epoch [1/2], Step [1913/3732], Loss: 3.4694, accuracy: 7.5928%\n",
      "Epoch [1/2], Step [1914/3732], Loss: 3.5106, accuracy: 7.5888%\n",
      "Epoch [1/2], Step [1915/3732], Loss: 3.6462, accuracy: 7.5914%\n",
      "Epoch [1/2], Step [1916/3732], Loss: 3.7186, accuracy: 7.5874%\n",
      "Epoch [1/2], Step [1917/3732], Loss: 3.7196, accuracy: 7.5900%\n",
      "Epoch [1/2], Step [1918/3732], Loss: 3.4781, accuracy: 7.5925%\n",
      "Epoch [1/2], Step [1919/3732], Loss: 3.6404, accuracy: 7.5886%\n",
      "Epoch [1/2], Step [1920/3732], Loss: 3.7678, accuracy: 7.5911%\n",
      "Epoch [1/2], Step [1921/3732], Loss: 3.4796, accuracy: 7.5937%\n",
      "Epoch [1/2], Step [1922/3732], Loss: 3.1766, accuracy: 7.5963%\n",
      "Epoch [1/2], Step [1923/3732], Loss: 3.6618, accuracy: 7.5988%\n",
      "Epoch [1/2], Step [1924/3732], Loss: 3.3465, accuracy: 7.6014%\n",
      "Epoch [1/2], Step [1925/3732], Loss: 3.4889, accuracy: 7.6104%\n",
      "Epoch [1/2], Step [1926/3732], Loss: 3.4476, accuracy: 7.6194%\n",
      "Epoch [1/2], Step [1927/3732], Loss: 3.1790, accuracy: 7.6284%\n",
      "Epoch [1/2], Step [1928/3732], Loss: 3.4388, accuracy: 7.6374%\n",
      "Epoch [1/2], Step [1929/3732], Loss: 3.9288, accuracy: 7.6335%\n",
      "Epoch [1/2], Step [1930/3732], Loss: 3.3067, accuracy: 7.6490%\n",
      "Epoch [1/2], Step [1931/3732], Loss: 3.5591, accuracy: 7.6515%\n",
      "Epoch [1/2], Step [1932/3732], Loss: 3.2904, accuracy: 7.6540%\n",
      "Epoch [1/2], Step [1933/3732], Loss: 4.0629, accuracy: 7.6630%\n",
      "Epoch [1/2], Step [1934/3732], Loss: 3.4931, accuracy: 7.6590%\n",
      "Epoch [1/2], Step [1935/3732], Loss: 3.1663, accuracy: 7.6615%\n",
      "Epoch [1/2], Step [1936/3732], Loss: 3.0985, accuracy: 7.6705%\n",
      "Epoch [1/2], Step [1937/3732], Loss: 4.1158, accuracy: 7.6665%\n",
      "Epoch [1/2], Step [1938/3732], Loss: 3.4994, accuracy: 7.6690%\n",
      "Epoch [1/2], Step [1939/3732], Loss: 3.6127, accuracy: 7.6650%\n",
      "Epoch [1/2], Step [1940/3732], Loss: 3.7175, accuracy: 7.6611%\n",
      "Epoch [1/2], Step [1941/3732], Loss: 3.7731, accuracy: 7.6700%\n",
      "Epoch [1/2], Step [1942/3732], Loss: 3.7249, accuracy: 7.6661%\n",
      "Epoch [1/2], Step [1943/3732], Loss: 2.6785, accuracy: 7.6750%\n",
      "Epoch [1/2], Step [1944/3732], Loss: 3.2826, accuracy: 7.6710%\n",
      "Epoch [1/2], Step [1945/3732], Loss: 3.3816, accuracy: 7.6735%\n",
      "Epoch [1/2], Step [1946/3732], Loss: 3.7564, accuracy: 7.6696%\n",
      "Epoch [1/2], Step [1947/3732], Loss: 3.8707, accuracy: 7.6656%\n",
      "Epoch [1/2], Step [1948/3732], Loss: 3.3500, accuracy: 7.6617%\n",
      "Epoch [1/2], Step [1949/3732], Loss: 2.8308, accuracy: 7.6770%\n",
      "Epoch [1/2], Step [1950/3732], Loss: 3.2520, accuracy: 7.6859%\n",
      "Epoch [1/2], Step [1951/3732], Loss: 2.9464, accuracy: 7.6884%\n",
      "Epoch [1/2], Step [1952/3732], Loss: 2.9544, accuracy: 7.6972%\n",
      "Epoch [1/2], Step [1953/3732], Loss: 2.7701, accuracy: 7.6997%\n",
      "Epoch [1/2], Step [1954/3732], Loss: 3.4049, accuracy: 7.7021%\n",
      "Epoch [1/2], Step [1955/3732], Loss: 3.3083, accuracy: 7.6982%\n",
      "Epoch [1/2], Step [1956/3732], Loss: 2.9789, accuracy: 7.7134%\n",
      "Epoch [1/2], Step [1957/3732], Loss: 3.1786, accuracy: 7.7287%\n",
      "Epoch [1/2], Step [1958/3732], Loss: 3.4189, accuracy: 7.7311%\n",
      "Epoch [1/2], Step [1959/3732], Loss: 3.8498, accuracy: 7.7272%\n",
      "Epoch [1/2], Step [1960/3732], Loss: 3.6435, accuracy: 7.7296%\n",
      "Epoch [1/2], Step [1961/3732], Loss: 3.3500, accuracy: 7.7320%\n",
      "Epoch [1/2], Step [1962/3732], Loss: 3.6266, accuracy: 7.7281%\n",
      "Epoch [1/2], Step [1963/3732], Loss: 3.3289, accuracy: 7.7305%\n",
      "Epoch [1/2], Step [1964/3732], Loss: 3.7089, accuracy: 7.7266%\n",
      "Epoch [1/2], Step [1965/3732], Loss: 3.0648, accuracy: 7.7354%\n",
      "Epoch [1/2], Step [1966/3732], Loss: 3.3750, accuracy: 7.7378%\n",
      "Epoch [1/2], Step [1967/3732], Loss: 3.4810, accuracy: 7.7402%\n",
      "Epoch [1/2], Step [1968/3732], Loss: 4.1267, accuracy: 7.7363%\n",
      "Epoch [1/2], Step [1969/3732], Loss: 3.4133, accuracy: 7.7387%\n",
      "Epoch [1/2], Step [1970/3732], Loss: 3.3603, accuracy: 7.7411%\n",
      "Epoch [1/2], Step [1971/3732], Loss: 3.4349, accuracy: 7.7435%\n",
      "Epoch [1/2], Step [1972/3732], Loss: 3.0754, accuracy: 7.7459%\n",
      "Epoch [1/2], Step [1973/3732], Loss: 3.2676, accuracy: 7.7420%\n",
      "Epoch [1/2], Step [1974/3732], Loss: 3.8711, accuracy: 7.7381%\n",
      "Epoch [1/2], Step [1975/3732], Loss: 3.5485, accuracy: 7.7405%\n",
      "Epoch [1/2], Step [1976/3732], Loss: 3.2469, accuracy: 7.7429%\n",
      "Epoch [1/2], Step [1977/3732], Loss: 3.3732, accuracy: 7.7516%\n",
      "Epoch [1/2], Step [1978/3732], Loss: 3.2287, accuracy: 7.7540%\n",
      "Epoch [1/2], Step [1979/3732], Loss: 3.7268, accuracy: 7.7501%\n",
      "Epoch [1/2], Step [1980/3732], Loss: 3.0687, accuracy: 7.7588%\n",
      "Epoch [1/2], Step [1981/3732], Loss: 3.6025, accuracy: 7.7739%\n",
      "Epoch [1/2], Step [1982/3732], Loss: 2.8772, accuracy: 7.7825%\n",
      "Epoch [1/2], Step [1983/3732], Loss: 3.5047, accuracy: 7.7849%\n",
      "Epoch [1/2], Step [1984/3732], Loss: 2.9691, accuracy: 7.7936%\n",
      "Epoch [1/2], Step [1985/3732], Loss: 3.3531, accuracy: 7.7960%\n",
      "Epoch [1/2], Step [1986/3732], Loss: 3.9231, accuracy: 7.7983%\n",
      "Epoch [1/2], Step [1987/3732], Loss: 3.3923, accuracy: 7.7944%\n",
      "Epoch [1/2], Step [1988/3732], Loss: 3.4229, accuracy: 7.8031%\n",
      "Epoch [1/2], Step [1989/3732], Loss: 3.9227, accuracy: 7.7991%\n",
      "Epoch [1/2], Step [1990/3732], Loss: 4.0484, accuracy: 7.7952%\n",
      "Epoch [1/2], Step [1991/3732], Loss: 3.0288, accuracy: 7.8039%\n",
      "Epoch [1/2], Step [1992/3732], Loss: 2.8293, accuracy: 7.8125%\n",
      "Epoch [1/2], Step [1993/3732], Loss: 3.3495, accuracy: 7.8086%\n",
      "Epoch [1/2], Step [1994/3732], Loss: 3.3409, accuracy: 7.8172%\n",
      "Epoch [1/2], Step [1995/3732], Loss: 3.7240, accuracy: 7.8258%\n",
      "Epoch [1/2], Step [1996/3732], Loss: 3.7593, accuracy: 7.8219%\n",
      "Epoch [1/2], Step [1997/3732], Loss: 3.4027, accuracy: 7.8180%\n",
      "Epoch [1/2], Step [1998/3732], Loss: 3.1569, accuracy: 7.8203%\n",
      "Epoch [1/2], Step [1999/3732], Loss: 3.4624, accuracy: 7.8227%\n",
      "Epoch [1/2], Step [2000/3732], Loss: 3.3907, accuracy: 7.8187%\n",
      "Epoch [1/2], Step [2001/3732], Loss: 2.9517, accuracy: 7.8211%\n",
      "Epoch [1/2], Step [2002/3732], Loss: 3.4537, accuracy: 7.8234%\n",
      "Epoch [1/2], Step [2003/3732], Loss: 3.1576, accuracy: 7.8258%\n",
      "Epoch [1/2], Step [2004/3732], Loss: 3.1587, accuracy: 7.8281%\n",
      "Epoch [1/2], Step [2005/3732], Loss: 4.2513, accuracy: 7.8242%\n",
      "Epoch [1/2], Step [2006/3732], Loss: 3.5236, accuracy: 7.8203%\n",
      "Epoch [1/2], Step [2007/3732], Loss: 3.8157, accuracy: 7.8226%\n",
      "Epoch [1/2], Step [2008/3732], Loss: 3.2403, accuracy: 7.8187%\n",
      "Epoch [1/2], Step [2009/3732], Loss: 3.5464, accuracy: 7.8211%\n",
      "Epoch [1/2], Step [2010/3732], Loss: 3.3777, accuracy: 7.8296%\n",
      "Epoch [1/2], Step [2011/3732], Loss: 3.5734, accuracy: 7.8257%\n",
      "Epoch [1/2], Step [2012/3732], Loss: 3.6278, accuracy: 7.8280%\n",
      "Epoch [1/2], Step [2013/3732], Loss: 3.1245, accuracy: 7.8366%\n",
      "Epoch [1/2], Step [2014/3732], Loss: 3.2946, accuracy: 7.8327%\n",
      "Epoch [1/2], Step [2015/3732], Loss: 3.7948, accuracy: 7.8350%\n",
      "Epoch [1/2], Step [2016/3732], Loss: 3.5944, accuracy: 7.8311%\n",
      "Epoch [1/2], Step [2017/3732], Loss: 3.6151, accuracy: 7.8272%\n",
      "Epoch [1/2], Step [2018/3732], Loss: 3.4796, accuracy: 7.8295%\n",
      "Epoch [1/2], Step [2019/3732], Loss: 3.6003, accuracy: 7.8318%\n",
      "Epoch [1/2], Step [2020/3732], Loss: 3.6991, accuracy: 7.8342%\n",
      "Epoch [1/2], Step [2021/3732], Loss: 3.3442, accuracy: 7.8303%\n",
      "Epoch [1/2], Step [2022/3732], Loss: 4.0235, accuracy: 7.8264%\n",
      "Epoch [1/2], Step [2023/3732], Loss: 3.4526, accuracy: 7.8287%\n",
      "Epoch [1/2], Step [2024/3732], Loss: 3.6994, accuracy: 7.8249%\n",
      "Epoch [1/2], Step [2025/3732], Loss: 3.1062, accuracy: 7.8395%\n",
      "Epoch [1/2], Step [2026/3732], Loss: 3.8641, accuracy: 7.8356%\n",
      "Epoch [1/2], Step [2027/3732], Loss: 3.5974, accuracy: 7.8379%\n",
      "Epoch [1/2], Step [2028/3732], Loss: 3.4363, accuracy: 7.8464%\n",
      "Epoch [1/2], Step [2029/3732], Loss: 3.5826, accuracy: 7.8425%\n",
      "Epoch [1/2], Step [2030/3732], Loss: 3.4606, accuracy: 7.8448%\n",
      "Epoch [1/2], Step [2031/3732], Loss: 3.5000, accuracy: 7.8533%\n",
      "Epoch [1/2], Step [2032/3732], Loss: 3.3332, accuracy: 7.8617%\n",
      "Epoch [1/2], Step [2033/3732], Loss: 3.7391, accuracy: 7.8578%\n",
      "Epoch [1/2], Step [2034/3732], Loss: 3.2821, accuracy: 7.8663%\n",
      "Epoch [1/2], Step [2035/3732], Loss: 4.0658, accuracy: 7.8624%\n",
      "Epoch [1/2], Step [2036/3732], Loss: 3.4896, accuracy: 7.8585%\n",
      "Epoch [1/2], Step [2037/3732], Loss: 3.8938, accuracy: 7.8608%\n",
      "Epoch [1/2], Step [2038/3732], Loss: 3.9178, accuracy: 7.8570%\n",
      "Epoch [1/2], Step [2039/3732], Loss: 3.4976, accuracy: 7.8592%\n",
      "Epoch [1/2], Step [2040/3732], Loss: 3.6716, accuracy: 7.8615%\n",
      "Epoch [1/2], Step [2041/3732], Loss: 3.2674, accuracy: 7.8699%\n",
      "Epoch [1/2], Step [2042/3732], Loss: 3.3107, accuracy: 7.8661%\n",
      "Epoch [1/2], Step [2043/3732], Loss: 3.7294, accuracy: 7.8622%\n",
      "Epoch [1/2], Step [2044/3732], Loss: 3.4598, accuracy: 7.8645%\n",
      "Epoch [1/2], Step [2045/3732], Loss: 3.6301, accuracy: 7.8667%\n",
      "Epoch [1/2], Step [2046/3732], Loss: 3.1593, accuracy: 7.8751%\n",
      "Epoch [1/2], Step [2047/3732], Loss: 3.5219, accuracy: 7.8774%\n",
      "Epoch [1/2], Step [2048/3732], Loss: 3.6054, accuracy: 7.8735%\n",
      "Epoch [1/2], Step [2049/3732], Loss: 3.4861, accuracy: 7.8758%\n",
      "Epoch [1/2], Step [2050/3732], Loss: 3.5234, accuracy: 7.8841%\n",
      "Epoch [1/2], Step [2051/3732], Loss: 3.3836, accuracy: 7.8864%\n",
      "Epoch [1/2], Step [2052/3732], Loss: 3.3909, accuracy: 7.8886%\n",
      "Epoch [1/2], Step [2053/3732], Loss: 3.8120, accuracy: 7.8909%\n",
      "Epoch [1/2], Step [2054/3732], Loss: 3.7107, accuracy: 7.8931%\n",
      "Epoch [1/2], Step [2055/3732], Loss: 3.4847, accuracy: 7.8954%\n",
      "Epoch [1/2], Step [2056/3732], Loss: 3.4108, accuracy: 7.8976%\n",
      "Epoch [1/2], Step [2057/3732], Loss: 3.6621, accuracy: 7.8938%\n",
      "Epoch [1/2], Step [2058/3732], Loss: 3.6803, accuracy: 7.8899%\n",
      "Epoch [1/2], Step [2059/3732], Loss: 3.5018, accuracy: 7.8861%\n",
      "Epoch [1/2], Step [2060/3732], Loss: 2.7658, accuracy: 7.8944%\n",
      "Epoch [1/2], Step [2061/3732], Loss: 3.4600, accuracy: 7.8967%\n",
      "Epoch [1/2], Step [2062/3732], Loss: 3.3356, accuracy: 7.8989%\n",
      "Epoch [1/2], Step [2063/3732], Loss: 3.6222, accuracy: 7.9011%\n",
      "Epoch [1/2], Step [2064/3732], Loss: 3.7372, accuracy: 7.8973%\n",
      "Epoch [1/2], Step [2065/3732], Loss: 3.3327, accuracy: 7.8995%\n",
      "Epoch [1/2], Step [2066/3732], Loss: 3.6008, accuracy: 7.8957%\n",
      "Epoch [1/2], Step [2067/3732], Loss: 2.9521, accuracy: 7.8979%\n",
      "Epoch [1/2], Step [2068/3732], Loss: 3.5265, accuracy: 7.9001%\n",
      "Epoch [1/2], Step [2069/3732], Loss: 3.8843, accuracy: 7.8963%\n",
      "Epoch [1/2], Step [2070/3732], Loss: 3.4949, accuracy: 7.8925%\n",
      "Epoch [1/2], Step [2071/3732], Loss: 2.8904, accuracy: 7.8947%\n",
      "Epoch [1/2], Step [2072/3732], Loss: 3.2877, accuracy: 7.8970%\n",
      "Epoch [1/2], Step [2073/3732], Loss: 3.5300, accuracy: 7.9052%\n",
      "Epoch [1/2], Step [2074/3732], Loss: 2.7992, accuracy: 7.9074%\n",
      "Epoch [1/2], Step [2075/3732], Loss: 3.9382, accuracy: 7.9036%\n",
      "Epoch [1/2], Step [2076/3732], Loss: 3.9918, accuracy: 7.9058%\n",
      "Epoch [1/2], Step [2077/3732], Loss: 3.2155, accuracy: 7.9141%\n",
      "Epoch [1/2], Step [2078/3732], Loss: 3.0174, accuracy: 7.9223%\n",
      "Epoch [1/2], Step [2079/3732], Loss: 3.1128, accuracy: 7.9185%\n",
      "Epoch [1/2], Step [2080/3732], Loss: 3.4419, accuracy: 7.9207%\n",
      "Epoch [1/2], Step [2081/3732], Loss: 4.2402, accuracy: 7.9169%\n",
      "Epoch [1/2], Step [2082/3732], Loss: 3.6075, accuracy: 7.9191%\n",
      "Epoch [1/2], Step [2083/3732], Loss: 3.5463, accuracy: 7.9153%\n",
      "Epoch [1/2], Step [2084/3732], Loss: 3.9312, accuracy: 7.9115%\n",
      "Epoch [1/2], Step [2085/3732], Loss: 3.5180, accuracy: 7.9137%\n",
      "Epoch [1/2], Step [2086/3732], Loss: 3.7539, accuracy: 7.9099%\n",
      "Epoch [1/2], Step [2087/3732], Loss: 3.6426, accuracy: 7.9121%\n",
      "Epoch [1/2], Step [2088/3732], Loss: 3.8797, accuracy: 7.9083%\n",
      "Epoch [1/2], Step [2089/3732], Loss: 2.8762, accuracy: 7.9225%\n",
      "Epoch [1/2], Step [2090/3732], Loss: 3.7536, accuracy: 7.9187%\n",
      "Epoch [1/2], Step [2091/3732], Loss: 3.3683, accuracy: 7.9149%\n",
      "Epoch [1/2], Step [2092/3732], Loss: 3.4622, accuracy: 7.9171%\n",
      "Epoch [1/2], Step [2093/3732], Loss: 3.5470, accuracy: 7.9133%\n",
      "Epoch [1/2], Step [2094/3732], Loss: 3.1605, accuracy: 7.9214%\n",
      "Epoch [1/2], Step [2095/3732], Loss: 3.5583, accuracy: 7.9177%\n",
      "Epoch [1/2], Step [2096/3732], Loss: 3.2759, accuracy: 7.9258%\n",
      "Epoch [1/2], Step [2097/3732], Loss: 3.6319, accuracy: 7.9280%\n",
      "Epoch [1/2], Step [2098/3732], Loss: 3.6055, accuracy: 7.9242%\n",
      "Epoch [1/2], Step [2099/3732], Loss: 3.5493, accuracy: 7.9264%\n",
      "Epoch [1/2], Step [2100/3732], Loss: 3.4501, accuracy: 7.9286%\n",
      "Epoch [1/2], Step [2101/3732], Loss: 3.5448, accuracy: 7.9248%\n",
      "Epoch [1/2], Step [2102/3732], Loss: 3.4488, accuracy: 7.9329%\n",
      "Epoch [1/2], Step [2103/3732], Loss: 3.0489, accuracy: 7.9351%\n",
      "Epoch [1/2], Step [2104/3732], Loss: 3.2986, accuracy: 7.9373%\n",
      "Epoch [1/2], Step [2105/3732], Loss: 3.5559, accuracy: 7.9394%\n",
      "Epoch [1/2], Step [2106/3732], Loss: 3.3279, accuracy: 7.9475%\n",
      "Epoch [1/2], Step [2107/3732], Loss: 3.7394, accuracy: 7.9438%\n",
      "Epoch [1/2], Step [2108/3732], Loss: 3.6986, accuracy: 7.9459%\n",
      "Epoch [1/2], Step [2109/3732], Loss: 3.4703, accuracy: 7.9481%\n",
      "Epoch [1/2], Step [2110/3732], Loss: 3.4431, accuracy: 7.9502%\n",
      "Epoch [1/2], Step [2111/3732], Loss: 3.0887, accuracy: 7.9583%\n",
      "Epoch [1/2], Step [2112/3732], Loss: 3.5523, accuracy: 7.9605%\n",
      "Epoch [1/2], Step [2113/3732], Loss: 3.1908, accuracy: 7.9626%\n",
      "Epoch [1/2], Step [2114/3732], Loss: 3.0902, accuracy: 7.9707%\n",
      "Epoch [1/2], Step [2115/3732], Loss: 3.8845, accuracy: 7.9669%\n",
      "Epoch [1/2], Step [2116/3732], Loss: 3.8834, accuracy: 7.9690%\n",
      "Epoch [1/2], Step [2117/3732], Loss: 3.6680, accuracy: 7.9712%\n",
      "Epoch [1/2], Step [2118/3732], Loss: 3.4124, accuracy: 7.9733%\n",
      "Epoch [1/2], Step [2119/3732], Loss: 3.8717, accuracy: 7.9696%\n",
      "Epoch [1/2], Step [2120/3732], Loss: 3.3092, accuracy: 7.9717%\n",
      "Epoch [1/2], Step [2121/3732], Loss: 3.5047, accuracy: 7.9738%\n",
      "Epoch [1/2], Step [2122/3732], Loss: 3.4295, accuracy: 7.9760%\n",
      "Epoch [1/2], Step [2123/3732], Loss: 3.6594, accuracy: 7.9781%\n",
      "Epoch [1/2], Step [2124/3732], Loss: 4.1302, accuracy: 7.9802%\n",
      "Epoch [1/2], Step [2125/3732], Loss: 3.6740, accuracy: 7.9765%\n",
      "Epoch [1/2], Step [2126/3732], Loss: 3.7832, accuracy: 7.9786%\n",
      "Epoch [1/2], Step [2127/3732], Loss: 3.5629, accuracy: 7.9748%\n",
      "Epoch [1/2], Step [2128/3732], Loss: 3.7500, accuracy: 7.9770%\n",
      "Epoch [1/2], Step [2129/3732], Loss: 3.6881, accuracy: 7.9791%\n",
      "Epoch [1/2], Step [2130/3732], Loss: 3.4967, accuracy: 7.9754%\n",
      "Epoch [1/2], Step [2131/3732], Loss: 3.3461, accuracy: 7.9833%\n",
      "Epoch [1/2], Step [2132/3732], Loss: 3.3478, accuracy: 7.9796%\n",
      "Epoch [1/2], Step [2133/3732], Loss: 2.7917, accuracy: 7.9934%\n",
      "Epoch [1/2], Step [2134/3732], Loss: 3.5733, accuracy: 7.9955%\n",
      "Epoch [1/2], Step [2135/3732], Loss: 3.6590, accuracy: 7.9977%\n",
      "Epoch [1/2], Step [2136/3732], Loss: 3.3751, accuracy: 7.9939%\n",
      "Epoch [1/2], Step [2137/3732], Loss: 3.3476, accuracy: 7.9960%\n",
      "Epoch [1/2], Step [2138/3732], Loss: 3.4615, accuracy: 7.9981%\n",
      "Epoch [1/2], Step [2139/3732], Loss: 3.4714, accuracy: 8.0002%\n",
      "Epoch [1/2], Step [2140/3732], Loss: 2.9558, accuracy: 8.0140%\n",
      "Epoch [1/2], Step [2141/3732], Loss: 3.0660, accuracy: 8.0278%\n",
      "Epoch [1/2], Step [2142/3732], Loss: 3.7540, accuracy: 8.0299%\n",
      "Epoch [1/2], Step [2143/3732], Loss: 3.8479, accuracy: 8.0320%\n",
      "Epoch [1/2], Step [2144/3732], Loss: 3.7781, accuracy: 8.0340%\n",
      "Epoch [1/2], Step [2145/3732], Loss: 3.4308, accuracy: 8.0361%\n",
      "Epoch [1/2], Step [2146/3732], Loss: 3.3304, accuracy: 8.0324%\n",
      "Epoch [1/2], Step [2147/3732], Loss: 4.2058, accuracy: 8.0345%\n",
      "Epoch [1/2], Step [2148/3732], Loss: 3.2493, accuracy: 8.0424%\n",
      "Epoch [1/2], Step [2149/3732], Loss: 3.3984, accuracy: 8.0444%\n",
      "Epoch [1/2], Step [2150/3732], Loss: 3.1564, accuracy: 8.0523%\n",
      "Epoch [1/2], Step [2151/3732], Loss: 3.4462, accuracy: 8.0486%\n",
      "Epoch [1/2], Step [2152/3732], Loss: 3.3718, accuracy: 8.0448%\n",
      "Epoch [1/2], Step [2153/3732], Loss: 3.1360, accuracy: 8.0527%\n",
      "Epoch [1/2], Step [2154/3732], Loss: 3.8947, accuracy: 8.0490%\n",
      "Epoch [1/2], Step [2155/3732], Loss: 3.6663, accuracy: 8.0452%\n",
      "Epoch [1/2], Step [2156/3732], Loss: 4.3333, accuracy: 8.0415%\n",
      "Epoch [1/2], Step [2157/3732], Loss: 3.1358, accuracy: 8.0436%\n",
      "Epoch [1/2], Step [2158/3732], Loss: 3.3042, accuracy: 8.0399%\n",
      "Epoch [1/2], Step [2159/3732], Loss: 3.4893, accuracy: 8.0419%\n",
      "Epoch [1/2], Step [2160/3732], Loss: 2.8677, accuracy: 8.0498%\n",
      "Epoch [1/2], Step [2161/3732], Loss: 3.5689, accuracy: 8.0576%\n",
      "Epoch [1/2], Step [2162/3732], Loss: 3.8789, accuracy: 8.0539%\n",
      "Epoch [1/2], Step [2163/3732], Loss: 3.7003, accuracy: 8.0502%\n",
      "Epoch [1/2], Step [2164/3732], Loss: 3.2283, accuracy: 8.0464%\n",
      "Epoch [1/2], Step [2165/3732], Loss: 3.4994, accuracy: 8.0427%\n",
      "Epoch [1/2], Step [2166/3732], Loss: 2.8539, accuracy: 8.0563%\n",
      "Epoch [1/2], Step [2167/3732], Loss: 3.1989, accuracy: 8.0641%\n",
      "Epoch [1/2], Step [2168/3732], Loss: 3.8745, accuracy: 8.0604%\n",
      "Epoch [1/2], Step [2169/3732], Loss: 3.7241, accuracy: 8.0567%\n",
      "Epoch [1/2], Step [2170/3732], Loss: 3.7612, accuracy: 8.0530%\n",
      "Epoch [1/2], Step [2171/3732], Loss: 3.6226, accuracy: 8.0493%\n",
      "Epoch [1/2], Step [2172/3732], Loss: 3.3672, accuracy: 8.0571%\n",
      "Epoch [1/2], Step [2173/3732], Loss: 3.1376, accuracy: 8.0591%\n",
      "Epoch [1/2], Step [2174/3732], Loss: 3.6856, accuracy: 8.0554%\n",
      "Epoch [1/2], Step [2175/3732], Loss: 3.7211, accuracy: 8.0517%\n",
      "Epoch [1/2], Step [2176/3732], Loss: 3.7944, accuracy: 8.0480%\n",
      "Epoch [1/2], Step [2177/3732], Loss: 3.1767, accuracy: 8.0501%\n",
      "Epoch [1/2], Step [2178/3732], Loss: 4.1229, accuracy: 8.0464%\n",
      "Epoch [1/2], Step [2179/3732], Loss: 3.1868, accuracy: 8.0484%\n",
      "Epoch [1/2], Step [2180/3732], Loss: 3.8374, accuracy: 8.0447%\n",
      "Epoch [1/2], Step [2181/3732], Loss: 3.0611, accuracy: 8.0525%\n",
      "Epoch [1/2], Step [2182/3732], Loss: 3.2693, accuracy: 8.0603%\n",
      "Epoch [1/2], Step [2183/3732], Loss: 3.6907, accuracy: 8.0623%\n",
      "Epoch [1/2], Step [2184/3732], Loss: 3.3177, accuracy: 8.0643%\n",
      "Epoch [1/2], Step [2185/3732], Loss: 3.3772, accuracy: 8.0606%\n",
      "Epoch [1/2], Step [2186/3732], Loss: 3.4944, accuracy: 8.0627%\n",
      "Epoch [1/2], Step [2187/3732], Loss: 3.5482, accuracy: 8.0704%\n",
      "Epoch [1/2], Step [2188/3732], Loss: 3.4950, accuracy: 8.0667%\n",
      "Epoch [1/2], Step [2189/3732], Loss: 3.1212, accuracy: 8.0688%\n",
      "Epoch [1/2], Step [2190/3732], Loss: 4.2791, accuracy: 8.0651%\n",
      "Epoch [1/2], Step [2191/3732], Loss: 2.9321, accuracy: 8.0671%\n",
      "Epoch [1/2], Step [2192/3732], Loss: 3.8586, accuracy: 8.0691%\n",
      "Epoch [1/2], Step [2193/3732], Loss: 3.4451, accuracy: 8.0711%\n",
      "Epoch [1/2], Step [2194/3732], Loss: 3.3149, accuracy: 8.0732%\n",
      "Epoch [1/2], Step [2195/3732], Loss: 3.6342, accuracy: 8.0752%\n",
      "Epoch [1/2], Step [2196/3732], Loss: 3.4476, accuracy: 8.0772%\n",
      "Epoch [1/2], Step [2197/3732], Loss: 2.8186, accuracy: 8.0849%\n",
      "Epoch [1/2], Step [2198/3732], Loss: 3.3271, accuracy: 8.0869%\n",
      "Epoch [1/2], Step [2199/3732], Loss: 3.2832, accuracy: 8.0946%\n",
      "Epoch [1/2], Step [2200/3732], Loss: 3.5303, accuracy: 8.0909%\n",
      "Epoch [1/2], Step [2201/3732], Loss: 3.9074, accuracy: 8.0929%\n",
      "Epoch [1/2], Step [2202/3732], Loss: 3.2251, accuracy: 8.0949%\n",
      "Epoch [1/2], Step [2203/3732], Loss: 3.2840, accuracy: 8.0969%\n",
      "Epoch [1/2], Step [2204/3732], Loss: 3.1933, accuracy: 8.1046%\n",
      "Epoch [1/2], Step [2205/3732], Loss: 3.7157, accuracy: 8.1009%\n",
      "Epoch [1/2], Step [2206/3732], Loss: 4.0037, accuracy: 8.0972%\n",
      "Epoch [1/2], Step [2207/3732], Loss: 3.4069, accuracy: 8.0936%\n",
      "Epoch [1/2], Step [2208/3732], Loss: 3.8539, accuracy: 8.0899%\n",
      "Epoch [1/2], Step [2209/3732], Loss: 3.2490, accuracy: 8.0919%\n",
      "Epoch [1/2], Step [2210/3732], Loss: 2.7937, accuracy: 8.1052%\n",
      "Epoch [1/2], Step [2211/3732], Loss: 3.5552, accuracy: 8.1015%\n",
      "Epoch [1/2], Step [2212/3732], Loss: 3.8902, accuracy: 8.0979%\n",
      "Epoch [1/2], Step [2213/3732], Loss: 3.2776, accuracy: 8.0942%\n",
      "Epoch [1/2], Step [2214/3732], Loss: 3.6325, accuracy: 8.0906%\n",
      "Epoch [1/2], Step [2215/3732], Loss: 3.3559, accuracy: 8.0926%\n",
      "Epoch [1/2], Step [2216/3732], Loss: 3.8047, accuracy: 8.0945%\n",
      "Epoch [1/2], Step [2217/3732], Loss: 3.2035, accuracy: 8.0909%\n",
      "Epoch [1/2], Step [2218/3732], Loss: 3.6560, accuracy: 8.0872%\n",
      "Epoch [1/2], Step [2219/3732], Loss: 3.1495, accuracy: 8.0892%\n",
      "Epoch [1/2], Step [2220/3732], Loss: 3.3856, accuracy: 8.0912%\n",
      "Epoch [1/2], Step [2221/3732], Loss: 3.4155, accuracy: 8.0876%\n",
      "Epoch [1/2], Step [2222/3732], Loss: 3.0158, accuracy: 8.0839%\n",
      "Epoch [1/2], Step [2223/3732], Loss: 3.6156, accuracy: 8.0803%\n",
      "Epoch [1/2], Step [2224/3732], Loss: 3.3735, accuracy: 8.0823%\n",
      "Epoch [1/2], Step [2225/3732], Loss: 3.6488, accuracy: 8.0843%\n",
      "Epoch [1/2], Step [2226/3732], Loss: 3.4489, accuracy: 8.0863%\n",
      "Epoch [1/2], Step [2227/3732], Loss: 3.2765, accuracy: 8.0882%\n",
      "Epoch [1/2], Step [2228/3732], Loss: 3.4322, accuracy: 8.0902%\n",
      "Epoch [1/2], Step [2229/3732], Loss: 3.9925, accuracy: 8.0866%\n",
      "Epoch [1/2], Step [2230/3732], Loss: 3.1777, accuracy: 8.0998%\n",
      "Epoch [1/2], Step [2231/3732], Loss: 3.1856, accuracy: 8.1074%\n",
      "Epoch [1/2], Step [2232/3732], Loss: 3.2783, accuracy: 8.1093%\n",
      "Epoch [1/2], Step [2233/3732], Loss: 3.1683, accuracy: 8.1169%\n",
      "Epoch [1/2], Step [2234/3732], Loss: 3.3461, accuracy: 8.1188%\n",
      "Epoch [1/2], Step [2235/3732], Loss: 3.7468, accuracy: 8.1208%\n",
      "Epoch [1/2], Step [2236/3732], Loss: 3.3158, accuracy: 8.1284%\n",
      "Epoch [1/2], Step [2237/3732], Loss: 3.7982, accuracy: 8.1247%\n",
      "Epoch [1/2], Step [2238/3732], Loss: 4.0753, accuracy: 8.1267%\n",
      "Epoch [1/2], Step [2239/3732], Loss: 3.9640, accuracy: 8.1230%\n",
      "Epoch [1/2], Step [2240/3732], Loss: 3.5899, accuracy: 8.1250%\n",
      "Epoch [1/2], Step [2241/3732], Loss: 3.0757, accuracy: 8.1325%\n",
      "Epoch [1/2], Step [2242/3732], Loss: 3.3440, accuracy: 8.1345%\n",
      "Epoch [1/2], Step [2243/3732], Loss: 3.3959, accuracy: 8.1364%\n",
      "Epoch [1/2], Step [2244/3732], Loss: 3.4743, accuracy: 8.1328%\n",
      "Epoch [1/2], Step [2245/3732], Loss: 3.7083, accuracy: 8.1292%\n",
      "Epoch [1/2], Step [2246/3732], Loss: 2.7978, accuracy: 8.1311%\n",
      "Epoch [1/2], Step [2247/3732], Loss: 3.0522, accuracy: 8.1386%\n",
      "Epoch [1/2], Step [2248/3732], Loss: 3.3581, accuracy: 8.1406%\n",
      "Epoch [1/2], Step [2249/3732], Loss: 3.4974, accuracy: 8.1369%\n",
      "Epoch [1/2], Step [2250/3732], Loss: 3.1842, accuracy: 8.1444%\n",
      "Epoch [1/2], Step [2251/3732], Loss: 2.8576, accuracy: 8.1519%\n",
      "Epoch [1/2], Step [2252/3732], Loss: 3.3952, accuracy: 8.1483%\n",
      "Epoch [1/2], Step [2253/3732], Loss: 3.5700, accuracy: 8.1447%\n",
      "Epoch [1/2], Step [2254/3732], Loss: 2.9407, accuracy: 8.1522%\n",
      "Epoch [1/2], Step [2255/3732], Loss: 3.5451, accuracy: 8.1486%\n",
      "Epoch [1/2], Step [2256/3732], Loss: 4.1474, accuracy: 8.1449%\n",
      "Epoch [1/2], Step [2257/3732], Loss: 3.0508, accuracy: 8.1524%\n",
      "Epoch [1/2], Step [2258/3732], Loss: 3.1954, accuracy: 8.1488%\n",
      "Epoch [1/2], Step [2259/3732], Loss: 2.9189, accuracy: 8.1618%\n",
      "Epoch [1/2], Step [2260/3732], Loss: 3.6376, accuracy: 8.1637%\n",
      "Epoch [1/2], Step [2261/3732], Loss: 3.1532, accuracy: 8.1712%\n",
      "Epoch [1/2], Step [2262/3732], Loss: 3.6593, accuracy: 8.1731%\n",
      "Epoch [1/2], Step [2263/3732], Loss: 3.7532, accuracy: 8.1695%\n",
      "Epoch [1/2], Step [2264/3732], Loss: 3.0141, accuracy: 8.1714%\n",
      "Epoch [1/2], Step [2265/3732], Loss: 3.2461, accuracy: 8.1678%\n",
      "Epoch [1/2], Step [2266/3732], Loss: 3.4137, accuracy: 8.1752%\n",
      "Epoch [1/2], Step [2267/3732], Loss: 3.7733, accuracy: 8.1716%\n",
      "Epoch [1/2], Step [2268/3732], Loss: 3.6711, accuracy: 8.1735%\n",
      "Epoch [1/2], Step [2269/3732], Loss: 3.3910, accuracy: 8.1699%\n",
      "Epoch [1/2], Step [2270/3732], Loss: 3.0878, accuracy: 8.1663%\n",
      "Epoch [1/2], Step [2271/3732], Loss: 3.5402, accuracy: 8.1682%\n",
      "Epoch [1/2], Step [2272/3732], Loss: 3.2994, accuracy: 8.1701%\n",
      "Epoch [1/2], Step [2273/3732], Loss: 3.2282, accuracy: 8.1720%\n",
      "Epoch [1/2], Step [2274/3732], Loss: 3.6164, accuracy: 8.1739%\n",
      "Epoch [1/2], Step [2275/3732], Loss: 3.3026, accuracy: 8.1703%\n",
      "Epoch [1/2], Step [2276/3732], Loss: 3.6854, accuracy: 8.1777%\n",
      "Epoch [1/2], Step [2277/3732], Loss: 2.9249, accuracy: 8.1851%\n",
      "Epoch [1/2], Step [2278/3732], Loss: 3.4199, accuracy: 8.1815%\n",
      "Epoch [1/2], Step [2279/3732], Loss: 2.7114, accuracy: 8.1889%\n",
      "Epoch [1/2], Step [2280/3732], Loss: 3.2193, accuracy: 8.1908%\n",
      "Epoch [1/2], Step [2281/3732], Loss: 3.3269, accuracy: 8.1872%\n",
      "Epoch [1/2], Step [2282/3732], Loss: 2.8440, accuracy: 8.1946%\n",
      "Epoch [1/2], Step [2283/3732], Loss: 3.7566, accuracy: 8.1910%\n",
      "Epoch [1/2], Step [2284/3732], Loss: 3.1494, accuracy: 8.1983%\n",
      "Epoch [1/2], Step [2285/3732], Loss: 2.9698, accuracy: 8.2002%\n",
      "Epoch [1/2], Step [2286/3732], Loss: 4.1938, accuracy: 8.1966%\n",
      "Epoch [1/2], Step [2287/3732], Loss: 3.8797, accuracy: 8.1930%\n",
      "Epoch [1/2], Step [2288/3732], Loss: 3.1518, accuracy: 8.1949%\n",
      "Epoch [1/2], Step [2289/3732], Loss: 3.7045, accuracy: 8.1913%\n",
      "Epoch [1/2], Step [2290/3732], Loss: 3.7899, accuracy: 8.1878%\n",
      "Epoch [1/2], Step [2291/3732], Loss: 2.9643, accuracy: 8.1897%\n",
      "Epoch [1/2], Step [2292/3732], Loss: 3.1487, accuracy: 8.1970%\n",
      "Epoch [1/2], Step [2293/3732], Loss: 3.3665, accuracy: 8.1934%\n",
      "Epoch [1/2], Step [2294/3732], Loss: 3.5444, accuracy: 8.1898%\n",
      "Epoch [1/2], Step [2295/3732], Loss: 3.8239, accuracy: 8.1917%\n",
      "Epoch [1/2], Step [2296/3732], Loss: 2.9070, accuracy: 8.1936%\n",
      "Epoch [1/2], Step [2297/3732], Loss: 3.4156, accuracy: 8.1955%\n",
      "Epoch [1/2], Step [2298/3732], Loss: 3.4953, accuracy: 8.1919%\n",
      "Epoch [1/2], Step [2299/3732], Loss: 3.4984, accuracy: 8.1938%\n",
      "Epoch [1/2], Step [2300/3732], Loss: 3.3740, accuracy: 8.2011%\n",
      "Epoch [1/2], Step [2301/3732], Loss: 2.9377, accuracy: 8.2030%\n",
      "Epoch [1/2], Step [2302/3732], Loss: 3.3786, accuracy: 8.1994%\n",
      "Epoch [1/2], Step [2303/3732], Loss: 3.5540, accuracy: 8.2067%\n",
      "Epoch [1/2], Step [2304/3732], Loss: 3.5446, accuracy: 8.2086%\n",
      "Epoch [1/2], Step [2305/3732], Loss: 3.6661, accuracy: 8.2050%\n",
      "Epoch [1/2], Step [2306/3732], Loss: 3.2547, accuracy: 8.2177%\n",
      "Epoch [1/2], Step [2307/3732], Loss: 3.2112, accuracy: 8.2195%\n",
      "Epoch [1/2], Step [2308/3732], Loss: 3.8686, accuracy: 8.2160%\n",
      "Epoch [1/2], Step [2309/3732], Loss: 3.6002, accuracy: 8.2124%\n",
      "Epoch [1/2], Step [2310/3732], Loss: 2.7724, accuracy: 8.2197%\n",
      "Epoch [1/2], Step [2311/3732], Loss: 4.0756, accuracy: 8.2215%\n",
      "Epoch [1/2], Step [2312/3732], Loss: 3.3124, accuracy: 8.2288%\n",
      "Epoch [1/2], Step [2313/3732], Loss: 3.2230, accuracy: 8.2252%\n",
      "Epoch [1/2], Step [2314/3732], Loss: 3.3246, accuracy: 8.2271%\n",
      "Epoch [1/2], Step [2315/3732], Loss: 3.3680, accuracy: 8.2235%\n",
      "Epoch [1/2], Step [2316/3732], Loss: 3.3255, accuracy: 8.2254%\n",
      "Epoch [1/2], Step [2317/3732], Loss: 3.8405, accuracy: 8.2218%\n",
      "Epoch [1/2], Step [2318/3732], Loss: 2.8424, accuracy: 8.2291%\n",
      "Epoch [1/2], Step [2319/3732], Loss: 3.4592, accuracy: 8.2255%\n",
      "Epoch [1/2], Step [2320/3732], Loss: 3.4079, accuracy: 8.2220%\n",
      "Epoch [1/2], Step [2321/3732], Loss: 2.9629, accuracy: 8.2292%\n",
      "Epoch [1/2], Step [2322/3732], Loss: 4.0940, accuracy: 8.2257%\n",
      "Epoch [1/2], Step [2323/3732], Loss: 3.0190, accuracy: 8.2221%\n",
      "Epoch [1/2], Step [2324/3732], Loss: 4.0220, accuracy: 8.2186%\n",
      "Epoch [1/2], Step [2325/3732], Loss: 3.2904, accuracy: 8.2204%\n",
      "Epoch [1/2], Step [2326/3732], Loss: 3.6170, accuracy: 8.2169%\n",
      "Epoch [1/2], Step [2327/3732], Loss: 3.4373, accuracy: 8.2134%\n",
      "Epoch [1/2], Step [2328/3732], Loss: 3.0239, accuracy: 8.2206%\n",
      "Epoch [1/2], Step [2329/3732], Loss: 3.7862, accuracy: 8.2170%\n",
      "Epoch [1/2], Step [2330/3732], Loss: 3.5010, accuracy: 8.2189%\n",
      "Epoch [1/2], Step [2331/3732], Loss: 3.5485, accuracy: 8.2207%\n",
      "Epoch [1/2], Step [2332/3732], Loss: 3.0032, accuracy: 8.2226%\n",
      "Epoch [1/2], Step [2333/3732], Loss: 3.2532, accuracy: 8.2297%\n",
      "Epoch [1/2], Step [2334/3732], Loss: 3.7682, accuracy: 8.2262%\n",
      "Epoch [1/2], Step [2335/3732], Loss: 3.5214, accuracy: 8.2227%\n",
      "Epoch [1/2], Step [2336/3732], Loss: 3.6936, accuracy: 8.2192%\n",
      "Epoch [1/2], Step [2337/3732], Loss: 3.4046, accuracy: 8.2157%\n",
      "Epoch [1/2], Step [2338/3732], Loss: 3.2043, accuracy: 8.2228%\n",
      "Epoch [1/2], Step [2339/3732], Loss: 3.6396, accuracy: 8.2193%\n",
      "Epoch [1/2], Step [2340/3732], Loss: 3.1727, accuracy: 8.2212%\n",
      "Epoch [1/2], Step [2341/3732], Loss: 3.7329, accuracy: 8.2176%\n",
      "Epoch [1/2], Step [2342/3732], Loss: 3.3738, accuracy: 8.2195%\n",
      "Epoch [1/2], Step [2343/3732], Loss: 3.1091, accuracy: 8.2213%\n",
      "Epoch [1/2], Step [2344/3732], Loss: 3.3510, accuracy: 8.2231%\n",
      "Epoch [1/2], Step [2345/3732], Loss: 3.6249, accuracy: 8.2249%\n",
      "Epoch [1/2], Step [2346/3732], Loss: 3.5444, accuracy: 8.2214%\n",
      "Epoch [1/2], Step [2347/3732], Loss: 3.7627, accuracy: 8.2233%\n",
      "Epoch [1/2], Step [2348/3732], Loss: 3.2121, accuracy: 8.2251%\n",
      "Epoch [1/2], Step [2349/3732], Loss: 3.0869, accuracy: 8.2216%\n",
      "Epoch [1/2], Step [2350/3732], Loss: 3.9015, accuracy: 8.2234%\n",
      "Epoch [1/2], Step [2351/3732], Loss: 3.5749, accuracy: 8.2199%\n",
      "Epoch [1/2], Step [2352/3732], Loss: 3.6893, accuracy: 8.2164%\n",
      "Epoch [1/2], Step [2353/3732], Loss: 3.6910, accuracy: 8.2129%\n",
      "Epoch [1/2], Step [2354/3732], Loss: 3.9969, accuracy: 8.2094%\n",
      "Epoch [1/2], Step [2355/3732], Loss: 3.8054, accuracy: 8.2113%\n",
      "Epoch [1/2], Step [2356/3732], Loss: 3.4298, accuracy: 8.2131%\n",
      "Epoch [1/2], Step [2357/3732], Loss: 3.5594, accuracy: 8.2096%\n",
      "Epoch [1/2], Step [2358/3732], Loss: 3.2817, accuracy: 8.2167%\n",
      "Epoch [1/2], Step [2359/3732], Loss: 3.3132, accuracy: 8.2291%\n",
      "Epoch [1/2], Step [2360/3732], Loss: 3.1995, accuracy: 8.2309%\n",
      "Epoch [1/2], Step [2361/3732], Loss: 3.2003, accuracy: 8.2380%\n",
      "Epoch [1/2], Step [2362/3732], Loss: 3.3118, accuracy: 8.2451%\n",
      "Epoch [1/2], Step [2363/3732], Loss: 3.4596, accuracy: 8.2469%\n",
      "Epoch [1/2], Step [2364/3732], Loss: 3.3347, accuracy: 8.2434%\n",
      "Epoch [1/2], Step [2365/3732], Loss: 3.7281, accuracy: 8.2505%\n",
      "Epoch [1/2], Step [2366/3732], Loss: 3.6092, accuracy: 8.2470%\n",
      "Epoch [1/2], Step [2367/3732], Loss: 3.2672, accuracy: 8.2541%\n",
      "Epoch [1/2], Step [2368/3732], Loss: 3.8702, accuracy: 8.2506%\n",
      "Epoch [1/2], Step [2369/3732], Loss: 3.6470, accuracy: 8.2472%\n",
      "Epoch [1/2], Step [2370/3732], Loss: 3.2270, accuracy: 8.2489%\n",
      "Epoch [1/2], Step [2371/3732], Loss: 2.8645, accuracy: 8.2560%\n",
      "Epoch [1/2], Step [2372/3732], Loss: 3.8285, accuracy: 8.2578%\n",
      "Epoch [1/2], Step [2373/3732], Loss: 4.4143, accuracy: 8.2543%\n",
      "Epoch [1/2], Step [2374/3732], Loss: 3.2170, accuracy: 8.2614%\n",
      "Epoch [1/2], Step [2375/3732], Loss: 2.9520, accuracy: 8.2684%\n",
      "Epoch [1/2], Step [2376/3732], Loss: 3.3596, accuracy: 8.2649%\n",
      "Epoch [1/2], Step [2377/3732], Loss: 3.5663, accuracy: 8.2667%\n",
      "Epoch [1/2], Step [2378/3732], Loss: 3.4894, accuracy: 8.2685%\n",
      "Epoch [1/2], Step [2379/3732], Loss: 3.3709, accuracy: 8.2703%\n",
      "Epoch [1/2], Step [2380/3732], Loss: 2.4517, accuracy: 8.2826%\n",
      "Epoch [1/2], Step [2381/3732], Loss: 3.4282, accuracy: 8.2896%\n",
      "Epoch [1/2], Step [2382/3732], Loss: 3.5146, accuracy: 8.2861%\n",
      "Epoch [1/2], Step [2383/3732], Loss: 3.6560, accuracy: 8.2826%\n",
      "Epoch [1/2], Step [2384/3732], Loss: 3.4674, accuracy: 8.2844%\n",
      "Epoch [1/2], Step [2385/3732], Loss: 3.9887, accuracy: 8.2809%\n",
      "Epoch [1/2], Step [2386/3732], Loss: 3.5075, accuracy: 8.2775%\n",
      "Epoch [1/2], Step [2387/3732], Loss: 3.4805, accuracy: 8.2740%\n",
      "Epoch [1/2], Step [2388/3732], Loss: 3.7347, accuracy: 8.2758%\n",
      "Epoch [1/2], Step [2389/3732], Loss: 3.4407, accuracy: 8.2775%\n",
      "Epoch [1/2], Step [2390/3732], Loss: 3.7693, accuracy: 8.2793%\n",
      "Epoch [1/2], Step [2391/3732], Loss: 3.5503, accuracy: 8.2758%\n",
      "Epoch [1/2], Step [2392/3732], Loss: 3.6273, accuracy: 8.2776%\n",
      "Epoch [1/2], Step [2393/3732], Loss: 3.7380, accuracy: 8.2794%\n",
      "Epoch [1/2], Step [2394/3732], Loss: 3.7105, accuracy: 8.2759%\n",
      "Epoch [1/2], Step [2395/3732], Loss: 3.3194, accuracy: 8.2777%\n",
      "Epoch [1/2], Step [2396/3732], Loss: 3.4963, accuracy: 8.2742%\n",
      "Epoch [1/2], Step [2397/3732], Loss: 3.6261, accuracy: 8.2760%\n",
      "Epoch [1/2], Step [2398/3732], Loss: 3.6439, accuracy: 8.2777%\n",
      "Epoch [1/2], Step [2399/3732], Loss: 2.9881, accuracy: 8.2899%\n",
      "Epoch [1/2], Step [2400/3732], Loss: 3.5007, accuracy: 8.2865%\n",
      "Epoch [1/2], Step [2401/3732], Loss: 3.0467, accuracy: 8.2882%\n",
      "Epoch [1/2], Step [2402/3732], Loss: 3.5194, accuracy: 8.2900%\n",
      "Epoch [1/2], Step [2403/3732], Loss: 3.3772, accuracy: 8.2917%\n",
      "Epoch [1/2], Step [2404/3732], Loss: 3.5994, accuracy: 8.2935%\n",
      "Epoch [1/2], Step [2405/3732], Loss: 3.3692, accuracy: 8.3004%\n",
      "Epoch [1/2], Step [2406/3732], Loss: 3.1254, accuracy: 8.3074%\n",
      "Epoch [1/2], Step [2407/3732], Loss: 3.6773, accuracy: 8.3039%\n",
      "Epoch [1/2], Step [2408/3732], Loss: 3.2670, accuracy: 8.3056%\n",
      "Epoch [1/2], Step [2409/3732], Loss: 3.6747, accuracy: 8.3022%\n",
      "Epoch [1/2], Step [2410/3732], Loss: 3.1022, accuracy: 8.3091%\n",
      "Epoch [1/2], Step [2411/3732], Loss: 3.2524, accuracy: 8.3109%\n",
      "Epoch [1/2], Step [2412/3732], Loss: 3.1558, accuracy: 8.3126%\n",
      "Epoch [1/2], Step [2413/3732], Loss: 3.2013, accuracy: 8.3195%\n",
      "Epoch [1/2], Step [2414/3732], Loss: 3.2486, accuracy: 8.3213%\n",
      "Epoch [1/2], Step [2415/3732], Loss: 3.1409, accuracy: 8.3230%\n",
      "Epoch [1/2], Step [2416/3732], Loss: 3.5825, accuracy: 8.3299%\n",
      "Epoch [1/2], Step [2417/3732], Loss: 2.7107, accuracy: 8.3316%\n",
      "Epoch [1/2], Step [2418/3732], Loss: 3.4007, accuracy: 8.3333%\n",
      "Epoch [1/2], Step [2419/3732], Loss: 2.6335, accuracy: 8.3454%\n",
      "Epoch [1/2], Step [2420/3732], Loss: 3.4023, accuracy: 8.3523%\n",
      "Epoch [1/2], Step [2421/3732], Loss: 3.2423, accuracy: 8.3488%\n",
      "Epoch [1/2], Step [2422/3732], Loss: 3.6402, accuracy: 8.3505%\n",
      "Epoch [1/2], Step [2423/3732], Loss: 3.8922, accuracy: 8.3522%\n",
      "Epoch [1/2], Step [2424/3732], Loss: 3.3616, accuracy: 8.3591%\n",
      "Epoch [1/2], Step [2425/3732], Loss: 3.2664, accuracy: 8.3608%\n",
      "Epoch [1/2], Step [2426/3732], Loss: 3.2382, accuracy: 8.3625%\n",
      "Epoch [1/2], Step [2427/3732], Loss: 2.7229, accuracy: 8.3694%\n",
      "Epoch [1/2], Step [2428/3732], Loss: 3.2744, accuracy: 8.3711%\n",
      "Epoch [1/2], Step [2429/3732], Loss: 3.2738, accuracy: 8.3779%\n",
      "Epoch [1/2], Step [2430/3732], Loss: 2.9752, accuracy: 8.3848%\n",
      "Epoch [1/2], Step [2431/3732], Loss: 3.4537, accuracy: 8.3865%\n",
      "Epoch [1/2], Step [2432/3732], Loss: 3.5167, accuracy: 8.3882%\n",
      "Epoch [1/2], Step [2433/3732], Loss: 3.6266, accuracy: 8.3847%\n",
      "Epoch [1/2], Step [2434/3732], Loss: 2.7627, accuracy: 8.3967%\n",
      "Epoch [1/2], Step [2435/3732], Loss: 3.4576, accuracy: 8.3932%\n",
      "Epoch [1/2], Step [2436/3732], Loss: 3.6939, accuracy: 8.3898%\n",
      "Epoch [1/2], Step [2437/3732], Loss: 3.4782, accuracy: 8.3863%\n",
      "Epoch [1/2], Step [2438/3732], Loss: 3.3157, accuracy: 8.3880%\n",
      "Epoch [1/2], Step [2439/3732], Loss: 3.6482, accuracy: 8.3846%\n",
      "Epoch [1/2], Step [2440/3732], Loss: 3.7424, accuracy: 8.3811%\n",
      "Epoch [1/2], Step [2441/3732], Loss: 3.2390, accuracy: 8.3828%\n",
      "Epoch [1/2], Step [2442/3732], Loss: 3.4319, accuracy: 8.3845%\n",
      "Epoch [1/2], Step [2443/3732], Loss: 3.1666, accuracy: 8.3913%\n",
      "Epoch [1/2], Step [2444/3732], Loss: 3.4255, accuracy: 8.3879%\n",
      "Epoch [1/2], Step [2445/3732], Loss: 3.5169, accuracy: 8.3896%\n",
      "Epoch [1/2], Step [2446/3732], Loss: 2.8301, accuracy: 8.3913%\n",
      "Epoch [1/2], Step [2447/3732], Loss: 3.4788, accuracy: 8.3878%\n",
      "Epoch [1/2], Step [2448/3732], Loss: 3.5375, accuracy: 8.3946%\n",
      "Epoch [1/2], Step [2449/3732], Loss: 3.8592, accuracy: 8.3912%\n",
      "Epoch [1/2], Step [2450/3732], Loss: 3.2716, accuracy: 8.3929%\n",
      "Epoch [1/2], Step [2451/3732], Loss: 3.0311, accuracy: 8.3945%\n",
      "Epoch [1/2], Step [2452/3732], Loss: 3.7844, accuracy: 8.3962%\n",
      "Epoch [1/2], Step [2453/3732], Loss: 3.5214, accuracy: 8.3928%\n",
      "Epoch [1/2], Step [2454/3732], Loss: 3.2861, accuracy: 8.3945%\n",
      "Epoch [1/2], Step [2455/3732], Loss: 3.4682, accuracy: 8.3961%\n",
      "Epoch [1/2], Step [2456/3732], Loss: 3.5622, accuracy: 8.3978%\n",
      "Epoch [1/2], Step [2457/3732], Loss: 3.6474, accuracy: 8.3944%\n",
      "Epoch [1/2], Step [2458/3732], Loss: 3.0789, accuracy: 8.4011%\n",
      "Epoch [1/2], Step [2459/3732], Loss: 3.4163, accuracy: 8.4028%\n",
      "Epoch [1/2], Step [2460/3732], Loss: 3.6099, accuracy: 8.3994%\n",
      "Epoch [1/2], Step [2461/3732], Loss: 3.3963, accuracy: 8.4011%\n",
      "Epoch [1/2], Step [2462/3732], Loss: 3.5124, accuracy: 8.4078%\n",
      "Epoch [1/2], Step [2463/3732], Loss: 3.3477, accuracy: 8.4095%\n",
      "Epoch [1/2], Step [2464/3732], Loss: 3.9081, accuracy: 8.4060%\n",
      "Epoch [1/2], Step [2465/3732], Loss: 3.3595, accuracy: 8.4026%\n",
      "Epoch [1/2], Step [2466/3732], Loss: 3.3483, accuracy: 8.4043%\n",
      "Epoch [1/2], Step [2467/3732], Loss: 3.3382, accuracy: 8.4060%\n",
      "Epoch [1/2], Step [2468/3732], Loss: 3.1386, accuracy: 8.4127%\n",
      "Epoch [1/2], Step [2469/3732], Loss: 3.2519, accuracy: 8.4093%\n",
      "Epoch [1/2], Step [2470/3732], Loss: 3.0050, accuracy: 8.4109%\n",
      "Epoch [1/2], Step [2471/3732], Loss: 3.0448, accuracy: 8.4227%\n",
      "Epoch [1/2], Step [2472/3732], Loss: 3.3153, accuracy: 8.4244%\n",
      "Epoch [1/2], Step [2473/3732], Loss: 2.9984, accuracy: 8.4311%\n",
      "Epoch [1/2], Step [2474/3732], Loss: 3.5268, accuracy: 8.4276%\n",
      "Epoch [1/2], Step [2475/3732], Loss: 3.3205, accuracy: 8.4293%\n",
      "Epoch [1/2], Step [2476/3732], Loss: 3.2557, accuracy: 8.4259%\n",
      "Epoch [1/2], Step [2477/3732], Loss: 3.5036, accuracy: 8.4275%\n",
      "Epoch [1/2], Step [2478/3732], Loss: 3.2485, accuracy: 8.4292%\n",
      "Epoch [1/2], Step [2479/3732], Loss: 2.5819, accuracy: 8.4359%\n",
      "Epoch [1/2], Step [2480/3732], Loss: 3.8156, accuracy: 8.4325%\n",
      "Epoch [1/2], Step [2481/3732], Loss: 3.7177, accuracy: 8.4341%\n",
      "Epoch [1/2], Step [2482/3732], Loss: 3.0546, accuracy: 8.4408%\n",
      "Epoch [1/2], Step [2483/3732], Loss: 3.5790, accuracy: 8.4374%\n",
      "Epoch [1/2], Step [2484/3732], Loss: 3.0324, accuracy: 8.4440%\n",
      "Epoch [1/2], Step [2485/3732], Loss: 2.7065, accuracy: 8.4507%\n",
      "Epoch [1/2], Step [2486/3732], Loss: 3.8134, accuracy: 8.4473%\n",
      "Epoch [1/2], Step [2487/3732], Loss: 3.5351, accuracy: 8.4489%\n",
      "Epoch [1/2], Step [2488/3732], Loss: 3.7257, accuracy: 8.4506%\n",
      "Epoch [1/2], Step [2489/3732], Loss: 4.5313, accuracy: 8.4522%\n",
      "Epoch [1/2], Step [2490/3732], Loss: 3.0312, accuracy: 8.4588%\n",
      "Epoch [1/2], Step [2491/3732], Loss: 3.6795, accuracy: 8.4605%\n",
      "Epoch [1/2], Step [2492/3732], Loss: 3.3924, accuracy: 8.4621%\n",
      "Epoch [1/2], Step [2493/3732], Loss: 3.6669, accuracy: 8.4637%\n",
      "Epoch [1/2], Step [2494/3732], Loss: 3.4778, accuracy: 8.4653%\n",
      "Epoch [1/2], Step [2495/3732], Loss: 3.2530, accuracy: 8.4719%\n",
      "Epoch [1/2], Step [2496/3732], Loss: 3.5974, accuracy: 8.4786%\n",
      "Epoch [1/2], Step [2497/3732], Loss: 3.4095, accuracy: 8.4752%\n",
      "Epoch [1/2], Step [2498/3732], Loss: 3.1420, accuracy: 8.4768%\n",
      "Epoch [1/2], Step [2499/3732], Loss: 2.9579, accuracy: 8.4834%\n",
      "Epoch [1/2], Step [2500/3732], Loss: 3.2844, accuracy: 8.4850%\n",
      "Epoch [1/2], Step [2501/3732], Loss: 2.9336, accuracy: 8.4966%\n",
      "Epoch [1/2], Step [2502/3732], Loss: 2.9917, accuracy: 8.4982%\n",
      "Epoch [1/2], Step [2503/3732], Loss: 3.7977, accuracy: 8.4948%\n",
      "Epoch [1/2], Step [2504/3732], Loss: 3.5041, accuracy: 8.4964%\n",
      "Epoch [1/2], Step [2505/3732], Loss: 3.2823, accuracy: 8.4930%\n",
      "Epoch [1/2], Step [2506/3732], Loss: 3.1652, accuracy: 8.4946%\n",
      "Epoch [1/2], Step [2507/3732], Loss: 3.1609, accuracy: 8.5012%\n",
      "Epoch [1/2], Step [2508/3732], Loss: 3.1522, accuracy: 8.4978%\n",
      "Epoch [1/2], Step [2509/3732], Loss: 3.3616, accuracy: 8.4994%\n",
      "Epoch [1/2], Step [2510/3732], Loss: 3.4515, accuracy: 8.5010%\n",
      "Epoch [1/2], Step [2511/3732], Loss: 3.5884, accuracy: 8.4976%\n",
      "Epoch [1/2], Step [2512/3732], Loss: 3.2400, accuracy: 8.5042%\n",
      "Epoch [1/2], Step [2513/3732], Loss: 3.8125, accuracy: 8.5008%\n",
      "Epoch [1/2], Step [2514/3732], Loss: 2.8559, accuracy: 8.5074%\n",
      "Epoch [1/2], Step [2515/3732], Loss: 3.9136, accuracy: 8.5040%\n",
      "Epoch [1/2], Step [2516/3732], Loss: 3.7488, accuracy: 8.5056%\n",
      "Epoch [1/2], Step [2517/3732], Loss: 2.9945, accuracy: 8.5121%\n",
      "Epoch [1/2], Step [2518/3732], Loss: 3.5910, accuracy: 8.5137%\n",
      "Epoch [1/2], Step [2519/3732], Loss: 3.6685, accuracy: 8.5103%\n",
      "Epoch [1/2], Step [2520/3732], Loss: 3.3767, accuracy: 8.5069%\n",
      "Epoch [1/2], Step [2521/3732], Loss: 3.5377, accuracy: 8.5085%\n",
      "Epoch [1/2], Step [2522/3732], Loss: 2.8713, accuracy: 8.5151%\n",
      "Epoch [1/2], Step [2523/3732], Loss: 3.2767, accuracy: 8.5216%\n",
      "Epoch [1/2], Step [2524/3732], Loss: 3.7064, accuracy: 8.5182%\n",
      "Epoch [1/2], Step [2525/3732], Loss: 3.6859, accuracy: 8.5149%\n",
      "Epoch [1/2], Step [2526/3732], Loss: 3.5178, accuracy: 8.5115%\n",
      "Epoch [1/2], Step [2527/3732], Loss: 3.2638, accuracy: 8.5180%\n",
      "Epoch [1/2], Step [2528/3732], Loss: 3.3167, accuracy: 8.5245%\n",
      "Epoch [1/2], Step [2529/3732], Loss: 3.5687, accuracy: 8.5261%\n",
      "Epoch [1/2], Step [2530/3732], Loss: 3.7660, accuracy: 8.5277%\n",
      "Epoch [1/2], Step [2531/3732], Loss: 2.8398, accuracy: 8.5342%\n",
      "Epoch [1/2], Step [2532/3732], Loss: 3.0372, accuracy: 8.5357%\n",
      "Epoch [1/2], Step [2533/3732], Loss: 3.7061, accuracy: 8.5373%\n",
      "Epoch [1/2], Step [2534/3732], Loss: 3.1458, accuracy: 8.5339%\n",
      "Epoch [1/2], Step [2535/3732], Loss: 3.0505, accuracy: 8.5404%\n",
      "Epoch [1/2], Step [2536/3732], Loss: 3.8015, accuracy: 8.5371%\n",
      "Epoch [1/2], Step [2537/3732], Loss: 2.7409, accuracy: 8.5386%\n",
      "Epoch [1/2], Step [2538/3732], Loss: 3.7554, accuracy: 8.5402%\n",
      "Epoch [1/2], Step [2539/3732], Loss: 2.9818, accuracy: 8.5417%\n",
      "Epoch [1/2], Step [2540/3732], Loss: 4.1124, accuracy: 8.5433%\n",
      "Epoch [1/2], Step [2541/3732], Loss: 2.5187, accuracy: 8.5498%\n",
      "Epoch [1/2], Step [2542/3732], Loss: 3.9638, accuracy: 8.5513%\n",
      "Epoch [1/2], Step [2543/3732], Loss: 3.7098, accuracy: 8.5529%\n",
      "Epoch [1/2], Step [2544/3732], Loss: 3.8811, accuracy: 8.5495%\n",
      "Epoch [1/2], Step [2545/3732], Loss: 2.6160, accuracy: 8.5560%\n",
      "Epoch [1/2], Step [2546/3732], Loss: 3.6570, accuracy: 8.5526%\n",
      "Epoch [1/2], Step [2547/3732], Loss: 3.3408, accuracy: 8.5542%\n",
      "Epoch [1/2], Step [2548/3732], Loss: 3.5300, accuracy: 8.5508%\n",
      "Epoch [1/2], Step [2549/3732], Loss: 3.3216, accuracy: 8.5524%\n",
      "Epoch [1/2], Step [2550/3732], Loss: 3.8839, accuracy: 8.5490%\n",
      "Epoch [1/2], Step [2551/3732], Loss: 3.2110, accuracy: 8.5506%\n",
      "Epoch [1/2], Step [2552/3732], Loss: 2.9168, accuracy: 8.5521%\n",
      "Epoch [1/2], Step [2553/3732], Loss: 3.8681, accuracy: 8.5586%\n",
      "Epoch [1/2], Step [2554/3732], Loss: 3.2138, accuracy: 8.5552%\n",
      "Epoch [1/2], Step [2555/3732], Loss: 3.4830, accuracy: 8.5568%\n",
      "Epoch [1/2], Step [2556/3732], Loss: 3.6059, accuracy: 8.5583%\n",
      "Epoch [1/2], Step [2557/3732], Loss: 3.6101, accuracy: 8.5598%\n",
      "Epoch [1/2], Step [2558/3732], Loss: 2.9897, accuracy: 8.5663%\n",
      "Epoch [1/2], Step [2559/3732], Loss: 3.3467, accuracy: 8.5629%\n",
      "Epoch [1/2], Step [2560/3732], Loss: 2.8874, accuracy: 8.5596%\n",
      "Epoch [1/2], Step [2561/3732], Loss: 3.0821, accuracy: 8.5611%\n",
      "Epoch [1/2], Step [2562/3732], Loss: 3.6311, accuracy: 8.5626%\n",
      "Epoch [1/2], Step [2563/3732], Loss: 3.4917, accuracy: 8.5691%\n",
      "Epoch [1/2], Step [2564/3732], Loss: 3.5442, accuracy: 8.5706%\n",
      "Epoch [1/2], Step [2565/3732], Loss: 3.3328, accuracy: 8.5673%\n",
      "Epoch [1/2], Step [2566/3732], Loss: 3.8939, accuracy: 8.5688%\n",
      "Epoch [1/2], Step [2567/3732], Loss: 3.0794, accuracy: 8.5654%\n",
      "Epoch [1/2], Step [2568/3732], Loss: 3.6918, accuracy: 8.5621%\n",
      "Epoch [1/2], Step [2569/3732], Loss: 3.3354, accuracy: 8.5685%\n",
      "Epoch [1/2], Step [2570/3732], Loss: 2.9967, accuracy: 8.5749%\n",
      "Epoch [1/2], Step [2571/3732], Loss: 3.5263, accuracy: 8.5716%\n",
      "Epoch [1/2], Step [2572/3732], Loss: 3.6663, accuracy: 8.5731%\n",
      "Epoch [1/2], Step [2573/3732], Loss: 3.5221, accuracy: 8.5698%\n",
      "Epoch [1/2], Step [2574/3732], Loss: 3.4540, accuracy: 8.5713%\n",
      "Epoch [1/2], Step [2575/3732], Loss: 2.7597, accuracy: 8.5777%\n",
      "Epoch [1/2], Step [2576/3732], Loss: 3.4276, accuracy: 8.5792%\n",
      "Epoch [1/2], Step [2577/3732], Loss: 3.6975, accuracy: 8.5807%\n",
      "Epoch [1/2], Step [2578/3732], Loss: 3.1257, accuracy: 8.5822%\n",
      "Epoch [1/2], Step [2579/3732], Loss: 3.2039, accuracy: 8.5789%\n",
      "Epoch [1/2], Step [2580/3732], Loss: 3.1944, accuracy: 8.5804%\n",
      "Epoch [1/2], Step [2581/3732], Loss: 3.3973, accuracy: 8.5868%\n",
      "Epoch [1/2], Step [2582/3732], Loss: 3.1653, accuracy: 8.5931%\n",
      "Epoch [1/2], Step [2583/3732], Loss: 3.2628, accuracy: 8.5898%\n",
      "Epoch [1/2], Step [2584/3732], Loss: 4.2571, accuracy: 8.5865%\n",
      "Epoch [1/2], Step [2585/3732], Loss: 2.4227, accuracy: 8.5977%\n",
      "Epoch [1/2], Step [2586/3732], Loss: 2.9752, accuracy: 8.5992%\n",
      "Epoch [1/2], Step [2587/3732], Loss: 3.2279, accuracy: 8.6007%\n",
      "Epoch [1/2], Step [2588/3732], Loss: 3.5378, accuracy: 8.6070%\n",
      "Epoch [1/2], Step [2589/3732], Loss: 3.8243, accuracy: 8.6037%\n",
      "Epoch [1/2], Step [2590/3732], Loss: 4.0539, accuracy: 8.6052%\n",
      "Epoch [1/2], Step [2591/3732], Loss: 2.7773, accuracy: 8.6164%\n",
      "Epoch [1/2], Step [2592/3732], Loss: 3.2094, accuracy: 8.6179%\n",
      "Epoch [1/2], Step [2593/3732], Loss: 4.2860, accuracy: 8.6194%\n",
      "Epoch [1/2], Step [2594/3732], Loss: 2.7117, accuracy: 8.6257%\n",
      "Epoch [1/2], Step [2595/3732], Loss: 2.9104, accuracy: 8.6368%\n",
      "Epoch [1/2], Step [2596/3732], Loss: 3.5118, accuracy: 8.6335%\n",
      "Epoch [1/2], Step [2597/3732], Loss: 2.9778, accuracy: 8.6398%\n",
      "Epoch [1/2], Step [2598/3732], Loss: 2.6784, accuracy: 8.6413%\n",
      "Epoch [1/2], Step [2599/3732], Loss: 2.7135, accuracy: 8.6476%\n",
      "Epoch [1/2], Step [2600/3732], Loss: 3.5899, accuracy: 8.6442%\n",
      "Epoch [1/2], Step [2601/3732], Loss: 3.0187, accuracy: 8.6553%\n",
      "Epoch [1/2], Step [2602/3732], Loss: 3.1994, accuracy: 8.6520%\n",
      "Epoch [1/2], Step [2603/3732], Loss: 3.4539, accuracy: 8.6487%\n",
      "Epoch [1/2], Step [2604/3732], Loss: 3.3930, accuracy: 8.6550%\n",
      "Epoch [1/2], Step [2605/3732], Loss: 3.5207, accuracy: 8.6564%\n",
      "Epoch [1/2], Step [2606/3732], Loss: 3.5140, accuracy: 8.6579%\n",
      "Epoch [1/2], Step [2607/3732], Loss: 3.4680, accuracy: 8.6642%\n",
      "Epoch [1/2], Step [2608/3732], Loss: 3.0648, accuracy: 8.6656%\n",
      "Epoch [1/2], Step [2609/3732], Loss: 3.6178, accuracy: 8.6671%\n",
      "Epoch [1/2], Step [2610/3732], Loss: 2.9499, accuracy: 8.6782%\n",
      "Epoch [1/2], Step [2611/3732], Loss: 3.2182, accuracy: 8.6844%\n",
      "Epoch [1/2], Step [2612/3732], Loss: 3.3654, accuracy: 8.6859%\n",
      "Epoch [1/2], Step [2613/3732], Loss: 3.4839, accuracy: 8.6873%\n",
      "Epoch [1/2], Step [2614/3732], Loss: 3.2985, accuracy: 8.6936%\n",
      "Epoch [1/2], Step [2615/3732], Loss: 3.2121, accuracy: 8.6950%\n",
      "Epoch [1/2], Step [2616/3732], Loss: 3.0005, accuracy: 8.6965%\n",
      "Epoch [1/2], Step [2617/3732], Loss: 3.1989, accuracy: 8.6932%\n",
      "Epoch [1/2], Step [2618/3732], Loss: 2.8684, accuracy: 8.6946%\n",
      "Epoch [1/2], Step [2619/3732], Loss: 3.9522, accuracy: 8.6913%\n",
      "Epoch [1/2], Step [2620/3732], Loss: 2.9139, accuracy: 8.6975%\n",
      "Epoch [1/2], Step [2621/3732], Loss: 3.4280, accuracy: 8.7037%\n",
      "Epoch [1/2], Step [2622/3732], Loss: 3.3835, accuracy: 8.7052%\n",
      "Epoch [1/2], Step [2623/3732], Loss: 3.1380, accuracy: 8.7114%\n",
      "Epoch [1/2], Step [2624/3732], Loss: 3.6546, accuracy: 8.7081%\n",
      "Epoch [1/2], Step [2625/3732], Loss: 3.2962, accuracy: 8.7048%\n",
      "Epoch [1/2], Step [2626/3732], Loss: 2.9692, accuracy: 8.7157%\n",
      "Epoch [1/2], Step [2627/3732], Loss: 3.5653, accuracy: 8.7124%\n",
      "Epoch [1/2], Step [2628/3732], Loss: 3.4757, accuracy: 8.7139%\n",
      "Epoch [1/2], Step [2629/3732], Loss: 3.3603, accuracy: 8.7105%\n",
      "Epoch [1/2], Step [2630/3732], Loss: 2.9084, accuracy: 8.7072%\n",
      "Epoch [1/2], Step [2631/3732], Loss: 3.3133, accuracy: 8.7087%\n",
      "Epoch [1/2], Step [2632/3732], Loss: 2.8449, accuracy: 8.7196%\n",
      "Epoch [1/2], Step [2633/3732], Loss: 3.4489, accuracy: 8.7210%\n",
      "Epoch [1/2], Step [2634/3732], Loss: 2.6788, accuracy: 8.7272%\n",
      "Epoch [1/2], Step [2635/3732], Loss: 3.4457, accuracy: 8.7381%\n",
      "Epoch [1/2], Step [2636/3732], Loss: 4.2169, accuracy: 8.7348%\n",
      "Epoch [1/2], Step [2637/3732], Loss: 3.0065, accuracy: 8.7410%\n",
      "Epoch [1/2], Step [2638/3732], Loss: 3.7841, accuracy: 8.7424%\n",
      "Epoch [1/2], Step [2639/3732], Loss: 3.3978, accuracy: 8.7438%\n",
      "Epoch [1/2], Step [2640/3732], Loss: 3.4692, accuracy: 8.7405%\n",
      "Epoch [1/2], Step [2641/3732], Loss: 3.6229, accuracy: 8.7372%\n",
      "Epoch [1/2], Step [2642/3732], Loss: 2.9064, accuracy: 8.7434%\n",
      "Epoch [1/2], Step [2643/3732], Loss: 3.5868, accuracy: 8.7401%\n",
      "Epoch [1/2], Step [2644/3732], Loss: 4.0435, accuracy: 8.7415%\n",
      "Epoch [1/2], Step [2645/3732], Loss: 3.7796, accuracy: 8.7429%\n",
      "Epoch [1/2], Step [2646/3732], Loss: 3.0862, accuracy: 8.7491%\n",
      "Epoch [1/2], Step [2647/3732], Loss: 2.9670, accuracy: 8.7599%\n",
      "Epoch [1/2], Step [2648/3732], Loss: 3.2453, accuracy: 8.7660%\n",
      "Epoch [1/2], Step [2649/3732], Loss: 2.6928, accuracy: 8.7675%\n",
      "Epoch [1/2], Step [2650/3732], Loss: 3.2652, accuracy: 8.7689%\n",
      "Epoch [1/2], Step [2651/3732], Loss: 3.1872, accuracy: 8.7750%\n",
      "Epoch [1/2], Step [2652/3732], Loss: 2.8974, accuracy: 8.7811%\n",
      "Epoch [1/2], Step [2653/3732], Loss: 3.7452, accuracy: 8.7778%\n",
      "Epoch [1/2], Step [2654/3732], Loss: 3.9984, accuracy: 8.7745%\n",
      "Epoch [1/2], Step [2655/3732], Loss: 2.8746, accuracy: 8.7759%\n",
      "Epoch [1/2], Step [2656/3732], Loss: 2.7572, accuracy: 8.7820%\n",
      "Epoch [1/2], Step [2657/3732], Loss: 3.2651, accuracy: 8.7834%\n",
      "Epoch [1/2], Step [2658/3732], Loss: 3.5949, accuracy: 8.7848%\n",
      "Epoch [1/2], Step [2659/3732], Loss: 2.7172, accuracy: 8.7909%\n",
      "Epoch [1/2], Step [2660/3732], Loss: 3.4726, accuracy: 8.7923%\n",
      "Epoch [1/2], Step [2661/3732], Loss: 3.6301, accuracy: 8.7937%\n",
      "Epoch [1/2], Step [2662/3732], Loss: 3.4120, accuracy: 8.7904%\n",
      "Epoch [1/2], Step [2663/3732], Loss: 2.9823, accuracy: 8.8059%\n",
      "Epoch [1/2], Step [2664/3732], Loss: 2.5397, accuracy: 8.8213%\n",
      "Epoch [1/2], Step [2665/3732], Loss: 3.3257, accuracy: 8.8180%\n",
      "Epoch [1/2], Step [2666/3732], Loss: 3.1589, accuracy: 8.8194%\n",
      "Epoch [1/2], Step [2667/3732], Loss: 3.3205, accuracy: 8.8208%\n",
      "Epoch [1/2], Step [2668/3732], Loss: 2.3448, accuracy: 8.8315%\n",
      "Epoch [1/2], Step [2669/3732], Loss: 3.6936, accuracy: 8.8329%\n",
      "Epoch [1/2], Step [2670/3732], Loss: 3.2902, accuracy: 8.8343%\n",
      "Epoch [1/2], Step [2671/3732], Loss: 3.7452, accuracy: 8.8310%\n",
      "Epoch [1/2], Step [2672/3732], Loss: 3.3308, accuracy: 8.8277%\n",
      "Epoch [1/2], Step [2673/3732], Loss: 3.5997, accuracy: 8.8290%\n",
      "Epoch [1/2], Step [2674/3732], Loss: 3.3869, accuracy: 8.8257%\n",
      "Epoch [1/2], Step [2675/3732], Loss: 3.4593, accuracy: 8.8271%\n",
      "Epoch [1/2], Step [2676/3732], Loss: 2.0362, accuracy: 8.8472%\n",
      "Epoch [1/2], Step [2677/3732], Loss: 2.9574, accuracy: 8.8485%\n",
      "Epoch [1/2], Step [2678/3732], Loss: 4.0081, accuracy: 8.8499%\n",
      "Epoch [1/2], Step [2679/3732], Loss: 3.4698, accuracy: 8.8466%\n",
      "Epoch [1/2], Step [2680/3732], Loss: 3.2554, accuracy: 8.8479%\n",
      "Epoch [1/2], Step [2681/3732], Loss: 3.5689, accuracy: 8.8493%\n",
      "Epoch [1/2], Step [2682/3732], Loss: 3.1387, accuracy: 8.8507%\n",
      "Epoch [1/2], Step [2683/3732], Loss: 3.3935, accuracy: 8.8474%\n",
      "Epoch [1/2], Step [2684/3732], Loss: 3.6599, accuracy: 8.8487%\n",
      "Epoch [1/2], Step [2685/3732], Loss: 3.1193, accuracy: 8.8501%\n",
      "Epoch [1/2], Step [2686/3732], Loss: 3.1260, accuracy: 8.8515%\n",
      "Epoch [1/2], Step [2687/3732], Loss: 2.2986, accuracy: 8.8575%\n",
      "Epoch [1/2], Step [2688/3732], Loss: 3.1892, accuracy: 8.8588%\n",
      "Epoch [1/2], Step [2689/3732], Loss: 3.4442, accuracy: 8.8602%\n",
      "Epoch [1/2], Step [2690/3732], Loss: 3.4033, accuracy: 8.8615%\n",
      "Epoch [1/2], Step [2691/3732], Loss: 4.0113, accuracy: 8.8629%\n",
      "Epoch [1/2], Step [2692/3732], Loss: 3.4416, accuracy: 8.8596%\n",
      "Epoch [1/2], Step [2693/3732], Loss: 3.5285, accuracy: 8.8609%\n",
      "Epoch [1/2], Step [2694/3732], Loss: 3.5860, accuracy: 8.8623%\n",
      "Epoch [1/2], Step [2695/3732], Loss: 3.2565, accuracy: 8.8636%\n",
      "Epoch [1/2], Step [2696/3732], Loss: 3.2566, accuracy: 8.8650%\n",
      "Epoch [1/2], Step [2697/3732], Loss: 3.4020, accuracy: 8.8663%\n",
      "Epoch [1/2], Step [2698/3732], Loss: 3.4605, accuracy: 8.8723%\n",
      "Epoch [1/2], Step [2699/3732], Loss: 3.2242, accuracy: 8.8737%\n",
      "Epoch [1/2], Step [2700/3732], Loss: 3.5113, accuracy: 8.8750%\n",
      "Epoch [1/2], Step [2701/3732], Loss: 3.0149, accuracy: 8.8810%\n",
      "Epoch [1/2], Step [2702/3732], Loss: 3.1437, accuracy: 8.8823%\n",
      "Epoch [1/2], Step [2703/3732], Loss: 2.5726, accuracy: 8.9021%\n",
      "Epoch [1/2], Step [2704/3732], Loss: 2.9460, accuracy: 8.9035%\n",
      "Epoch [1/2], Step [2705/3732], Loss: 3.0235, accuracy: 8.9002%\n",
      "Epoch [1/2], Step [2706/3732], Loss: 3.5310, accuracy: 8.9108%\n",
      "Epoch [1/2], Step [2707/3732], Loss: 3.8185, accuracy: 8.9167%\n",
      "Epoch [1/2], Step [2708/3732], Loss: 3.6621, accuracy: 8.9134%\n",
      "Epoch [1/2], Step [2709/3732], Loss: 3.3429, accuracy: 8.9147%\n",
      "Epoch [1/2], Step [2710/3732], Loss: 3.7511, accuracy: 8.9114%\n",
      "Epoch [1/2], Step [2711/3732], Loss: 2.8923, accuracy: 8.9128%\n",
      "Epoch [1/2], Step [2712/3732], Loss: 3.1674, accuracy: 8.9141%\n",
      "Epoch [1/2], Step [2713/3732], Loss: 3.1497, accuracy: 8.9108%\n",
      "Epoch [1/2], Step [2714/3732], Loss: 2.9080, accuracy: 8.9075%\n",
      "Epoch [1/2], Step [2715/3732], Loss: 3.4609, accuracy: 8.9088%\n",
      "Epoch [1/2], Step [2716/3732], Loss: 3.4603, accuracy: 8.9056%\n",
      "Epoch [1/2], Step [2717/3732], Loss: 2.7609, accuracy: 8.9115%\n",
      "Epoch [1/2], Step [2718/3732], Loss: 3.1467, accuracy: 8.9174%\n",
      "Epoch [1/2], Step [2719/3732], Loss: 3.1508, accuracy: 8.9141%\n",
      "Epoch [1/2], Step [2720/3732], Loss: 3.0875, accuracy: 8.9154%\n",
      "Epoch [1/2], Step [2721/3732], Loss: 3.3953, accuracy: 8.9122%\n",
      "Epoch [1/2], Step [2722/3732], Loss: 2.7429, accuracy: 8.9227%\n",
      "Epoch [1/2], Step [2723/3732], Loss: 3.9788, accuracy: 8.9240%\n",
      "Epoch [1/2], Step [2724/3732], Loss: 3.7156, accuracy: 8.9207%\n",
      "Epoch [1/2], Step [2725/3732], Loss: 2.6126, accuracy: 8.9266%\n",
      "Epoch [1/2], Step [2726/3732], Loss: 3.2337, accuracy: 8.9233%\n",
      "Epoch [1/2], Step [2727/3732], Loss: 3.0993, accuracy: 8.9292%\n",
      "Epoch [1/2], Step [2728/3732], Loss: 3.2931, accuracy: 8.9260%\n",
      "Epoch [1/2], Step [2729/3732], Loss: 3.7833, accuracy: 8.9227%\n",
      "Epoch [1/2], Step [2730/3732], Loss: 3.2208, accuracy: 8.9194%\n",
      "Epoch [1/2], Step [2731/3732], Loss: 3.7631, accuracy: 8.9207%\n",
      "Epoch [1/2], Step [2732/3732], Loss: 3.1340, accuracy: 8.9266%\n",
      "Epoch [1/2], Step [2733/3732], Loss: 2.7347, accuracy: 8.9325%\n",
      "Epoch [1/2], Step [2734/3732], Loss: 3.2807, accuracy: 8.9475%\n",
      "Epoch [1/2], Step [2735/3732], Loss: 3.4422, accuracy: 8.9488%\n",
      "Epoch [1/2], Step [2736/3732], Loss: 3.2558, accuracy: 8.9501%\n",
      "Epoch [1/2], Step [2737/3732], Loss: 2.7465, accuracy: 8.9651%\n",
      "Epoch [1/2], Step [2738/3732], Loss: 3.2758, accuracy: 8.9664%\n",
      "Epoch [1/2], Step [2739/3732], Loss: 3.3515, accuracy: 8.9631%\n",
      "Epoch [1/2], Step [2740/3732], Loss: 3.0908, accuracy: 8.9690%\n",
      "Epoch [1/2], Step [2741/3732], Loss: 3.5623, accuracy: 8.9703%\n",
      "Epoch [1/2], Step [2742/3732], Loss: 4.4152, accuracy: 8.9716%\n",
      "Epoch [1/2], Step [2743/3732], Loss: 3.6466, accuracy: 8.9683%\n",
      "Epoch [1/2], Step [2744/3732], Loss: 3.3324, accuracy: 8.9696%\n",
      "Epoch [1/2], Step [2745/3732], Loss: 3.9980, accuracy: 8.9709%\n",
      "Epoch [1/2], Step [2746/3732], Loss: 3.5470, accuracy: 8.9721%\n",
      "Epoch [1/2], Step [2747/3732], Loss: 3.4405, accuracy: 8.9780%\n",
      "Epoch [1/2], Step [2748/3732], Loss: 3.7067, accuracy: 8.9793%\n",
      "Epoch [1/2], Step [2749/3732], Loss: 3.8393, accuracy: 8.9760%\n",
      "Epoch [1/2], Step [2750/3732], Loss: 3.7127, accuracy: 8.9727%\n",
      "Epoch [1/2], Step [2751/3732], Loss: 3.3439, accuracy: 8.9695%\n",
      "Epoch [1/2], Step [2752/3732], Loss: 4.0793, accuracy: 8.9662%\n",
      "Epoch [1/2], Step [2753/3732], Loss: 2.9137, accuracy: 8.9720%\n",
      "Epoch [1/2], Step [2754/3732], Loss: 3.0575, accuracy: 8.9824%\n",
      "Epoch [1/2], Step [2755/3732], Loss: 3.4167, accuracy: 8.9837%\n",
      "Epoch [1/2], Step [2756/3732], Loss: 3.1055, accuracy: 8.9895%\n",
      "Epoch [1/2], Step [2757/3732], Loss: 3.1423, accuracy: 8.9862%\n",
      "Epoch [1/2], Step [2758/3732], Loss: 3.7797, accuracy: 8.9830%\n",
      "Epoch [1/2], Step [2759/3732], Loss: 2.9837, accuracy: 8.9888%\n",
      "Epoch [1/2], Step [2760/3732], Loss: 3.3356, accuracy: 8.9900%\n",
      "Epoch [1/2], Step [2761/3732], Loss: 2.8208, accuracy: 8.9958%\n",
      "Epoch [1/2], Step [2762/3732], Loss: 3.4066, accuracy: 8.9926%\n",
      "Epoch [1/2], Step [2763/3732], Loss: 3.3434, accuracy: 9.0029%\n",
      "Epoch [1/2], Step [2764/3732], Loss: 3.6460, accuracy: 8.9996%\n",
      "Epoch [1/2], Step [2765/3732], Loss: 3.0692, accuracy: 9.0009%\n",
      "Epoch [1/2], Step [2766/3732], Loss: 3.5531, accuracy: 8.9977%\n",
      "Epoch [1/2], Step [2767/3732], Loss: 4.0134, accuracy: 8.9989%\n",
      "Epoch [1/2], Step [2768/3732], Loss: 3.6306, accuracy: 8.9957%\n",
      "Epoch [1/2], Step [2769/3732], Loss: 3.4131, accuracy: 8.9924%\n",
      "Epoch [1/2], Step [2770/3732], Loss: 3.4589, accuracy: 8.9937%\n",
      "Epoch [1/2], Step [2771/3732], Loss: 3.4790, accuracy: 8.9949%\n",
      "Epoch [1/2], Step [2772/3732], Loss: 3.8834, accuracy: 8.9917%\n",
      "Epoch [1/2], Step [2773/3732], Loss: 3.4174, accuracy: 8.9930%\n",
      "Epoch [1/2], Step [2774/3732], Loss: 3.2254, accuracy: 8.9942%\n",
      "Epoch [1/2], Step [2775/3732], Loss: 3.5273, accuracy: 8.9910%\n",
      "Epoch [1/2], Step [2776/3732], Loss: 3.0777, accuracy: 8.9923%\n",
      "Epoch [1/2], Step [2777/3732], Loss: 3.3590, accuracy: 8.9980%\n",
      "Epoch [1/2], Step [2778/3732], Loss: 3.3486, accuracy: 8.9993%\n",
      "Epoch [1/2], Step [2779/3732], Loss: 3.1306, accuracy: 9.0095%\n",
      "Epoch [1/2], Step [2780/3732], Loss: 3.1884, accuracy: 9.0108%\n",
      "Epoch [1/2], Step [2781/3732], Loss: 3.6780, accuracy: 9.0076%\n",
      "Epoch [1/2], Step [2782/3732], Loss: 3.6968, accuracy: 9.0133%\n",
      "Epoch [1/2], Step [2783/3732], Loss: 2.8951, accuracy: 9.0190%\n",
      "Epoch [1/2], Step [2784/3732], Loss: 3.0327, accuracy: 9.0203%\n",
      "Epoch [1/2], Step [2785/3732], Loss: 3.7013, accuracy: 9.0215%\n",
      "Epoch [1/2], Step [2786/3732], Loss: 3.1593, accuracy: 9.0183%\n",
      "Epoch [1/2], Step [2787/3732], Loss: 3.3099, accuracy: 9.0196%\n",
      "Epoch [1/2], Step [2788/3732], Loss: 3.8533, accuracy: 9.0208%\n",
      "Epoch [1/2], Step [2789/3732], Loss: 3.3166, accuracy: 9.0265%\n",
      "Epoch [1/2], Step [2790/3732], Loss: 3.9896, accuracy: 9.0278%\n",
      "Epoch [1/2], Step [2791/3732], Loss: 3.6728, accuracy: 9.0245%\n",
      "Epoch [1/2], Step [2792/3732], Loss: 3.3158, accuracy: 9.0213%\n",
      "Epoch [1/2], Step [2793/3732], Loss: 3.3382, accuracy: 9.0226%\n",
      "Epoch [1/2], Step [2794/3732], Loss: 3.6129, accuracy: 9.0238%\n",
      "Epoch [1/2], Step [2795/3732], Loss: 3.5991, accuracy: 9.0250%\n",
      "Epoch [1/2], Step [2796/3732], Loss: 3.7523, accuracy: 9.0218%\n",
      "Epoch [1/2], Step [2797/3732], Loss: 3.5481, accuracy: 9.0275%\n",
      "Epoch [1/2], Step [2798/3732], Loss: 2.6464, accuracy: 9.0332%\n",
      "Epoch [1/2], Step [2799/3732], Loss: 3.3884, accuracy: 9.0389%\n",
      "Epoch [1/2], Step [2800/3732], Loss: 2.6089, accuracy: 9.0491%\n",
      "Epoch [1/2], Step [2801/3732], Loss: 3.5891, accuracy: 9.0459%\n",
      "Epoch [1/2], Step [2802/3732], Loss: 3.5101, accuracy: 9.0426%\n",
      "Epoch [1/2], Step [2803/3732], Loss: 3.4973, accuracy: 9.0439%\n",
      "Epoch [1/2], Step [2804/3732], Loss: 3.2476, accuracy: 9.0496%\n",
      "Epoch [1/2], Step [2805/3732], Loss: 2.9054, accuracy: 9.0508%\n",
      "Epoch [1/2], Step [2806/3732], Loss: 3.5677, accuracy: 9.0476%\n",
      "Epoch [1/2], Step [2807/3732], Loss: 3.1715, accuracy: 9.0533%\n",
      "Epoch [1/2], Step [2808/3732], Loss: 3.0047, accuracy: 9.0545%\n",
      "Epoch [1/2], Step [2809/3732], Loss: 3.5757, accuracy: 9.0513%\n",
      "Epoch [1/2], Step [2810/3732], Loss: 3.6341, accuracy: 9.0525%\n",
      "Epoch [1/2], Step [2811/3732], Loss: 3.0226, accuracy: 9.0537%\n",
      "Epoch [1/2], Step [2812/3732], Loss: 3.0070, accuracy: 9.0549%\n",
      "Epoch [1/2], Step [2813/3732], Loss: 2.9311, accuracy: 9.0517%\n",
      "Epoch [1/2], Step [2814/3732], Loss: 3.1766, accuracy: 9.0485%\n",
      "Epoch [1/2], Step [2815/3732], Loss: 3.7052, accuracy: 9.0497%\n",
      "Epoch [1/2], Step [2816/3732], Loss: 3.1151, accuracy: 9.0510%\n",
      "Epoch [1/2], Step [2817/3732], Loss: 2.6883, accuracy: 9.0566%\n",
      "Epoch [1/2], Step [2818/3732], Loss: 3.0779, accuracy: 9.0623%\n",
      "Epoch [1/2], Step [2819/3732], Loss: 3.3858, accuracy: 9.0591%\n",
      "Epoch [1/2], Step [2820/3732], Loss: 2.9710, accuracy: 9.0603%\n",
      "Epoch [1/2], Step [2821/3732], Loss: 3.4828, accuracy: 9.0615%\n",
      "Epoch [1/2], Step [2822/3732], Loss: 3.1632, accuracy: 9.0583%\n",
      "Epoch [1/2], Step [2823/3732], Loss: 2.9811, accuracy: 9.0595%\n",
      "Epoch [1/2], Step [2824/3732], Loss: 3.2896, accuracy: 9.0607%\n",
      "Epoch [1/2], Step [2825/3732], Loss: 4.1729, accuracy: 9.0619%\n",
      "Epoch [1/2], Step [2826/3732], Loss: 2.8722, accuracy: 9.0632%\n",
      "Epoch [1/2], Step [2827/3732], Loss: 3.0170, accuracy: 9.0644%\n",
      "Epoch [1/2], Step [2828/3732], Loss: 3.4539, accuracy: 9.0656%\n",
      "Epoch [1/2], Step [2829/3732], Loss: 2.8884, accuracy: 9.0668%\n",
      "Epoch [1/2], Step [2830/3732], Loss: 3.6123, accuracy: 9.0636%\n",
      "Epoch [1/2], Step [2831/3732], Loss: 3.1619, accuracy: 9.0604%\n",
      "Epoch [1/2], Step [2832/3732], Loss: 3.3010, accuracy: 9.0572%\n",
      "Epoch [1/2], Step [2833/3732], Loss: 3.0883, accuracy: 9.0628%\n",
      "Epoch [1/2], Step [2834/3732], Loss: 2.2364, accuracy: 9.0685%\n",
      "Epoch [1/2], Step [2835/3732], Loss: 3.2747, accuracy: 9.0697%\n",
      "Epoch [1/2], Step [2836/3732], Loss: 3.8066, accuracy: 9.0665%\n",
      "Epoch [1/2], Step [2837/3732], Loss: 3.1668, accuracy: 9.0677%\n",
      "Epoch [1/2], Step [2838/3732], Loss: 3.4432, accuracy: 9.0645%\n",
      "Epoch [1/2], Step [2839/3732], Loss: 3.2236, accuracy: 9.0745%\n",
      "Epoch [1/2], Step [2840/3732], Loss: 3.5718, accuracy: 9.0757%\n",
      "Epoch [1/2], Step [2841/3732], Loss: 4.0802, accuracy: 9.0769%\n",
      "Epoch [1/2], Step [2842/3732], Loss: 3.3333, accuracy: 9.0825%\n",
      "Epoch [1/2], Step [2843/3732], Loss: 3.5153, accuracy: 9.0837%\n",
      "Epoch [1/2], Step [2844/3732], Loss: 4.0137, accuracy: 9.0805%\n",
      "Epoch [1/2], Step [2845/3732], Loss: 3.0168, accuracy: 9.0905%\n",
      "Epoch [1/2], Step [2846/3732], Loss: 3.4593, accuracy: 9.0917%\n",
      "Epoch [1/2], Step [2847/3732], Loss: 3.8499, accuracy: 9.0885%\n",
      "Epoch [1/2], Step [2848/3732], Loss: 3.7454, accuracy: 9.0853%\n",
      "Epoch [1/2], Step [2849/3732], Loss: 3.4675, accuracy: 9.0865%\n",
      "Epoch [1/2], Step [2850/3732], Loss: 2.9013, accuracy: 9.0965%\n",
      "Epoch [1/2], Step [2851/3732], Loss: 3.3348, accuracy: 9.1021%\n",
      "Epoch [1/2], Step [2852/3732], Loss: 3.5733, accuracy: 9.1033%\n",
      "Epoch [1/2], Step [2853/3732], Loss: 3.2552, accuracy: 9.1045%\n",
      "Epoch [1/2], Step [2854/3732], Loss: 3.2828, accuracy: 9.1056%\n",
      "Epoch [1/2], Step [2855/3732], Loss: 2.7649, accuracy: 9.1112%\n",
      "Epoch [1/2], Step [2856/3732], Loss: 3.5328, accuracy: 9.1168%\n",
      "Epoch [1/2], Step [2857/3732], Loss: 2.9862, accuracy: 9.1180%\n",
      "Epoch [1/2], Step [2858/3732], Loss: 3.0079, accuracy: 9.1235%\n",
      "Epoch [1/2], Step [2859/3732], Loss: 4.0760, accuracy: 9.1203%\n",
      "Epoch [1/2], Step [2860/3732], Loss: 3.5163, accuracy: 9.1171%\n",
      "Epoch [1/2], Step [2861/3732], Loss: 3.5896, accuracy: 9.1227%\n",
      "Epoch [1/2], Step [2862/3732], Loss: 3.1501, accuracy: 9.1282%\n",
      "Epoch [1/2], Step [2863/3732], Loss: 3.1493, accuracy: 9.1294%\n",
      "Epoch [1/2], Step [2864/3732], Loss: 3.3955, accuracy: 9.1262%\n",
      "Epoch [1/2], Step [2865/3732], Loss: 2.8352, accuracy: 9.1274%\n",
      "Epoch [1/2], Step [2866/3732], Loss: 4.1858, accuracy: 9.1242%\n",
      "Epoch [1/2], Step [2867/3732], Loss: 3.0987, accuracy: 9.1254%\n",
      "Epoch [1/2], Step [2868/3732], Loss: 3.0775, accuracy: 9.1309%\n",
      "Epoch [1/2], Step [2869/3732], Loss: 3.1419, accuracy: 9.1321%\n",
      "Epoch [1/2], Step [2870/3732], Loss: 4.5103, accuracy: 9.1289%\n",
      "Epoch [1/2], Step [2871/3732], Loss: 3.0431, accuracy: 9.1344%\n",
      "Epoch [1/2], Step [2872/3732], Loss: 2.7280, accuracy: 9.1356%\n",
      "Epoch [1/2], Step [2873/3732], Loss: 2.8776, accuracy: 9.1368%\n",
      "Epoch [1/2], Step [2874/3732], Loss: 3.9125, accuracy: 9.1380%\n",
      "Epoch [1/2], Step [2875/3732], Loss: 3.5615, accuracy: 9.1391%\n",
      "Epoch [1/2], Step [2876/3732], Loss: 3.4193, accuracy: 9.1403%\n",
      "Epoch [1/2], Step [2877/3732], Loss: 3.2510, accuracy: 9.1371%\n",
      "Epoch [1/2], Step [2878/3732], Loss: 4.0087, accuracy: 9.1339%\n",
      "Epoch [1/2], Step [2879/3732], Loss: 3.4161, accuracy: 9.1351%\n",
      "Epoch [1/2], Step [2880/3732], Loss: 3.5570, accuracy: 9.1363%\n",
      "Epoch [1/2], Step [2881/3732], Loss: 2.5647, accuracy: 9.1418%\n",
      "Epoch [1/2], Step [2882/3732], Loss: 4.0447, accuracy: 9.1386%\n",
      "Epoch [1/2], Step [2883/3732], Loss: 3.4835, accuracy: 9.1441%\n",
      "Epoch [1/2], Step [2884/3732], Loss: 3.1729, accuracy: 9.1496%\n",
      "Epoch [1/2], Step [2885/3732], Loss: 3.7289, accuracy: 9.1464%\n",
      "Epoch [1/2], Step [2886/3732], Loss: 4.1987, accuracy: 9.1433%\n",
      "Epoch [1/2], Step [2887/3732], Loss: 3.2376, accuracy: 9.1401%\n",
      "Epoch [1/2], Step [2888/3732], Loss: 3.8097, accuracy: 9.1369%\n",
      "Epoch [1/2], Step [2889/3732], Loss: 3.0056, accuracy: 9.1381%\n",
      "Epoch [1/2], Step [2890/3732], Loss: 4.1325, accuracy: 9.1349%\n",
      "Epoch [1/2], Step [2891/3732], Loss: 3.6798, accuracy: 9.1318%\n",
      "Epoch [1/2], Step [2892/3732], Loss: 3.2021, accuracy: 9.1330%\n",
      "Epoch [1/2], Step [2893/3732], Loss: 3.9910, accuracy: 9.1341%\n",
      "Epoch [1/2], Step [2894/3732], Loss: 2.8705, accuracy: 9.1353%\n",
      "Epoch [1/2], Step [2895/3732], Loss: 3.7424, accuracy: 9.1364%\n",
      "Epoch [1/2], Step [2896/3732], Loss: 2.7605, accuracy: 9.1376%\n",
      "Epoch [1/2], Step [2897/3732], Loss: 3.8703, accuracy: 9.1344%\n",
      "Epoch [1/2], Step [2898/3732], Loss: 3.2017, accuracy: 9.1356%\n",
      "Epoch [1/2], Step [2899/3732], Loss: 3.1619, accuracy: 9.1411%\n",
      "Epoch [1/2], Step [2900/3732], Loss: 3.4732, accuracy: 9.1422%\n",
      "Epoch [1/2], Step [2901/3732], Loss: 3.5540, accuracy: 9.1391%\n",
      "Epoch [1/2], Step [2902/3732], Loss: 2.7528, accuracy: 9.1489%\n",
      "Epoch [1/2], Step [2903/3732], Loss: 3.9352, accuracy: 9.1457%\n",
      "Epoch [1/2], Step [2904/3732], Loss: 2.6390, accuracy: 9.1598%\n",
      "Epoch [1/2], Step [2905/3732], Loss: 3.3095, accuracy: 9.1609%\n",
      "Epoch [1/2], Step [2906/3732], Loss: 2.4695, accuracy: 9.1707%\n",
      "Epoch [1/2], Step [2907/3732], Loss: 2.8070, accuracy: 9.1804%\n",
      "Epoch [1/2], Step [2908/3732], Loss: 3.5642, accuracy: 9.1773%\n",
      "Epoch [1/2], Step [2909/3732], Loss: 3.5501, accuracy: 9.1784%\n",
      "Epoch [1/2], Step [2910/3732], Loss: 3.2675, accuracy: 9.1881%\n",
      "Epoch [1/2], Step [2911/3732], Loss: 3.2162, accuracy: 9.1850%\n",
      "Epoch [1/2], Step [2912/3732], Loss: 3.1243, accuracy: 9.1861%\n",
      "Epoch [1/2], Step [2913/3732], Loss: 3.4107, accuracy: 9.1830%\n",
      "Epoch [1/2], Step [2914/3732], Loss: 3.7787, accuracy: 9.1841%\n",
      "Epoch [1/2], Step [2915/3732], Loss: 3.5448, accuracy: 9.1895%\n",
      "Epoch [1/2], Step [2916/3732], Loss: 3.4608, accuracy: 9.1950%\n",
      "Epoch [1/2], Step [2917/3732], Loss: 3.4861, accuracy: 9.1918%\n",
      "Epoch [1/2], Step [2918/3732], Loss: 3.5463, accuracy: 9.1887%\n",
      "Epoch [1/2], Step [2919/3732], Loss: 3.4804, accuracy: 9.1855%\n",
      "Epoch [1/2], Step [2920/3732], Loss: 3.3797, accuracy: 9.1952%\n",
      "Epoch [1/2], Step [2921/3732], Loss: 3.0466, accuracy: 9.2006%\n",
      "Epoch [1/2], Step [2922/3732], Loss: 4.0625, accuracy: 9.1975%\n",
      "Epoch [1/2], Step [2923/3732], Loss: 3.5001, accuracy: 9.1986%\n",
      "Epoch [1/2], Step [2924/3732], Loss: 3.6385, accuracy: 9.1997%\n",
      "Epoch [1/2], Step [2925/3732], Loss: 3.5457, accuracy: 9.2009%\n",
      "Epoch [1/2], Step [2926/3732], Loss: 3.7757, accuracy: 9.2020%\n",
      "Epoch [1/2], Step [2927/3732], Loss: 3.6645, accuracy: 9.1988%\n",
      "Epoch [1/2], Step [2928/3732], Loss: 3.4500, accuracy: 9.1957%\n",
      "Epoch [1/2], Step [2929/3732], Loss: 3.4784, accuracy: 9.1968%\n",
      "Epoch [1/2], Step [2930/3732], Loss: 3.1097, accuracy: 9.1980%\n",
      "Epoch [1/2], Step [2931/3732], Loss: 3.6038, accuracy: 9.1991%\n",
      "Epoch [1/2], Step [2932/3732], Loss: 3.8949, accuracy: 9.1959%\n",
      "Epoch [1/2], Step [2933/3732], Loss: 3.5454, accuracy: 9.1971%\n",
      "Epoch [1/2], Step [2934/3732], Loss: 3.1466, accuracy: 9.1982%\n",
      "Epoch [1/2], Step [2935/3732], Loss: 3.5871, accuracy: 9.1951%\n",
      "Epoch [1/2], Step [2936/3732], Loss: 3.2225, accuracy: 9.2047%\n",
      "Epoch [1/2], Step [2937/3732], Loss: 3.0117, accuracy: 9.2101%\n",
      "Epoch [1/2], Step [2938/3732], Loss: 3.2115, accuracy: 9.2112%\n",
      "Epoch [1/2], Step [2939/3732], Loss: 3.6888, accuracy: 9.2081%\n",
      "Epoch [1/2], Step [2940/3732], Loss: 3.8645, accuracy: 9.2049%\n",
      "Epoch [1/2], Step [2941/3732], Loss: 2.9068, accuracy: 9.2146%\n",
      "Epoch [1/2], Step [2942/3732], Loss: 3.6438, accuracy: 9.2114%\n",
      "Epoch [1/2], Step [2943/3732], Loss: 3.7331, accuracy: 9.2083%\n",
      "Epoch [1/2], Step [2944/3732], Loss: 2.9743, accuracy: 9.2137%\n",
      "Epoch [1/2], Step [2945/3732], Loss: 3.3302, accuracy: 9.2148%\n",
      "Epoch [1/2], Step [2946/3732], Loss: 3.7131, accuracy: 9.2116%\n",
      "Epoch [1/2], Step [2947/3732], Loss: 3.4464, accuracy: 9.2128%\n",
      "Epoch [1/2], Step [2948/3732], Loss: 2.7672, accuracy: 9.2181%\n",
      "Epoch [1/2], Step [2949/3732], Loss: 3.1174, accuracy: 9.2192%\n",
      "Epoch [1/2], Step [2950/3732], Loss: 3.0063, accuracy: 9.2203%\n",
      "Epoch [1/2], Step [2951/3732], Loss: 2.9492, accuracy: 9.2257%\n",
      "Epoch [1/2], Step [2952/3732], Loss: 3.1766, accuracy: 9.2310%\n",
      "Epoch [1/2], Step [2953/3732], Loss: 3.1795, accuracy: 9.2321%\n",
      "Epoch [1/2], Step [2954/3732], Loss: 3.8099, accuracy: 9.2290%\n",
      "Epoch [1/2], Step [2955/3732], Loss: 3.0907, accuracy: 9.2343%\n",
      "Epoch [1/2], Step [2956/3732], Loss: 4.0045, accuracy: 9.2312%\n",
      "Epoch [1/2], Step [2957/3732], Loss: 3.1187, accuracy: 9.2408%\n",
      "Epoch [1/2], Step [2958/3732], Loss: 2.7187, accuracy: 9.2503%\n",
      "Epoch [1/2], Step [2959/3732], Loss: 3.2905, accuracy: 9.2514%\n",
      "Epoch [1/2], Step [2960/3732], Loss: 3.1125, accuracy: 9.2525%\n",
      "Epoch [1/2], Step [2961/3732], Loss: 2.7916, accuracy: 9.2621%\n",
      "Epoch [1/2], Step [2962/3732], Loss: 3.4909, accuracy: 9.2716%\n",
      "Epoch [1/2], Step [2963/3732], Loss: 3.1927, accuracy: 9.2811%\n",
      "Epoch [1/2], Step [2964/3732], Loss: 3.6235, accuracy: 9.2822%\n",
      "Epoch [1/2], Step [2965/3732], Loss: 3.5834, accuracy: 9.2833%\n",
      "Epoch [1/2], Step [2966/3732], Loss: 3.7633, accuracy: 9.2802%\n",
      "Epoch [1/2], Step [2967/3732], Loss: 3.1390, accuracy: 9.2813%\n",
      "Epoch [1/2], Step [2968/3732], Loss: 2.5482, accuracy: 9.2908%\n",
      "Epoch [1/2], Step [2969/3732], Loss: 3.3177, accuracy: 9.2876%\n",
      "Epoch [1/2], Step [2970/3732], Loss: 2.7567, accuracy: 9.2929%\n",
      "Epoch [1/2], Step [2971/3732], Loss: 2.8775, accuracy: 9.2982%\n",
      "Epoch [1/2], Step [2972/3732], Loss: 3.0515, accuracy: 9.3077%\n",
      "Epoch [1/2], Step [2973/3732], Loss: 3.4390, accuracy: 9.3130%\n",
      "Epoch [1/2], Step [2974/3732], Loss: 2.9699, accuracy: 9.3141%\n",
      "Epoch [1/2], Step [2975/3732], Loss: 2.9517, accuracy: 9.3235%\n",
      "Epoch [1/2], Step [2976/3732], Loss: 3.4550, accuracy: 9.3288%\n",
      "Epoch [1/2], Step [2977/3732], Loss: 2.8816, accuracy: 9.3257%\n",
      "Epoch [1/2], Step [2978/3732], Loss: 3.4172, accuracy: 9.3225%\n",
      "Epoch [1/2], Step [2979/3732], Loss: 3.1696, accuracy: 9.3236%\n",
      "Epoch [1/2], Step [2980/3732], Loss: 3.8808, accuracy: 9.3247%\n",
      "Epoch [1/2], Step [2981/3732], Loss: 3.3479, accuracy: 9.3215%\n",
      "Epoch [1/2], Step [2982/3732], Loss: 3.8836, accuracy: 9.3226%\n",
      "Epoch [1/2], Step [2983/3732], Loss: 3.2381, accuracy: 9.3195%\n",
      "Epoch [1/2], Step [2984/3732], Loss: 3.4343, accuracy: 9.3247%\n",
      "Epoch [1/2], Step [2985/3732], Loss: 3.4812, accuracy: 9.3258%\n",
      "Epoch [1/2], Step [2986/3732], Loss: 3.7725, accuracy: 9.3227%\n",
      "Epoch [1/2], Step [2987/3732], Loss: 3.2348, accuracy: 9.3279%\n",
      "Epoch [1/2], Step [2988/3732], Loss: 3.1560, accuracy: 9.3248%\n",
      "Epoch [1/2], Step [2989/3732], Loss: 3.7305, accuracy: 9.3217%\n",
      "Epoch [1/2], Step [2990/3732], Loss: 3.2870, accuracy: 9.3269%\n",
      "Epoch [1/2], Step [2991/3732], Loss: 3.1341, accuracy: 9.3280%\n",
      "Epoch [1/2], Step [2992/3732], Loss: 2.8197, accuracy: 9.3374%\n",
      "Epoch [1/2], Step [2993/3732], Loss: 2.9797, accuracy: 9.3426%\n",
      "Epoch [1/2], Step [2994/3732], Loss: 3.5798, accuracy: 9.3437%\n",
      "Epoch [1/2], Step [2995/3732], Loss: 3.0388, accuracy: 9.3489%\n",
      "Epoch [1/2], Step [2996/3732], Loss: 3.3279, accuracy: 9.3500%\n",
      "Epoch [1/2], Step [2997/3732], Loss: 3.1911, accuracy: 9.3552%\n",
      "Epoch [1/2], Step [2998/3732], Loss: 3.6396, accuracy: 9.3604%\n",
      "Epoch [1/2], Step [2999/3732], Loss: 3.1193, accuracy: 9.3656%\n",
      "Epoch [1/2], Step [3000/3732], Loss: 3.2909, accuracy: 9.3667%\n",
      "Epoch [1/2], Step [3001/3732], Loss: 3.4013, accuracy: 9.3677%\n",
      "Epoch [1/2], Step [3002/3732], Loss: 3.3955, accuracy: 9.3688%\n",
      "Epoch [1/2], Step [3003/3732], Loss: 2.9669, accuracy: 9.3740%\n",
      "Epoch [1/2], Step [3004/3732], Loss: 3.9277, accuracy: 9.3750%\n",
      "Epoch [1/2], Step [3005/3732], Loss: 3.8726, accuracy: 9.3719%\n",
      "Epoch [1/2], Step [3006/3732], Loss: 3.3009, accuracy: 9.3771%\n",
      "Epoch [1/2], Step [3007/3732], Loss: 3.3490, accuracy: 9.3740%\n",
      "Epoch [1/2], Step [3008/3732], Loss: 3.3806, accuracy: 9.3750%\n",
      "Epoch [1/2], Step [3009/3732], Loss: 2.8430, accuracy: 9.3802%\n",
      "Epoch [1/2], Step [3010/3732], Loss: 3.4680, accuracy: 9.3854%\n",
      "Epoch [1/2], Step [3011/3732], Loss: 2.7726, accuracy: 9.3864%\n",
      "Epoch [1/2], Step [3012/3732], Loss: 3.1261, accuracy: 9.3916%\n",
      "Epoch [1/2], Step [3013/3732], Loss: 3.4584, accuracy: 9.3885%\n",
      "Epoch [1/2], Step [3014/3732], Loss: 2.8934, accuracy: 9.3937%\n",
      "Epoch [1/2], Step [3015/3732], Loss: 2.5844, accuracy: 9.3988%\n",
      "Epoch [1/2], Step [3016/3732], Loss: 3.2456, accuracy: 9.3957%\n",
      "Epoch [1/2], Step [3017/3732], Loss: 3.3703, accuracy: 9.3968%\n",
      "Epoch [1/2], Step [3018/3732], Loss: 2.8909, accuracy: 9.4019%\n",
      "Epoch [1/2], Step [3019/3732], Loss: 3.8948, accuracy: 9.3988%\n",
      "Epoch [1/2], Step [3020/3732], Loss: 3.0676, accuracy: 9.3998%\n",
      "Epoch [1/2], Step [3021/3732], Loss: 2.9968, accuracy: 9.4050%\n",
      "Epoch [1/2], Step [3022/3732], Loss: 3.4169, accuracy: 9.4060%\n",
      "Epoch [1/2], Step [3023/3732], Loss: 2.8811, accuracy: 9.4112%\n",
      "Epoch [1/2], Step [3024/3732], Loss: 3.4138, accuracy: 9.4081%\n",
      "Epoch [1/2], Step [3025/3732], Loss: 2.9692, accuracy: 9.4091%\n",
      "Epoch [1/2], Step [3026/3732], Loss: 2.5838, accuracy: 9.4142%\n",
      "Epoch [1/2], Step [3027/3732], Loss: 3.2596, accuracy: 9.4111%\n",
      "Epoch [1/2], Step [3028/3732], Loss: 2.9789, accuracy: 9.4122%\n",
      "Epoch [1/2], Step [3029/3732], Loss: 3.1474, accuracy: 9.4132%\n",
      "Epoch [1/2], Step [3030/3732], Loss: 3.5394, accuracy: 9.4101%\n",
      "Epoch [1/2], Step [3031/3732], Loss: 3.7614, accuracy: 9.4070%\n",
      "Epoch [1/2], Step [3032/3732], Loss: 3.3721, accuracy: 9.4121%\n",
      "Epoch [1/2], Step [3033/3732], Loss: 3.3325, accuracy: 9.4131%\n",
      "Epoch [1/2], Step [3034/3732], Loss: 3.2336, accuracy: 9.4183%\n",
      "Epoch [1/2], Step [3035/3732], Loss: 2.7776, accuracy: 9.4234%\n",
      "Epoch [1/2], Step [3036/3732], Loss: 2.5732, accuracy: 9.4409%\n",
      "Epoch [1/2], Step [3037/3732], Loss: 3.6085, accuracy: 9.4378%\n",
      "Epoch [1/2], Step [3038/3732], Loss: 2.8801, accuracy: 9.4388%\n",
      "Epoch [1/2], Step [3039/3732], Loss: 2.6234, accuracy: 9.4439%\n",
      "Epoch [1/2], Step [3040/3732], Loss: 3.8283, accuracy: 9.4408%\n",
      "Epoch [1/2], Step [3041/3732], Loss: 3.0585, accuracy: 9.4377%\n",
      "Epoch [1/2], Step [3042/3732], Loss: 3.2181, accuracy: 9.4387%\n",
      "Epoch [1/2], Step [3043/3732], Loss: 2.5874, accuracy: 9.4520%\n",
      "Epoch [1/2], Step [3044/3732], Loss: 2.4338, accuracy: 9.4571%\n",
      "Epoch [1/2], Step [3045/3732], Loss: 3.8463, accuracy: 9.4581%\n",
      "Epoch [1/2], Step [3046/3732], Loss: 2.7692, accuracy: 9.4550%\n",
      "Epoch [1/2], Step [3047/3732], Loss: 3.8667, accuracy: 9.4560%\n",
      "Epoch [1/2], Step [3048/3732], Loss: 2.6425, accuracy: 9.4693%\n",
      "Epoch [1/2], Step [3049/3732], Loss: 3.7988, accuracy: 9.4662%\n",
      "Epoch [1/2], Step [3050/3732], Loss: 3.2758, accuracy: 9.4672%\n",
      "Epoch [1/2], Step [3051/3732], Loss: 3.5767, accuracy: 9.4682%\n",
      "Epoch [1/2], Step [3052/3732], Loss: 3.1033, accuracy: 9.4733%\n",
      "Epoch [1/2], Step [3053/3732], Loss: 2.4111, accuracy: 9.4784%\n",
      "Epoch [1/2], Step [3054/3732], Loss: 3.2700, accuracy: 9.4753%\n",
      "Epoch [1/2], Step [3055/3732], Loss: 3.7051, accuracy: 9.4722%\n",
      "Epoch [1/2], Step [3056/3732], Loss: 3.1587, accuracy: 9.4732%\n",
      "Epoch [1/2], Step [3057/3732], Loss: 3.3303, accuracy: 9.4701%\n",
      "Epoch [1/2], Step [3058/3732], Loss: 3.6217, accuracy: 9.4751%\n",
      "Epoch [1/2], Step [3059/3732], Loss: 3.2672, accuracy: 9.4802%\n",
      "Epoch [1/2], Step [3060/3732], Loss: 3.3418, accuracy: 9.4812%\n",
      "Epoch [1/2], Step [3061/3732], Loss: 3.3493, accuracy: 9.4822%\n",
      "Epoch [1/2], Step [3062/3732], Loss: 3.3697, accuracy: 9.4832%\n",
      "Epoch [1/2], Step [3063/3732], Loss: 3.4112, accuracy: 9.4882%\n",
      "Epoch [1/2], Step [3064/3732], Loss: 3.1671, accuracy: 9.4933%\n",
      "Epoch [1/2], Step [3065/3732], Loss: 3.2779, accuracy: 9.4902%\n",
      "Epoch [1/2], Step [3066/3732], Loss: 3.5088, accuracy: 9.4871%\n",
      "Epoch [1/2], Step [3067/3732], Loss: 3.0039, accuracy: 9.4881%\n",
      "Epoch [1/2], Step [3068/3732], Loss: 2.3785, accuracy: 9.5013%\n",
      "Epoch [1/2], Step [3069/3732], Loss: 3.6026, accuracy: 9.5023%\n",
      "Epoch [1/2], Step [3070/3732], Loss: 3.1301, accuracy: 9.5033%\n",
      "Epoch [1/2], Step [3071/3732], Loss: 3.0946, accuracy: 9.5083%\n",
      "Epoch [1/2], Step [3072/3732], Loss: 3.1844, accuracy: 9.5133%\n",
      "Epoch [1/2], Step [3073/3732], Loss: 3.0617, accuracy: 9.5103%\n",
      "Epoch [1/2], Step [3074/3732], Loss: 3.9256, accuracy: 9.5072%\n",
      "Epoch [1/2], Step [3075/3732], Loss: 3.0888, accuracy: 9.5041%\n",
      "Epoch [1/2], Step [3076/3732], Loss: 3.2288, accuracy: 9.5050%\n",
      "Epoch [1/2], Step [3077/3732], Loss: 3.3758, accuracy: 9.5101%\n",
      "Epoch [1/2], Step [3078/3732], Loss: 3.6622, accuracy: 9.5070%\n",
      "Epoch [1/2], Step [3079/3732], Loss: 3.8427, accuracy: 9.5039%\n",
      "Epoch [1/2], Step [3080/3732], Loss: 2.9425, accuracy: 9.5089%\n",
      "Epoch [1/2], Step [3081/3732], Loss: 2.6214, accuracy: 9.5180%\n",
      "Epoch [1/2], Step [3082/3732], Loss: 3.4737, accuracy: 9.5149%\n",
      "Epoch [1/2], Step [3083/3732], Loss: 2.8566, accuracy: 9.5159%\n",
      "Epoch [1/2], Step [3084/3732], Loss: 2.9033, accuracy: 9.5209%\n",
      "Epoch [1/2], Step [3085/3732], Loss: 3.3671, accuracy: 9.5259%\n",
      "Epoch [1/2], Step [3086/3732], Loss: 3.3302, accuracy: 9.5269%\n",
      "Epoch [1/2], Step [3087/3732], Loss: 4.0327, accuracy: 9.5238%\n",
      "Epoch [1/2], Step [3088/3732], Loss: 3.4358, accuracy: 9.5248%\n",
      "Epoch [1/2], Step [3089/3732], Loss: 3.7359, accuracy: 9.5217%\n",
      "Epoch [1/2], Step [3090/3732], Loss: 3.2826, accuracy: 9.5186%\n",
      "Epoch [1/2], Step [3091/3732], Loss: 3.3146, accuracy: 9.5277%\n",
      "Epoch [1/2], Step [3092/3732], Loss: 3.7093, accuracy: 9.5246%\n",
      "Epoch [1/2], Step [3093/3732], Loss: 3.6457, accuracy: 9.5215%\n",
      "Epoch [1/2], Step [3094/3732], Loss: 2.8623, accuracy: 9.5225%\n",
      "Epoch [1/2], Step [3095/3732], Loss: 3.9182, accuracy: 9.5194%\n",
      "Epoch [1/2], Step [3096/3732], Loss: 3.1831, accuracy: 9.5244%\n",
      "Epoch [1/2], Step [3097/3732], Loss: 3.9086, accuracy: 9.5213%\n",
      "Epoch [1/2], Step [3098/3732], Loss: 3.2553, accuracy: 9.5263%\n",
      "Epoch [1/2], Step [3099/3732], Loss: 3.2197, accuracy: 9.5232%\n",
      "Epoch [1/2], Step [3100/3732], Loss: 3.2978, accuracy: 9.5282%\n",
      "Epoch [1/2], Step [3101/3732], Loss: 2.7683, accuracy: 9.5332%\n",
      "Epoch [1/2], Step [3102/3732], Loss: 3.3535, accuracy: 9.5382%\n",
      "Epoch [1/2], Step [3103/3732], Loss: 3.7741, accuracy: 9.5392%\n",
      "Epoch [1/2], Step [3104/3732], Loss: 2.8059, accuracy: 9.5441%\n",
      "Epoch [1/2], Step [3105/3732], Loss: 3.4078, accuracy: 9.5451%\n",
      "Epoch [1/2], Step [3106/3732], Loss: 4.5253, accuracy: 9.5420%\n",
      "Epoch [1/2], Step [3107/3732], Loss: 3.5492, accuracy: 9.5389%\n",
      "Epoch [1/2], Step [3108/3732], Loss: 3.8618, accuracy: 9.5359%\n",
      "Epoch [1/2], Step [3109/3732], Loss: 3.3751, accuracy: 9.5408%\n",
      "Epoch [1/2], Step [3110/3732], Loss: 3.3677, accuracy: 9.5418%\n",
      "Epoch [1/2], Step [3111/3732], Loss: 2.9959, accuracy: 9.5428%\n",
      "Epoch [1/2], Step [3112/3732], Loss: 3.3774, accuracy: 9.5437%\n",
      "Epoch [1/2], Step [3113/3732], Loss: 3.2318, accuracy: 9.5406%\n",
      "Epoch [1/2], Step [3114/3732], Loss: 3.5955, accuracy: 9.5376%\n",
      "Epoch [1/2], Step [3115/3732], Loss: 3.0948, accuracy: 9.5425%\n",
      "Epoch [1/2], Step [3116/3732], Loss: 4.1276, accuracy: 9.5435%\n",
      "Epoch [1/2], Step [3117/3732], Loss: 2.8523, accuracy: 9.5525%\n",
      "Epoch [1/2], Step [3118/3732], Loss: 2.9117, accuracy: 9.5494%\n",
      "Epoch [1/2], Step [3119/3732], Loss: 3.8824, accuracy: 9.5463%\n",
      "Epoch [1/2], Step [3120/3732], Loss: 4.0273, accuracy: 9.5433%\n",
      "Epoch [1/2], Step [3121/3732], Loss: 3.5714, accuracy: 9.5402%\n",
      "Epoch [1/2], Step [3122/3732], Loss: 3.7948, accuracy: 9.5372%\n",
      "Epoch [1/2], Step [3123/3732], Loss: 3.8352, accuracy: 9.5421%\n",
      "Epoch [1/2], Step [3124/3732], Loss: 2.9112, accuracy: 9.5391%\n",
      "Epoch [1/2], Step [3125/3732], Loss: 3.2132, accuracy: 9.5400%\n",
      "Epoch [1/2], Step [3126/3732], Loss: 3.1541, accuracy: 9.5409%\n",
      "Epoch [1/2], Step [3127/3732], Loss: 3.5972, accuracy: 9.5379%\n",
      "Epoch [1/2], Step [3128/3732], Loss: 3.9104, accuracy: 9.5348%\n",
      "Epoch [1/2], Step [3129/3732], Loss: 2.8457, accuracy: 9.5398%\n",
      "Epoch [1/2], Step [3130/3732], Loss: 2.0547, accuracy: 9.5567%\n",
      "Epoch [1/2], Step [3131/3732], Loss: 2.9708, accuracy: 9.5537%\n",
      "Epoch [1/2], Step [3132/3732], Loss: 3.8354, accuracy: 9.5506%\n",
      "Epoch [1/2], Step [3133/3732], Loss: 3.5591, accuracy: 9.5515%\n",
      "Epoch [1/2], Step [3134/3732], Loss: 2.7220, accuracy: 9.5605%\n",
      "Epoch [1/2], Step [3135/3732], Loss: 3.1678, accuracy: 9.5654%\n",
      "Epoch [1/2], Step [3136/3732], Loss: 3.0703, accuracy: 9.5703%\n",
      "Epoch [1/2], Step [3137/3732], Loss: 3.3544, accuracy: 9.5712%\n",
      "Epoch [1/2], Step [3138/3732], Loss: 3.3090, accuracy: 9.5722%\n",
      "Epoch [1/2], Step [3139/3732], Loss: 3.2790, accuracy: 9.5771%\n",
      "Epoch [1/2], Step [3140/3732], Loss: 3.5515, accuracy: 9.5820%\n",
      "Epoch [1/2], Step [3141/3732], Loss: 3.5153, accuracy: 9.5829%\n",
      "Epoch [1/2], Step [3142/3732], Loss: 3.6692, accuracy: 9.5799%\n",
      "Epoch [1/2], Step [3143/3732], Loss: 2.9508, accuracy: 9.5808%\n",
      "Epoch [1/2], Step [3144/3732], Loss: 3.7738, accuracy: 9.5857%\n",
      "Epoch [1/2], Step [3145/3732], Loss: 3.5051, accuracy: 9.5866%\n",
      "Epoch [1/2], Step [3146/3732], Loss: 2.9576, accuracy: 9.5876%\n",
      "Epoch [1/2], Step [3147/3732], Loss: 3.4383, accuracy: 9.5845%\n",
      "Epoch [1/2], Step [3148/3732], Loss: 3.1121, accuracy: 9.5855%\n",
      "Epoch [1/2], Step [3149/3732], Loss: 3.2231, accuracy: 9.5824%\n",
      "Epoch [1/2], Step [3150/3732], Loss: 3.8414, accuracy: 9.5794%\n",
      "Epoch [1/2], Step [3151/3732], Loss: 2.6560, accuracy: 9.5803%\n",
      "Epoch [1/2], Step [3152/3732], Loss: 3.7976, accuracy: 9.5812%\n",
      "Epoch [1/2], Step [3153/3732], Loss: 3.1205, accuracy: 9.5821%\n",
      "Epoch [1/2], Step [3154/3732], Loss: 3.7144, accuracy: 9.5831%\n",
      "Epoch [1/2], Step [3155/3732], Loss: 3.1275, accuracy: 9.5840%\n",
      "Epoch [1/2], Step [3156/3732], Loss: 3.3896, accuracy: 9.5849%\n",
      "Epoch [1/2], Step [3157/3732], Loss: 3.2214, accuracy: 9.5819%\n",
      "Epoch [1/2], Step [3158/3732], Loss: 3.2344, accuracy: 9.5788%\n",
      "Epoch [1/2], Step [3159/3732], Loss: 3.2916, accuracy: 9.5837%\n",
      "Epoch [1/2], Step [3160/3732], Loss: 3.0182, accuracy: 9.5886%\n",
      "Epoch [1/2], Step [3161/3732], Loss: 3.2685, accuracy: 9.5895%\n",
      "Epoch [1/2], Step [3162/3732], Loss: 3.7154, accuracy: 9.5904%\n",
      "Epoch [1/2], Step [3163/3732], Loss: 3.0114, accuracy: 9.5874%\n",
      "Epoch [1/2], Step [3164/3732], Loss: 2.9435, accuracy: 9.5923%\n",
      "Epoch [1/2], Step [3165/3732], Loss: 3.1279, accuracy: 9.5932%\n",
      "Epoch [1/2], Step [3166/3732], Loss: 3.6476, accuracy: 9.5902%\n",
      "Epoch [1/2], Step [3167/3732], Loss: 3.8839, accuracy: 9.5911%\n",
      "Epoch [1/2], Step [3168/3732], Loss: 3.1848, accuracy: 9.5920%\n",
      "Epoch [1/2], Step [3169/3732], Loss: 3.8611, accuracy: 9.5890%\n",
      "Epoch [1/2], Step [3170/3732], Loss: 3.5208, accuracy: 9.5899%\n",
      "Epoch [1/2], Step [3171/3732], Loss: 3.3466, accuracy: 9.5869%\n",
      "Epoch [1/2], Step [3172/3732], Loss: 3.8540, accuracy: 9.5878%\n",
      "Epoch [1/2], Step [3173/3732], Loss: 3.4688, accuracy: 9.5887%\n",
      "Epoch [1/2], Step [3174/3732], Loss: 3.3878, accuracy: 9.5896%\n",
      "Epoch [1/2], Step [3175/3732], Loss: 3.1927, accuracy: 9.5945%\n",
      "Epoch [1/2], Step [3176/3732], Loss: 3.4184, accuracy: 9.5915%\n",
      "Epoch [1/2], Step [3177/3732], Loss: 3.1567, accuracy: 9.5924%\n",
      "Epoch [1/2], Step [3178/3732], Loss: 3.0820, accuracy: 9.6012%\n",
      "Epoch [1/2], Step [3179/3732], Loss: 2.6141, accuracy: 9.6099%\n",
      "Epoch [1/2], Step [3180/3732], Loss: 3.4283, accuracy: 9.6069%\n",
      "Epoch [1/2], Step [3181/3732], Loss: 3.0320, accuracy: 9.6157%\n",
      "Epoch [1/2], Step [3182/3732], Loss: 4.4616, accuracy: 9.6127%\n",
      "Epoch [1/2], Step [3183/3732], Loss: 3.6407, accuracy: 9.6096%\n",
      "Epoch [1/2], Step [3184/3732], Loss: 3.9828, accuracy: 9.6066%\n",
      "Epoch [1/2], Step [3185/3732], Loss: 3.4030, accuracy: 9.6036%\n",
      "Epoch [1/2], Step [3186/3732], Loss: 3.2769, accuracy: 9.6006%\n",
      "Epoch [1/2], Step [3187/3732], Loss: 3.7184, accuracy: 9.6015%\n",
      "Epoch [1/2], Step [3188/3732], Loss: 3.6722, accuracy: 9.6024%\n",
      "Epoch [1/2], Step [3189/3732], Loss: 3.1565, accuracy: 9.6072%\n",
      "Epoch [1/2], Step [3190/3732], Loss: 3.2232, accuracy: 9.6121%\n",
      "Epoch [1/2], Step [3191/3732], Loss: 3.4144, accuracy: 9.6091%\n",
      "Epoch [1/2], Step [3192/3732], Loss: 3.0563, accuracy: 9.6139%\n",
      "Epoch [1/2], Step [3193/3732], Loss: 3.5818, accuracy: 9.6148%\n",
      "Epoch [1/2], Step [3194/3732], Loss: 3.5067, accuracy: 9.6118%\n",
      "Epoch [1/2], Step [3195/3732], Loss: 2.6995, accuracy: 9.6166%\n",
      "Epoch [1/2], Step [3196/3732], Loss: 3.5439, accuracy: 9.6136%\n",
      "Epoch [1/2], Step [3197/3732], Loss: 3.3784, accuracy: 9.6145%\n",
      "Epoch [1/2], Step [3198/3732], Loss: 3.2190, accuracy: 9.6154%\n",
      "Epoch [1/2], Step [3199/3732], Loss: 3.2353, accuracy: 9.6124%\n",
      "Epoch [1/2], Step [3200/3732], Loss: 3.3987, accuracy: 9.6094%\n",
      "Epoch [1/2], Step [3201/3732], Loss: 3.0650, accuracy: 9.6142%\n",
      "Epoch [1/2], Step [3202/3732], Loss: 3.0360, accuracy: 9.6229%\n",
      "Epoch [1/2], Step [3203/3732], Loss: 2.9335, accuracy: 9.6277%\n",
      "Epoch [1/2], Step [3204/3732], Loss: 2.7706, accuracy: 9.6247%\n",
      "Epoch [1/2], Step [3205/3732], Loss: 3.4948, accuracy: 9.6256%\n",
      "Epoch [1/2], Step [3206/3732], Loss: 2.9912, accuracy: 9.6304%\n",
      "Epoch [1/2], Step [3207/3732], Loss: 3.1590, accuracy: 9.6352%\n",
      "Epoch [1/2], Step [3208/3732], Loss: 3.0984, accuracy: 9.6400%\n",
      "Epoch [1/2], Step [3209/3732], Loss: 3.6851, accuracy: 9.6370%\n",
      "Epoch [1/2], Step [3210/3732], Loss: 3.2720, accuracy: 9.6379%\n",
      "Epoch [1/2], Step [3211/3732], Loss: 2.7293, accuracy: 9.6465%\n",
      "Epoch [1/2], Step [3212/3732], Loss: 2.9501, accuracy: 9.6552%\n",
      "Epoch [1/2], Step [3213/3732], Loss: 2.9646, accuracy: 9.6561%\n",
      "Epoch [1/2], Step [3214/3732], Loss: 3.1301, accuracy: 9.6531%\n",
      "Epoch [1/2], Step [3215/3732], Loss: 3.9739, accuracy: 9.6540%\n",
      "Epoch [1/2], Step [3216/3732], Loss: 4.1575, accuracy: 9.6510%\n",
      "Epoch [1/2], Step [3217/3732], Loss: 2.7170, accuracy: 9.6518%\n",
      "Epoch [1/2], Step [3218/3732], Loss: 3.5195, accuracy: 9.6489%\n",
      "Epoch [1/2], Step [3219/3732], Loss: 2.8425, accuracy: 9.6536%\n",
      "Epoch [1/2], Step [3220/3732], Loss: 3.3532, accuracy: 9.6584%\n",
      "Epoch [1/2], Step [3221/3732], Loss: 2.6244, accuracy: 9.6670%\n",
      "Epoch [1/2], Step [3222/3732], Loss: 3.4195, accuracy: 9.6640%\n",
      "Epoch [1/2], Step [3223/3732], Loss: 3.1741, accuracy: 9.6688%\n",
      "Epoch [1/2], Step [3224/3732], Loss: 3.2208, accuracy: 9.6735%\n",
      "Epoch [1/2], Step [3225/3732], Loss: 3.6453, accuracy: 9.6783%\n",
      "Epoch [1/2], Step [3226/3732], Loss: 3.2193, accuracy: 9.6792%\n",
      "Epoch [1/2], Step [3227/3732], Loss: 3.7149, accuracy: 9.6762%\n",
      "Epoch [1/2], Step [3228/3732], Loss: 3.7598, accuracy: 9.6770%\n",
      "Epoch [1/2], Step [3229/3732], Loss: 3.5439, accuracy: 9.6818%\n",
      "Epoch [1/2], Step [3230/3732], Loss: 3.0610, accuracy: 9.6865%\n",
      "Epoch [1/2], Step [3231/3732], Loss: 3.2892, accuracy: 9.6874%\n",
      "Epoch [1/2], Step [3232/3732], Loss: 3.1451, accuracy: 9.6844%\n",
      "Epoch [1/2], Step [3233/3732], Loss: 3.4031, accuracy: 9.6853%\n",
      "Epoch [1/2], Step [3234/3732], Loss: 3.0253, accuracy: 9.6861%\n",
      "Epoch [1/2], Step [3235/3732], Loss: 2.8531, accuracy: 9.6832%\n",
      "Epoch [1/2], Step [3236/3732], Loss: 3.2324, accuracy: 9.6840%\n",
      "Epoch [1/2], Step [3237/3732], Loss: 3.5963, accuracy: 9.6849%\n",
      "Epoch [1/2], Step [3238/3732], Loss: 3.1683, accuracy: 9.6896%\n",
      "Epoch [1/2], Step [3239/3732], Loss: 2.5491, accuracy: 9.6944%\n",
      "Epoch [1/2], Step [3240/3732], Loss: 3.9587, accuracy: 9.6914%\n",
      "Epoch [1/2], Step [3241/3732], Loss: 2.9180, accuracy: 9.6922%\n",
      "Epoch [1/2], Step [3242/3732], Loss: 3.6893, accuracy: 9.6931%\n",
      "Epoch [1/2], Step [3243/3732], Loss: 4.0295, accuracy: 9.6940%\n",
      "Epoch [1/2], Step [3244/3732], Loss: 3.1368, accuracy: 9.6948%\n",
      "Epoch [1/2], Step [3245/3732], Loss: 2.7532, accuracy: 9.7034%\n",
      "Epoch [1/2], Step [3246/3732], Loss: 3.4144, accuracy: 9.7043%\n",
      "Epoch [1/2], Step [3247/3732], Loss: 3.2166, accuracy: 9.7051%\n",
      "Epoch [1/2], Step [3248/3732], Loss: 3.1565, accuracy: 9.7137%\n",
      "Epoch [1/2], Step [3249/3732], Loss: 3.0744, accuracy: 9.7145%\n",
      "Epoch [1/2], Step [3250/3732], Loss: 3.3757, accuracy: 9.7154%\n",
      "Epoch [1/2], Step [3251/3732], Loss: 3.5088, accuracy: 9.7201%\n",
      "Epoch [1/2], Step [3252/3732], Loss: 3.9212, accuracy: 9.7171%\n",
      "Epoch [1/2], Step [3253/3732], Loss: 3.2624, accuracy: 9.7141%\n",
      "Epoch [1/2], Step [3254/3732], Loss: 4.0101, accuracy: 9.7150%\n",
      "Epoch [1/2], Step [3255/3732], Loss: 3.7018, accuracy: 9.7120%\n",
      "Epoch [1/2], Step [3256/3732], Loss: 3.3204, accuracy: 9.7128%\n",
      "Epoch [1/2], Step [3257/3732], Loss: 3.2104, accuracy: 9.7214%\n",
      "Epoch [1/2], Step [3258/3732], Loss: 3.3351, accuracy: 9.7222%\n",
      "Epoch [1/2], Step [3259/3732], Loss: 3.4754, accuracy: 9.7231%\n",
      "Epoch [1/2], Step [3260/3732], Loss: 3.6623, accuracy: 9.7201%\n",
      "Epoch [1/2], Step [3261/3732], Loss: 3.2750, accuracy: 9.7248%\n",
      "Epoch [1/2], Step [3262/3732], Loss: 3.2975, accuracy: 9.7295%\n",
      "Epoch [1/2], Step [3263/3732], Loss: 3.2261, accuracy: 9.7303%\n",
      "Epoch [1/2], Step [3264/3732], Loss: 3.0890, accuracy: 9.7312%\n",
      "Epoch [1/2], Step [3265/3732], Loss: 3.4746, accuracy: 9.7282%\n",
      "Epoch [1/2], Step [3266/3732], Loss: 3.7017, accuracy: 9.7290%\n",
      "Epoch [1/2], Step [3267/3732], Loss: 3.4477, accuracy: 9.7299%\n",
      "Epoch [1/2], Step [3268/3732], Loss: 3.5395, accuracy: 9.7307%\n",
      "Epoch [1/2], Step [3269/3732], Loss: 2.9941, accuracy: 9.7316%\n",
      "Epoch [1/2], Step [3270/3732], Loss: 2.3355, accuracy: 9.7439%\n",
      "Epoch [1/2], Step [3271/3732], Loss: 3.5241, accuracy: 9.7447%\n",
      "Epoch [1/2], Step [3272/3732], Loss: 3.2832, accuracy: 9.7456%\n",
      "Epoch [1/2], Step [3273/3732], Loss: 3.7623, accuracy: 9.7426%\n",
      "Epoch [1/2], Step [3274/3732], Loss: 2.8887, accuracy: 9.7434%\n",
      "Epoch [1/2], Step [3275/3732], Loss: 2.7711, accuracy: 9.7481%\n",
      "Epoch [1/2], Step [3276/3732], Loss: 3.5042, accuracy: 9.7527%\n",
      "Epoch [1/2], Step [3277/3732], Loss: 2.8533, accuracy: 9.7574%\n",
      "Epoch [1/2], Step [3278/3732], Loss: 2.9558, accuracy: 9.7659%\n",
      "Epoch [1/2], Step [3279/3732], Loss: 3.3246, accuracy: 9.7629%\n",
      "Epoch [1/2], Step [3280/3732], Loss: 2.8819, accuracy: 9.7713%\n",
      "Epoch [1/2], Step [3281/3732], Loss: 2.8911, accuracy: 9.7722%\n",
      "Epoch [1/2], Step [3282/3732], Loss: 3.4555, accuracy: 9.7730%\n",
      "Epoch [1/2], Step [3283/3732], Loss: 3.1335, accuracy: 9.7700%\n",
      "Epoch [1/2], Step [3284/3732], Loss: 3.3993, accuracy: 9.7747%\n",
      "Epoch [1/2], Step [3285/3732], Loss: 3.3489, accuracy: 9.7717%\n",
      "Epoch [1/2], Step [3286/3732], Loss: 2.8292, accuracy: 9.7725%\n",
      "Epoch [1/2], Step [3287/3732], Loss: 3.0308, accuracy: 9.7810%\n",
      "Epoch [1/2], Step [3288/3732], Loss: 4.0524, accuracy: 9.7818%\n",
      "Epoch [1/2], Step [3289/3732], Loss: 3.7650, accuracy: 9.7826%\n",
      "Epoch [1/2], Step [3290/3732], Loss: 3.1410, accuracy: 9.7796%\n",
      "Epoch [1/2], Step [3291/3732], Loss: 2.7827, accuracy: 9.7805%\n",
      "Epoch [1/2], Step [3292/3732], Loss: 3.3341, accuracy: 9.7813%\n",
      "Epoch [1/2], Step [3293/3732], Loss: 3.1265, accuracy: 9.7821%\n",
      "Epoch [1/2], Step [3294/3732], Loss: 3.7354, accuracy: 9.7791%\n",
      "Epoch [1/2], Step [3295/3732], Loss: 3.7383, accuracy: 9.7762%\n",
      "Epoch [1/2], Step [3296/3732], Loss: 3.2099, accuracy: 9.7770%\n",
      "Epoch [1/2], Step [3297/3732], Loss: 4.2853, accuracy: 9.7778%\n",
      "Epoch [1/2], Step [3298/3732], Loss: 2.7045, accuracy: 9.7862%\n",
      "Epoch [1/2], Step [3299/3732], Loss: 3.4673, accuracy: 9.7871%\n",
      "Epoch [1/2], Step [3300/3732], Loss: 3.4534, accuracy: 9.7879%\n",
      "Epoch [1/2], Step [3301/3732], Loss: 2.8694, accuracy: 9.7925%\n",
      "Epoch [1/2], Step [3302/3732], Loss: 2.9003, accuracy: 9.7971%\n",
      "Epoch [1/2], Step [3303/3732], Loss: 3.7050, accuracy: 9.7941%\n",
      "Epoch [1/2], Step [3304/3732], Loss: 3.0664, accuracy: 9.7912%\n",
      "Epoch [1/2], Step [3305/3732], Loss: 3.3050, accuracy: 9.7958%\n",
      "Epoch [1/2], Step [3306/3732], Loss: 3.9368, accuracy: 9.7966%\n",
      "Epoch [1/2], Step [3307/3732], Loss: 2.7240, accuracy: 9.8012%\n",
      "Epoch [1/2], Step [3308/3732], Loss: 3.5247, accuracy: 9.7982%\n",
      "Epoch [1/2], Step [3309/3732], Loss: 3.1635, accuracy: 9.8028%\n",
      "Epoch [1/2], Step [3310/3732], Loss: 3.2548, accuracy: 9.8036%\n",
      "Epoch [1/2], Step [3311/3732], Loss: 3.7179, accuracy: 9.8007%\n",
      "Epoch [1/2], Step [3312/3732], Loss: 3.4376, accuracy: 9.8015%\n",
      "Epoch [1/2], Step [3313/3732], Loss: 3.1306, accuracy: 9.8023%\n",
      "Epoch [1/2], Step [3314/3732], Loss: 3.7172, accuracy: 9.8031%\n",
      "Epoch [1/2], Step [3315/3732], Loss: 2.9923, accuracy: 9.8039%\n",
      "Epoch [1/2], Step [3316/3732], Loss: 2.7338, accuracy: 9.8085%\n",
      "Epoch [1/2], Step [3317/3732], Loss: 3.2453, accuracy: 9.8093%\n",
      "Epoch [1/2], Step [3318/3732], Loss: 3.7806, accuracy: 9.8101%\n",
      "Epoch [1/2], Step [3319/3732], Loss: 3.0350, accuracy: 9.8147%\n",
      "Epoch [1/2], Step [3320/3732], Loss: 3.6432, accuracy: 9.8117%\n",
      "Epoch [1/2], Step [3321/3732], Loss: 3.3375, accuracy: 9.8163%\n",
      "Epoch [1/2], Step [3322/3732], Loss: 3.2359, accuracy: 9.8209%\n",
      "Epoch [1/2], Step [3323/3732], Loss: 2.3578, accuracy: 9.8255%\n",
      "Epoch [1/2], Step [3324/3732], Loss: 2.9744, accuracy: 9.8300%\n",
      "Epoch [1/2], Step [3325/3732], Loss: 2.7592, accuracy: 9.8383%\n",
      "Epoch [1/2], Step [3326/3732], Loss: 3.4914, accuracy: 9.8354%\n",
      "Epoch [1/2], Step [3327/3732], Loss: 3.6803, accuracy: 9.8324%\n",
      "Epoch [1/2], Step [3328/3732], Loss: 3.4700, accuracy: 9.8295%\n",
      "Epoch [1/2], Step [3329/3732], Loss: 3.0202, accuracy: 9.8303%\n",
      "Epoch [1/2], Step [3330/3732], Loss: 3.1459, accuracy: 9.8348%\n",
      "Epoch [1/2], Step [3331/3732], Loss: 2.8088, accuracy: 9.8394%\n",
      "Epoch [1/2], Step [3332/3732], Loss: 3.2158, accuracy: 9.8439%\n",
      "Epoch [1/2], Step [3333/3732], Loss: 3.2157, accuracy: 9.8522%\n",
      "Epoch [1/2], Step [3334/3732], Loss: 2.9904, accuracy: 9.8493%\n",
      "Epoch [1/2], Step [3335/3732], Loss: 4.2134, accuracy: 9.8501%\n",
      "Epoch [1/2], Step [3336/3732], Loss: 3.1551, accuracy: 9.8509%\n",
      "Epoch [1/2], Step [3337/3732], Loss: 3.4907, accuracy: 9.8479%\n",
      "Epoch [1/2], Step [3338/3732], Loss: 4.3032, accuracy: 9.8450%\n",
      "Epoch [1/2], Step [3339/3732], Loss: 3.4020, accuracy: 9.8458%\n",
      "Epoch [1/2], Step [3340/3732], Loss: 3.4641, accuracy: 9.8428%\n",
      "Epoch [1/2], Step [3341/3732], Loss: 3.0376, accuracy: 9.8436%\n",
      "Epoch [1/2], Step [3342/3732], Loss: 3.2894, accuracy: 9.8481%\n",
      "Epoch [1/2], Step [3343/3732], Loss: 2.9284, accuracy: 9.8489%\n",
      "Epoch [1/2], Step [3344/3732], Loss: 3.0524, accuracy: 9.8460%\n",
      "Epoch [1/2], Step [3345/3732], Loss: 2.9968, accuracy: 9.8505%\n",
      "Epoch [1/2], Step [3346/3732], Loss: 3.7942, accuracy: 9.8476%\n",
      "Epoch [1/2], Step [3347/3732], Loss: 3.1219, accuracy: 9.8484%\n",
      "Epoch [1/2], Step [3348/3732], Loss: 3.3017, accuracy: 9.8492%\n",
      "Epoch [1/2], Step [3349/3732], Loss: 2.8372, accuracy: 9.8537%\n",
      "Epoch [1/2], Step [3350/3732], Loss: 3.2854, accuracy: 9.8545%\n",
      "Epoch [1/2], Step [3351/3732], Loss: 3.4206, accuracy: 9.8553%\n",
      "Epoch [1/2], Step [3352/3732], Loss: 3.4057, accuracy: 9.8523%\n",
      "Epoch [1/2], Step [3353/3732], Loss: 2.8641, accuracy: 9.8531%\n",
      "Epoch [1/2], Step [3354/3732], Loss: 2.5922, accuracy: 9.8576%\n",
      "Epoch [1/2], Step [3355/3732], Loss: 3.4810, accuracy: 9.8584%\n",
      "Epoch [1/2], Step [3356/3732], Loss: 3.4947, accuracy: 9.8629%\n",
      "Epoch [1/2], Step [3357/3732], Loss: 3.0876, accuracy: 9.8637%\n",
      "Epoch [1/2], Step [3358/3732], Loss: 4.1172, accuracy: 9.8608%\n",
      "Epoch [1/2], Step [3359/3732], Loss: 3.3748, accuracy: 9.8578%\n",
      "Epoch [1/2], Step [3360/3732], Loss: 2.6266, accuracy: 9.8624%\n",
      "Epoch [1/2], Step [3361/3732], Loss: 3.9985, accuracy: 9.8594%\n",
      "Epoch [1/2], Step [3362/3732], Loss: 3.2823, accuracy: 9.8565%\n",
      "Epoch [1/2], Step [3363/3732], Loss: 3.4603, accuracy: 9.8610%\n",
      "Epoch [1/2], Step [3364/3732], Loss: 2.8156, accuracy: 9.8618%\n",
      "Epoch [1/2], Step [3365/3732], Loss: 3.0475, accuracy: 9.8663%\n",
      "Epoch [1/2], Step [3366/3732], Loss: 3.0802, accuracy: 9.8782%\n",
      "Epoch [1/2], Step [3367/3732], Loss: 3.3821, accuracy: 9.8864%\n",
      "Epoch [1/2], Step [3368/3732], Loss: 3.3680, accuracy: 9.8909%\n",
      "Epoch [1/2], Step [3369/3732], Loss: 3.2981, accuracy: 9.8917%\n",
      "Epoch [1/2], Step [3370/3732], Loss: 3.0668, accuracy: 9.8924%\n",
      "Epoch [1/2], Step [3371/3732], Loss: 3.9278, accuracy: 9.8895%\n",
      "Epoch [1/2], Step [3372/3732], Loss: 2.6334, accuracy: 9.8977%\n",
      "Epoch [1/2], Step [3373/3732], Loss: 3.2767, accuracy: 9.8985%\n",
      "Epoch [1/2], Step [3374/3732], Loss: 3.1909, accuracy: 9.8992%\n",
      "Epoch [1/2], Step [3375/3732], Loss: 3.5986, accuracy: 9.9037%\n",
      "Epoch [1/2], Step [3376/3732], Loss: 3.6200, accuracy: 9.9008%\n",
      "Epoch [1/2], Step [3377/3732], Loss: 3.0637, accuracy: 9.9015%\n",
      "Epoch [1/2], Step [3378/3732], Loss: 3.2918, accuracy: 9.9097%\n",
      "Epoch [1/2], Step [3379/3732], Loss: 3.6748, accuracy: 9.9068%\n",
      "Epoch [1/2], Step [3380/3732], Loss: 4.1759, accuracy: 9.9038%\n",
      "Epoch [1/2], Step [3381/3732], Loss: 3.4355, accuracy: 9.9046%\n",
      "Epoch [1/2], Step [3382/3732], Loss: 3.3635, accuracy: 9.9017%\n",
      "Epoch [1/2], Step [3383/3732], Loss: 3.1943, accuracy: 9.8988%\n",
      "Epoch [1/2], Step [3384/3732], Loss: 4.0099, accuracy: 9.8995%\n",
      "Epoch [1/2], Step [3385/3732], Loss: 3.2525, accuracy: 9.9003%\n",
      "Epoch [1/2], Step [3386/3732], Loss: 2.7660, accuracy: 9.9048%\n",
      "Epoch [1/2], Step [3387/3732], Loss: 3.1668, accuracy: 9.9092%\n",
      "Epoch [1/2], Step [3388/3732], Loss: 3.4488, accuracy: 9.9100%\n",
      "Epoch [1/2], Step [3389/3732], Loss: 3.0320, accuracy: 9.9144%\n",
      "Epoch [1/2], Step [3390/3732], Loss: 2.9785, accuracy: 9.9152%\n",
      "Epoch [1/2], Step [3391/3732], Loss: 3.9265, accuracy: 9.9123%\n",
      "Epoch [1/2], Step [3392/3732], Loss: 3.0301, accuracy: 9.9093%\n",
      "Epoch [1/2], Step [3393/3732], Loss: 3.0369, accuracy: 9.9175%\n",
      "Epoch [1/2], Step [3394/3732], Loss: 3.8533, accuracy: 9.9182%\n",
      "Epoch [1/2], Step [3395/3732], Loss: 3.6129, accuracy: 9.9190%\n",
      "Epoch [1/2], Step [3396/3732], Loss: 3.2209, accuracy: 9.9198%\n",
      "Epoch [1/2], Step [3397/3732], Loss: 3.2973, accuracy: 9.9205%\n",
      "Epoch [1/2], Step [3398/3732], Loss: 2.7320, accuracy: 9.9250%\n",
      "Epoch [1/2], Step [3399/3732], Loss: 3.7393, accuracy: 9.9257%\n",
      "Epoch [1/2], Step [3400/3732], Loss: 3.2454, accuracy: 9.9265%\n",
      "Epoch [1/2], Step [3401/3732], Loss: 4.0911, accuracy: 9.9236%\n",
      "Epoch [1/2], Step [3402/3732], Loss: 3.2393, accuracy: 9.9280%\n",
      "Epoch [1/2], Step [3403/3732], Loss: 3.3872, accuracy: 9.9287%\n",
      "Epoch [1/2], Step [3404/3732], Loss: 3.3995, accuracy: 9.9258%\n",
      "Epoch [1/2], Step [3405/3732], Loss: 3.0345, accuracy: 9.9266%\n",
      "Epoch [1/2], Step [3406/3732], Loss: 3.1359, accuracy: 9.9273%\n",
      "Epoch [1/2], Step [3407/3732], Loss: 3.2674, accuracy: 9.9281%\n",
      "Epoch [1/2], Step [3408/3732], Loss: 3.2048, accuracy: 9.9288%\n",
      "Epoch [1/2], Step [3409/3732], Loss: 3.4181, accuracy: 9.9296%\n",
      "Epoch [1/2], Step [3410/3732], Loss: 3.2443, accuracy: 9.9304%\n",
      "Epoch [1/2], Step [3411/3732], Loss: 3.3747, accuracy: 9.9274%\n",
      "Epoch [1/2], Step [3412/3732], Loss: 3.7726, accuracy: 9.9245%\n",
      "Epoch [1/2], Step [3413/3732], Loss: 3.3008, accuracy: 9.9289%\n",
      "Epoch [1/2], Step [3414/3732], Loss: 3.0368, accuracy: 9.9297%\n",
      "Epoch [1/2], Step [3415/3732], Loss: 3.2904, accuracy: 9.9378%\n",
      "Epoch [1/2], Step [3416/3732], Loss: 3.1002, accuracy: 9.9385%\n",
      "Epoch [1/2], Step [3417/3732], Loss: 2.1395, accuracy: 9.9429%\n",
      "Epoch [1/2], Step [3418/3732], Loss: 3.1544, accuracy: 9.9437%\n",
      "Epoch [1/2], Step [3419/3732], Loss: 3.6494, accuracy: 9.9444%\n",
      "Epoch [1/2], Step [3420/3732], Loss: 3.3206, accuracy: 9.9488%\n",
      "Epoch [1/2], Step [3421/3732], Loss: 2.8091, accuracy: 9.9532%\n",
      "Epoch [1/2], Step [3422/3732], Loss: 3.2277, accuracy: 9.9540%\n",
      "Epoch [1/2], Step [3423/3732], Loss: 2.6987, accuracy: 9.9584%\n",
      "Epoch [1/2], Step [3424/3732], Loss: 2.9767, accuracy: 9.9664%\n",
      "Epoch [1/2], Step [3425/3732], Loss: 2.9381, accuracy: 9.9745%\n",
      "Epoch [1/2], Step [3426/3732], Loss: 3.2076, accuracy: 9.9752%\n",
      "Epoch [1/2], Step [3427/3732], Loss: 3.5365, accuracy: 9.9759%\n",
      "Epoch [1/2], Step [3428/3732], Loss: 3.1288, accuracy: 9.9730%\n",
      "Epoch [1/2], Step [3429/3732], Loss: 2.5252, accuracy: 9.9774%\n",
      "Epoch [1/2], Step [3430/3732], Loss: 2.7631, accuracy: 9.9818%\n",
      "Epoch [1/2], Step [3431/3732], Loss: 3.7455, accuracy: 9.9789%\n",
      "Epoch [1/2], Step [3432/3732], Loss: 3.6653, accuracy: 9.9760%\n",
      "Epoch [1/2], Step [3433/3732], Loss: 3.5667, accuracy: 9.9767%\n",
      "Epoch [1/2], Step [3434/3732], Loss: 2.7127, accuracy: 9.9774%\n",
      "Epoch [1/2], Step [3435/3732], Loss: 3.2572, accuracy: 9.9818%\n",
      "Epoch [1/2], Step [3436/3732], Loss: 3.2902, accuracy: 9.9862%\n",
      "Epoch [1/2], Step [3437/3732], Loss: 3.8920, accuracy: 9.9869%\n",
      "Epoch [1/2], Step [3438/3732], Loss: 3.7575, accuracy: 9.9840%\n",
      "Epoch [1/2], Step [3439/3732], Loss: 2.6963, accuracy: 9.9884%\n",
      "Epoch [1/2], Step [3440/3732], Loss: 2.6401, accuracy: 9.9927%\n",
      "Epoch [1/2], Step [3441/3732], Loss: 2.7024, accuracy: 10.0007%\n",
      "Epoch [1/2], Step [3442/3732], Loss: 3.3254, accuracy: 10.0051%\n",
      "Epoch [1/2], Step [3443/3732], Loss: 3.5312, accuracy: 10.0058%\n",
      "Epoch [1/2], Step [3444/3732], Loss: 2.5795, accuracy: 10.0138%\n",
      "Epoch [1/2], Step [3445/3732], Loss: 2.8251, accuracy: 10.0218%\n",
      "Epoch [1/2], Step [3446/3732], Loss: 3.2467, accuracy: 10.0225%\n",
      "Epoch [1/2], Step [3447/3732], Loss: 3.1125, accuracy: 10.0305%\n",
      "Epoch [1/2], Step [3448/3732], Loss: 3.1658, accuracy: 10.0348%\n",
      "Epoch [1/2], Step [3449/3732], Loss: 3.7619, accuracy: 10.0319%\n",
      "Epoch [1/2], Step [3450/3732], Loss: 3.2126, accuracy: 10.0290%\n",
      "Epoch [1/2], Step [3451/3732], Loss: 3.4688, accuracy: 10.0261%\n",
      "Epoch [1/2], Step [3452/3732], Loss: 2.8817, accuracy: 10.0377%\n",
      "Epoch [1/2], Step [3453/3732], Loss: 4.1261, accuracy: 10.0384%\n",
      "Epoch [1/2], Step [3454/3732], Loss: 3.4286, accuracy: 10.0427%\n",
      "Epoch [1/2], Step [3455/3732], Loss: 3.1227, accuracy: 10.0470%\n",
      "Epoch [1/2], Step [3456/3732], Loss: 3.6785, accuracy: 10.0441%\n",
      "Epoch [1/2], Step [3457/3732], Loss: 3.3388, accuracy: 10.0448%\n",
      "Epoch [1/2], Step [3458/3732], Loss: 2.4912, accuracy: 10.0528%\n",
      "Epoch [1/2], Step [3459/3732], Loss: 3.6820, accuracy: 10.0571%\n",
      "Epoch [1/2], Step [3460/3732], Loss: 3.3917, accuracy: 10.0578%\n",
      "Epoch [1/2], Step [3461/3732], Loss: 2.6356, accuracy: 10.0621%\n",
      "Epoch [1/2], Step [3462/3732], Loss: 3.1257, accuracy: 10.0628%\n",
      "Epoch [1/2], Step [3463/3732], Loss: 3.3579, accuracy: 10.0635%\n",
      "Epoch [1/2], Step [3464/3732], Loss: 3.6014, accuracy: 10.0642%\n",
      "Epoch [1/2], Step [3465/3732], Loss: 3.4352, accuracy: 10.0649%\n",
      "Epoch [1/2], Step [3466/3732], Loss: 4.2039, accuracy: 10.0620%\n",
      "Epoch [1/2], Step [3467/3732], Loss: 2.6640, accuracy: 10.0627%\n",
      "Epoch [1/2], Step [3468/3732], Loss: 3.2668, accuracy: 10.0670%\n",
      "Epoch [1/2], Step [3469/3732], Loss: 2.9831, accuracy: 10.0713%\n",
      "Epoch [1/2], Step [3470/3732], Loss: 3.1182, accuracy: 10.0756%\n",
      "Epoch [1/2], Step [3471/3732], Loss: 3.3251, accuracy: 10.0799%\n",
      "Epoch [1/2], Step [3472/3732], Loss: 3.1707, accuracy: 10.0806%\n",
      "Epoch [1/2], Step [3473/3732], Loss: 2.7473, accuracy: 10.0885%\n",
      "Epoch [1/2], Step [3474/3732], Loss: 4.0572, accuracy: 10.0856%\n",
      "Epoch [1/2], Step [3475/3732], Loss: 3.0439, accuracy: 10.0935%\n",
      "Epoch [1/2], Step [3476/3732], Loss: 3.2969, accuracy: 10.0978%\n",
      "Epoch [1/2], Step [3477/3732], Loss: 3.0867, accuracy: 10.0985%\n",
      "Epoch [1/2], Step [3478/3732], Loss: 3.7300, accuracy: 10.0956%\n",
      "Epoch [1/2], Step [3479/3732], Loss: 3.0463, accuracy: 10.0963%\n",
      "Epoch [1/2], Step [3480/3732], Loss: 3.1084, accuracy: 10.0970%\n",
      "Epoch [1/2], Step [3481/3732], Loss: 3.6483, accuracy: 10.1013%\n",
      "Epoch [1/2], Step [3482/3732], Loss: 3.0014, accuracy: 10.1020%\n",
      "Epoch [1/2], Step [3483/3732], Loss: 3.2697, accuracy: 10.1098%\n",
      "Epoch [1/2], Step [3484/3732], Loss: 3.4698, accuracy: 10.1105%\n",
      "Epoch [1/2], Step [3485/3732], Loss: 3.7234, accuracy: 10.1112%\n",
      "Epoch [1/2], Step [3486/3732], Loss: 2.6550, accuracy: 10.1119%\n",
      "Epoch [1/2], Step [3487/3732], Loss: 3.0972, accuracy: 10.1126%\n",
      "Epoch [1/2], Step [3488/3732], Loss: 2.8073, accuracy: 10.1204%\n",
      "Epoch [1/2], Step [3489/3732], Loss: 3.5984, accuracy: 10.1175%\n",
      "Epoch [1/2], Step [3490/3732], Loss: 2.9530, accuracy: 10.1182%\n",
      "Epoch [1/2], Step [3491/3732], Loss: 2.5043, accuracy: 10.1260%\n",
      "Epoch [1/2], Step [3492/3732], Loss: 3.4505, accuracy: 10.1231%\n",
      "Epoch [1/2], Step [3493/3732], Loss: 3.0408, accuracy: 10.1238%\n",
      "Epoch [1/2], Step [3494/3732], Loss: 2.4143, accuracy: 10.1281%\n",
      "Epoch [1/2], Step [3495/3732], Loss: 2.7072, accuracy: 10.1359%\n",
      "Epoch [1/2], Step [3496/3732], Loss: 2.7911, accuracy: 10.1330%\n",
      "Epoch [1/2], Step [3497/3732], Loss: 3.3198, accuracy: 10.1337%\n",
      "Epoch [1/2], Step [3498/3732], Loss: 3.2051, accuracy: 10.1308%\n",
      "Epoch [1/2], Step [3499/3732], Loss: 3.5259, accuracy: 10.1279%\n",
      "Epoch [1/2], Step [3500/3732], Loss: 3.4053, accuracy: 10.1286%\n",
      "Epoch [1/2], Step [3501/3732], Loss: 3.3291, accuracy: 10.1257%\n",
      "Epoch [1/2], Step [3502/3732], Loss: 3.8589, accuracy: 10.1228%\n",
      "Epoch [1/2], Step [3503/3732], Loss: 3.0858, accuracy: 10.1235%\n",
      "Epoch [1/2], Step [3504/3732], Loss: 2.4216, accuracy: 10.1277%\n",
      "Epoch [1/2], Step [3505/3732], Loss: 2.8379, accuracy: 10.1320%\n",
      "Epoch [1/2], Step [3506/3732], Loss: 3.4877, accuracy: 10.1326%\n",
      "Epoch [1/2], Step [3507/3732], Loss: 3.0883, accuracy: 10.1333%\n",
      "Epoch [1/2], Step [3508/3732], Loss: 3.3922, accuracy: 10.1340%\n",
      "Epoch [1/2], Step [3509/3732], Loss: 3.2748, accuracy: 10.1382%\n",
      "Epoch [1/2], Step [3510/3732], Loss: 2.9725, accuracy: 10.1460%\n",
      "Epoch [1/2], Step [3511/3732], Loss: 3.3153, accuracy: 10.1431%\n",
      "Epoch [1/2], Step [3512/3732], Loss: 3.5078, accuracy: 10.1402%\n",
      "Epoch [1/2], Step [3513/3732], Loss: 3.7251, accuracy: 10.1373%\n",
      "Epoch [1/2], Step [3514/3732], Loss: 2.3600, accuracy: 10.1522%\n",
      "Epoch [1/2], Step [3515/3732], Loss: 3.7742, accuracy: 10.1494%\n",
      "Epoch [1/2], Step [3516/3732], Loss: 3.2908, accuracy: 10.1465%\n",
      "Epoch [1/2], Step [3517/3732], Loss: 3.0126, accuracy: 10.1471%\n",
      "Epoch [1/2], Step [3518/3732], Loss: 2.3287, accuracy: 10.1585%\n",
      "Epoch [1/2], Step [3519/3732], Loss: 3.6486, accuracy: 10.1556%\n",
      "Epoch [1/2], Step [3520/3732], Loss: 3.4861, accuracy: 10.1598%\n",
      "Epoch [1/2], Step [3521/3732], Loss: 2.9327, accuracy: 10.1605%\n",
      "Epoch [1/2], Step [3522/3732], Loss: 2.4836, accuracy: 10.1682%\n",
      "Epoch [1/2], Step [3523/3732], Loss: 2.9612, accuracy: 10.1724%\n",
      "Epoch [1/2], Step [3524/3732], Loss: 2.9395, accuracy: 10.1766%\n",
      "Epoch [1/2], Step [3525/3732], Loss: 3.1748, accuracy: 10.1809%\n",
      "Epoch [1/2], Step [3526/3732], Loss: 3.4676, accuracy: 10.1851%\n",
      "Epoch [1/2], Step [3527/3732], Loss: 2.8418, accuracy: 10.1893%\n",
      "Epoch [1/2], Step [3528/3732], Loss: 2.6409, accuracy: 10.2005%\n",
      "Epoch [1/2], Step [3529/3732], Loss: 3.3407, accuracy: 10.1976%\n",
      "Epoch [1/2], Step [3530/3732], Loss: 3.7702, accuracy: 10.1948%\n",
      "Epoch [1/2], Step [3531/3732], Loss: 4.0450, accuracy: 10.1990%\n",
      "Epoch [1/2], Step [3532/3732], Loss: 2.9383, accuracy: 10.2031%\n",
      "Epoch [1/2], Step [3533/3732], Loss: 3.5665, accuracy: 10.2038%\n",
      "Epoch [1/2], Step [3534/3732], Loss: 3.2236, accuracy: 10.2115%\n",
      "Epoch [1/2], Step [3535/3732], Loss: 3.4244, accuracy: 10.2122%\n",
      "Epoch [1/2], Step [3536/3732], Loss: 3.8427, accuracy: 10.2093%\n",
      "Epoch [1/2], Step [3537/3732], Loss: 2.9211, accuracy: 10.2135%\n",
      "Epoch [1/2], Step [3538/3732], Loss: 3.3023, accuracy: 10.2106%\n",
      "Epoch [1/2], Step [3539/3732], Loss: 3.1169, accuracy: 10.2112%\n",
      "Epoch [1/2], Step [3540/3732], Loss: 2.8213, accuracy: 10.2154%\n",
      "Epoch [1/2], Step [3541/3732], Loss: 3.2433, accuracy: 10.2196%\n",
      "Epoch [1/2], Step [3542/3732], Loss: 3.6290, accuracy: 10.2202%\n",
      "Epoch [1/2], Step [3543/3732], Loss: 2.6744, accuracy: 10.2314%\n",
      "Epoch [1/2], Step [3544/3732], Loss: 3.4463, accuracy: 10.2286%\n",
      "Epoch [1/2], Step [3545/3732], Loss: 3.4865, accuracy: 10.2257%\n",
      "Epoch [1/2], Step [3546/3732], Loss: 3.0946, accuracy: 10.2334%\n",
      "Epoch [1/2], Step [3547/3732], Loss: 3.3052, accuracy: 10.2375%\n",
      "Epoch [1/2], Step [3548/3732], Loss: 3.4926, accuracy: 10.2382%\n",
      "Epoch [1/2], Step [3549/3732], Loss: 2.6954, accuracy: 10.2423%\n",
      "Epoch [1/2], Step [3550/3732], Loss: 3.0254, accuracy: 10.2465%\n",
      "Epoch [1/2], Step [3551/3732], Loss: 3.7102, accuracy: 10.2436%\n",
      "Epoch [1/2], Step [3552/3732], Loss: 2.5827, accuracy: 10.2477%\n",
      "Epoch [1/2], Step [3553/3732], Loss: 2.8507, accuracy: 10.2484%\n",
      "Epoch [1/2], Step [3554/3732], Loss: 3.7732, accuracy: 10.2490%\n",
      "Epoch [1/2], Step [3555/3732], Loss: 3.3001, accuracy: 10.2496%\n",
      "Epoch [1/2], Step [3556/3732], Loss: 3.2192, accuracy: 10.2503%\n",
      "Epoch [1/2], Step [3557/3732], Loss: 3.4049, accuracy: 10.2509%\n",
      "Epoch [1/2], Step [3558/3732], Loss: 2.8088, accuracy: 10.2515%\n",
      "Epoch [1/2], Step [3559/3732], Loss: 2.8253, accuracy: 10.2557%\n",
      "Epoch [1/2], Step [3560/3732], Loss: 2.6230, accuracy: 10.2598%\n",
      "Epoch [1/2], Step [3561/3732], Loss: 3.6269, accuracy: 10.2570%\n",
      "Epoch [1/2], Step [3562/3732], Loss: 3.1503, accuracy: 10.2541%\n",
      "Epoch [1/2], Step [3563/3732], Loss: 2.9602, accuracy: 10.2582%\n",
      "Epoch [1/2], Step [3564/3732], Loss: 3.4934, accuracy: 10.2553%\n",
      "Epoch [1/2], Step [3565/3732], Loss: 3.6520, accuracy: 10.2560%\n",
      "Epoch [1/2], Step [3566/3732], Loss: 2.7088, accuracy: 10.2601%\n",
      "Epoch [1/2], Step [3567/3732], Loss: 1.9487, accuracy: 10.2712%\n",
      "Epoch [1/2], Step [3568/3732], Loss: 3.0584, accuracy: 10.2684%\n",
      "Epoch [1/2], Step [3569/3732], Loss: 3.0645, accuracy: 10.2725%\n",
      "Epoch [1/2], Step [3570/3732], Loss: 3.1503, accuracy: 10.2696%\n",
      "Epoch [1/2], Step [3571/3732], Loss: 2.9736, accuracy: 10.2702%\n",
      "Epoch [1/2], Step [3572/3732], Loss: 2.5182, accuracy: 10.2814%\n",
      "Epoch [1/2], Step [3573/3732], Loss: 3.1316, accuracy: 10.2820%\n",
      "Epoch [1/2], Step [3574/3732], Loss: 3.5481, accuracy: 10.2826%\n",
      "Epoch [1/2], Step [3575/3732], Loss: 3.3623, accuracy: 10.2832%\n",
      "Epoch [1/2], Step [3576/3732], Loss: 3.4621, accuracy: 10.2873%\n",
      "Epoch [1/2], Step [3577/3732], Loss: 2.5528, accuracy: 10.2914%\n",
      "Epoch [1/2], Step [3578/3732], Loss: 3.1023, accuracy: 10.2886%\n",
      "Epoch [1/2], Step [3579/3732], Loss: 3.1897, accuracy: 10.2927%\n",
      "Epoch [1/2], Step [3580/3732], Loss: 2.3926, accuracy: 10.3003%\n",
      "Epoch [1/2], Step [3581/3732], Loss: 3.6492, accuracy: 10.2974%\n",
      "Epoch [1/2], Step [3582/3732], Loss: 3.8424, accuracy: 10.2945%\n",
      "Epoch [1/2], Step [3583/3732], Loss: 2.9362, accuracy: 10.3056%\n",
      "Epoch [1/2], Step [3584/3732], Loss: 2.8481, accuracy: 10.3167%\n",
      "Epoch [1/2], Step [3585/3732], Loss: 4.2096, accuracy: 10.3173%\n",
      "Epoch [1/2], Step [3586/3732], Loss: 3.0203, accuracy: 10.3179%\n",
      "Epoch [1/2], Step [3587/3732], Loss: 3.5937, accuracy: 10.3150%\n",
      "Epoch [1/2], Step [3588/3732], Loss: 3.9730, accuracy: 10.3122%\n",
      "Epoch [1/2], Step [3589/3732], Loss: 3.8535, accuracy: 10.3128%\n",
      "Epoch [1/2], Step [3590/3732], Loss: 2.9519, accuracy: 10.3169%\n",
      "Epoch [1/2], Step [3591/3732], Loss: 2.7818, accuracy: 10.3209%\n",
      "Epoch [1/2], Step [3592/3732], Loss: 3.0252, accuracy: 10.3250%\n",
      "Epoch [1/2], Step [3593/3732], Loss: 2.8947, accuracy: 10.3291%\n",
      "Epoch [1/2], Step [3594/3732], Loss: 3.1785, accuracy: 10.3332%\n",
      "Epoch [1/2], Step [3595/3732], Loss: 3.5678, accuracy: 10.3338%\n",
      "Epoch [1/2], Step [3596/3732], Loss: 3.1718, accuracy: 10.3344%\n",
      "Epoch [1/2], Step [3597/3732], Loss: 2.7193, accuracy: 10.3385%\n",
      "Epoch [1/2], Step [3598/3732], Loss: 3.5871, accuracy: 10.3426%\n",
      "Epoch [1/2], Step [3599/3732], Loss: 3.7002, accuracy: 10.3397%\n",
      "Epoch [1/2], Step [3600/3732], Loss: 2.8078, accuracy: 10.3438%\n",
      "Epoch [1/2], Step [3601/3732], Loss: 3.1904, accuracy: 10.3478%\n",
      "Epoch [1/2], Step [3602/3732], Loss: 4.0683, accuracy: 10.3449%\n",
      "Epoch [1/2], Step [3603/3732], Loss: 2.8898, accuracy: 10.3455%\n",
      "Epoch [1/2], Step [3604/3732], Loss: 3.1864, accuracy: 10.3496%\n",
      "Epoch [1/2], Step [3605/3732], Loss: 3.4067, accuracy: 10.3502%\n",
      "Epoch [1/2], Step [3606/3732], Loss: 4.0689, accuracy: 10.3508%\n",
      "Epoch [1/2], Step [3607/3732], Loss: 3.7107, accuracy: 10.3479%\n",
      "Epoch [1/2], Step [3608/3732], Loss: 3.8752, accuracy: 10.3451%\n",
      "Epoch [1/2], Step [3609/3732], Loss: 2.9480, accuracy: 10.3491%\n",
      "Epoch [1/2], Step [3610/3732], Loss: 3.6090, accuracy: 10.3497%\n",
      "Epoch [1/2], Step [3611/3732], Loss: 3.3968, accuracy: 10.3503%\n",
      "Epoch [1/2], Step [3612/3732], Loss: 3.6169, accuracy: 10.3475%\n",
      "Epoch [1/2], Step [3613/3732], Loss: 3.2528, accuracy: 10.3515%\n",
      "Epoch [1/2], Step [3614/3732], Loss: 3.7642, accuracy: 10.3486%\n",
      "Epoch [1/2], Step [3615/3732], Loss: 3.3046, accuracy: 10.3492%\n",
      "Epoch [1/2], Step [3616/3732], Loss: 3.0364, accuracy: 10.3567%\n",
      "Epoch [1/2], Step [3617/3732], Loss: 3.5489, accuracy: 10.3573%\n",
      "Epoch [1/2], Step [3618/3732], Loss: 3.5738, accuracy: 10.3614%\n",
      "Epoch [1/2], Step [3619/3732], Loss: 3.9635, accuracy: 10.3585%\n",
      "Epoch [1/2], Step [3620/3732], Loss: 2.9898, accuracy: 10.3557%\n",
      "Epoch [1/2], Step [3621/3732], Loss: 3.3712, accuracy: 10.3597%\n",
      "Epoch [1/2], Step [3622/3732], Loss: 3.4396, accuracy: 10.3568%\n",
      "Epoch [1/2], Step [3623/3732], Loss: 3.1587, accuracy: 10.3609%\n",
      "Epoch [1/2], Step [3624/3732], Loss: 3.2509, accuracy: 10.3615%\n",
      "Epoch [1/2], Step [3625/3732], Loss: 3.2716, accuracy: 10.3621%\n",
      "Epoch [1/2], Step [3626/3732], Loss: 3.2192, accuracy: 10.3627%\n",
      "Epoch [1/2], Step [3627/3732], Loss: 3.0803, accuracy: 10.3667%\n",
      "Epoch [1/2], Step [3628/3732], Loss: 2.5613, accuracy: 10.3673%\n",
      "Epoch [1/2], Step [3629/3732], Loss: 3.4315, accuracy: 10.3644%\n",
      "Epoch [1/2], Step [3630/3732], Loss: 2.9554, accuracy: 10.3685%\n",
      "Epoch [1/2], Step [3631/3732], Loss: 3.2621, accuracy: 10.3656%\n",
      "Epoch [1/2], Step [3632/3732], Loss: 3.1553, accuracy: 10.3662%\n",
      "Epoch [1/2], Step [3633/3732], Loss: 3.4503, accuracy: 10.3633%\n",
      "Epoch [1/2], Step [3634/3732], Loss: 3.3300, accuracy: 10.3639%\n",
      "Epoch [1/2], Step [3635/3732], Loss: 3.1216, accuracy: 10.3645%\n",
      "Epoch [1/2], Step [3636/3732], Loss: 3.6050, accuracy: 10.3651%\n",
      "Epoch [1/2], Step [3637/3732], Loss: 2.7384, accuracy: 10.3691%\n",
      "Epoch [1/2], Step [3638/3732], Loss: 3.4415, accuracy: 10.3663%\n",
      "Epoch [1/2], Step [3639/3732], Loss: 3.6595, accuracy: 10.3669%\n",
      "Epoch [1/2], Step [3640/3732], Loss: 3.0705, accuracy: 10.3743%\n",
      "Epoch [1/2], Step [3641/3732], Loss: 3.2934, accuracy: 10.3749%\n",
      "Epoch [1/2], Step [3642/3732], Loss: 2.9647, accuracy: 10.3789%\n",
      "Epoch [1/2], Step [3643/3732], Loss: 2.9561, accuracy: 10.3829%\n",
      "Epoch [1/2], Step [3644/3732], Loss: 3.5266, accuracy: 10.3801%\n",
      "Epoch [1/2], Step [3645/3732], Loss: 3.3703, accuracy: 10.3772%\n",
      "Epoch [1/2], Step [3646/3732], Loss: 3.1944, accuracy: 10.3812%\n",
      "Epoch [1/2], Step [3647/3732], Loss: 3.2694, accuracy: 10.3818%\n",
      "Epoch [1/2], Step [3648/3732], Loss: 3.4297, accuracy: 10.3790%\n",
      "Epoch [1/2], Step [3649/3732], Loss: 2.8069, accuracy: 10.3796%\n",
      "Epoch [1/2], Step [3650/3732], Loss: 3.0248, accuracy: 10.3801%\n",
      "Epoch [1/2], Step [3651/3732], Loss: 2.8464, accuracy: 10.3876%\n",
      "Epoch [1/2], Step [3652/3732], Loss: 3.4387, accuracy: 10.3881%\n",
      "Epoch [1/2], Step [3653/3732], Loss: 4.0492, accuracy: 10.3853%\n",
      "Epoch [1/2], Step [3654/3732], Loss: 3.4142, accuracy: 10.3893%\n",
      "Epoch [1/2], Step [3655/3732], Loss: 2.4245, accuracy: 10.4001%\n",
      "Epoch [1/2], Step [3656/3732], Loss: 3.8419, accuracy: 10.3973%\n",
      "Epoch [1/2], Step [3657/3732], Loss: 2.7847, accuracy: 10.3979%\n",
      "Epoch [1/2], Step [3658/3732], Loss: 2.5748, accuracy: 10.4053%\n",
      "Epoch [1/2], Step [3659/3732], Loss: 3.6405, accuracy: 10.4058%\n",
      "Epoch [1/2], Step [3660/3732], Loss: 3.6264, accuracy: 10.4030%\n",
      "Epoch [1/2], Step [3661/3732], Loss: 2.6335, accuracy: 10.4036%\n",
      "Epoch [1/2], Step [3662/3732], Loss: 3.8408, accuracy: 10.4007%\n",
      "Epoch [1/2], Step [3663/3732], Loss: 2.6871, accuracy: 10.4013%\n",
      "Epoch [1/2], Step [3664/3732], Loss: 3.3122, accuracy: 10.4019%\n",
      "Epoch [1/2], Step [3665/3732], Loss: 3.3609, accuracy: 10.3990%\n",
      "Epoch [1/2], Step [3666/3732], Loss: 3.7743, accuracy: 10.3996%\n",
      "Epoch [1/2], Step [3667/3732], Loss: 3.0603, accuracy: 10.3968%\n",
      "Epoch [1/2], Step [3668/3732], Loss: 2.8934, accuracy: 10.3974%\n",
      "Epoch [1/2], Step [3669/3732], Loss: 2.2344, accuracy: 10.4081%\n",
      "Epoch [1/2], Step [3670/3732], Loss: 3.1108, accuracy: 10.4053%\n",
      "Epoch [1/2], Step [3671/3732], Loss: 3.1734, accuracy: 10.4025%\n",
      "Epoch [1/2], Step [3672/3732], Loss: 2.8811, accuracy: 10.4065%\n",
      "Epoch [1/2], Step [3673/3732], Loss: 3.2880, accuracy: 10.4070%\n",
      "Epoch [1/2], Step [3674/3732], Loss: 3.5480, accuracy: 10.4110%\n",
      "Epoch [1/2], Step [3675/3732], Loss: 2.9091, accuracy: 10.4116%\n",
      "Epoch [1/2], Step [3676/3732], Loss: 3.2489, accuracy: 10.4087%\n",
      "Epoch [1/2], Step [3677/3732], Loss: 3.0463, accuracy: 10.4059%\n",
      "Epoch [1/2], Step [3678/3732], Loss: 2.9946, accuracy: 10.4133%\n",
      "Epoch [1/2], Step [3679/3732], Loss: 3.0537, accuracy: 10.4104%\n",
      "Epoch [1/2], Step [3680/3732], Loss: 3.0293, accuracy: 10.4110%\n",
      "Epoch [1/2], Step [3681/3732], Loss: 3.2441, accuracy: 10.4116%\n",
      "Epoch [1/2], Step [3682/3732], Loss: 2.4649, accuracy: 10.4189%\n",
      "Epoch [1/2], Step [3683/3732], Loss: 3.8607, accuracy: 10.4161%\n",
      "Epoch [1/2], Step [3684/3732], Loss: 3.4624, accuracy: 10.4133%\n",
      "Epoch [1/2], Step [3685/3732], Loss: 3.9366, accuracy: 10.4104%\n",
      "Epoch [1/2], Step [3686/3732], Loss: 2.6269, accuracy: 10.4110%\n",
      "Epoch [1/2], Step [3687/3732], Loss: 3.9528, accuracy: 10.4082%\n",
      "Epoch [1/2], Step [3688/3732], Loss: 3.4198, accuracy: 10.4088%\n",
      "Epoch [1/2], Step [3689/3732], Loss: 3.1308, accuracy: 10.4127%\n",
      "Epoch [1/2], Step [3690/3732], Loss: 3.6799, accuracy: 10.4099%\n",
      "Epoch [1/2], Step [3691/3732], Loss: 2.9328, accuracy: 10.4105%\n",
      "Epoch [1/2], Step [3692/3732], Loss: 3.0161, accuracy: 10.4144%\n",
      "Epoch [1/2], Step [3693/3732], Loss: 3.5388, accuracy: 10.4150%\n",
      "Epoch [1/2], Step [3694/3732], Loss: 2.5980, accuracy: 10.4189%\n",
      "Epoch [1/2], Step [3695/3732], Loss: 3.0058, accuracy: 10.4263%\n",
      "Epoch [1/2], Step [3696/3732], Loss: 3.5194, accuracy: 10.4268%\n",
      "Epoch [1/2], Step [3697/3732], Loss: 3.3158, accuracy: 10.4308%\n",
      "Epoch [1/2], Step [3698/3732], Loss: 3.0007, accuracy: 10.4381%\n",
      "Epoch [1/2], Step [3699/3732], Loss: 3.7878, accuracy: 10.4386%\n",
      "Epoch [1/2], Step [3700/3732], Loss: 2.8371, accuracy: 10.4392%\n",
      "Epoch [1/2], Step [3701/3732], Loss: 3.4412, accuracy: 10.4431%\n",
      "Epoch [1/2], Step [3702/3732], Loss: 3.1146, accuracy: 10.4504%\n",
      "Epoch [1/2], Step [3703/3732], Loss: 3.4643, accuracy: 10.4510%\n",
      "Epoch [1/2], Step [3704/3732], Loss: 3.0744, accuracy: 10.4515%\n",
      "Epoch [1/2], Step [3705/3732], Loss: 3.3287, accuracy: 10.4487%\n",
      "Epoch [1/2], Step [3706/3732], Loss: 3.2497, accuracy: 10.4493%\n",
      "Epoch [1/2], Step [3707/3732], Loss: 2.8824, accuracy: 10.4465%\n",
      "Epoch [1/2], Step [3708/3732], Loss: 3.6342, accuracy: 10.4436%\n",
      "Epoch [1/2], Step [3709/3732], Loss: 3.3851, accuracy: 10.4476%\n",
      "Epoch [1/2], Step [3710/3732], Loss: 2.9502, accuracy: 10.4515%\n",
      "Epoch [1/2], Step [3711/3732], Loss: 2.9111, accuracy: 10.4554%\n",
      "Epoch [1/2], Step [3712/3732], Loss: 3.1566, accuracy: 10.4593%\n",
      "Epoch [1/2], Step [3713/3732], Loss: 3.5158, accuracy: 10.4565%\n",
      "Epoch [1/2], Step [3714/3732], Loss: 2.6286, accuracy: 10.4604%\n",
      "Epoch [1/2], Step [3715/3732], Loss: 3.4169, accuracy: 10.4576%\n",
      "Epoch [1/2], Step [3716/3732], Loss: 2.9879, accuracy: 10.4615%\n",
      "Epoch [1/2], Step [3717/3732], Loss: 3.8864, accuracy: 10.4621%\n",
      "Epoch [1/2], Step [3718/3732], Loss: 3.8449, accuracy: 10.4626%\n",
      "Epoch [1/2], Step [3719/3732], Loss: 2.9316, accuracy: 10.4665%\n",
      "Epoch [1/2], Step [3720/3732], Loss: 3.6638, accuracy: 10.4671%\n",
      "Epoch [1/2], Step [3721/3732], Loss: 2.4717, accuracy: 10.4710%\n",
      "Epoch [1/2], Step [3722/3732], Loss: 2.8019, accuracy: 10.4749%\n",
      "Epoch [1/2], Step [3723/3732], Loss: 3.3443, accuracy: 10.4754%\n",
      "Epoch [1/2], Step [3724/3732], Loss: 3.8411, accuracy: 10.4726%\n",
      "Epoch [1/2], Step [3725/3732], Loss: 3.6515, accuracy: 10.4732%\n",
      "Epoch [1/2], Step [3726/3732], Loss: 3.3602, accuracy: 10.4737%\n",
      "Epoch [1/2], Step [3727/3732], Loss: 2.3827, accuracy: 10.4809%\n",
      "Epoch [1/2], Step [3728/3732], Loss: 2.9960, accuracy: 10.4815%\n",
      "Epoch [1/2], Step [3729/3732], Loss: 2.9638, accuracy: 10.4887%\n",
      "Epoch [1/2], Step [3730/3732], Loss: 2.9152, accuracy: 10.4926%\n",
      "Epoch [1/2], Step [3731/3732], Loss: 3.3804, accuracy: 10.4932%\n",
      "Epoch [1/2], Step [3732/3732], Loss: 3.5433, accuracy: 10.4937%\n",
      "Epoch [2/2], Step [1/3732], Loss: 2.9834, accuracy: 10.4909%\n",
      "Epoch [2/2], Step [2/3732], Loss: 3.2311, accuracy: 10.4881%\n",
      "Epoch [2/2], Step [3/3732], Loss: 2.5236, accuracy: 10.4953%\n",
      "Epoch [2/2], Step [4/3732], Loss: 3.6611, accuracy: 10.4959%\n",
      "Epoch [2/2], Step [5/3732], Loss: 3.4136, accuracy: 10.4997%\n",
      "Epoch [2/2], Step [6/3732], Loss: 3.1442, accuracy: 10.5003%\n",
      "Epoch [2/2], Step [7/3732], Loss: 3.2001, accuracy: 10.4975%\n",
      "Epoch [2/2], Step [8/3732], Loss: 3.4246, accuracy: 10.4980%\n",
      "Epoch [2/2], Step [9/3732], Loss: 2.9777, accuracy: 10.5019%\n",
      "Epoch [2/2], Step [10/3732], Loss: 2.7081, accuracy: 10.5091%\n",
      "Epoch [2/2], Step [11/3732], Loss: 3.4757, accuracy: 10.5063%\n",
      "Epoch [2/2], Step [12/3732], Loss: 3.2927, accuracy: 10.5101%\n",
      "Epoch [2/2], Step [13/3732], Loss: 2.5976, accuracy: 10.5174%\n",
      "Epoch [2/2], Step [14/3732], Loss: 3.4243, accuracy: 10.5179%\n",
      "Epoch [2/2], Step [15/3732], Loss: 3.1708, accuracy: 10.5184%\n",
      "Epoch [2/2], Step [16/3732], Loss: 3.4183, accuracy: 10.5189%\n",
      "Epoch [2/2], Step [17/3732], Loss: 3.1453, accuracy: 10.5195%\n",
      "Epoch [2/2], Step [18/3732], Loss: 2.8501, accuracy: 10.5233%\n",
      "Epoch [2/2], Step [19/3732], Loss: 3.0299, accuracy: 10.5205%\n",
      "Epoch [2/2], Step [20/3732], Loss: 2.0789, accuracy: 10.5244%\n",
      "Epoch [2/2], Step [21/3732], Loss: 3.1207, accuracy: 10.5216%\n",
      "Epoch [2/2], Step [22/3732], Loss: 3.4653, accuracy: 10.5221%\n",
      "Epoch [2/2], Step [23/3732], Loss: 3.9716, accuracy: 10.5226%\n",
      "Epoch [2/2], Step [24/3732], Loss: 2.7373, accuracy: 10.5298%\n",
      "Epoch [2/2], Step [25/3732], Loss: 3.1934, accuracy: 10.5370%\n",
      "Epoch [2/2], Step [26/3732], Loss: 2.2809, accuracy: 10.5475%\n",
      "Epoch [2/2], Step [27/3732], Loss: 2.9126, accuracy: 10.5480%\n",
      "Epoch [2/2], Step [28/3732], Loss: 3.5178, accuracy: 10.5452%\n",
      "Epoch [2/2], Step [29/3732], Loss: 3.0090, accuracy: 10.5557%\n",
      "Epoch [2/2], Step [30/3732], Loss: 2.8666, accuracy: 10.5562%\n",
      "Epoch [2/2], Step [31/3732], Loss: 2.5768, accuracy: 10.5567%\n",
      "Epoch [2/2], Step [32/3732], Loss: 3.0145, accuracy: 10.5606%\n",
      "Epoch [2/2], Step [33/3732], Loss: 3.5560, accuracy: 10.5578%\n",
      "Epoch [2/2], Step [34/3732], Loss: 3.1032, accuracy: 10.5682%\n",
      "Epoch [2/2], Step [35/3732], Loss: 2.5928, accuracy: 10.5721%\n",
      "Epoch [2/2], Step [36/3732], Loss: 2.7326, accuracy: 10.5825%\n",
      "Epoch [2/2], Step [37/3732], Loss: 3.5988, accuracy: 10.5864%\n",
      "Epoch [2/2], Step [38/3732], Loss: 3.5505, accuracy: 10.5836%\n",
      "Epoch [2/2], Step [39/3732], Loss: 3.1467, accuracy: 10.5807%\n",
      "Epoch [2/2], Step [40/3732], Loss: 3.3784, accuracy: 10.5779%\n",
      "Epoch [2/2], Step [41/3732], Loss: 3.2298, accuracy: 10.5785%\n",
      "Epoch [2/2], Step [42/3732], Loss: 3.8678, accuracy: 10.5756%\n",
      "Epoch [2/2], Step [43/3732], Loss: 3.5196, accuracy: 10.5795%\n",
      "Epoch [2/2], Step [44/3732], Loss: 2.9410, accuracy: 10.5800%\n",
      "Epoch [2/2], Step [45/3732], Loss: 3.1218, accuracy: 10.5805%\n",
      "Epoch [2/2], Step [46/3732], Loss: 3.1403, accuracy: 10.5810%\n",
      "Epoch [2/2], Step [47/3732], Loss: 2.9392, accuracy: 10.5848%\n",
      "Epoch [2/2], Step [48/3732], Loss: 2.9040, accuracy: 10.5853%\n",
      "Epoch [2/2], Step [49/3732], Loss: 3.6726, accuracy: 10.5825%\n",
      "Epoch [2/2], Step [50/3732], Loss: 2.5091, accuracy: 10.5896%\n",
      "Epoch [2/2], Step [51/3732], Loss: 3.4702, accuracy: 10.5901%\n",
      "Epoch [2/2], Step [52/3732], Loss: 2.6415, accuracy: 10.5973%\n",
      "Epoch [2/2], Step [53/3732], Loss: 3.5612, accuracy: 10.5978%\n",
      "Epoch [2/2], Step [54/3732], Loss: 3.3345, accuracy: 10.6016%\n",
      "Epoch [2/2], Step [55/3732], Loss: 3.9906, accuracy: 10.5988%\n",
      "Epoch [2/2], Step [56/3732], Loss: 3.2118, accuracy: 10.5993%\n",
      "Epoch [2/2], Step [57/3732], Loss: 2.8216, accuracy: 10.6097%\n",
      "Epoch [2/2], Step [58/3732], Loss: 2.7260, accuracy: 10.6168%\n",
      "Epoch [2/2], Step [59/3732], Loss: 3.8565, accuracy: 10.6140%\n",
      "Epoch [2/2], Step [60/3732], Loss: 3.6561, accuracy: 10.6145%\n",
      "Epoch [2/2], Step [61/3732], Loss: 2.3645, accuracy: 10.6182%\n",
      "Epoch [2/2], Step [62/3732], Loss: 3.7551, accuracy: 10.6154%\n",
      "Epoch [2/2], Step [63/3732], Loss: 3.4102, accuracy: 10.6159%\n",
      "Epoch [2/2], Step [64/3732], Loss: 3.6401, accuracy: 10.6164%\n",
      "Epoch [2/2], Step [65/3732], Loss: 3.5321, accuracy: 10.6169%\n",
      "Epoch [2/2], Step [66/3732], Loss: 2.4557, accuracy: 10.6240%\n",
      "Epoch [2/2], Step [67/3732], Loss: 2.6417, accuracy: 10.6245%\n",
      "Epoch [2/2], Step [68/3732], Loss: 3.3079, accuracy: 10.6283%\n",
      "Epoch [2/2], Step [69/3732], Loss: 2.3937, accuracy: 10.6354%\n",
      "Epoch [2/2], Step [70/3732], Loss: 3.0278, accuracy: 10.6391%\n",
      "Epoch [2/2], Step [71/3732], Loss: 3.8596, accuracy: 10.6363%\n",
      "Epoch [2/2], Step [72/3732], Loss: 3.6547, accuracy: 10.6335%\n",
      "Epoch [2/2], Step [73/3732], Loss: 2.4508, accuracy: 10.6373%\n",
      "Epoch [2/2], Step [74/3732], Loss: 2.7689, accuracy: 10.6378%\n",
      "Epoch [2/2], Step [75/3732], Loss: 2.8493, accuracy: 10.6449%\n",
      "Epoch [2/2], Step [76/3732], Loss: 2.9023, accuracy: 10.6486%\n",
      "Epoch [2/2], Step [77/3732], Loss: 3.5686, accuracy: 10.6458%\n",
      "Epoch [2/2], Step [78/3732], Loss: 3.8576, accuracy: 10.6496%\n",
      "Epoch [2/2], Step [79/3732], Loss: 2.2637, accuracy: 10.6534%\n",
      "Epoch [2/2], Step [80/3732], Loss: 2.6652, accuracy: 10.6571%\n",
      "Epoch [2/2], Step [81/3732], Loss: 3.0610, accuracy: 10.6576%\n",
      "Epoch [2/2], Step [82/3732], Loss: 4.1356, accuracy: 10.6548%\n",
      "Epoch [2/2], Step [83/3732], Loss: 3.1309, accuracy: 10.6586%\n",
      "Epoch [2/2], Step [84/3732], Loss: 2.9477, accuracy: 10.6623%\n",
      "Epoch [2/2], Step [85/3732], Loss: 3.2991, accuracy: 10.6661%\n",
      "Epoch [2/2], Step [86/3732], Loss: 2.9164, accuracy: 10.6666%\n",
      "Epoch [2/2], Step [87/3732], Loss: 3.6441, accuracy: 10.6671%\n",
      "Epoch [2/2], Step [88/3732], Loss: 3.3174, accuracy: 10.6643%\n",
      "Epoch [2/2], Step [89/3732], Loss: 3.3095, accuracy: 10.6615%\n",
      "Epoch [2/2], Step [90/3732], Loss: 3.6211, accuracy: 10.6652%\n",
      "Epoch [2/2], Step [91/3732], Loss: 1.8705, accuracy: 10.6755%\n",
      "Epoch [2/2], Step [92/3732], Loss: 3.8770, accuracy: 10.6727%\n",
      "Epoch [2/2], Step [93/3732], Loss: 3.5199, accuracy: 10.6732%\n",
      "Epoch [2/2], Step [94/3732], Loss: 2.4218, accuracy: 10.6835%\n",
      "Epoch [2/2], Step [95/3732], Loss: 3.1302, accuracy: 10.6807%\n",
      "Epoch [2/2], Step [96/3732], Loss: 3.7470, accuracy: 10.6779%\n",
      "Epoch [2/2], Step [97/3732], Loss: 3.3489, accuracy: 10.6784%\n",
      "Epoch [2/2], Step [98/3732], Loss: 2.0996, accuracy: 10.6919%\n",
      "Epoch [2/2], Step [99/3732], Loss: 2.7389, accuracy: 10.6956%\n",
      "Epoch [2/2], Step [100/3732], Loss: 2.8278, accuracy: 10.6928%\n",
      "Epoch [2/2], Step [101/3732], Loss: 3.4773, accuracy: 10.6901%\n",
      "Epoch [2/2], Step [102/3732], Loss: 3.5996, accuracy: 10.6873%\n",
      "Epoch [2/2], Step [103/3732], Loss: 2.6760, accuracy: 10.6910%\n",
      "Epoch [2/2], Step [104/3732], Loss: 3.2152, accuracy: 10.6882%\n",
      "Epoch [2/2], Step [105/3732], Loss: 3.1626, accuracy: 10.6854%\n",
      "Epoch [2/2], Step [106/3732], Loss: 3.1697, accuracy: 10.6859%\n",
      "Epoch [2/2], Step [107/3732], Loss: 3.1082, accuracy: 10.6896%\n",
      "Epoch [2/2], Step [108/3732], Loss: 2.9248, accuracy: 10.6934%\n",
      "Epoch [2/2], Step [109/3732], Loss: 2.9966, accuracy: 10.6906%\n",
      "Epoch [2/2], Step [110/3732], Loss: 2.7065, accuracy: 10.6943%\n",
      "Epoch [2/2], Step [111/3732], Loss: 2.9328, accuracy: 10.7078%\n",
      "Epoch [2/2], Step [112/3732], Loss: 3.1384, accuracy: 10.7082%\n",
      "Epoch [2/2], Step [113/3732], Loss: 3.6691, accuracy: 10.7087%\n",
      "Epoch [2/2], Step [114/3732], Loss: 2.9217, accuracy: 10.7092%\n",
      "Epoch [2/2], Step [115/3732], Loss: 2.6963, accuracy: 10.7161%\n",
      "Epoch [2/2], Step [116/3732], Loss: 2.9574, accuracy: 10.7199%\n",
      "Epoch [2/2], Step [117/3732], Loss: 2.7794, accuracy: 10.7268%\n",
      "Epoch [2/2], Step [118/3732], Loss: 3.0805, accuracy: 10.7305%\n",
      "Epoch [2/2], Step [119/3732], Loss: 3.7067, accuracy: 10.7310%\n",
      "Epoch [2/2], Step [120/3732], Loss: 3.2542, accuracy: 10.7347%\n",
      "Epoch [2/2], Step [121/3732], Loss: 3.2093, accuracy: 10.7351%\n",
      "Epoch [2/2], Step [122/3732], Loss: 3.0633, accuracy: 10.7388%\n",
      "Epoch [2/2], Step [123/3732], Loss: 3.5375, accuracy: 10.7425%\n",
      "Epoch [2/2], Step [124/3732], Loss: 3.5345, accuracy: 10.7398%\n",
      "Epoch [2/2], Step [125/3732], Loss: 3.2908, accuracy: 10.7402%\n",
      "Epoch [2/2], Step [126/3732], Loss: 3.1366, accuracy: 10.7471%\n",
      "Epoch [2/2], Step [127/3732], Loss: 3.2564, accuracy: 10.7508%\n",
      "Epoch [2/2], Step [128/3732], Loss: 3.0526, accuracy: 10.7545%\n",
      "Epoch [2/2], Step [129/3732], Loss: 2.8578, accuracy: 10.7582%\n",
      "Epoch [2/2], Step [130/3732], Loss: 2.8745, accuracy: 10.7651%\n",
      "Epoch [2/2], Step [131/3732], Loss: 2.3221, accuracy: 10.7721%\n",
      "Epoch [2/2], Step [132/3732], Loss: 3.2249, accuracy: 10.7693%\n",
      "Epoch [2/2], Step [133/3732], Loss: 3.0118, accuracy: 10.7730%\n",
      "Epoch [2/2], Step [134/3732], Loss: 3.7637, accuracy: 10.7702%\n",
      "Epoch [2/2], Step [135/3732], Loss: 3.2987, accuracy: 10.7674%\n",
      "Epoch [2/2], Step [136/3732], Loss: 3.0477, accuracy: 10.7678%\n",
      "Epoch [2/2], Step [137/3732], Loss: 3.6571, accuracy: 10.7683%\n",
      "Epoch [2/2], Step [138/3732], Loss: 2.7999, accuracy: 10.7655%\n",
      "Epoch [2/2], Step [139/3732], Loss: 3.4557, accuracy: 10.7627%\n",
      "Epoch [2/2], Step [140/3732], Loss: 3.1398, accuracy: 10.7664%\n",
      "Epoch [2/2], Step [141/3732], Loss: 2.5599, accuracy: 10.7701%\n",
      "Epoch [2/2], Step [142/3732], Loss: 2.5872, accuracy: 10.7737%\n",
      "Epoch [2/2], Step [143/3732], Loss: 2.8524, accuracy: 10.7742%\n",
      "Epoch [2/2], Step [144/3732], Loss: 3.0916, accuracy: 10.7746%\n",
      "Epoch [2/2], Step [145/3732], Loss: 3.0443, accuracy: 10.7751%\n",
      "Epoch [2/2], Step [146/3732], Loss: 3.8859, accuracy: 10.7788%\n",
      "Epoch [2/2], Step [147/3732], Loss: 2.6406, accuracy: 10.7889%\n",
      "Epoch [2/2], Step [148/3732], Loss: 3.1277, accuracy: 10.7893%\n",
      "Epoch [2/2], Step [149/3732], Loss: 3.0354, accuracy: 10.7930%\n",
      "Epoch [2/2], Step [150/3732], Loss: 3.6339, accuracy: 10.7902%\n",
      "Epoch [2/2], Step [151/3732], Loss: 3.6261, accuracy: 10.7938%\n",
      "Epoch [2/2], Step [152/3732], Loss: 2.9369, accuracy: 10.8007%\n",
      "Epoch [2/2], Step [153/3732], Loss: 2.9442, accuracy: 10.8012%\n",
      "Epoch [2/2], Step [154/3732], Loss: 3.0106, accuracy: 10.7984%\n",
      "Epoch [2/2], Step [155/3732], Loss: 2.9661, accuracy: 10.7956%\n",
      "Epoch [2/2], Step [156/3732], Loss: 3.2888, accuracy: 10.7928%\n",
      "Epoch [2/2], Step [157/3732], Loss: 3.1098, accuracy: 10.7933%\n",
      "Epoch [2/2], Step [158/3732], Loss: 3.1434, accuracy: 10.7937%\n",
      "Epoch [2/2], Step [159/3732], Loss: 3.7952, accuracy: 10.7909%\n",
      "Epoch [2/2], Step [160/3732], Loss: 3.0993, accuracy: 10.7914%\n",
      "Epoch [2/2], Step [161/3732], Loss: 3.1489, accuracy: 10.7886%\n",
      "Epoch [2/2], Step [162/3732], Loss: 3.6064, accuracy: 10.7890%\n",
      "Epoch [2/2], Step [163/3732], Loss: 3.5443, accuracy: 10.7895%\n",
      "Epoch [2/2], Step [164/3732], Loss: 3.0970, accuracy: 10.7931%\n",
      "Epoch [2/2], Step [165/3732], Loss: 2.8653, accuracy: 10.7936%\n",
      "Epoch [2/2], Step [166/3732], Loss: 2.4359, accuracy: 10.8036%\n",
      "Epoch [2/2], Step [167/3732], Loss: 3.3794, accuracy: 10.8041%\n",
      "Epoch [2/2], Step [168/3732], Loss: 3.5219, accuracy: 10.8013%\n",
      "Epoch [2/2], Step [169/3732], Loss: 3.2958, accuracy: 10.7985%\n",
      "Epoch [2/2], Step [170/3732], Loss: 3.0364, accuracy: 10.8054%\n",
      "Epoch [2/2], Step [171/3732], Loss: 2.8755, accuracy: 10.8122%\n",
      "Epoch [2/2], Step [172/3732], Loss: 3.5957, accuracy: 10.8094%\n",
      "Epoch [2/2], Step [173/3732], Loss: 2.6334, accuracy: 10.8099%\n",
      "Epoch [2/2], Step [174/3732], Loss: 2.9984, accuracy: 10.8135%\n",
      "Epoch [2/2], Step [175/3732], Loss: 3.0744, accuracy: 10.8139%\n",
      "Epoch [2/2], Step [176/3732], Loss: 3.0807, accuracy: 10.8144%\n",
      "Epoch [2/2], Step [177/3732], Loss: 2.4364, accuracy: 10.8212%\n",
      "Epoch [2/2], Step [178/3732], Loss: 3.1290, accuracy: 10.8248%\n",
      "Epoch [2/2], Step [179/3732], Loss: 3.4034, accuracy: 10.8220%\n",
      "Epoch [2/2], Step [180/3732], Loss: 3.2666, accuracy: 10.8193%\n",
      "Epoch [2/2], Step [181/3732], Loss: 2.4322, accuracy: 10.8261%\n",
      "Epoch [2/2], Step [182/3732], Loss: 3.6488, accuracy: 10.8297%\n",
      "Epoch [2/2], Step [183/3732], Loss: 3.3674, accuracy: 10.8301%\n",
      "Epoch [2/2], Step [184/3732], Loss: 3.2877, accuracy: 10.8306%\n",
      "Epoch [2/2], Step [185/3732], Loss: 2.5435, accuracy: 10.8342%\n",
      "Epoch [2/2], Step [186/3732], Loss: 3.1211, accuracy: 10.8346%\n",
      "Epoch [2/2], Step [187/3732], Loss: 2.8778, accuracy: 10.8350%\n",
      "Epoch [2/2], Step [188/3732], Loss: 3.3856, accuracy: 10.8355%\n",
      "Epoch [2/2], Step [189/3732], Loss: 3.0360, accuracy: 10.8359%\n",
      "Epoch [2/2], Step [190/3732], Loss: 3.6205, accuracy: 10.8363%\n",
      "Epoch [2/2], Step [191/3732], Loss: 1.9165, accuracy: 10.8527%\n",
      "Epoch [2/2], Step [192/3732], Loss: 2.4208, accuracy: 10.8595%\n",
      "Epoch [2/2], Step [193/3732], Loss: 2.9716, accuracy: 10.8662%\n",
      "Epoch [2/2], Step [194/3732], Loss: 3.2577, accuracy: 10.8635%\n",
      "Epoch [2/2], Step [195/3732], Loss: 3.5851, accuracy: 10.8607%\n",
      "Epoch [2/2], Step [196/3732], Loss: 3.4582, accuracy: 10.8611%\n",
      "Epoch [2/2], Step [197/3732], Loss: 2.4579, accuracy: 10.8679%\n",
      "Epoch [2/2], Step [198/3732], Loss: 3.3128, accuracy: 10.8715%\n",
      "Epoch [2/2], Step [199/3732], Loss: 3.1392, accuracy: 10.8687%\n",
      "Epoch [2/2], Step [200/3732], Loss: 3.5160, accuracy: 10.8660%\n",
      "Epoch [2/2], Step [201/3732], Loss: 3.5408, accuracy: 10.8664%\n",
      "Epoch [2/2], Step [202/3732], Loss: 3.6227, accuracy: 10.8700%\n",
      "Epoch [2/2], Step [203/3732], Loss: 3.4920, accuracy: 10.8672%\n",
      "Epoch [2/2], Step [204/3732], Loss: 3.5181, accuracy: 10.8708%\n",
      "Epoch [2/2], Step [205/3732], Loss: 3.5234, accuracy: 10.8680%\n",
      "Epoch [2/2], Step [206/3732], Loss: 3.5416, accuracy: 10.8685%\n",
      "Epoch [2/2], Step [207/3732], Loss: 3.0637, accuracy: 10.8689%\n",
      "Epoch [2/2], Step [208/3732], Loss: 3.0365, accuracy: 10.8725%\n",
      "Epoch [2/2], Step [209/3732], Loss: 3.2741, accuracy: 10.8729%\n",
      "Epoch [2/2], Step [210/3732], Loss: 2.6751, accuracy: 10.8765%\n",
      "Epoch [2/2], Step [211/3732], Loss: 3.6234, accuracy: 10.8737%\n",
      "Epoch [2/2], Step [212/3732], Loss: 2.9777, accuracy: 10.8805%\n",
      "Epoch [2/2], Step [213/3732], Loss: 3.2525, accuracy: 10.8809%\n",
      "Epoch [2/2], Step [214/3732], Loss: 3.2921, accuracy: 10.8813%\n",
      "Epoch [2/2], Step [215/3732], Loss: 2.8112, accuracy: 10.8848%\n",
      "Epoch [2/2], Step [216/3732], Loss: 3.1505, accuracy: 10.8853%\n",
      "Epoch [2/2], Step [217/3732], Loss: 3.3488, accuracy: 10.8857%\n",
      "Epoch [2/2], Step [218/3732], Loss: 3.3992, accuracy: 10.8861%\n",
      "Epoch [2/2], Step [219/3732], Loss: 3.4024, accuracy: 10.8833%\n",
      "Epoch [2/2], Step [220/3732], Loss: 3.1138, accuracy: 10.8837%\n",
      "Epoch [2/2], Step [221/3732], Loss: 3.5301, accuracy: 10.8841%\n",
      "Epoch [2/2], Step [222/3732], Loss: 3.3198, accuracy: 10.8814%\n",
      "Epoch [2/2], Step [223/3732], Loss: 2.5734, accuracy: 10.8850%\n",
      "Epoch [2/2], Step [224/3732], Loss: 3.0890, accuracy: 10.8917%\n",
      "Epoch [2/2], Step [225/3732], Loss: 2.5656, accuracy: 10.9016%\n",
      "Epoch [2/2], Step [226/3732], Loss: 3.4786, accuracy: 10.8988%\n",
      "Epoch [2/2], Step [227/3732], Loss: 3.0919, accuracy: 10.9024%\n",
      "Epoch [2/2], Step [228/3732], Loss: 3.1221, accuracy: 10.9028%\n",
      "Epoch [2/2], Step [229/3732], Loss: 4.0735, accuracy: 10.9032%\n",
      "Epoch [2/2], Step [230/3732], Loss: 3.5141, accuracy: 10.9067%\n",
      "Epoch [2/2], Step [231/3732], Loss: 2.5251, accuracy: 10.9103%\n",
      "Epoch [2/2], Step [232/3732], Loss: 3.2696, accuracy: 10.9138%\n",
      "Epoch [2/2], Step [233/3732], Loss: 2.1113, accuracy: 10.9269%\n",
      "Epoch [2/2], Step [234/3732], Loss: 3.2878, accuracy: 10.9273%\n",
      "Epoch [2/2], Step [235/3732], Loss: 2.7966, accuracy: 10.9277%\n",
      "Epoch [2/2], Step [236/3732], Loss: 2.6853, accuracy: 10.9280%\n",
      "Epoch [2/2], Step [237/3732], Loss: 3.6684, accuracy: 10.9284%\n",
      "Epoch [2/2], Step [238/3732], Loss: 2.5108, accuracy: 10.9383%\n",
      "Epoch [2/2], Step [239/3732], Loss: 2.6933, accuracy: 10.9450%\n",
      "Epoch [2/2], Step [240/3732], Loss: 2.9137, accuracy: 10.9485%\n",
      "Epoch [2/2], Step [241/3732], Loss: 3.7906, accuracy: 10.9489%\n",
      "Epoch [2/2], Step [242/3732], Loss: 2.7639, accuracy: 10.9493%\n",
      "Epoch [2/2], Step [243/3732], Loss: 3.4092, accuracy: 10.9465%\n",
      "Epoch [2/2], Step [244/3732], Loss: 3.4591, accuracy: 10.9501%\n",
      "Epoch [2/2], Step [245/3732], Loss: 3.0215, accuracy: 10.9536%\n",
      "Epoch [2/2], Step [246/3732], Loss: 3.0896, accuracy: 10.9509%\n",
      "Epoch [2/2], Step [247/3732], Loss: 2.7332, accuracy: 10.9544%\n",
      "Epoch [2/2], Step [248/3732], Loss: 3.1576, accuracy: 10.9548%\n",
      "Epoch [2/2], Step [249/3732], Loss: 3.2901, accuracy: 10.9552%\n",
      "Epoch [2/2], Step [250/3732], Loss: 3.0301, accuracy: 10.9555%\n",
      "Epoch [2/2], Step [251/3732], Loss: 2.6881, accuracy: 10.9559%\n",
      "Epoch [2/2], Step [252/3732], Loss: 4.2557, accuracy: 10.9532%\n",
      "Epoch [2/2], Step [253/3732], Loss: 4.3773, accuracy: 10.9504%\n",
      "Epoch [2/2], Step [254/3732], Loss: 3.4199, accuracy: 10.9508%\n",
      "Epoch [2/2], Step [255/3732], Loss: 3.3152, accuracy: 10.9481%\n",
      "Epoch [2/2], Step [256/3732], Loss: 4.3479, accuracy: 10.9453%\n",
      "Epoch [2/2], Step [257/3732], Loss: 3.7710, accuracy: 10.9426%\n",
      "Epoch [2/2], Step [258/3732], Loss: 2.6732, accuracy: 10.9461%\n",
      "Epoch [2/2], Step [259/3732], Loss: 3.0466, accuracy: 10.9434%\n",
      "Epoch [2/2], Step [260/3732], Loss: 3.3861, accuracy: 10.9469%\n",
      "Epoch [2/2], Step [261/3732], Loss: 2.9897, accuracy: 10.9535%\n",
      "Epoch [2/2], Step [262/3732], Loss: 3.6884, accuracy: 10.9539%\n",
      "Epoch [2/2], Step [263/3732], Loss: 3.2062, accuracy: 10.9543%\n",
      "Epoch [2/2], Step [264/3732], Loss: 2.7377, accuracy: 10.9610%\n",
      "Epoch [2/2], Step [265/3732], Loss: 3.2028, accuracy: 10.9645%\n",
      "Epoch [2/2], Step [266/3732], Loss: 3.0725, accuracy: 10.9680%\n",
      "Epoch [2/2], Step [267/3732], Loss: 3.1804, accuracy: 10.9684%\n",
      "Epoch [2/2], Step [268/3732], Loss: 3.2261, accuracy: 10.9688%\n",
      "Epoch [2/2], Step [269/3732], Loss: 2.3670, accuracy: 10.9785%\n",
      "Epoch [2/2], Step [270/3732], Loss: 3.4289, accuracy: 10.9820%\n",
      "Epoch [2/2], Step [271/3732], Loss: 3.2543, accuracy: 10.9824%\n",
      "Epoch [2/2], Step [272/3732], Loss: 3.5835, accuracy: 10.9828%\n",
      "Epoch [2/2], Step [273/3732], Loss: 2.7495, accuracy: 10.9894%\n",
      "Epoch [2/2], Step [274/3732], Loss: 3.6555, accuracy: 10.9866%\n",
      "Epoch [2/2], Step [275/3732], Loss: 3.1752, accuracy: 10.9870%\n",
      "Epoch [2/2], Step [276/3732], Loss: 2.9079, accuracy: 10.9874%\n",
      "Epoch [2/2], Step [277/3732], Loss: 2.9869, accuracy: 10.9909%\n",
      "Epoch [2/2], Step [278/3732], Loss: 3.6519, accuracy: 10.9913%\n",
      "Epoch [2/2], Step [279/3732], Loss: 3.0383, accuracy: 10.9948%\n",
      "Epoch [2/2], Step [280/3732], Loss: 3.7970, accuracy: 10.9951%\n",
      "Epoch [2/2], Step [281/3732], Loss: 4.6397, accuracy: 10.9924%\n",
      "Epoch [2/2], Step [282/3732], Loss: 2.8424, accuracy: 10.9928%\n",
      "Epoch [2/2], Step [283/3732], Loss: 3.3515, accuracy: 10.9932%\n",
      "Epoch [2/2], Step [284/3732], Loss: 2.8071, accuracy: 11.0029%\n",
      "Epoch [2/2], Step [285/3732], Loss: 2.9192, accuracy: 11.0095%\n",
      "Epoch [2/2], Step [286/3732], Loss: 3.5750, accuracy: 11.0098%\n",
      "Epoch [2/2], Step [287/3732], Loss: 3.3485, accuracy: 11.0133%\n",
      "Epoch [2/2], Step [288/3732], Loss: 3.2697, accuracy: 11.0168%\n",
      "Epoch [2/2], Step [289/3732], Loss: 3.0358, accuracy: 11.0203%\n",
      "Epoch [2/2], Step [290/3732], Loss: 3.3151, accuracy: 11.0175%\n",
      "Epoch [2/2], Step [291/3732], Loss: 3.7212, accuracy: 11.0148%\n",
      "Epoch [2/2], Step [292/3732], Loss: 3.3335, accuracy: 11.0152%\n",
      "Epoch [2/2], Step [293/3732], Loss: 3.5911, accuracy: 11.0124%\n",
      "Epoch [2/2], Step [294/3732], Loss: 2.8181, accuracy: 11.0221%\n",
      "Epoch [2/2], Step [295/3732], Loss: 3.1186, accuracy: 11.0256%\n",
      "Epoch [2/2], Step [296/3732], Loss: 3.2026, accuracy: 11.0290%\n",
      "Epoch [2/2], Step [297/3732], Loss: 3.1481, accuracy: 11.0325%\n",
      "Epoch [2/2], Step [298/3732], Loss: 2.5919, accuracy: 11.0360%\n",
      "Epoch [2/2], Step [299/3732], Loss: 2.8413, accuracy: 11.0363%\n",
      "Epoch [2/2], Step [300/3732], Loss: 3.5707, accuracy: 11.0336%\n",
      "Epoch [2/2], Step [301/3732], Loss: 3.1197, accuracy: 11.0402%\n",
      "Epoch [2/2], Step [302/3732], Loss: 3.7480, accuracy: 11.0374%\n",
      "Epoch [2/2], Step [303/3732], Loss: 3.1369, accuracy: 11.0378%\n",
      "Epoch [2/2], Step [304/3732], Loss: 2.4899, accuracy: 11.0382%\n",
      "Epoch [2/2], Step [305/3732], Loss: 2.9917, accuracy: 11.0385%\n",
      "Epoch [2/2], Step [306/3732], Loss: 3.3488, accuracy: 11.0389%\n",
      "Epoch [2/2], Step [307/3732], Loss: 3.8234, accuracy: 11.0392%\n",
      "Epoch [2/2], Step [308/3732], Loss: 3.1388, accuracy: 11.0396%\n",
      "Epoch [2/2], Step [309/3732], Loss: 3.7446, accuracy: 11.0369%\n",
      "Epoch [2/2], Step [310/3732], Loss: 2.7270, accuracy: 11.0434%\n",
      "Epoch [2/2], Step [311/3732], Loss: 3.7252, accuracy: 11.0469%\n",
      "Epoch [2/2], Step [312/3732], Loss: 3.4930, accuracy: 11.0441%\n",
      "Epoch [2/2], Step [313/3732], Loss: 3.2413, accuracy: 11.0414%\n",
      "Epoch [2/2], Step [314/3732], Loss: 4.2799, accuracy: 11.0387%\n",
      "Epoch [2/2], Step [315/3732], Loss: 3.2602, accuracy: 11.0390%\n",
      "Epoch [2/2], Step [316/3732], Loss: 2.7111, accuracy: 11.0425%\n",
      "Epoch [2/2], Step [317/3732], Loss: 3.0253, accuracy: 11.0459%\n",
      "Epoch [2/2], Step [318/3732], Loss: 3.3401, accuracy: 11.0463%\n",
      "Epoch [2/2], Step [319/3732], Loss: 2.2200, accuracy: 11.0497%\n",
      "Epoch [2/2], Step [320/3732], Loss: 3.3451, accuracy: 11.0470%\n",
      "Epoch [2/2], Step [321/3732], Loss: 3.4888, accuracy: 11.0443%\n",
      "Epoch [2/2], Step [322/3732], Loss: 2.9190, accuracy: 11.0477%\n",
      "Epoch [2/2], Step [323/3732], Loss: 2.9487, accuracy: 11.0543%\n",
      "Epoch [2/2], Step [324/3732], Loss: 3.7327, accuracy: 11.0515%\n",
      "Epoch [2/2], Step [325/3732], Loss: 3.8128, accuracy: 11.0488%\n",
      "Epoch [2/2], Step [326/3732], Loss: 3.0889, accuracy: 11.0461%\n",
      "Epoch [2/2], Step [327/3732], Loss: 3.0287, accuracy: 11.0495%\n",
      "Epoch [2/2], Step [328/3732], Loss: 3.0156, accuracy: 11.0560%\n",
      "Epoch [2/2], Step [329/3732], Loss: 2.6777, accuracy: 11.0595%\n",
      "Epoch [2/2], Step [330/3732], Loss: 3.0867, accuracy: 11.0629%\n",
      "Epoch [2/2], Step [331/3732], Loss: 2.9009, accuracy: 11.0663%\n",
      "Epoch [2/2], Step [332/3732], Loss: 2.6488, accuracy: 11.0698%\n",
      "Epoch [2/2], Step [333/3732], Loss: 3.4243, accuracy: 11.0701%\n",
      "Epoch [2/2], Step [334/3732], Loss: 3.3767, accuracy: 11.0674%\n",
      "Epoch [2/2], Step [335/3732], Loss: 3.3489, accuracy: 11.0647%\n",
      "Epoch [2/2], Step [336/3732], Loss: 3.2002, accuracy: 11.0650%\n",
      "Epoch [2/2], Step [337/3732], Loss: 3.0018, accuracy: 11.0654%\n",
      "Epoch [2/2], Step [338/3732], Loss: 3.1410, accuracy: 11.0657%\n",
      "Epoch [2/2], Step [339/3732], Loss: 2.5143, accuracy: 11.0722%\n",
      "Epoch [2/2], Step [340/3732], Loss: 3.3278, accuracy: 11.0756%\n",
      "Epoch [2/2], Step [341/3732], Loss: 3.3550, accuracy: 11.0760%\n",
      "Epoch [2/2], Step [342/3732], Loss: 3.4768, accuracy: 11.0733%\n",
      "Epoch [2/2], Step [343/3732], Loss: 2.8598, accuracy: 11.0767%\n",
      "Epoch [2/2], Step [344/3732], Loss: 2.8549, accuracy: 11.0832%\n",
      "Epoch [2/2], Step [345/3732], Loss: 2.8851, accuracy: 11.0896%\n",
      "Epoch [2/2], Step [346/3732], Loss: 3.5535, accuracy: 11.0900%\n",
      "Epoch [2/2], Step [347/3732], Loss: 3.1609, accuracy: 11.0903%\n",
      "Epoch [2/2], Step [348/3732], Loss: 3.0218, accuracy: 11.0938%\n",
      "Epoch [2/2], Step [349/3732], Loss: 4.4566, accuracy: 11.0941%\n",
      "Epoch [2/2], Step [350/3732], Loss: 3.0381, accuracy: 11.0975%\n",
      "Epoch [2/2], Step [351/3732], Loss: 2.4044, accuracy: 11.1070%\n",
      "Epoch [2/2], Step [352/3732], Loss: 3.1711, accuracy: 11.1074%\n",
      "Epoch [2/2], Step [353/3732], Loss: 3.4983, accuracy: 11.1077%\n",
      "Epoch [2/2], Step [354/3732], Loss: 2.1649, accuracy: 11.1203%\n",
      "Epoch [2/2], Step [355/3732], Loss: 3.5046, accuracy: 11.1206%\n",
      "Epoch [2/2], Step [356/3732], Loss: 3.1072, accuracy: 11.1210%\n",
      "Epoch [2/2], Step [357/3732], Loss: 3.3496, accuracy: 11.1182%\n",
      "Epoch [2/2], Step [358/3732], Loss: 3.1244, accuracy: 11.1186%\n",
      "Epoch [2/2], Step [359/3732], Loss: 2.7345, accuracy: 11.1189%\n",
      "Epoch [2/2], Step [360/3732], Loss: 3.4499, accuracy: 11.1162%\n",
      "Epoch [2/2], Step [361/3732], Loss: 3.2470, accuracy: 11.1135%\n",
      "Epoch [2/2], Step [362/3732], Loss: 3.0263, accuracy: 11.1108%\n",
      "Epoch [2/2], Step [363/3732], Loss: 3.2208, accuracy: 11.1111%\n",
      "Epoch [2/2], Step [364/3732], Loss: 3.1145, accuracy: 11.1115%\n",
      "Epoch [2/2], Step [365/3732], Loss: 2.8489, accuracy: 11.1148%\n",
      "Epoch [2/2], Step [366/3732], Loss: 2.7113, accuracy: 11.1152%\n",
      "Epoch [2/2], Step [367/3732], Loss: 3.2336, accuracy: 11.1155%\n",
      "Epoch [2/2], Step [368/3732], Loss: 3.2790, accuracy: 11.1159%\n",
      "Epoch [2/2], Step [369/3732], Loss: 3.0854, accuracy: 11.1131%\n",
      "Epoch [2/2], Step [370/3732], Loss: 3.3810, accuracy: 11.1104%\n",
      "Epoch [2/2], Step [371/3732], Loss: 2.6883, accuracy: 11.1199%\n",
      "Epoch [2/2], Step [372/3732], Loss: 3.3220, accuracy: 11.1172%\n",
      "Epoch [2/2], Step [373/3732], Loss: 2.7137, accuracy: 11.1236%\n",
      "Epoch [2/2], Step [374/3732], Loss: 2.6679, accuracy: 11.1240%\n",
      "Epoch [2/2], Step [375/3732], Loss: 2.9636, accuracy: 11.1273%\n",
      "Epoch [2/2], Step [376/3732], Loss: 3.2749, accuracy: 11.1246%\n",
      "Epoch [2/2], Step [377/3732], Loss: 3.2787, accuracy: 11.1280%\n",
      "Epoch [2/2], Step [378/3732], Loss: 3.7986, accuracy: 11.1253%\n",
      "Epoch [2/2], Step [379/3732], Loss: 2.8106, accuracy: 11.1287%\n",
      "Epoch [2/2], Step [380/3732], Loss: 2.9929, accuracy: 11.1321%\n",
      "Epoch [2/2], Step [381/3732], Loss: 3.3562, accuracy: 11.1293%\n",
      "Epoch [2/2], Step [382/3732], Loss: 3.2340, accuracy: 11.1297%\n",
      "Epoch [2/2], Step [383/3732], Loss: 2.9941, accuracy: 11.1270%\n",
      "Epoch [2/2], Step [384/3732], Loss: 3.2909, accuracy: 11.1243%\n",
      "Epoch [2/2], Step [385/3732], Loss: 3.1108, accuracy: 11.1276%\n",
      "Epoch [2/2], Step [386/3732], Loss: 3.1469, accuracy: 11.1280%\n",
      "Epoch [2/2], Step [387/3732], Loss: 2.7609, accuracy: 11.1283%\n",
      "Epoch [2/2], Step [388/3732], Loss: 2.4205, accuracy: 11.1377%\n",
      "Epoch [2/2], Step [389/3732], Loss: 3.3270, accuracy: 11.1381%\n",
      "Epoch [2/2], Step [390/3732], Loss: 2.6242, accuracy: 11.1384%\n",
      "Epoch [2/2], Step [391/3732], Loss: 2.7772, accuracy: 11.1387%\n",
      "Epoch [2/2], Step [392/3732], Loss: 2.4983, accuracy: 11.1482%\n",
      "Epoch [2/2], Step [393/3732], Loss: 3.7680, accuracy: 11.1455%\n",
      "Epoch [2/2], Step [394/3732], Loss: 2.6315, accuracy: 11.1549%\n",
      "Epoch [2/2], Step [395/3732], Loss: 2.7697, accuracy: 11.1582%\n",
      "Epoch [2/2], Step [396/3732], Loss: 2.8625, accuracy: 11.1586%\n",
      "Epoch [2/2], Step [397/3732], Loss: 3.0353, accuracy: 11.1619%\n",
      "Epoch [2/2], Step [398/3732], Loss: 3.1013, accuracy: 11.1653%\n",
      "Epoch [2/2], Step [399/3732], Loss: 3.1789, accuracy: 11.1686%\n",
      "Epoch [2/2], Step [400/3732], Loss: 2.4995, accuracy: 11.1780%\n",
      "Epoch [2/2], Step [401/3732], Loss: 3.1937, accuracy: 11.1783%\n",
      "Epoch [2/2], Step [402/3732], Loss: 3.1083, accuracy: 11.1786%\n",
      "Epoch [2/2], Step [403/3732], Loss: 3.4925, accuracy: 11.1820%\n",
      "Epoch [2/2], Step [404/3732], Loss: 3.6611, accuracy: 11.1853%\n",
      "Epoch [2/2], Step [405/3732], Loss: 3.2265, accuracy: 11.1856%\n",
      "Epoch [2/2], Step [406/3732], Loss: 3.5866, accuracy: 11.1829%\n",
      "Epoch [2/2], Step [407/3732], Loss: 2.7544, accuracy: 11.1863%\n",
      "Epoch [2/2], Step [408/3732], Loss: 3.0361, accuracy: 11.1896%\n",
      "Epoch [2/2], Step [409/3732], Loss: 2.3084, accuracy: 11.1990%\n",
      "Epoch [2/2], Step [410/3732], Loss: 3.1491, accuracy: 11.1993%\n",
      "Epoch [2/2], Step [411/3732], Loss: 2.6256, accuracy: 11.2056%\n",
      "Epoch [2/2], Step [412/3732], Loss: 2.7662, accuracy: 11.2060%\n",
      "Epoch [2/2], Step [413/3732], Loss: 3.2784, accuracy: 11.2063%\n",
      "Epoch [2/2], Step [414/3732], Loss: 2.8627, accuracy: 11.2096%\n",
      "Epoch [2/2], Step [415/3732], Loss: 4.7224, accuracy: 11.2099%\n",
      "Epoch [2/2], Step [416/3732], Loss: 2.6777, accuracy: 11.2162%\n",
      "Epoch [2/2], Step [417/3732], Loss: 2.8341, accuracy: 11.2166%\n",
      "Epoch [2/2], Step [418/3732], Loss: 2.7932, accuracy: 11.2139%\n",
      "Epoch [2/2], Step [419/3732], Loss: 3.1496, accuracy: 11.2112%\n",
      "Epoch [2/2], Step [420/3732], Loss: 2.9020, accuracy: 11.2175%\n",
      "Epoch [2/2], Step [421/3732], Loss: 3.7163, accuracy: 11.2178%\n",
      "Epoch [2/2], Step [422/3732], Loss: 2.4811, accuracy: 11.2181%\n",
      "Epoch [2/2], Step [423/3732], Loss: 3.7981, accuracy: 11.2154%\n",
      "Epoch [2/2], Step [424/3732], Loss: 3.1668, accuracy: 11.2187%\n",
      "Epoch [2/2], Step [425/3732], Loss: 3.0971, accuracy: 11.2220%\n",
      "Epoch [2/2], Step [426/3732], Loss: 3.5900, accuracy: 11.2193%\n",
      "Epoch [2/2], Step [427/3732], Loss: 2.5953, accuracy: 11.2287%\n",
      "Epoch [2/2], Step [428/3732], Loss: 3.2677, accuracy: 11.2290%\n",
      "Epoch [2/2], Step [429/3732], Loss: 3.4779, accuracy: 11.2293%\n",
      "Epoch [2/2], Step [430/3732], Loss: 2.9833, accuracy: 11.2296%\n",
      "Epoch [2/2], Step [431/3732], Loss: 3.2045, accuracy: 11.2269%\n",
      "Epoch [2/2], Step [432/3732], Loss: 3.1885, accuracy: 11.2272%\n",
      "Epoch [2/2], Step [433/3732], Loss: 3.3030, accuracy: 11.2275%\n",
      "Epoch [2/2], Step [434/3732], Loss: 3.5332, accuracy: 11.2278%\n",
      "Epoch [2/2], Step [435/3732], Loss: 2.9685, accuracy: 11.2341%\n",
      "Epoch [2/2], Step [436/3732], Loss: 3.8856, accuracy: 11.2314%\n",
      "Epoch [2/2], Step [437/3732], Loss: 3.3900, accuracy: 11.2347%\n",
      "Epoch [2/2], Step [438/3732], Loss: 3.6650, accuracy: 11.2350%\n",
      "Epoch [2/2], Step [439/3732], Loss: 2.8405, accuracy: 11.2383%\n",
      "Epoch [2/2], Step [440/3732], Loss: 2.8552, accuracy: 11.2386%\n",
      "Epoch [2/2], Step [441/3732], Loss: 3.5615, accuracy: 11.2389%\n",
      "Epoch [2/2], Step [442/3732], Loss: 3.5128, accuracy: 11.2362%\n",
      "Epoch [2/2], Step [443/3732], Loss: 3.3082, accuracy: 11.2365%\n",
      "Epoch [2/2], Step [444/3732], Loss: 3.7935, accuracy: 11.2368%\n",
      "Epoch [2/2], Step [445/3732], Loss: 2.9500, accuracy: 11.2401%\n",
      "Epoch [2/2], Step [446/3732], Loss: 2.9778, accuracy: 11.2404%\n",
      "Epoch [2/2], Step [447/3732], Loss: 3.2318, accuracy: 11.2407%\n",
      "Epoch [2/2], Step [448/3732], Loss: 3.0119, accuracy: 11.2440%\n",
      "Epoch [2/2], Step [449/3732], Loss: 2.6686, accuracy: 11.2443%\n",
      "Epoch [2/2], Step [450/3732], Loss: 2.3363, accuracy: 11.2476%\n",
      "Epoch [2/2], Step [451/3732], Loss: 2.7162, accuracy: 11.2479%\n",
      "Epoch [2/2], Step [452/3732], Loss: 3.5540, accuracy: 11.2452%\n",
      "Epoch [2/2], Step [453/3732], Loss: 3.8863, accuracy: 11.2485%\n",
      "Epoch [2/2], Step [454/3732], Loss: 4.0033, accuracy: 11.2458%\n",
      "Epoch [2/2], Step [455/3732], Loss: 2.8017, accuracy: 11.2461%\n",
      "Epoch [2/2], Step [456/3732], Loss: 2.9683, accuracy: 11.2494%\n",
      "Epoch [2/2], Step [457/3732], Loss: 3.3160, accuracy: 11.2467%\n",
      "Epoch [2/2], Step [458/3732], Loss: 3.3246, accuracy: 11.2440%\n",
      "Epoch [2/2], Step [459/3732], Loss: 2.9992, accuracy: 11.2473%\n",
      "Epoch [2/2], Step [460/3732], Loss: 3.5215, accuracy: 11.2476%\n",
      "Epoch [2/2], Step [461/3732], Loss: 3.2803, accuracy: 11.2479%\n",
      "Epoch [2/2], Step [462/3732], Loss: 3.4264, accuracy: 11.2452%\n",
      "Epoch [2/2], Step [463/3732], Loss: 3.2791, accuracy: 11.2485%\n",
      "Epoch [2/2], Step [464/3732], Loss: 3.1105, accuracy: 11.2518%\n",
      "Epoch [2/2], Step [465/3732], Loss: 3.5492, accuracy: 11.2521%\n",
      "Epoch [2/2], Step [466/3732], Loss: 3.2556, accuracy: 11.2524%\n",
      "Epoch [2/2], Step [467/3732], Loss: 3.0390, accuracy: 11.2527%\n",
      "Epoch [2/2], Step [468/3732], Loss: 2.6909, accuracy: 11.2560%\n",
      "Epoch [2/2], Step [469/3732], Loss: 3.0243, accuracy: 11.2562%\n",
      "Epoch [2/2], Step [470/3732], Loss: 2.7027, accuracy: 11.2625%\n",
      "Epoch [2/2], Step [471/3732], Loss: 3.2279, accuracy: 11.2628%\n",
      "Epoch [2/2], Step [472/3732], Loss: 3.2387, accuracy: 11.2631%\n",
      "Epoch [2/2], Step [473/3732], Loss: 2.8947, accuracy: 11.2634%\n",
      "Epoch [2/2], Step [474/3732], Loss: 3.4132, accuracy: 11.2607%\n",
      "Epoch [2/2], Step [475/3732], Loss: 3.1501, accuracy: 11.2580%\n",
      "Epoch [2/2], Step [476/3732], Loss: 3.0903, accuracy: 11.2613%\n",
      "Epoch [2/2], Step [477/3732], Loss: 3.1751, accuracy: 11.2616%\n",
      "Epoch [2/2], Step [478/3732], Loss: 3.7266, accuracy: 11.2589%\n",
      "Epoch [2/2], Step [479/3732], Loss: 3.2465, accuracy: 11.2592%\n",
      "Epoch [2/2], Step [480/3732], Loss: 2.6595, accuracy: 11.2625%\n",
      "Epoch [2/2], Step [481/3732], Loss: 3.1710, accuracy: 11.2598%\n",
      "Epoch [2/2], Step [482/3732], Loss: 3.4793, accuracy: 11.2601%\n",
      "Epoch [2/2], Step [483/3732], Loss: 3.2281, accuracy: 11.2574%\n",
      "Epoch [2/2], Step [484/3732], Loss: 3.4475, accuracy: 11.2547%\n",
      "Epoch [2/2], Step [485/3732], Loss: 3.1564, accuracy: 11.2550%\n",
      "Epoch [2/2], Step [486/3732], Loss: 3.2341, accuracy: 11.2524%\n",
      "Epoch [2/2], Step [487/3732], Loss: 3.4593, accuracy: 11.2497%\n",
      "Epoch [2/2], Step [488/3732], Loss: 3.6806, accuracy: 11.2530%\n",
      "Epoch [2/2], Step [489/3732], Loss: 3.4393, accuracy: 11.2562%\n",
      "Epoch [2/2], Step [490/3732], Loss: 2.6970, accuracy: 11.2624%\n",
      "Epoch [2/2], Step [491/3732], Loss: 3.9753, accuracy: 11.2627%\n",
      "Epoch [2/2], Step [492/3732], Loss: 3.4804, accuracy: 11.2660%\n",
      "Epoch [2/2], Step [493/3732], Loss: 3.4424, accuracy: 11.2722%\n",
      "Epoch [2/2], Step [494/3732], Loss: 3.1323, accuracy: 11.2725%\n",
      "Epoch [2/2], Step [495/3732], Loss: 2.5756, accuracy: 11.2728%\n",
      "Epoch [2/2], Step [496/3732], Loss: 3.4642, accuracy: 11.2701%\n",
      "Epoch [2/2], Step [497/3732], Loss: 2.9602, accuracy: 11.2704%\n",
      "Epoch [2/2], Step [498/3732], Loss: 3.1506, accuracy: 11.2677%\n",
      "Epoch [2/2], Step [499/3732], Loss: 2.4461, accuracy: 11.2739%\n",
      "Epoch [2/2], Step [500/3732], Loss: 3.0677, accuracy: 11.2713%\n",
      "Epoch [2/2], Step [501/3732], Loss: 3.0023, accuracy: 11.2716%\n",
      "Epoch [2/2], Step [502/3732], Loss: 2.8475, accuracy: 11.2778%\n",
      "Epoch [2/2], Step [503/3732], Loss: 2.9510, accuracy: 11.2780%\n",
      "Epoch [2/2], Step [504/3732], Loss: 3.1914, accuracy: 11.2754%\n",
      "Epoch [2/2], Step [505/3732], Loss: 2.5901, accuracy: 11.2786%\n",
      "Epoch [2/2], Step [506/3732], Loss: 3.1287, accuracy: 11.2819%\n",
      "Epoch [2/2], Step [507/3732], Loss: 3.1032, accuracy: 11.2851%\n",
      "Epoch [2/2], Step [508/3732], Loss: 2.9450, accuracy: 11.2883%\n",
      "Epoch [2/2], Step [509/3732], Loss: 3.9119, accuracy: 11.2886%\n",
      "Epoch [2/2], Step [510/3732], Loss: 3.3027, accuracy: 11.2860%\n",
      "Epoch [2/2], Step [511/3732], Loss: 2.7991, accuracy: 11.2862%\n",
      "Epoch [2/2], Step [512/3732], Loss: 2.8100, accuracy: 11.2895%\n",
      "Epoch [2/2], Step [513/3732], Loss: 2.1969, accuracy: 11.2986%\n",
      "Epoch [2/2], Step [514/3732], Loss: 2.7460, accuracy: 11.3018%\n",
      "Epoch [2/2], Step [515/3732], Loss: 3.4428, accuracy: 11.3021%\n",
      "Epoch [2/2], Step [516/3732], Loss: 3.2554, accuracy: 11.3024%\n",
      "Epoch [2/2], Step [517/3732], Loss: 3.6360, accuracy: 11.2997%\n",
      "Epoch [2/2], Step [518/3732], Loss: 2.5891, accuracy: 11.3000%\n",
      "Epoch [2/2], Step [519/3732], Loss: 3.0485, accuracy: 11.3032%\n",
      "Epoch [2/2], Step [520/3732], Loss: 2.8020, accuracy: 11.3064%\n",
      "Epoch [2/2], Step [521/3732], Loss: 3.3882, accuracy: 11.3097%\n",
      "Epoch [2/2], Step [522/3732], Loss: 3.3336, accuracy: 11.3070%\n",
      "Epoch [2/2], Step [523/3732], Loss: 3.3722, accuracy: 11.3102%\n",
      "Epoch [2/2], Step [524/3732], Loss: 3.0720, accuracy: 11.3076%\n",
      "Epoch [2/2], Step [525/3732], Loss: 2.6081, accuracy: 11.3108%\n",
      "Epoch [2/2], Step [526/3732], Loss: 3.2180, accuracy: 11.3111%\n",
      "Epoch [2/2], Step [527/3732], Loss: 3.0992, accuracy: 11.3143%\n",
      "Epoch [2/2], Step [528/3732], Loss: 2.4939, accuracy: 11.3146%\n",
      "Epoch [2/2], Step [529/3732], Loss: 2.4307, accuracy: 11.3178%\n",
      "Epoch [2/2], Step [530/3732], Loss: 3.0056, accuracy: 11.3180%\n",
      "Epoch [2/2], Step [531/3732], Loss: 3.2535, accuracy: 11.3242%\n",
      "Epoch [2/2], Step [532/3732], Loss: 2.9085, accuracy: 11.3274%\n",
      "Epoch [2/2], Step [533/3732], Loss: 3.4168, accuracy: 11.3277%\n",
      "Epoch [2/2], Step [534/3732], Loss: 2.8148, accuracy: 11.3309%\n",
      "Epoch [2/2], Step [535/3732], Loss: 2.8787, accuracy: 11.3341%\n",
      "Epoch [2/2], Step [536/3732], Loss: 3.6193, accuracy: 11.3314%\n",
      "Epoch [2/2], Step [537/3732], Loss: 3.3827, accuracy: 11.3317%\n",
      "Epoch [2/2], Step [538/3732], Loss: 2.6424, accuracy: 11.3320%\n",
      "Epoch [2/2], Step [539/3732], Loss: 2.8634, accuracy: 11.3352%\n",
      "Epoch [2/2], Step [540/3732], Loss: 3.3656, accuracy: 11.3354%\n",
      "Epoch [2/2], Step [541/3732], Loss: 3.0822, accuracy: 11.3386%\n",
      "Epoch [2/2], Step [542/3732], Loss: 2.9277, accuracy: 11.3389%\n",
      "Epoch [2/2], Step [543/3732], Loss: 2.4863, accuracy: 11.3480%\n",
      "Epoch [2/2], Step [544/3732], Loss: 3.5063, accuracy: 11.3482%\n",
      "Epoch [2/2], Step [545/3732], Loss: 2.6490, accuracy: 11.3514%\n",
      "Epoch [2/2], Step [546/3732], Loss: 3.0692, accuracy: 11.3517%\n",
      "Epoch [2/2], Step [547/3732], Loss: 3.0330, accuracy: 11.3520%\n",
      "Epoch [2/2], Step [548/3732], Loss: 2.6817, accuracy: 11.3581%\n",
      "Epoch [2/2], Step [549/3732], Loss: 3.3399, accuracy: 11.3612%\n",
      "Epoch [2/2], Step [550/3732], Loss: 3.1495, accuracy: 11.3644%\n",
      "Epoch [2/2], Step [551/3732], Loss: 3.0295, accuracy: 11.3647%\n",
      "Epoch [2/2], Step [552/3732], Loss: 3.5431, accuracy: 11.3650%\n",
      "Epoch [2/2], Step [553/3732], Loss: 2.3820, accuracy: 11.3681%\n",
      "Epoch [2/2], Step [554/3732], Loss: 3.5945, accuracy: 11.3684%\n",
      "Epoch [2/2], Step [555/3732], Loss: 2.9273, accuracy: 11.3687%\n",
      "Epoch [2/2], Step [556/3732], Loss: 2.6038, accuracy: 11.3719%\n",
      "Epoch [2/2], Step [557/3732], Loss: 3.1648, accuracy: 11.3750%\n",
      "Epoch [2/2], Step [558/3732], Loss: 4.2345, accuracy: 11.3724%\n",
      "Epoch [2/2], Step [559/3732], Loss: 2.8270, accuracy: 11.3726%\n",
      "Epoch [2/2], Step [560/3732], Loss: 3.0256, accuracy: 11.3729%\n",
      "Epoch [2/2], Step [561/3732], Loss: 3.0980, accuracy: 11.3732%\n",
      "Epoch [2/2], Step [562/3732], Loss: 3.0722, accuracy: 11.3734%\n",
      "Epoch [2/2], Step [563/3732], Loss: 3.5467, accuracy: 11.3737%\n",
      "Epoch [2/2], Step [564/3732], Loss: 3.1090, accuracy: 11.3769%\n",
      "Epoch [2/2], Step [565/3732], Loss: 3.1696, accuracy: 11.3800%\n",
      "Epoch [2/2], Step [566/3732], Loss: 2.4536, accuracy: 11.3832%\n",
      "Epoch [2/2], Step [567/3732], Loss: 2.9758, accuracy: 11.3864%\n",
      "Epoch [2/2], Step [568/3732], Loss: 3.7747, accuracy: 11.3837%\n",
      "Epoch [2/2], Step [569/3732], Loss: 3.1056, accuracy: 11.3811%\n",
      "Epoch [2/2], Step [570/3732], Loss: 4.0412, accuracy: 11.3784%\n",
      "Epoch [2/2], Step [571/3732], Loss: 2.8372, accuracy: 11.3874%\n",
      "Epoch [2/2], Step [572/3732], Loss: 3.9966, accuracy: 11.3848%\n",
      "Epoch [2/2], Step [573/3732], Loss: 2.8840, accuracy: 11.3850%\n",
      "Epoch [2/2], Step [574/3732], Loss: 3.0337, accuracy: 11.3882%\n",
      "Epoch [2/2], Step [575/3732], Loss: 3.4146, accuracy: 11.3884%\n",
      "Epoch [2/2], Step [576/3732], Loss: 3.1524, accuracy: 11.3887%\n",
      "Epoch [2/2], Step [577/3732], Loss: 3.5639, accuracy: 11.3890%\n",
      "Epoch [2/2], Step [578/3732], Loss: 3.2552, accuracy: 11.3921%\n",
      "Epoch [2/2], Step [579/3732], Loss: 2.3687, accuracy: 11.3982%\n",
      "Epoch [2/2], Step [580/3732], Loss: 2.3362, accuracy: 11.4042%\n",
      "Epoch [2/2], Step [581/3732], Loss: 3.5933, accuracy: 11.4016%\n",
      "Epoch [2/2], Step [582/3732], Loss: 3.6078, accuracy: 11.3989%\n",
      "Epoch [2/2], Step [583/3732], Loss: 2.9606, accuracy: 11.3992%\n",
      "Epoch [2/2], Step [584/3732], Loss: 3.5989, accuracy: 11.3994%\n",
      "Epoch [2/2], Step [585/3732], Loss: 2.2699, accuracy: 11.4055%\n",
      "Epoch [2/2], Step [586/3732], Loss: 3.3660, accuracy: 11.4028%\n",
      "Epoch [2/2], Step [587/3732], Loss: 3.5369, accuracy: 11.4031%\n",
      "Epoch [2/2], Step [588/3732], Loss: 2.9504, accuracy: 11.4062%\n",
      "Epoch [2/2], Step [589/3732], Loss: 2.9293, accuracy: 11.4094%\n",
      "Epoch [2/2], Step [590/3732], Loss: 3.1683, accuracy: 11.4154%\n",
      "Epoch [2/2], Step [591/3732], Loss: 3.3688, accuracy: 11.4157%\n",
      "Epoch [2/2], Step [592/3732], Loss: 2.7784, accuracy: 11.4217%\n",
      "Epoch [2/2], Step [593/3732], Loss: 2.6582, accuracy: 11.4277%\n",
      "Epoch [2/2], Step [594/3732], Loss: 2.7044, accuracy: 11.4367%\n",
      "Epoch [2/2], Step [595/3732], Loss: 3.0875, accuracy: 11.4369%\n",
      "Epoch [2/2], Step [596/3732], Loss: 3.1945, accuracy: 11.4429%\n",
      "Epoch [2/2], Step [597/3732], Loss: 3.3912, accuracy: 11.4432%\n",
      "Epoch [2/2], Step [598/3732], Loss: 3.3925, accuracy: 11.4405%\n",
      "Epoch [2/2], Step [599/3732], Loss: 2.9279, accuracy: 11.4437%\n",
      "Epoch [2/2], Step [600/3732], Loss: 3.0171, accuracy: 11.4410%\n",
      "Epoch [2/2], Step [601/3732], Loss: 4.0007, accuracy: 11.4384%\n",
      "Epoch [2/2], Step [602/3732], Loss: 3.3353, accuracy: 11.4357%\n",
      "Epoch [2/2], Step [603/3732], Loss: 3.0063, accuracy: 11.4389%\n",
      "Epoch [2/2], Step [604/3732], Loss: 3.3393, accuracy: 11.4362%\n",
      "Epoch [2/2], Step [605/3732], Loss: 3.1093, accuracy: 11.4365%\n",
      "Epoch [2/2], Step [606/3732], Loss: 3.1928, accuracy: 11.4396%\n",
      "Epoch [2/2], Step [607/3732], Loss: 2.8342, accuracy: 11.4398%\n",
      "Epoch [2/2], Step [608/3732], Loss: 3.0390, accuracy: 11.4401%\n",
      "Epoch [2/2], Step [609/3732], Loss: 3.1787, accuracy: 11.4403%\n",
      "Epoch [2/2], Step [610/3732], Loss: 3.1027, accuracy: 11.4435%\n",
      "Epoch [2/2], Step [611/3732], Loss: 2.9618, accuracy: 11.4437%\n",
      "Epoch [2/2], Step [612/3732], Loss: 2.8875, accuracy: 11.4468%\n",
      "Epoch [2/2], Step [613/3732], Loss: 3.5958, accuracy: 11.4471%\n",
      "Epoch [2/2], Step [614/3732], Loss: 3.0752, accuracy: 11.4502%\n",
      "Epoch [2/2], Step [615/3732], Loss: 2.9150, accuracy: 11.4533%\n",
      "Epoch [2/2], Step [616/3732], Loss: 2.7225, accuracy: 11.4593%\n",
      "Epoch [2/2], Step [617/3732], Loss: 2.9097, accuracy: 11.4595%\n",
      "Epoch [2/2], Step [618/3732], Loss: 2.9247, accuracy: 11.4598%\n",
      "Epoch [2/2], Step [619/3732], Loss: 3.0822, accuracy: 11.4629%\n",
      "Epoch [2/2], Step [620/3732], Loss: 2.9501, accuracy: 11.4631%\n",
      "Epoch [2/2], Step [621/3732], Loss: 3.2691, accuracy: 11.4634%\n",
      "Epoch [2/2], Step [622/3732], Loss: 3.5975, accuracy: 11.4607%\n",
      "Epoch [2/2], Step [623/3732], Loss: 3.7806, accuracy: 11.4610%\n",
      "Epoch [2/2], Step [624/3732], Loss: 3.7104, accuracy: 11.4583%\n",
      "Epoch [2/2], Step [625/3732], Loss: 2.7429, accuracy: 11.4586%\n",
      "Epoch [2/2], Step [626/3732], Loss: 2.8485, accuracy: 11.4617%\n",
      "Epoch [2/2], Step [627/3732], Loss: 3.7920, accuracy: 11.4591%\n",
      "Epoch [2/2], Step [628/3732], Loss: 3.3684, accuracy: 11.4564%\n",
      "Epoch [2/2], Step [629/3732], Loss: 2.6699, accuracy: 11.4595%\n",
      "Epoch [2/2], Step [630/3732], Loss: 3.5039, accuracy: 11.4626%\n",
      "Epoch [2/2], Step [631/3732], Loss: 3.3796, accuracy: 11.4629%\n",
      "Epoch [2/2], Step [632/3732], Loss: 3.2630, accuracy: 11.4631%\n",
      "Epoch [2/2], Step [633/3732], Loss: 3.0825, accuracy: 11.4691%\n",
      "Epoch [2/2], Step [634/3732], Loss: 3.2139, accuracy: 11.4693%\n",
      "Epoch [2/2], Step [635/3732], Loss: 3.8358, accuracy: 11.4695%\n",
      "Epoch [2/2], Step [636/3732], Loss: 4.1559, accuracy: 11.4669%\n",
      "Epoch [2/2], Step [637/3732], Loss: 3.3365, accuracy: 11.4643%\n",
      "Epoch [2/2], Step [638/3732], Loss: 2.5797, accuracy: 11.4703%\n",
      "Epoch [2/2], Step [639/3732], Loss: 3.1652, accuracy: 11.4676%\n",
      "Epoch [2/2], Step [640/3732], Loss: 2.9742, accuracy: 11.4707%\n",
      "Epoch [2/2], Step [641/3732], Loss: 2.6552, accuracy: 11.4795%\n",
      "Epoch [2/2], Step [642/3732], Loss: 3.7040, accuracy: 11.4769%\n",
      "Epoch [2/2], Step [643/3732], Loss: 3.6971, accuracy: 11.4771%\n",
      "Epoch [2/2], Step [644/3732], Loss: 2.8979, accuracy: 11.4831%\n",
      "Epoch [2/2], Step [645/3732], Loss: 2.3593, accuracy: 11.4862%\n",
      "Epoch [2/2], Step [646/3732], Loss: 3.1621, accuracy: 11.4864%\n",
      "Epoch [2/2], Step [647/3732], Loss: 3.4318, accuracy: 11.4895%\n",
      "Epoch [2/2], Step [648/3732], Loss: 2.9522, accuracy: 11.4897%\n",
      "Epoch [2/2], Step [649/3732], Loss: 2.9211, accuracy: 11.4900%\n",
      "Epoch [2/2], Step [650/3732], Loss: 4.0186, accuracy: 11.4902%\n",
      "Epoch [2/2], Step [651/3732], Loss: 2.7983, accuracy: 11.4933%\n",
      "Epoch [2/2], Step [652/3732], Loss: 3.7848, accuracy: 11.4935%\n",
      "Epoch [2/2], Step [653/3732], Loss: 3.2500, accuracy: 11.4909%\n",
      "Epoch [2/2], Step [654/3732], Loss: 3.1146, accuracy: 11.4911%\n",
      "Epoch [2/2], Step [655/3732], Loss: 3.3419, accuracy: 11.4885%\n",
      "Epoch [2/2], Step [656/3732], Loss: 2.5378, accuracy: 11.4916%\n",
      "Epoch [2/2], Step [657/3732], Loss: 3.4138, accuracy: 11.4918%\n",
      "Epoch [2/2], Step [658/3732], Loss: 2.6939, accuracy: 11.4892%\n",
      "Epoch [2/2], Step [659/3732], Loss: 2.2559, accuracy: 11.4980%\n",
      "Epoch [2/2], Step [660/3732], Loss: 3.2766, accuracy: 11.5010%\n",
      "Epoch [2/2], Step [661/3732], Loss: 3.1988, accuracy: 11.5041%\n",
      "Epoch [2/2], Step [662/3732], Loss: 3.6641, accuracy: 11.5043%\n",
      "Epoch [2/2], Step [663/3732], Loss: 3.0007, accuracy: 11.5074%\n",
      "Epoch [2/2], Step [664/3732], Loss: 3.5632, accuracy: 11.5076%\n",
      "Epoch [2/2], Step [665/3732], Loss: 3.5689, accuracy: 11.5050%\n",
      "Epoch [2/2], Step [666/3732], Loss: 2.3531, accuracy: 11.5109%\n",
      "Epoch [2/2], Step [667/3732], Loss: 3.0326, accuracy: 11.5111%\n",
      "Epoch [2/2], Step [668/3732], Loss: 2.5974, accuracy: 11.5114%\n",
      "Epoch [2/2], Step [669/3732], Loss: 3.6651, accuracy: 11.5087%\n",
      "Epoch [2/2], Step [670/3732], Loss: 3.3903, accuracy: 11.5090%\n",
      "Epoch [2/2], Step [671/3732], Loss: 3.2115, accuracy: 11.5120%\n",
      "Epoch [2/2], Step [672/3732], Loss: 3.5160, accuracy: 11.5123%\n",
      "Epoch [2/2], Step [673/3732], Loss: 2.4692, accuracy: 11.5153%\n",
      "Epoch [2/2], Step [674/3732], Loss: 2.5398, accuracy: 11.5184%\n",
      "Epoch [2/2], Step [675/3732], Loss: 2.5308, accuracy: 11.5186%\n",
      "Epoch [2/2], Step [676/3732], Loss: 3.6081, accuracy: 11.5160%\n",
      "Epoch [2/2], Step [677/3732], Loss: 2.9568, accuracy: 11.5162%\n",
      "Epoch [2/2], Step [678/3732], Loss: 3.1847, accuracy: 11.5164%\n",
      "Epoch [2/2], Step [679/3732], Loss: 3.0700, accuracy: 11.5167%\n",
      "Epoch [2/2], Step [680/3732], Loss: 3.2384, accuracy: 11.5197%\n",
      "Epoch [2/2], Step [681/3732], Loss: 2.6290, accuracy: 11.5284%\n",
      "Epoch [2/2], Step [682/3732], Loss: 2.9660, accuracy: 11.5315%\n",
      "Epoch [2/2], Step [683/3732], Loss: 3.5704, accuracy: 11.5289%\n",
      "Epoch [2/2], Step [684/3732], Loss: 2.8389, accuracy: 11.5319%\n",
      "Epoch [2/2], Step [685/3732], Loss: 3.3851, accuracy: 11.5321%\n",
      "Epoch [2/2], Step [686/3732], Loss: 3.3370, accuracy: 11.5324%\n",
      "Epoch [2/2], Step [687/3732], Loss: 3.0013, accuracy: 11.5326%\n",
      "Epoch [2/2], Step [688/3732], Loss: 3.3479, accuracy: 11.5300%\n",
      "Epoch [2/2], Step [689/3732], Loss: 3.3991, accuracy: 11.5274%\n",
      "Epoch [2/2], Step [690/3732], Loss: 3.1044, accuracy: 11.5276%\n",
      "Epoch [2/2], Step [691/3732], Loss: 2.8599, accuracy: 11.5278%\n",
      "Epoch [2/2], Step [692/3732], Loss: 2.8805, accuracy: 11.5252%\n",
      "Epoch [2/2], Step [693/3732], Loss: 2.3674, accuracy: 11.5282%\n",
      "Epoch [2/2], Step [694/3732], Loss: 3.3942, accuracy: 11.5256%\n",
      "Epoch [2/2], Step [695/3732], Loss: 2.9888, accuracy: 11.5230%\n",
      "Epoch [2/2], Step [696/3732], Loss: 2.7759, accuracy: 11.5233%\n",
      "Epoch [2/2], Step [697/3732], Loss: 3.1950, accuracy: 11.5263%\n",
      "Epoch [2/2], Step [698/3732], Loss: 3.2713, accuracy: 11.5265%\n",
      "Epoch [2/2], Step [699/3732], Loss: 2.4807, accuracy: 11.5296%\n",
      "Epoch [2/2], Step [700/3732], Loss: 2.8013, accuracy: 11.5326%\n",
      "Epoch [2/2], Step [701/3732], Loss: 3.4002, accuracy: 11.5328%\n",
      "Epoch [2/2], Step [702/3732], Loss: 3.0953, accuracy: 11.5302%\n",
      "Epoch [2/2], Step [703/3732], Loss: 2.9117, accuracy: 11.5304%\n",
      "Epoch [2/2], Step [704/3732], Loss: 2.8077, accuracy: 11.5335%\n",
      "Epoch [2/2], Step [705/3732], Loss: 3.1942, accuracy: 11.5337%\n",
      "Epoch [2/2], Step [706/3732], Loss: 2.8138, accuracy: 11.5367%\n",
      "Epoch [2/2], Step [707/3732], Loss: 2.8069, accuracy: 11.5341%\n",
      "Epoch [2/2], Step [708/3732], Loss: 3.1173, accuracy: 11.5343%\n",
      "Epoch [2/2], Step [709/3732], Loss: 3.1533, accuracy: 11.5346%\n",
      "Epoch [2/2], Step [710/3732], Loss: 3.6558, accuracy: 11.5348%\n",
      "Epoch [2/2], Step [711/3732], Loss: 2.4431, accuracy: 11.5434%\n",
      "Epoch [2/2], Step [712/3732], Loss: 3.7757, accuracy: 11.5408%\n",
      "Epoch [2/2], Step [713/3732], Loss: 3.9767, accuracy: 11.5382%\n",
      "Epoch [2/2], Step [714/3732], Loss: 3.2726, accuracy: 11.5385%\n",
      "Epoch [2/2], Step [715/3732], Loss: 3.7083, accuracy: 11.5387%\n",
      "Epoch [2/2], Step [716/3732], Loss: 2.5173, accuracy: 11.5417%\n",
      "Epoch [2/2], Step [717/3732], Loss: 2.8910, accuracy: 11.5475%\n",
      "Epoch [2/2], Step [718/3732], Loss: 2.6925, accuracy: 11.5506%\n",
      "Epoch [2/2], Step [719/3732], Loss: 3.2419, accuracy: 11.5480%\n",
      "Epoch [2/2], Step [720/3732], Loss: 3.6626, accuracy: 11.5454%\n",
      "Epoch [2/2], Step [721/3732], Loss: 2.9979, accuracy: 11.5484%\n",
      "Epoch [2/2], Step [722/3732], Loss: 3.5883, accuracy: 11.5486%\n",
      "Epoch [2/2], Step [723/3732], Loss: 2.9834, accuracy: 11.5488%\n",
      "Epoch [2/2], Step [724/3732], Loss: 3.1341, accuracy: 11.5490%\n",
      "Epoch [2/2], Step [725/3732], Loss: 3.7826, accuracy: 11.5464%\n",
      "Epoch [2/2], Step [726/3732], Loss: 3.7417, accuracy: 11.5467%\n",
      "Epoch [2/2], Step [727/3732], Loss: 3.0280, accuracy: 11.5469%\n",
      "Epoch [2/2], Step [728/3732], Loss: 2.7768, accuracy: 11.5527%\n",
      "Epoch [2/2], Step [729/3732], Loss: 3.0210, accuracy: 11.5529%\n",
      "Epoch [2/2], Step [730/3732], Loss: 3.0532, accuracy: 11.5559%\n",
      "Epoch [2/2], Step [731/3732], Loss: 4.3921, accuracy: 11.5533%\n",
      "Epoch [2/2], Step [732/3732], Loss: 3.4757, accuracy: 11.5535%\n",
      "Epoch [2/2], Step [733/3732], Loss: 3.3053, accuracy: 11.5510%\n",
      "Epoch [2/2], Step [734/3732], Loss: 3.0177, accuracy: 11.5540%\n",
      "Epoch [2/2], Step [735/3732], Loss: 2.5184, accuracy: 11.5598%\n",
      "Epoch [2/2], Step [736/3732], Loss: 2.8755, accuracy: 11.5684%\n",
      "Epoch [2/2], Step [737/3732], Loss: 2.8005, accuracy: 11.5742%\n",
      "Epoch [2/2], Step [738/3732], Loss: 3.5316, accuracy: 11.5744%\n",
      "Epoch [2/2], Step [739/3732], Loss: 3.1475, accuracy: 11.5746%\n",
      "Epoch [2/2], Step [740/3732], Loss: 2.8511, accuracy: 11.5776%\n",
      "Epoch [2/2], Step [741/3732], Loss: 3.1014, accuracy: 11.5806%\n",
      "Epoch [2/2], Step [742/3732], Loss: 3.1351, accuracy: 11.5808%\n",
      "Epoch [2/2], Step [743/3732], Loss: 3.6265, accuracy: 11.5782%\n",
      "Epoch [2/2], Step [744/3732], Loss: 2.7463, accuracy: 11.5840%\n",
      "Epoch [2/2], Step [745/3732], Loss: 3.0979, accuracy: 11.5842%\n",
      "Epoch [2/2], Step [746/3732], Loss: 3.5533, accuracy: 11.5844%\n",
      "Epoch [2/2], Step [747/3732], Loss: 2.8527, accuracy: 11.5874%\n",
      "Epoch [2/2], Step [748/3732], Loss: 2.4980, accuracy: 11.5904%\n",
      "Epoch [2/2], Step [749/3732], Loss: 2.9193, accuracy: 11.5906%\n",
      "Epoch [2/2], Step [750/3732], Loss: 3.3037, accuracy: 11.5908%\n",
      "Epoch [2/2], Step [751/3732], Loss: 3.3331, accuracy: 11.5882%\n",
      "Epoch [2/2], Step [752/3732], Loss: 2.4948, accuracy: 11.5940%\n",
      "Epoch [2/2], Step [753/3732], Loss: 3.4210, accuracy: 11.5942%\n",
      "Epoch [2/2], Step [754/3732], Loss: 4.4142, accuracy: 11.5972%\n",
      "Epoch [2/2], Step [755/3732], Loss: 2.7327, accuracy: 11.6002%\n",
      "Epoch [2/2], Step [756/3732], Loss: 3.2898, accuracy: 11.5976%\n",
      "Epoch [2/2], Step [757/3732], Loss: 3.1207, accuracy: 11.5978%\n",
      "Epoch [2/2], Step [758/3732], Loss: 3.5446, accuracy: 11.5952%\n",
      "Epoch [2/2], Step [759/3732], Loss: 4.2177, accuracy: 11.5926%\n",
      "Epoch [2/2], Step [760/3732], Loss: 3.8903, accuracy: 11.5900%\n",
      "Epoch [2/2], Step [761/3732], Loss: 3.3000, accuracy: 11.5903%\n",
      "Epoch [2/2], Step [762/3732], Loss: 3.4519, accuracy: 11.5905%\n",
      "Epoch [2/2], Step [763/3732], Loss: 3.1989, accuracy: 11.5907%\n",
      "Epoch [2/2], Step [764/3732], Loss: 3.4984, accuracy: 11.5881%\n",
      "Epoch [2/2], Step [765/3732], Loss: 2.8914, accuracy: 11.5883%\n",
      "Epoch [2/2], Step [766/3732], Loss: 3.5714, accuracy: 11.5885%\n",
      "Epoch [2/2], Step [767/3732], Loss: 2.8471, accuracy: 11.5887%\n",
      "Epoch [2/2], Step [768/3732], Loss: 3.0147, accuracy: 11.5861%\n",
      "Epoch [2/2], Step [769/3732], Loss: 3.9430, accuracy: 11.5835%\n",
      "Epoch [2/2], Step [770/3732], Loss: 3.5602, accuracy: 11.5810%\n",
      "Epoch [2/2], Step [771/3732], Loss: 3.1428, accuracy: 11.5784%\n",
      "Epoch [2/2], Step [772/3732], Loss: 3.4125, accuracy: 11.5786%\n",
      "Epoch [2/2], Step [773/3732], Loss: 2.7692, accuracy: 11.5788%\n",
      "Epoch [2/2], Step [774/3732], Loss: 2.9256, accuracy: 11.5818%\n",
      "Epoch [2/2], Step [775/3732], Loss: 3.2324, accuracy: 11.5875%\n",
      "Epoch [2/2], Step [776/3732], Loss: 3.4557, accuracy: 11.5850%\n",
      "Epoch [2/2], Step [777/3732], Loss: 3.2044, accuracy: 11.5852%\n",
      "Epoch [2/2], Step [778/3732], Loss: 3.1989, accuracy: 11.5881%\n",
      "Epoch [2/2], Step [779/3732], Loss: 3.5133, accuracy: 11.5883%\n",
      "Epoch [2/2], Step [780/3732], Loss: 3.5915, accuracy: 11.5885%\n",
      "Epoch [2/2], Step [781/3732], Loss: 2.6915, accuracy: 11.5887%\n",
      "Epoch [2/2], Step [782/3732], Loss: 3.7827, accuracy: 11.5862%\n",
      "Epoch [2/2], Step [783/3732], Loss: 3.5075, accuracy: 11.5836%\n",
      "Epoch [2/2], Step [784/3732], Loss: 2.8442, accuracy: 11.5838%\n",
      "Epoch [2/2], Step [785/3732], Loss: 2.9922, accuracy: 11.5812%\n",
      "Epoch [2/2], Step [786/3732], Loss: 3.6606, accuracy: 11.5787%\n",
      "Epoch [2/2], Step [787/3732], Loss: 3.5930, accuracy: 11.5789%\n",
      "Epoch [2/2], Step [788/3732], Loss: 2.7365, accuracy: 11.5846%\n",
      "Epoch [2/2], Step [789/3732], Loss: 2.9584, accuracy: 11.5848%\n",
      "Epoch [2/2], Step [790/3732], Loss: 3.5605, accuracy: 11.5850%\n",
      "Epoch [2/2], Step [791/3732], Loss: 3.4115, accuracy: 11.5825%\n",
      "Epoch [2/2], Step [792/3732], Loss: 3.8045, accuracy: 11.5854%\n",
      "Epoch [2/2], Step [793/3732], Loss: 3.2802, accuracy: 11.5856%\n",
      "Epoch [2/2], Step [794/3732], Loss: 3.5845, accuracy: 11.5831%\n",
      "Epoch [2/2], Step [795/3732], Loss: 2.9982, accuracy: 11.5888%\n",
      "Epoch [2/2], Step [796/3732], Loss: 2.2786, accuracy: 11.5945%\n",
      "Epoch [2/2], Step [797/3732], Loss: 3.1439, accuracy: 11.5920%\n",
      "Epoch [2/2], Step [798/3732], Loss: 3.2421, accuracy: 11.5894%\n",
      "Epoch [2/2], Step [799/3732], Loss: 3.6240, accuracy: 11.5868%\n",
      "Epoch [2/2], Step [800/3732], Loss: 2.9015, accuracy: 11.5898%\n",
      "Epoch [2/2], Step [801/3732], Loss: 2.8491, accuracy: 11.5928%\n",
      "Epoch [2/2], Step [802/3732], Loss: 3.0648, accuracy: 11.5930%\n",
      "Epoch [2/2], Step [803/3732], Loss: 3.1138, accuracy: 11.5959%\n",
      "Epoch [2/2], Step [804/3732], Loss: 3.1731, accuracy: 11.5989%\n",
      "Epoch [2/2], Step [805/3732], Loss: 3.0806, accuracy: 11.6018%\n",
      "Epoch [2/2], Step [806/3732], Loss: 3.4517, accuracy: 11.6020%\n",
      "Epoch [2/2], Step [807/3732], Loss: 2.7829, accuracy: 11.6022%\n",
      "Epoch [2/2], Step [808/3732], Loss: 2.9585, accuracy: 11.6024%\n",
      "Epoch [2/2], Step [809/3732], Loss: 2.8434, accuracy: 11.6026%\n",
      "Epoch [2/2], Step [810/3732], Loss: 3.0058, accuracy: 11.6001%\n",
      "Epoch [2/2], Step [811/3732], Loss: 3.5289, accuracy: 11.5975%\n",
      "Epoch [2/2], Step [812/3732], Loss: 3.3857, accuracy: 11.6005%\n",
      "Epoch [2/2], Step [813/3732], Loss: 3.2720, accuracy: 11.5979%\n",
      "Epoch [2/2], Step [814/3732], Loss: 3.6605, accuracy: 11.5981%\n",
      "Epoch [2/2], Step [815/3732], Loss: 2.9420, accuracy: 11.5983%\n",
      "Epoch [2/2], Step [816/3732], Loss: 3.2111, accuracy: 11.5985%\n",
      "Epoch [2/2], Step [817/3732], Loss: 2.8224, accuracy: 11.6015%\n",
      "Epoch [2/2], Step [818/3732], Loss: 2.4822, accuracy: 11.6016%\n",
      "Epoch [2/2], Step [819/3732], Loss: 4.7282, accuracy: 11.5991%\n",
      "Epoch [2/2], Step [820/3732], Loss: 3.6665, accuracy: 11.5966%\n",
      "Epoch [2/2], Step [821/3732], Loss: 3.4130, accuracy: 11.5995%\n",
      "Epoch [2/2], Step [822/3732], Loss: 2.9282, accuracy: 11.6024%\n",
      "Epoch [2/2], Step [823/3732], Loss: 2.5875, accuracy: 11.6081%\n",
      "Epoch [2/2], Step [824/3732], Loss: 3.0814, accuracy: 11.6083%\n",
      "Epoch [2/2], Step [825/3732], Loss: 2.6945, accuracy: 11.6113%\n",
      "Epoch [2/2], Step [826/3732], Loss: 3.6868, accuracy: 11.6087%\n",
      "Epoch [2/2], Step [827/3732], Loss: 2.8860, accuracy: 11.6089%\n",
      "Epoch [2/2], Step [828/3732], Loss: 3.3927, accuracy: 11.6091%\n",
      "Epoch [2/2], Step [829/3732], Loss: 3.5975, accuracy: 11.6066%\n",
      "Epoch [2/2], Step [830/3732], Loss: 3.0358, accuracy: 11.6095%\n",
      "Epoch [2/2], Step [831/3732], Loss: 3.6330, accuracy: 11.6097%\n",
      "Epoch [2/2], Step [832/3732], Loss: 3.1719, accuracy: 11.6099%\n",
      "Epoch [2/2], Step [833/3732], Loss: 3.3937, accuracy: 11.6101%\n",
      "Epoch [2/2], Step [834/3732], Loss: 2.7878, accuracy: 11.6130%\n",
      "Epoch [2/2], Step [835/3732], Loss: 3.5668, accuracy: 11.6105%\n",
      "Epoch [2/2], Step [836/3732], Loss: 3.3179, accuracy: 11.6079%\n",
      "Epoch [2/2], Step [837/3732], Loss: 3.2560, accuracy: 11.6081%\n",
      "Epoch [2/2], Step [838/3732], Loss: 3.3746, accuracy: 11.6083%\n",
      "Epoch [2/2], Step [839/3732], Loss: 2.9358, accuracy: 11.6112%\n",
      "Epoch [2/2], Step [840/3732], Loss: 3.1887, accuracy: 11.6114%\n",
      "Epoch [2/2], Step [841/3732], Loss: 2.6316, accuracy: 11.6198%\n",
      "Epoch [2/2], Step [842/3732], Loss: 2.6106, accuracy: 11.6228%\n",
      "Epoch [2/2], Step [843/3732], Loss: 2.8312, accuracy: 11.6257%\n",
      "Epoch [2/2], Step [844/3732], Loss: 3.4935, accuracy: 11.6259%\n",
      "Epoch [2/2], Step [845/3732], Loss: 3.3712, accuracy: 11.6315%\n",
      "Epoch [2/2], Step [846/3732], Loss: 2.9036, accuracy: 11.6372%\n",
      "Epoch [2/2], Step [847/3732], Loss: 3.3762, accuracy: 11.6374%\n",
      "Epoch [2/2], Step [848/3732], Loss: 2.2189, accuracy: 11.6430%\n",
      "Epoch [2/2], Step [849/3732], Loss: 2.7039, accuracy: 11.6459%\n",
      "Epoch [2/2], Step [850/3732], Loss: 3.2231, accuracy: 11.6461%\n",
      "Epoch [2/2], Step [851/3732], Loss: 3.1923, accuracy: 11.6463%\n",
      "Epoch [2/2], Step [852/3732], Loss: 3.3786, accuracy: 11.6465%\n",
      "Epoch [2/2], Step [853/3732], Loss: 2.6170, accuracy: 11.6494%\n",
      "Epoch [2/2], Step [854/3732], Loss: 2.7520, accuracy: 11.6550%\n",
      "Epoch [2/2], Step [855/3732], Loss: 3.3155, accuracy: 11.6579%\n",
      "Epoch [2/2], Step [856/3732], Loss: 2.9117, accuracy: 11.6581%\n",
      "Epoch [2/2], Step [857/3732], Loss: 2.9566, accuracy: 11.6583%\n",
      "Epoch [2/2], Step [858/3732], Loss: 2.4730, accuracy: 11.6612%\n",
      "Epoch [2/2], Step [859/3732], Loss: 2.9017, accuracy: 11.6587%\n",
      "Epoch [2/2], Step [860/3732], Loss: 3.4203, accuracy: 11.6616%\n",
      "Epoch [2/2], Step [861/3732], Loss: 2.2377, accuracy: 11.6618%\n",
      "Epoch [2/2], Step [862/3732], Loss: 2.3556, accuracy: 11.6647%\n",
      "Epoch [2/2], Step [863/3732], Loss: 2.8996, accuracy: 11.6703%\n",
      "Epoch [2/2], Step [864/3732], Loss: 3.0607, accuracy: 11.6732%\n",
      "Epoch [2/2], Step [865/3732], Loss: 3.4908, accuracy: 11.6707%\n",
      "Epoch [2/2], Step [866/3732], Loss: 3.5236, accuracy: 11.6736%\n",
      "Epoch [2/2], Step [867/3732], Loss: 2.4292, accuracy: 11.6792%\n",
      "Epoch [2/2], Step [868/3732], Loss: 2.6353, accuracy: 11.6875%\n",
      "Epoch [2/2], Step [869/3732], Loss: 2.9036, accuracy: 11.6931%\n",
      "Epoch [2/2], Step [870/3732], Loss: 3.6588, accuracy: 11.6906%\n",
      "Epoch [2/2], Step [871/3732], Loss: 3.9278, accuracy: 11.6907%\n",
      "Epoch [2/2], Step [872/3732], Loss: 3.1385, accuracy: 11.6909%\n",
      "Epoch [2/2], Step [873/3732], Loss: 3.0195, accuracy: 11.6938%\n",
      "Epoch [2/2], Step [874/3732], Loss: 2.7590, accuracy: 11.6913%\n",
      "Epoch [2/2], Step [875/3732], Loss: 2.7401, accuracy: 11.6887%\n",
      "Epoch [2/2], Step [876/3732], Loss: 3.3937, accuracy: 11.6916%\n",
      "Epoch [2/2], Step [877/3732], Loss: 3.3148, accuracy: 11.6918%\n",
      "Epoch [2/2], Step [878/3732], Loss: 3.7025, accuracy: 11.6920%\n",
      "Epoch [2/2], Step [879/3732], Loss: 2.7558, accuracy: 11.6976%\n",
      "Epoch [2/2], Step [880/3732], Loss: 4.0695, accuracy: 11.6950%\n",
      "Epoch [2/2], Step [881/3732], Loss: 4.0674, accuracy: 11.6925%\n",
      "Epoch [2/2], Step [882/3732], Loss: 2.7130, accuracy: 11.6981%\n",
      "Epoch [2/2], Step [883/3732], Loss: 2.1698, accuracy: 11.7064%\n",
      "Epoch [2/2], Step [884/3732], Loss: 3.7452, accuracy: 11.7039%\n",
      "Epoch [2/2], Step [885/3732], Loss: 3.5396, accuracy: 11.7040%\n",
      "Epoch [2/2], Step [886/3732], Loss: 2.4989, accuracy: 11.7042%\n",
      "Epoch [2/2], Step [887/3732], Loss: 2.6973, accuracy: 11.7071%\n",
      "Epoch [2/2], Step [888/3732], Loss: 2.5667, accuracy: 11.7127%\n",
      "Epoch [2/2], Step [889/3732], Loss: 3.4806, accuracy: 11.7101%\n",
      "Epoch [2/2], Step [890/3732], Loss: 2.4944, accuracy: 11.7184%\n",
      "Epoch [2/2], Step [891/3732], Loss: 3.0119, accuracy: 11.7240%\n",
      "Epoch [2/2], Step [892/3732], Loss: 3.4483, accuracy: 11.7242%\n",
      "Epoch [2/2], Step [893/3732], Loss: 3.1806, accuracy: 11.7270%\n",
      "Epoch [2/2], Step [894/3732], Loss: 3.8256, accuracy: 11.7272%\n",
      "Epoch [2/2], Step [895/3732], Loss: 2.6002, accuracy: 11.7328%\n",
      "Epoch [2/2], Step [896/3732], Loss: 3.4929, accuracy: 11.7329%\n",
      "Epoch [2/2], Step [897/3732], Loss: 3.2597, accuracy: 11.7331%\n",
      "Epoch [2/2], Step [898/3732], Loss: 2.6577, accuracy: 11.7387%\n",
      "Epoch [2/2], Step [899/3732], Loss: 2.6795, accuracy: 11.7415%\n",
      "Epoch [2/2], Step [900/3732], Loss: 2.8263, accuracy: 11.7444%\n",
      "Epoch [2/2], Step [901/3732], Loss: 2.3739, accuracy: 11.7526%\n",
      "Epoch [2/2], Step [902/3732], Loss: 3.0042, accuracy: 11.7555%\n",
      "Epoch [2/2], Step [903/3732], Loss: 3.0817, accuracy: 11.7557%\n",
      "Epoch [2/2], Step [904/3732], Loss: 4.1668, accuracy: 11.7531%\n",
      "Epoch [2/2], Step [905/3732], Loss: 4.2562, accuracy: 11.7506%\n",
      "Epoch [2/2], Step [906/3732], Loss: 2.7794, accuracy: 11.7508%\n",
      "Epoch [2/2], Step [907/3732], Loss: 2.5977, accuracy: 11.7536%\n",
      "Epoch [2/2], Step [908/3732], Loss: 3.1111, accuracy: 11.7565%\n",
      "Epoch [2/2], Step [909/3732], Loss: 3.1185, accuracy: 11.7566%\n",
      "Epoch [2/2], Step [910/3732], Loss: 3.4694, accuracy: 11.7568%\n",
      "Epoch [2/2], Step [911/3732], Loss: 2.2559, accuracy: 11.7623%\n",
      "Epoch [2/2], Step [912/3732], Loss: 3.6691, accuracy: 11.7625%\n",
      "Epoch [2/2], Step [913/3732], Loss: 2.8563, accuracy: 11.7680%\n",
      "Epoch [2/2], Step [914/3732], Loss: 3.7664, accuracy: 11.7655%\n",
      "Epoch [2/2], Step [915/3732], Loss: 3.5884, accuracy: 11.7630%\n",
      "Epoch [2/2], Step [916/3732], Loss: 3.0246, accuracy: 11.7604%\n",
      "Epoch [2/2], Step [917/3732], Loss: 2.7912, accuracy: 11.7660%\n",
      "Epoch [2/2], Step [918/3732], Loss: 3.2879, accuracy: 11.7661%\n",
      "Epoch [2/2], Step [919/3732], Loss: 3.1197, accuracy: 11.7663%\n",
      "Epoch [2/2], Step [920/3732], Loss: 3.3372, accuracy: 11.7691%\n",
      "Epoch [2/2], Step [921/3732], Loss: 2.9512, accuracy: 11.7720%\n",
      "Epoch [2/2], Step [922/3732], Loss: 3.3393, accuracy: 11.7694%\n",
      "Epoch [2/2], Step [923/3732], Loss: 3.6432, accuracy: 11.7696%\n",
      "Epoch [2/2], Step [924/3732], Loss: 3.4745, accuracy: 11.7698%\n",
      "Epoch [2/2], Step [925/3732], Loss: 2.5915, accuracy: 11.7726%\n",
      "Epoch [2/2], Step [926/3732], Loss: 3.2801, accuracy: 11.7728%\n",
      "Epoch [2/2], Step [927/3732], Loss: 2.9652, accuracy: 11.7756%\n",
      "Epoch [2/2], Step [928/3732], Loss: 3.0285, accuracy: 11.7784%\n",
      "Epoch [2/2], Step [929/3732], Loss: 2.9420, accuracy: 11.7759%\n",
      "Epoch [2/2], Step [930/3732], Loss: 3.1329, accuracy: 11.7761%\n",
      "Epoch [2/2], Step [931/3732], Loss: 2.6437, accuracy: 11.7843%\n",
      "Epoch [2/2], Step [932/3732], Loss: 3.2688, accuracy: 11.7871%\n",
      "Epoch [2/2], Step [933/3732], Loss: 2.9873, accuracy: 11.7899%\n",
      "Epoch [2/2], Step [934/3732], Loss: 2.7506, accuracy: 11.7928%\n",
      "Epoch [2/2], Step [935/3732], Loss: 3.3785, accuracy: 11.7902%\n",
      "Epoch [2/2], Step [936/3732], Loss: 2.7247, accuracy: 11.7957%\n",
      "Epoch [2/2], Step [937/3732], Loss: 3.0893, accuracy: 11.7986%\n",
      "Epoch [2/2], Step [938/3732], Loss: 2.9790, accuracy: 11.7987%\n",
      "Epoch [2/2], Step [939/3732], Loss: 2.9305, accuracy: 11.7989%\n",
      "Epoch [2/2], Step [940/3732], Loss: 4.1192, accuracy: 11.7963%\n",
      "Epoch [2/2], Step [941/3732], Loss: 3.9755, accuracy: 11.7965%\n",
      "Epoch [2/2], Step [942/3732], Loss: 2.8794, accuracy: 11.7993%\n",
      "Epoch [2/2], Step [943/3732], Loss: 2.7209, accuracy: 11.8075%\n",
      "Epoch [2/2], Step [944/3732], Loss: 3.2874, accuracy: 11.8103%\n",
      "Epoch [2/2], Step [945/3732], Loss: 2.5550, accuracy: 11.8131%\n",
      "Epoch [2/2], Step [946/3732], Loss: 2.4058, accuracy: 11.8240%\n",
      "Epoch [2/2], Step [947/3732], Loss: 2.5979, accuracy: 11.8268%\n",
      "Epoch [2/2], Step [948/3732], Loss: 3.1309, accuracy: 11.8269%\n",
      "Epoch [2/2], Step [949/3732], Loss: 2.7700, accuracy: 11.8271%\n",
      "Epoch [2/2], Step [950/3732], Loss: 3.2056, accuracy: 11.8245%\n",
      "Epoch [2/2], Step [951/3732], Loss: 3.0605, accuracy: 11.8247%\n",
      "Epoch [2/2], Step [952/3732], Loss: 3.9802, accuracy: 11.8222%\n",
      "Epoch [2/2], Step [953/3732], Loss: 2.8089, accuracy: 11.8250%\n",
      "Epoch [2/2], Step [954/3732], Loss: 3.1385, accuracy: 11.8278%\n",
      "Epoch [2/2], Step [955/3732], Loss: 3.4250, accuracy: 11.8279%\n",
      "Epoch [2/2], Step [956/3732], Loss: 3.2568, accuracy: 11.8281%\n",
      "Epoch [2/2], Step [957/3732], Loss: 3.0286, accuracy: 11.8309%\n",
      "Epoch [2/2], Step [958/3732], Loss: 3.7746, accuracy: 11.8310%\n",
      "Epoch [2/2], Step [959/3732], Loss: 3.6454, accuracy: 11.8312%\n",
      "Epoch [2/2], Step [960/3732], Loss: 3.8849, accuracy: 11.8286%\n",
      "Epoch [2/2], Step [961/3732], Loss: 3.3489, accuracy: 11.8288%\n",
      "Epoch [2/2], Step [962/3732], Loss: 3.1690, accuracy: 11.8343%\n",
      "Epoch [2/2], Step [963/3732], Loss: 2.8199, accuracy: 11.8344%\n",
      "Epoch [2/2], Step [964/3732], Loss: 2.7914, accuracy: 11.8372%\n",
      "Epoch [2/2], Step [965/3732], Loss: 2.6473, accuracy: 11.8427%\n",
      "Epoch [2/2], Step [966/3732], Loss: 3.7358, accuracy: 11.8428%\n",
      "Epoch [2/2], Step [967/3732], Loss: 4.3138, accuracy: 11.8403%\n",
      "Epoch [2/2], Step [968/3732], Loss: 2.7573, accuracy: 11.8404%\n",
      "Epoch [2/2], Step [969/3732], Loss: 3.1874, accuracy: 11.8432%\n",
      "Epoch [2/2], Step [970/3732], Loss: 2.9237, accuracy: 11.8460%\n",
      "Epoch [2/2], Step [971/3732], Loss: 2.7661, accuracy: 11.8488%\n",
      "Epoch [2/2], Step [972/3732], Loss: 3.0169, accuracy: 11.8516%\n",
      "Epoch [2/2], Step [973/3732], Loss: 2.7510, accuracy: 11.8571%\n",
      "Epoch [2/2], Step [974/3732], Loss: 3.6592, accuracy: 11.8625%\n",
      "Epoch [2/2], Step [975/3732], Loss: 2.8232, accuracy: 11.8600%\n",
      "Epoch [2/2], Step [976/3732], Loss: 3.4734, accuracy: 11.8601%\n",
      "Epoch [2/2], Step [977/3732], Loss: 2.4512, accuracy: 11.8603%\n",
      "Epoch [2/2], Step [978/3732], Loss: 3.5707, accuracy: 11.8577%\n",
      "Epoch [2/2], Step [979/3732], Loss: 3.2384, accuracy: 11.8579%\n",
      "Epoch [2/2], Step [980/3732], Loss: 3.1834, accuracy: 11.8580%\n",
      "Epoch [2/2], Step [981/3732], Loss: 3.5655, accuracy: 11.8582%\n",
      "Epoch [2/2], Step [982/3732], Loss: 3.0360, accuracy: 11.8583%\n",
      "Epoch [2/2], Step [983/3732], Loss: 2.5518, accuracy: 11.8637%\n",
      "Epoch [2/2], Step [984/3732], Loss: 3.0338, accuracy: 11.8665%\n",
      "Epoch [2/2], Step [985/3732], Loss: 3.3090, accuracy: 11.8667%\n",
      "Epoch [2/2], Step [986/3732], Loss: 3.2889, accuracy: 11.8694%\n",
      "Epoch [2/2], Step [987/3732], Loss: 3.0171, accuracy: 11.8722%\n",
      "Epoch [2/2], Step [988/3732], Loss: 2.8273, accuracy: 11.8750%\n",
      "Epoch [2/2], Step [989/3732], Loss: 3.1282, accuracy: 11.8751%\n",
      "Epoch [2/2], Step [990/3732], Loss: 2.8474, accuracy: 11.8779%\n",
      "Epoch [2/2], Step [991/3732], Loss: 3.2692, accuracy: 11.8807%\n",
      "Epoch [2/2], Step [992/3732], Loss: 2.9238, accuracy: 11.8835%\n",
      "Epoch [2/2], Step [993/3732], Loss: 3.4095, accuracy: 11.8836%\n",
      "Epoch [2/2], Step [994/3732], Loss: 3.2462, accuracy: 11.8837%\n",
      "Epoch [2/2], Step [995/3732], Loss: 2.9625, accuracy: 11.8865%\n",
      "Epoch [2/2], Step [996/3732], Loss: 2.6317, accuracy: 11.8893%\n",
      "Epoch [2/2], Step [997/3732], Loss: 3.0646, accuracy: 11.8894%\n",
      "Epoch [2/2], Step [998/3732], Loss: 3.1503, accuracy: 11.8948%\n",
      "Epoch [2/2], Step [999/3732], Loss: 3.2309, accuracy: 11.8949%\n",
      "Epoch [2/2], Step [1000/3732], Loss: 3.1319, accuracy: 11.8977%\n",
      "Epoch [2/2], Step [1001/3732], Loss: 3.3393, accuracy: 11.8978%\n",
      "Epoch [2/2], Step [1002/3732], Loss: 2.9868, accuracy: 11.9006%\n",
      "Epoch [2/2], Step [1003/3732], Loss: 3.2304, accuracy: 11.9007%\n",
      "Epoch [2/2], Step [1004/3732], Loss: 3.3219, accuracy: 11.9009%\n",
      "Epoch [2/2], Step [1005/3732], Loss: 3.9821, accuracy: 11.9010%\n",
      "Epoch [2/2], Step [1006/3732], Loss: 3.0854, accuracy: 11.9011%\n",
      "Epoch [2/2], Step [1007/3732], Loss: 3.2042, accuracy: 11.9012%\n",
      "Epoch [2/2], Step [1008/3732], Loss: 2.9074, accuracy: 11.8987%\n",
      "Epoch [2/2], Step [1009/3732], Loss: 3.4830, accuracy: 11.8962%\n",
      "Epoch [2/2], Step [1010/3732], Loss: 3.3203, accuracy: 11.8964%\n",
      "Epoch [2/2], Step [1011/3732], Loss: 3.2979, accuracy: 11.8938%\n",
      "Epoch [2/2], Step [1012/3732], Loss: 2.9850, accuracy: 11.8966%\n",
      "Epoch [2/2], Step [1013/3732], Loss: 2.8073, accuracy: 11.8967%\n",
      "Epoch [2/2], Step [1014/3732], Loss: 3.3487, accuracy: 11.8942%\n",
      "Epoch [2/2], Step [1015/3732], Loss: 3.4061, accuracy: 11.8944%\n",
      "Epoch [2/2], Step [1016/3732], Loss: 3.1127, accuracy: 11.8945%\n",
      "Epoch [2/2], Step [1017/3732], Loss: 3.3310, accuracy: 11.8920%\n",
      "Epoch [2/2], Step [1018/3732], Loss: 3.7093, accuracy: 11.8921%\n",
      "Epoch [2/2], Step [1019/3732], Loss: 3.0971, accuracy: 11.8922%\n",
      "Epoch [2/2], Step [1020/3732], Loss: 3.0369, accuracy: 11.8924%\n",
      "Epoch [2/2], Step [1021/3732], Loss: 2.6838, accuracy: 11.8951%\n",
      "Epoch [2/2], Step [1022/3732], Loss: 3.8635, accuracy: 11.8952%\n",
      "Epoch [2/2], Step [1023/3732], Loss: 3.0729, accuracy: 11.8954%\n",
      "Epoch [2/2], Step [1024/3732], Loss: 2.7729, accuracy: 11.9008%\n",
      "Epoch [2/2], Step [1025/3732], Loss: 2.5800, accuracy: 11.9035%\n",
      "Epoch [2/2], Step [1026/3732], Loss: 3.8417, accuracy: 11.9010%\n",
      "Epoch [2/2], Step [1027/3732], Loss: 3.2705, accuracy: 11.9038%\n",
      "Epoch [2/2], Step [1028/3732], Loss: 3.8670, accuracy: 11.9013%\n",
      "Epoch [2/2], Step [1029/3732], Loss: 2.8807, accuracy: 11.9040%\n",
      "Epoch [2/2], Step [1030/3732], Loss: 3.1827, accuracy: 11.9041%\n",
      "Epoch [2/2], Step [1031/3732], Loss: 3.5530, accuracy: 11.9069%\n",
      "Epoch [2/2], Step [1032/3732], Loss: 3.1189, accuracy: 11.9070%\n",
      "Epoch [2/2], Step [1033/3732], Loss: 3.5834, accuracy: 11.9045%\n",
      "Epoch [2/2], Step [1034/3732], Loss: 2.6454, accuracy: 11.9099%\n",
      "Epoch [2/2], Step [1035/3732], Loss: 3.1587, accuracy: 11.9153%\n",
      "Epoch [2/2], Step [1036/3732], Loss: 3.4485, accuracy: 11.9154%\n",
      "Epoch [2/2], Step [1037/3732], Loss: 4.0196, accuracy: 11.9129%\n",
      "Epoch [2/2], Step [1038/3732], Loss: 2.8944, accuracy: 11.9156%\n",
      "Epoch [2/2], Step [1039/3732], Loss: 3.4301, accuracy: 11.9157%\n",
      "Epoch [2/2], Step [1040/3732], Loss: 3.0441, accuracy: 11.9159%\n",
      "Epoch [2/2], Step [1041/3732], Loss: 3.5963, accuracy: 11.9160%\n",
      "Epoch [2/2], Step [1042/3732], Loss: 2.9176, accuracy: 11.9187%\n",
      "Epoch [2/2], Step [1043/3732], Loss: 3.0761, accuracy: 11.9215%\n",
      "Epoch [2/2], Step [1044/3732], Loss: 3.0513, accuracy: 11.9216%\n",
      "Epoch [2/2], Step [1045/3732], Loss: 2.8903, accuracy: 11.9191%\n",
      "Epoch [2/2], Step [1046/3732], Loss: 2.1800, accuracy: 11.9192%\n",
      "Epoch [2/2], Step [1047/3732], Loss: 3.2910, accuracy: 11.9220%\n",
      "Epoch [2/2], Step [1048/3732], Loss: 3.3246, accuracy: 11.9221%\n",
      "Epoch [2/2], Step [1049/3732], Loss: 4.1849, accuracy: 11.9196%\n",
      "Epoch [2/2], Step [1050/3732], Loss: 3.4329, accuracy: 11.9171%\n",
      "Epoch [2/2], Step [1051/3732], Loss: 3.3556, accuracy: 11.9172%\n",
      "Epoch [2/2], Step [1052/3732], Loss: 3.4273, accuracy: 11.9147%\n",
      "Epoch [2/2], Step [1053/3732], Loss: 3.4158, accuracy: 11.9122%\n",
      "Epoch [2/2], Step [1054/3732], Loss: 2.7669, accuracy: 11.9176%\n",
      "Epoch [2/2], Step [1055/3732], Loss: 2.7308, accuracy: 11.9177%\n",
      "Epoch [2/2], Step [1056/3732], Loss: 2.6359, accuracy: 11.9204%\n",
      "Epoch [2/2], Step [1057/3732], Loss: 3.5470, accuracy: 11.9205%\n",
      "Epoch [2/2], Step [1058/3732], Loss: 3.4762, accuracy: 11.9207%\n",
      "Epoch [2/2], Step [1059/3732], Loss: 3.7161, accuracy: 11.9208%\n",
      "Epoch [2/2], Step [1060/3732], Loss: 1.9352, accuracy: 11.9261%\n",
      "Epoch [2/2], Step [1061/3732], Loss: 3.5577, accuracy: 11.9262%\n",
      "Epoch [2/2], Step [1062/3732], Loss: 3.0723, accuracy: 11.9290%\n",
      "Epoch [2/2], Step [1063/3732], Loss: 2.6732, accuracy: 11.9343%\n",
      "Epoch [2/2], Step [1064/3732], Loss: 3.0861, accuracy: 11.9344%\n",
      "Epoch [2/2], Step [1065/3732], Loss: 3.4461, accuracy: 11.9319%\n",
      "Epoch [2/2], Step [1066/3732], Loss: 2.8711, accuracy: 11.9347%\n",
      "Epoch [2/2], Step [1067/3732], Loss: 3.6939, accuracy: 11.9348%\n",
      "Epoch [2/2], Step [1068/3732], Loss: 3.8060, accuracy: 11.9323%\n",
      "Epoch [2/2], Step [1069/3732], Loss: 3.2966, accuracy: 11.9350%\n",
      "Epoch [2/2], Step [1070/3732], Loss: 3.4872, accuracy: 11.9351%\n",
      "Epoch [2/2], Step [1071/3732], Loss: 3.7527, accuracy: 11.9326%\n",
      "Epoch [2/2], Step [1072/3732], Loss: 3.1261, accuracy: 11.9354%\n",
      "Epoch [2/2], Step [1073/3732], Loss: 3.3037, accuracy: 11.9381%\n",
      "Epoch [2/2], Step [1074/3732], Loss: 3.1275, accuracy: 11.9382%\n",
      "Epoch [2/2], Step [1075/3732], Loss: 3.7556, accuracy: 11.9383%\n",
      "Epoch [2/2], Step [1076/3732], Loss: 3.3168, accuracy: 11.9410%\n",
      "Epoch [2/2], Step [1077/3732], Loss: 2.9996, accuracy: 11.9438%\n",
      "Epoch [2/2], Step [1078/3732], Loss: 3.8323, accuracy: 11.9439%\n",
      "Epoch [2/2], Step [1079/3732], Loss: 2.6153, accuracy: 11.9492%\n",
      "Epoch [2/2], Step [1080/3732], Loss: 3.3202, accuracy: 11.9493%\n",
      "Epoch [2/2], Step [1081/3732], Loss: 2.8665, accuracy: 11.9520%\n",
      "Epoch [2/2], Step [1082/3732], Loss: 2.9774, accuracy: 11.9521%\n",
      "Epoch [2/2], Step [1083/3732], Loss: 3.2353, accuracy: 11.9522%\n",
      "Epoch [2/2], Step [1084/3732], Loss: 3.7735, accuracy: 11.9498%\n",
      "Epoch [2/2], Step [1085/3732], Loss: 2.5173, accuracy: 11.9525%\n",
      "Epoch [2/2], Step [1086/3732], Loss: 2.6256, accuracy: 11.9552%\n",
      "Epoch [2/2], Step [1087/3732], Loss: 3.3954, accuracy: 11.9527%\n",
      "Epoch [2/2], Step [1088/3732], Loss: 2.5788, accuracy: 11.9606%\n",
      "Epoch [2/2], Step [1089/3732], Loss: 3.6398, accuracy: 11.9581%\n",
      "Epoch [2/2], Step [1090/3732], Loss: 3.3497, accuracy: 11.9556%\n",
      "Epoch [2/2], Step [1091/3732], Loss: 3.3987, accuracy: 11.9531%\n",
      "Epoch [2/2], Step [1092/3732], Loss: 2.8414, accuracy: 11.9584%\n",
      "Epoch [2/2], Step [1093/3732], Loss: 3.7521, accuracy: 11.9560%\n",
      "Epoch [2/2], Step [1094/3732], Loss: 2.8716, accuracy: 11.9561%\n",
      "Epoch [2/2], Step [1095/3732], Loss: 3.1996, accuracy: 11.9562%\n",
      "Epoch [2/2], Step [1096/3732], Loss: 3.0593, accuracy: 11.9563%\n",
      "Epoch [2/2], Step [1097/3732], Loss: 2.7803, accuracy: 11.9590%\n",
      "Epoch [2/2], Step [1098/3732], Loss: 3.0519, accuracy: 11.9591%\n",
      "Epoch [2/2], Step [1099/3732], Loss: 3.1324, accuracy: 11.9644%\n",
      "Epoch [2/2], Step [1100/3732], Loss: 3.1145, accuracy: 11.9645%\n",
      "Epoch [2/2], Step [1101/3732], Loss: 3.4296, accuracy: 11.9672%\n",
      "Epoch [2/2], Step [1102/3732], Loss: 2.5521, accuracy: 11.9777%\n",
      "Epoch [2/2], Step [1103/3732], Loss: 2.1725, accuracy: 11.9855%\n",
      "Epoch [2/2], Step [1104/3732], Loss: 3.2418, accuracy: 11.9856%\n",
      "Epoch [2/2], Step [1105/3732], Loss: 2.9707, accuracy: 11.9883%\n",
      "Epoch [2/2], Step [1106/3732], Loss: 3.1427, accuracy: 11.9910%\n",
      "Epoch [2/2], Step [1107/3732], Loss: 3.0126, accuracy: 11.9885%\n",
      "Epoch [2/2], Step [1108/3732], Loss: 3.4209, accuracy: 11.9861%\n",
      "Epoch [2/2], Step [1109/3732], Loss: 2.6307, accuracy: 11.9862%\n",
      "Epoch [2/2], Step [1110/3732], Loss: 3.0004, accuracy: 11.9888%\n",
      "Epoch [2/2], Step [1111/3732], Loss: 3.4715, accuracy: 11.9864%\n",
      "Epoch [2/2], Step [1112/3732], Loss: 2.8344, accuracy: 11.9916%\n",
      "Epoch [2/2], Step [1113/3732], Loss: 3.6087, accuracy: 11.9943%\n",
      "Epoch [2/2], Step [1114/3732], Loss: 2.5709, accuracy: 11.9970%\n",
      "Epoch [2/2], Step [1115/3732], Loss: 2.3884, accuracy: 11.9971%\n",
      "Epoch [2/2], Step [1116/3732], Loss: 2.5553, accuracy: 11.9998%\n",
      "Epoch [2/2], Step [1117/3732], Loss: 3.1924, accuracy: 11.9999%\n",
      "Epoch [2/2], Step [1118/3732], Loss: 3.2128, accuracy: 12.0026%\n",
      "Epoch [2/2], Step [1119/3732], Loss: 2.9074, accuracy: 12.0053%\n",
      "Epoch [2/2], Step [1120/3732], Loss: 3.2871, accuracy: 12.0054%\n",
      "Epoch [2/2], Step [1121/3732], Loss: 3.0757, accuracy: 12.0055%\n",
      "Epoch [2/2], Step [1122/3732], Loss: 2.4034, accuracy: 12.0056%\n",
      "Epoch [2/2], Step [1123/3732], Loss: 1.9254, accuracy: 12.0134%\n",
      "Epoch [2/2], Step [1124/3732], Loss: 3.2596, accuracy: 12.0161%\n",
      "Epoch [2/2], Step [1125/3732], Loss: 3.0017, accuracy: 12.0162%\n",
      "Epoch [2/2], Step [1126/3732], Loss: 2.2069, accuracy: 12.0266%\n",
      "Epoch [2/2], Step [1127/3732], Loss: 2.7457, accuracy: 12.0267%\n",
      "Epoch [2/2], Step [1128/3732], Loss: 3.0112, accuracy: 12.0293%\n",
      "Epoch [2/2], Step [1129/3732], Loss: 3.3928, accuracy: 12.0294%\n",
      "Epoch [2/2], Step [1130/3732], Loss: 3.5644, accuracy: 12.0321%\n",
      "Epoch [2/2], Step [1131/3732], Loss: 3.0881, accuracy: 12.0322%\n",
      "Epoch [2/2], Step [1132/3732], Loss: 2.4185, accuracy: 12.0374%\n",
      "Epoch [2/2], Step [1133/3732], Loss: 2.9641, accuracy: 12.0375%\n",
      "Epoch [2/2], Step [1134/3732], Loss: 3.4115, accuracy: 12.0376%\n",
      "Epoch [2/2], Step [1135/3732], Loss: 3.1139, accuracy: 12.0377%\n",
      "Epoch [2/2], Step [1136/3732], Loss: 3.0205, accuracy: 12.0404%\n",
      "Epoch [2/2], Step [1137/3732], Loss: 2.7929, accuracy: 12.0405%\n",
      "Epoch [2/2], Step [1138/3732], Loss: 1.9118, accuracy: 12.0534%\n",
      "Epoch [2/2], Step [1139/3732], Loss: 4.2087, accuracy: 12.0509%\n",
      "Epoch [2/2], Step [1140/3732], Loss: 3.5147, accuracy: 12.0484%\n",
      "Epoch [2/2], Step [1141/3732], Loss: 3.3316, accuracy: 12.0511%\n",
      "Epoch [2/2], Step [1142/3732], Loss: 3.7329, accuracy: 12.0486%\n",
      "Epoch [2/2], Step [1143/3732], Loss: 3.1148, accuracy: 12.0487%\n",
      "Epoch [2/2], Step [1144/3732], Loss: 3.5269, accuracy: 12.0488%\n",
      "Epoch [2/2], Step [1145/3732], Loss: 3.3502, accuracy: 12.0515%\n",
      "Epoch [2/2], Step [1146/3732], Loss: 3.0042, accuracy: 12.0541%\n",
      "Epoch [2/2], Step [1147/3732], Loss: 3.4409, accuracy: 12.0516%\n",
      "Epoch [2/2], Step [1148/3732], Loss: 3.4671, accuracy: 12.0517%\n",
      "Epoch [2/2], Step [1149/3732], Loss: 3.3034, accuracy: 12.0493%\n",
      "Epoch [2/2], Step [1150/3732], Loss: 2.9649, accuracy: 12.0519%\n",
      "Epoch [2/2], Step [1151/3732], Loss: 3.1765, accuracy: 12.0495%\n",
      "Epoch [2/2], Step [1152/3732], Loss: 2.5298, accuracy: 12.0547%\n",
      "Epoch [2/2], Step [1153/3732], Loss: 3.4528, accuracy: 12.0522%\n",
      "Epoch [2/2], Step [1154/3732], Loss: 2.5445, accuracy: 12.0523%\n",
      "Epoch [2/2], Step [1155/3732], Loss: 3.6900, accuracy: 12.0498%\n",
      "Epoch [2/2], Step [1156/3732], Loss: 2.6783, accuracy: 12.0525%\n",
      "Epoch [2/2], Step [1157/3732], Loss: 3.4510, accuracy: 12.0526%\n",
      "Epoch [2/2], Step [1158/3732], Loss: 2.9682, accuracy: 12.0552%\n",
      "Epoch [2/2], Step [1159/3732], Loss: 2.8497, accuracy: 12.0604%\n",
      "Epoch [2/2], Step [1160/3732], Loss: 3.3404, accuracy: 12.0580%\n",
      "Epoch [2/2], Step [1161/3732], Loss: 2.5928, accuracy: 12.0580%\n",
      "Epoch [2/2], Step [1162/3732], Loss: 2.7693, accuracy: 12.0632%\n",
      "Epoch [2/2], Step [1163/3732], Loss: 3.3031, accuracy: 12.0608%\n",
      "Epoch [2/2], Step [1164/3732], Loss: 3.6450, accuracy: 12.0583%\n",
      "Epoch [2/2], Step [1165/3732], Loss: 2.5493, accuracy: 12.0610%\n",
      "Epoch [2/2], Step [1166/3732], Loss: 3.3873, accuracy: 12.0636%\n",
      "Epoch [2/2], Step [1167/3732], Loss: 2.7980, accuracy: 12.0662%\n",
      "Epoch [2/2], Step [1168/3732], Loss: 3.5691, accuracy: 12.0663%\n",
      "Epoch [2/2], Step [1169/3732], Loss: 2.8906, accuracy: 12.0690%\n",
      "Epoch [2/2], Step [1170/3732], Loss: 2.9932, accuracy: 12.0691%\n",
      "Epoch [2/2], Step [1171/3732], Loss: 2.9654, accuracy: 12.0691%\n",
      "Epoch [2/2], Step [1172/3732], Loss: 3.1455, accuracy: 12.0692%\n",
      "Epoch [2/2], Step [1173/3732], Loss: 2.9926, accuracy: 12.0693%\n",
      "Epoch [2/2], Step [1174/3732], Loss: 2.3927, accuracy: 12.0694%\n",
      "Epoch [2/2], Step [1175/3732], Loss: 3.7865, accuracy: 12.0695%\n",
      "Epoch [2/2], Step [1176/3732], Loss: 3.0048, accuracy: 12.0721%\n",
      "Epoch [2/2], Step [1177/3732], Loss: 2.7419, accuracy: 12.0748%\n",
      "Epoch [2/2], Step [1178/3732], Loss: 3.3080, accuracy: 12.0774%\n",
      "Epoch [2/2], Step [1179/3732], Loss: 2.7663, accuracy: 12.0800%\n",
      "Epoch [2/2], Step [1180/3732], Loss: 2.8128, accuracy: 12.0801%\n",
      "Epoch [2/2], Step [1181/3732], Loss: 3.2858, accuracy: 12.0802%\n",
      "Epoch [2/2], Step [1182/3732], Loss: 3.4882, accuracy: 12.0803%\n",
      "Epoch [2/2], Step [1183/3732], Loss: 2.4319, accuracy: 12.0855%\n",
      "Epoch [2/2], Step [1184/3732], Loss: 1.8155, accuracy: 12.0957%\n",
      "Epoch [2/2], Step [1185/3732], Loss: 3.3473, accuracy: 12.0958%\n",
      "Epoch [2/2], Step [1186/3732], Loss: 2.9937, accuracy: 12.0984%\n",
      "Epoch [2/2], Step [1187/3732], Loss: 2.9179, accuracy: 12.0985%\n",
      "Epoch [2/2], Step [1188/3732], Loss: 3.2350, accuracy: 12.0986%\n",
      "Epoch [2/2], Step [1189/3732], Loss: 3.4191, accuracy: 12.1012%\n",
      "Epoch [2/2], Step [1190/3732], Loss: 2.9600, accuracy: 12.1013%\n",
      "Epoch [2/2], Step [1191/3732], Loss: 3.0687, accuracy: 12.1039%\n",
      "Epoch [2/2], Step [1192/3732], Loss: 2.7464, accuracy: 12.1091%\n",
      "Epoch [2/2], Step [1193/3732], Loss: 2.7228, accuracy: 12.1066%\n",
      "Epoch [2/2], Step [1194/3732], Loss: 3.2629, accuracy: 12.1067%\n",
      "Epoch [2/2], Step [1195/3732], Loss: 2.8631, accuracy: 12.1068%\n",
      "Epoch [2/2], Step [1196/3732], Loss: 2.5938, accuracy: 12.1094%\n",
      "Epoch [2/2], Step [1197/3732], Loss: 2.7764, accuracy: 12.1120%\n",
      "Epoch [2/2], Step [1198/3732], Loss: 2.8734, accuracy: 12.1121%\n",
      "Epoch [2/2], Step [1199/3732], Loss: 3.6179, accuracy: 12.1096%\n",
      "Epoch [2/2], Step [1200/3732], Loss: 3.6229, accuracy: 12.1122%\n",
      "Epoch [2/2], Step [1201/3732], Loss: 2.8328, accuracy: 12.1123%\n",
      "Epoch [2/2], Step [1202/3732], Loss: 3.3092, accuracy: 12.1124%\n",
      "Epoch [2/2], Step [1203/3732], Loss: 3.1011, accuracy: 12.1125%\n",
      "Epoch [2/2], Step [1204/3732], Loss: 3.2340, accuracy: 12.1125%\n",
      "Epoch [2/2], Step [1205/3732], Loss: 2.5015, accuracy: 12.1152%\n",
      "Epoch [2/2], Step [1206/3732], Loss: 3.6281, accuracy: 12.1152%\n",
      "Epoch [2/2], Step [1207/3732], Loss: 3.5038, accuracy: 12.1153%\n",
      "Epoch [2/2], Step [1208/3732], Loss: 3.4244, accuracy: 12.1179%\n",
      "Epoch [2/2], Step [1209/3732], Loss: 2.8971, accuracy: 12.1180%\n",
      "Epoch [2/2], Step [1210/3732], Loss: 2.8249, accuracy: 12.1231%\n",
      "Epoch [2/2], Step [1211/3732], Loss: 3.4626, accuracy: 12.1207%\n",
      "Epoch [2/2], Step [1212/3732], Loss: 2.5846, accuracy: 12.1309%\n",
      "Epoch [2/2], Step [1213/3732], Loss: 4.0714, accuracy: 12.1284%\n",
      "Epoch [2/2], Step [1214/3732], Loss: 3.3411, accuracy: 12.1260%\n",
      "Epoch [2/2], Step [1215/3732], Loss: 3.1568, accuracy: 12.1311%\n",
      "Epoch [2/2], Step [1216/3732], Loss: 3.1535, accuracy: 12.1312%\n",
      "Epoch [2/2], Step [1217/3732], Loss: 2.9838, accuracy: 12.1388%\n",
      "Epoch [2/2], Step [1218/3732], Loss: 3.1996, accuracy: 12.1364%\n",
      "Epoch [2/2], Step [1219/3732], Loss: 3.1285, accuracy: 12.1364%\n",
      "Epoch [2/2], Step [1220/3732], Loss: 3.1864, accuracy: 12.1365%\n",
      "Epoch [2/2], Step [1221/3732], Loss: 2.4794, accuracy: 12.1391%\n",
      "Epoch [2/2], Step [1222/3732], Loss: 3.2363, accuracy: 12.1392%\n",
      "Epoch [2/2], Step [1223/3732], Loss: 2.9369, accuracy: 12.1468%\n",
      "Epoch [2/2], Step [1224/3732], Loss: 3.4630, accuracy: 12.1519%\n",
      "Epoch [2/2], Step [1225/3732], Loss: 3.0571, accuracy: 12.1520%\n",
      "Epoch [2/2], Step [1226/3732], Loss: 3.4267, accuracy: 12.1496%\n",
      "Epoch [2/2], Step [1227/3732], Loss: 2.9476, accuracy: 12.1521%\n",
      "Epoch [2/2], Step [1228/3732], Loss: 3.9963, accuracy: 12.1497%\n",
      "Epoch [2/2], Step [1229/3732], Loss: 3.0505, accuracy: 12.1498%\n",
      "Epoch [2/2], Step [1230/3732], Loss: 2.5757, accuracy: 12.1524%\n",
      "Epoch [2/2], Step [1231/3732], Loss: 3.2707, accuracy: 12.1524%\n",
      "Epoch [2/2], Step [1232/3732], Loss: 3.3076, accuracy: 12.1525%\n",
      "Epoch [2/2], Step [1233/3732], Loss: 3.2063, accuracy: 12.1501%\n",
      "Epoch [2/2], Step [1234/3732], Loss: 2.7452, accuracy: 12.1552%\n",
      "Epoch [2/2], Step [1235/3732], Loss: 2.4050, accuracy: 12.1577%\n",
      "Epoch [2/2], Step [1236/3732], Loss: 2.9918, accuracy: 12.1628%\n",
      "Epoch [2/2], Step [1237/3732], Loss: 2.8535, accuracy: 12.1654%\n",
      "Epoch [2/2], Step [1238/3732], Loss: 1.5582, accuracy: 12.1730%\n",
      "Epoch [2/2], Step [1239/3732], Loss: 3.2001, accuracy: 12.1706%\n",
      "Epoch [2/2], Step [1240/3732], Loss: 3.6309, accuracy: 12.1681%\n",
      "Epoch [2/2], Step [1241/3732], Loss: 3.4184, accuracy: 12.1707%\n",
      "Epoch [2/2], Step [1242/3732], Loss: 3.0215, accuracy: 12.1708%\n",
      "Epoch [2/2], Step [1243/3732], Loss: 3.2564, accuracy: 12.1709%\n",
      "Epoch [2/2], Step [1244/3732], Loss: 3.4627, accuracy: 12.1709%\n",
      "Epoch [2/2], Step [1245/3732], Loss: 3.2431, accuracy: 12.1685%\n",
      "Epoch [2/2], Step [1246/3732], Loss: 3.1639, accuracy: 12.1711%\n",
      "Epoch [2/2], Step [1247/3732], Loss: 3.4855, accuracy: 12.1711%\n",
      "Epoch [2/2], Step [1248/3732], Loss: 3.3796, accuracy: 12.1712%\n",
      "Epoch [2/2], Step [1249/3732], Loss: 3.2631, accuracy: 12.1738%\n",
      "Epoch [2/2], Step [1250/3732], Loss: 2.9602, accuracy: 12.1788%\n",
      "Epoch [2/2], Step [1251/3732], Loss: 2.8881, accuracy: 12.1839%\n",
      "Epoch [2/2], Step [1252/3732], Loss: 3.4361, accuracy: 12.1815%\n",
      "Epoch [2/2], Step [1253/3732], Loss: 3.5538, accuracy: 12.1841%\n",
      "Epoch [2/2], Step [1254/3732], Loss: 2.8111, accuracy: 12.1866%\n",
      "Epoch [2/2], Step [1255/3732], Loss: 2.5724, accuracy: 12.1917%\n",
      "Epoch [2/2], Step [1256/3732], Loss: 2.7512, accuracy: 12.1968%\n",
      "Epoch [2/2], Step [1257/3732], Loss: 3.6105, accuracy: 12.1993%\n",
      "Epoch [2/2], Step [1258/3732], Loss: 2.0458, accuracy: 12.2044%\n",
      "Epoch [2/2], Step [1259/3732], Loss: 3.2240, accuracy: 12.2070%\n",
      "Epoch [2/2], Step [1260/3732], Loss: 3.6356, accuracy: 12.2070%\n",
      "Epoch [2/2], Step [1261/3732], Loss: 3.6254, accuracy: 12.2071%\n",
      "Epoch [2/2], Step [1262/3732], Loss: 3.7570, accuracy: 12.2046%\n",
      "Epoch [2/2], Step [1263/3732], Loss: 3.2198, accuracy: 12.2047%\n",
      "Epoch [2/2], Step [1264/3732], Loss: 3.4851, accuracy: 12.2023%\n",
      "Epoch [2/2], Step [1265/3732], Loss: 3.0228, accuracy: 12.2048%\n",
      "Epoch [2/2], Step [1266/3732], Loss: 2.7284, accuracy: 12.2049%\n",
      "Epoch [2/2], Step [1267/3732], Loss: 3.1632, accuracy: 12.2024%\n",
      "Epoch [2/2], Step [1268/3732], Loss: 3.5742, accuracy: 12.2000%\n",
      "Epoch [2/2], Step [1269/3732], Loss: 1.7330, accuracy: 12.2076%\n",
      "Epoch [2/2], Step [1270/3732], Loss: 3.1300, accuracy: 12.2126%\n",
      "Epoch [2/2], Step [1271/3732], Loss: 3.5066, accuracy: 12.2152%\n",
      "Epoch [2/2], Step [1272/3732], Loss: 3.6034, accuracy: 12.2152%\n",
      "Epoch [2/2], Step [1273/3732], Loss: 2.8180, accuracy: 12.2178%\n",
      "Epoch [2/2], Step [1274/3732], Loss: 3.5651, accuracy: 12.2178%\n",
      "Epoch [2/2], Step [1275/3732], Loss: 2.9196, accuracy: 12.2229%\n",
      "Epoch [2/2], Step [1276/3732], Loss: 3.1521, accuracy: 12.2229%\n",
      "Epoch [2/2], Step [1277/3732], Loss: 2.7010, accuracy: 12.2280%\n",
      "Epoch [2/2], Step [1278/3732], Loss: 2.6570, accuracy: 12.2355%\n",
      "Epoch [2/2], Step [1279/3732], Loss: 3.9000, accuracy: 12.2331%\n",
      "Epoch [2/2], Step [1280/3732], Loss: 3.1597, accuracy: 12.2331%\n",
      "Epoch [2/2], Step [1281/3732], Loss: 3.4382, accuracy: 12.2307%\n",
      "Epoch [2/2], Step [1282/3732], Loss: 3.0691, accuracy: 12.2332%\n",
      "Epoch [2/2], Step [1283/3732], Loss: 3.0120, accuracy: 12.2308%\n",
      "Epoch [2/2], Step [1284/3732], Loss: 2.9663, accuracy: 12.2309%\n",
      "Epoch [2/2], Step [1285/3732], Loss: 2.1757, accuracy: 12.2359%\n",
      "Epoch [2/2], Step [1286/3732], Loss: 3.7379, accuracy: 12.2360%\n",
      "Epoch [2/2], Step [1287/3732], Loss: 3.4498, accuracy: 12.2360%\n",
      "Epoch [2/2], Step [1288/3732], Loss: 3.5016, accuracy: 12.2361%\n",
      "Epoch [2/2], Step [1289/3732], Loss: 3.2236, accuracy: 12.2336%\n",
      "Epoch [2/2], Step [1290/3732], Loss: 3.1763, accuracy: 12.2337%\n",
      "Epoch [2/2], Step [1291/3732], Loss: 2.4523, accuracy: 12.2387%\n",
      "Epoch [2/2], Step [1292/3732], Loss: 2.8597, accuracy: 12.2412%\n",
      "Epoch [2/2], Step [1293/3732], Loss: 3.4963, accuracy: 12.2388%\n",
      "Epoch [2/2], Step [1294/3732], Loss: 3.1589, accuracy: 12.2413%\n",
      "Epoch [2/2], Step [1295/3732], Loss: 2.8298, accuracy: 12.2439%\n",
      "Epoch [2/2], Step [1296/3732], Loss: 3.5494, accuracy: 12.2439%\n",
      "Epoch [2/2], Step [1297/3732], Loss: 3.1803, accuracy: 12.2465%\n",
      "Epoch [2/2], Step [1298/3732], Loss: 3.4390, accuracy: 12.2440%\n",
      "Epoch [2/2], Step [1299/3732], Loss: 3.3216, accuracy: 12.2466%\n",
      "Epoch [2/2], Step [1300/3732], Loss: 3.1802, accuracy: 12.2491%\n",
      "Epoch [2/2], Step [1301/3732], Loss: 3.0539, accuracy: 12.2492%\n",
      "Epoch [2/2], Step [1302/3732], Loss: 3.2278, accuracy: 12.2517%\n",
      "Epoch [2/2], Step [1303/3732], Loss: 3.4100, accuracy: 12.2517%\n",
      "Epoch [2/2], Step [1304/3732], Loss: 3.0548, accuracy: 12.2518%\n",
      "Epoch [2/2], Step [1305/3732], Loss: 3.2402, accuracy: 12.2494%\n",
      "Epoch [2/2], Step [1306/3732], Loss: 3.3215, accuracy: 12.2519%\n",
      "Epoch [2/2], Step [1307/3732], Loss: 3.1502, accuracy: 12.2519%\n",
      "Epoch [2/2], Step [1308/3732], Loss: 2.5297, accuracy: 12.2569%\n",
      "Epoch [2/2], Step [1309/3732], Loss: 2.9647, accuracy: 12.2620%\n",
      "Epoch [2/2], Step [1310/3732], Loss: 3.3350, accuracy: 12.2620%\n",
      "Epoch [2/2], Step [1311/3732], Loss: 3.7430, accuracy: 12.2596%\n",
      "Epoch [2/2], Step [1312/3732], Loss: 3.2013, accuracy: 12.2621%\n",
      "Epoch [2/2], Step [1313/3732], Loss: 2.6655, accuracy: 12.2621%\n",
      "Epoch [2/2], Step [1314/3732], Loss: 2.6216, accuracy: 12.2671%\n",
      "Epoch [2/2], Step [1315/3732], Loss: 3.2651, accuracy: 12.2647%\n",
      "Epoch [2/2], Step [1316/3732], Loss: 2.6652, accuracy: 12.2672%\n",
      "Epoch [2/2], Step [1317/3732], Loss: 2.7403, accuracy: 12.2698%\n",
      "Epoch [2/2], Step [1318/3732], Loss: 2.9882, accuracy: 12.2723%\n",
      "Epoch [2/2], Step [1319/3732], Loss: 2.5922, accuracy: 12.2748%\n",
      "Epoch [2/2], Step [1320/3732], Loss: 3.4637, accuracy: 12.2748%\n",
      "Epoch [2/2], Step [1321/3732], Loss: 4.1941, accuracy: 12.2724%\n",
      "Epoch [2/2], Step [1322/3732], Loss: 1.9971, accuracy: 12.2774%\n",
      "Epoch [2/2], Step [1323/3732], Loss: 3.1189, accuracy: 12.2774%\n",
      "Epoch [2/2], Step [1324/3732], Loss: 2.1812, accuracy: 12.2824%\n",
      "Epoch [2/2], Step [1325/3732], Loss: 2.8476, accuracy: 12.2850%\n",
      "Epoch [2/2], Step [1326/3732], Loss: 2.6298, accuracy: 12.2924%\n",
      "Epoch [2/2], Step [1327/3732], Loss: 2.7713, accuracy: 12.2924%\n",
      "Epoch [2/2], Step [1328/3732], Loss: 2.3577, accuracy: 12.2974%\n",
      "Epoch [2/2], Step [1329/3732], Loss: 3.3475, accuracy: 12.2975%\n",
      "Epoch [2/2], Step [1330/3732], Loss: 3.4783, accuracy: 12.2950%\n",
      "Epoch [2/2], Step [1331/3732], Loss: 2.9295, accuracy: 12.2976%\n",
      "Epoch [2/2], Step [1332/3732], Loss: 2.2635, accuracy: 12.3050%\n",
      "Epoch [2/2], Step [1333/3732], Loss: 3.4789, accuracy: 12.3050%\n",
      "Epoch [2/2], Step [1334/3732], Loss: 3.2293, accuracy: 12.3075%\n",
      "Epoch [2/2], Step [1335/3732], Loss: 2.5998, accuracy: 12.3125%\n",
      "Epoch [2/2], Step [1336/3732], Loss: 4.7960, accuracy: 12.3101%\n",
      "Epoch [2/2], Step [1337/3732], Loss: 3.6722, accuracy: 12.3101%\n",
      "Epoch [2/2], Step [1338/3732], Loss: 3.0692, accuracy: 12.3102%\n",
      "Epoch [2/2], Step [1339/3732], Loss: 3.5536, accuracy: 12.3102%\n",
      "Epoch [2/2], Step [1340/3732], Loss: 2.9991, accuracy: 12.3102%\n",
      "Epoch [2/2], Step [1341/3732], Loss: 3.1856, accuracy: 12.3078%\n",
      "Epoch [2/2], Step [1342/3732], Loss: 3.2611, accuracy: 12.3078%\n",
      "Epoch [2/2], Step [1343/3732], Loss: 2.8724, accuracy: 12.3103%\n",
      "Epoch [2/2], Step [1344/3732], Loss: 2.7383, accuracy: 12.3104%\n",
      "Epoch [2/2], Step [1345/3732], Loss: 3.1604, accuracy: 12.3080%\n",
      "Epoch [2/2], Step [1346/3732], Loss: 2.7780, accuracy: 12.3105%\n",
      "Epoch [2/2], Step [1347/3732], Loss: 2.6950, accuracy: 12.3154%\n",
      "Epoch [2/2], Step [1348/3732], Loss: 2.7367, accuracy: 12.3155%\n",
      "Epoch [2/2], Step [1349/3732], Loss: 3.5254, accuracy: 12.3155%\n",
      "Epoch [2/2], Step [1350/3732], Loss: 2.3628, accuracy: 12.3180%\n",
      "Epoch [2/2], Step [1351/3732], Loss: 3.3281, accuracy: 12.3156%\n",
      "Epoch [2/2], Step [1352/3732], Loss: 2.4785, accuracy: 12.3230%\n",
      "Epoch [2/2], Step [1353/3732], Loss: 3.5781, accuracy: 12.3230%\n",
      "Epoch [2/2], Step [1354/3732], Loss: 2.8336, accuracy: 12.3255%\n",
      "Epoch [2/2], Step [1355/3732], Loss: 3.9438, accuracy: 12.3255%\n",
      "Epoch [2/2], Step [1356/3732], Loss: 3.6377, accuracy: 12.3256%\n",
      "Epoch [2/2], Step [1357/3732], Loss: 2.6359, accuracy: 12.3305%\n",
      "Epoch [2/2], Step [1358/3732], Loss: 2.8708, accuracy: 12.3330%\n",
      "Epoch [2/2], Step [1359/3732], Loss: 3.3002, accuracy: 12.3379%\n",
      "Epoch [2/2], Step [1360/3732], Loss: 2.7405, accuracy: 12.3404%\n",
      "Epoch [2/2], Step [1361/3732], Loss: 2.5871, accuracy: 12.3429%\n",
      "Epoch [2/2], Step [1362/3732], Loss: 3.6816, accuracy: 12.3405%\n",
      "Epoch [2/2], Step [1363/3732], Loss: 2.4530, accuracy: 12.3454%\n",
      "Epoch [2/2], Step [1364/3732], Loss: 3.1649, accuracy: 12.3479%\n",
      "Epoch [2/2], Step [1365/3732], Loss: 3.2548, accuracy: 12.3479%\n",
      "Epoch [2/2], Step [1366/3732], Loss: 3.4101, accuracy: 12.3455%\n",
      "Epoch [2/2], Step [1367/3732], Loss: 2.4804, accuracy: 12.3480%\n",
      "Epoch [2/2], Step [1368/3732], Loss: 2.6254, accuracy: 12.3529%\n",
      "Epoch [2/2], Step [1369/3732], Loss: 2.7009, accuracy: 12.3579%\n",
      "Epoch [2/2], Step [1370/3732], Loss: 3.5798, accuracy: 12.3603%\n",
      "Epoch [2/2], Step [1371/3732], Loss: 2.7755, accuracy: 12.3604%\n",
      "Epoch [2/2], Step [1372/3732], Loss: 1.7702, accuracy: 12.3678%\n",
      "Epoch [2/2], Step [1373/3732], Loss: 3.4520, accuracy: 12.3678%\n",
      "Epoch [2/2], Step [1374/3732], Loss: 2.6482, accuracy: 12.3727%\n",
      "Epoch [2/2], Step [1375/3732], Loss: 2.6721, accuracy: 12.3752%\n",
      "Epoch [2/2], Step [1376/3732], Loss: 3.3623, accuracy: 12.3727%\n",
      "Epoch [2/2], Step [1377/3732], Loss: 3.1852, accuracy: 12.3728%\n",
      "Epoch [2/2], Step [1378/3732], Loss: 2.9276, accuracy: 12.3752%\n",
      "Epoch [2/2], Step [1379/3732], Loss: 3.2362, accuracy: 12.3728%\n",
      "Epoch [2/2], Step [1380/3732], Loss: 2.9084, accuracy: 12.3753%\n",
      "Epoch [2/2], Step [1381/3732], Loss: 3.0881, accuracy: 12.3753%\n",
      "Epoch [2/2], Step [1382/3732], Loss: 3.4803, accuracy: 12.3753%\n",
      "Epoch [2/2], Step [1383/3732], Loss: 3.2750, accuracy: 12.3778%\n",
      "Epoch [2/2], Step [1384/3732], Loss: 3.0297, accuracy: 12.3778%\n",
      "Epoch [2/2], Step [1385/3732], Loss: 2.6451, accuracy: 12.3827%\n",
      "Epoch [2/2], Step [1386/3732], Loss: 3.7567, accuracy: 12.3828%\n",
      "Epoch [2/2], Step [1387/3732], Loss: 3.3955, accuracy: 12.3852%\n",
      "Epoch [2/2], Step [1388/3732], Loss: 2.9755, accuracy: 12.3853%\n",
      "Epoch [2/2], Step [1389/3732], Loss: 3.0607, accuracy: 12.3828%\n",
      "Epoch [2/2], Step [1390/3732], Loss: 2.6553, accuracy: 12.3877%\n",
      "Epoch [2/2], Step [1391/3732], Loss: 2.7469, accuracy: 12.3878%\n",
      "Epoch [2/2], Step [1392/3732], Loss: 3.9595, accuracy: 12.3878%\n",
      "Epoch [2/2], Step [1393/3732], Loss: 3.0803, accuracy: 12.3927%\n",
      "Epoch [2/2], Step [1394/3732], Loss: 2.2970, accuracy: 12.3951%\n",
      "Epoch [2/2], Step [1395/3732], Loss: 3.3829, accuracy: 12.3952%\n",
      "Epoch [2/2], Step [1396/3732], Loss: 3.2444, accuracy: 12.3927%\n",
      "Epoch [2/2], Step [1397/3732], Loss: 2.7569, accuracy: 12.3952%\n",
      "Epoch [2/2], Step [1398/3732], Loss: 3.2283, accuracy: 12.3952%\n",
      "Epoch [2/2], Step [1399/3732], Loss: 3.9026, accuracy: 12.3952%\n",
      "Epoch [2/2], Step [1400/3732], Loss: 2.8183, accuracy: 12.4001%\n",
      "Epoch [2/2], Step [1401/3732], Loss: 2.6714, accuracy: 12.4050%\n",
      "Epoch [2/2], Step [1402/3732], Loss: 3.3974, accuracy: 12.4026%\n",
      "Epoch [2/2], Step [1403/3732], Loss: 3.4103, accuracy: 12.4002%\n",
      "Epoch [2/2], Step [1404/3732], Loss: 3.5612, accuracy: 12.4002%\n",
      "Epoch [2/2], Step [1405/3732], Loss: 2.9633, accuracy: 12.4027%\n",
      "Epoch [2/2], Step [1406/3732], Loss: 2.4239, accuracy: 12.4051%\n",
      "Epoch [2/2], Step [1407/3732], Loss: 2.9979, accuracy: 12.4051%\n",
      "Epoch [2/2], Step [1408/3732], Loss: 3.9232, accuracy: 12.4027%\n",
      "Epoch [2/2], Step [1409/3732], Loss: 3.4795, accuracy: 12.4003%\n",
      "Epoch [2/2], Step [1410/3732], Loss: 3.0079, accuracy: 12.4052%\n",
      "Epoch [2/2], Step [1411/3732], Loss: 3.0876, accuracy: 12.4076%\n",
      "Epoch [2/2], Step [1412/3732], Loss: 3.7064, accuracy: 12.4077%\n",
      "Epoch [2/2], Step [1413/3732], Loss: 2.5507, accuracy: 12.4101%\n",
      "Epoch [2/2], Step [1414/3732], Loss: 2.8011, accuracy: 12.4150%\n",
      "Epoch [2/2], Step [1415/3732], Loss: 2.6314, accuracy: 12.4223%\n",
      "Epoch [2/2], Step [1416/3732], Loss: 2.4662, accuracy: 12.4272%\n",
      "Epoch [2/2], Step [1417/3732], Loss: 2.8848, accuracy: 12.4296%\n",
      "Epoch [2/2], Step [1418/3732], Loss: 3.2266, accuracy: 12.4369%\n",
      "Epoch [2/2], Step [1419/3732], Loss: 2.8380, accuracy: 12.4393%\n",
      "Epoch [2/2], Step [1420/3732], Loss: 3.2199, accuracy: 12.4393%\n",
      "Epoch [2/2], Step [1421/3732], Loss: 2.7157, accuracy: 12.4442%\n",
      "Epoch [2/2], Step [1422/3732], Loss: 3.8354, accuracy: 12.4466%\n",
      "Epoch [2/2], Step [1423/3732], Loss: 3.5018, accuracy: 12.4442%\n",
      "Epoch [2/2], Step [1424/3732], Loss: 3.8140, accuracy: 12.4418%\n",
      "Epoch [2/2], Step [1425/3732], Loss: 2.8452, accuracy: 12.4418%\n",
      "Epoch [2/2], Step [1426/3732], Loss: 3.5562, accuracy: 12.4394%\n",
      "Epoch [2/2], Step [1427/3732], Loss: 2.8822, accuracy: 12.4370%\n",
      "Epoch [2/2], Step [1428/3732], Loss: 3.0589, accuracy: 12.4370%\n",
      "Epoch [2/2], Step [1429/3732], Loss: 2.9463, accuracy: 12.4394%\n",
      "Epoch [2/2], Step [1430/3732], Loss: 3.1149, accuracy: 12.4443%\n",
      "Epoch [2/2], Step [1431/3732], Loss: 3.0177, accuracy: 12.4419%\n",
      "Epoch [2/2], Step [1432/3732], Loss: 2.7505, accuracy: 12.4443%\n",
      "Epoch [2/2], Step [1433/3732], Loss: 3.4083, accuracy: 12.4443%\n",
      "Epoch [2/2], Step [1434/3732], Loss: 2.9685, accuracy: 12.4443%\n",
      "Epoch [2/2], Step [1435/3732], Loss: 2.7228, accuracy: 12.4492%\n",
      "Epoch [2/2], Step [1436/3732], Loss: 2.1094, accuracy: 12.4516%\n",
      "Epoch [2/2], Step [1437/3732], Loss: 2.9097, accuracy: 12.4541%\n",
      "Epoch [2/2], Step [1438/3732], Loss: 3.1675, accuracy: 12.4541%\n",
      "Epoch [2/2], Step [1439/3732], Loss: 3.4779, accuracy: 12.4517%\n",
      "Epoch [2/2], Step [1440/3732], Loss: 3.0211, accuracy: 12.4517%\n",
      "Epoch [2/2], Step [1441/3732], Loss: 2.6385, accuracy: 12.4589%\n",
      "Epoch [2/2], Step [1442/3732], Loss: 3.6761, accuracy: 12.4589%\n",
      "Epoch [2/2], Step [1443/3732], Loss: 2.7187, accuracy: 12.4589%\n",
      "Epoch [2/2], Step [1444/3732], Loss: 2.5912, accuracy: 12.4638%\n",
      "Epoch [2/2], Step [1445/3732], Loss: 2.9608, accuracy: 12.4638%\n",
      "Epoch [2/2], Step [1446/3732], Loss: 2.9714, accuracy: 12.4662%\n",
      "Epoch [2/2], Step [1447/3732], Loss: 3.4704, accuracy: 12.4638%\n",
      "Epoch [2/2], Step [1448/3732], Loss: 2.3321, accuracy: 12.4759%\n",
      "Epoch [2/2], Step [1449/3732], Loss: 3.1220, accuracy: 12.4807%\n",
      "Epoch [2/2], Step [1450/3732], Loss: 3.3704, accuracy: 12.4807%\n",
      "Epoch [2/2], Step [1451/3732], Loss: 3.2458, accuracy: 12.4807%\n",
      "Epoch [2/2], Step [1452/3732], Loss: 2.9868, accuracy: 12.4807%\n",
      "Epoch [2/2], Step [1453/3732], Loss: 2.6305, accuracy: 12.4855%\n",
      "Epoch [2/2], Step [1454/3732], Loss: 2.3833, accuracy: 12.4879%\n",
      "Epoch [2/2], Step [1455/3732], Loss: 2.8409, accuracy: 12.4928%\n",
      "Epoch [2/2], Step [1456/3732], Loss: 3.4537, accuracy: 12.4952%\n",
      "Epoch [2/2], Step [1457/3732], Loss: 3.5129, accuracy: 12.4952%\n",
      "Epoch [2/2], Step [1458/3732], Loss: 2.9602, accuracy: 12.5000%\n",
      "Epoch [2/2], Step [1459/3732], Loss: 2.6410, accuracy: 12.4976%\n",
      "Epoch [2/2], Step [1460/3732], Loss: 2.5928, accuracy: 12.5000%\n",
      "Epoch [2/2], Step [1461/3732], Loss: 2.8883, accuracy: 12.5000%\n",
      "Epoch [2/2], Step [1462/3732], Loss: 3.1333, accuracy: 12.5048%\n",
      "Epoch [2/2], Step [1463/3732], Loss: 2.7450, accuracy: 12.5048%\n",
      "Epoch [2/2], Step [1464/3732], Loss: 3.4521, accuracy: 12.5024%\n",
      "Epoch [2/2], Step [1465/3732], Loss: 3.7096, accuracy: 12.5024%\n",
      "Epoch [2/2], Step [1466/3732], Loss: 3.1592, accuracy: 12.5024%\n",
      "Epoch [2/2], Step [1467/3732], Loss: 3.5853, accuracy: 12.5072%\n",
      "Epoch [2/2], Step [1468/3732], Loss: 2.7125, accuracy: 12.5096%\n",
      "Epoch [2/2], Step [1469/3732], Loss: 3.3955, accuracy: 12.5096%\n",
      "Epoch [2/2], Step [1470/3732], Loss: 2.6979, accuracy: 12.5120%\n",
      "Epoch [2/2], Step [1471/3732], Loss: 2.8244, accuracy: 12.5168%\n",
      "Epoch [2/2], Step [1472/3732], Loss: 3.8105, accuracy: 12.5144%\n",
      "Epoch [2/2], Step [1473/3732], Loss: 2.2549, accuracy: 12.5216%\n",
      "Epoch [2/2], Step [1474/3732], Loss: 2.9892, accuracy: 12.5264%\n",
      "Epoch [2/2], Step [1475/3732], Loss: 2.6470, accuracy: 12.5264%\n",
      "Epoch [2/2], Step [1476/3732], Loss: 2.9809, accuracy: 12.5264%\n",
      "Epoch [2/2], Step [1477/3732], Loss: 3.0896, accuracy: 12.5288%\n",
      "Epoch [2/2], Step [1478/3732], Loss: 3.0979, accuracy: 12.5336%\n",
      "Epoch [2/2], Step [1479/3732], Loss: 2.4034, accuracy: 12.5384%\n",
      "Epoch [2/2], Step [1480/3732], Loss: 2.8425, accuracy: 12.5408%\n",
      "Epoch [2/2], Step [1481/3732], Loss: 3.2403, accuracy: 12.5384%\n",
      "Epoch [2/2], Step [1482/3732], Loss: 3.6663, accuracy: 12.5360%\n",
      "Epoch [2/2], Step [1483/3732], Loss: 3.9964, accuracy: 12.5336%\n",
      "Epoch [2/2], Step [1484/3732], Loss: 3.1194, accuracy: 12.5336%\n",
      "Epoch [2/2], Step [1485/3732], Loss: 2.4598, accuracy: 12.5359%\n",
      "Epoch [2/2], Step [1486/3732], Loss: 3.6039, accuracy: 12.5335%\n",
      "Epoch [2/2], Step [1487/3732], Loss: 3.2466, accuracy: 12.5311%\n",
      "Epoch [2/2], Step [1488/3732], Loss: 2.9474, accuracy: 12.5311%\n",
      "Epoch [2/2], Step [1489/3732], Loss: 3.9102, accuracy: 12.5287%\n",
      "Epoch [2/2], Step [1490/3732], Loss: 2.9552, accuracy: 12.5335%\n",
      "Epoch [2/2], Step [1491/3732], Loss: 2.8323, accuracy: 12.5359%\n",
      "Epoch [2/2], Step [1492/3732], Loss: 3.5905, accuracy: 12.5359%\n",
      "Epoch [2/2], Step [1493/3732], Loss: 3.3504, accuracy: 12.5359%\n",
      "Epoch [2/2], Step [1494/3732], Loss: 3.6888, accuracy: 12.5359%\n",
      "Epoch [2/2], Step [1495/3732], Loss: 2.9877, accuracy: 12.5383%\n",
      "Epoch [2/2], Step [1496/3732], Loss: 2.4820, accuracy: 12.5454%\n",
      "Epoch [2/2], Step [1497/3732], Loss: 2.7322, accuracy: 12.5478%\n",
      "Epoch [2/2], Step [1498/3732], Loss: 3.0709, accuracy: 12.5478%\n",
      "Epoch [2/2], Step [1499/3732], Loss: 2.5731, accuracy: 12.5478%\n",
      "Epoch [2/2], Step [1500/3732], Loss: 3.5652, accuracy: 12.5478%\n",
      "Epoch [2/2], Step [1501/3732], Loss: 3.3799, accuracy: 12.5478%\n",
      "Epoch [2/2], Step [1502/3732], Loss: 2.6788, accuracy: 12.5502%\n",
      "Epoch [2/2], Step [1503/3732], Loss: 3.2087, accuracy: 12.5478%\n",
      "Epoch [2/2], Step [1504/3732], Loss: 3.6941, accuracy: 12.5454%\n",
      "Epoch [2/2], Step [1505/3732], Loss: 3.7731, accuracy: 12.5477%\n",
      "Epoch [2/2], Step [1506/3732], Loss: 2.8615, accuracy: 12.5501%\n",
      "Epoch [2/2], Step [1507/3732], Loss: 2.3926, accuracy: 12.5596%\n",
      "Epoch [2/2], Step [1508/3732], Loss: 3.6753, accuracy: 12.5620%\n",
      "Epoch [2/2], Step [1509/3732], Loss: 3.3742, accuracy: 12.5620%\n",
      "Epoch [2/2], Step [1510/3732], Loss: 2.4206, accuracy: 12.5668%\n",
      "Epoch [2/2], Step [1511/3732], Loss: 3.6263, accuracy: 12.5668%\n",
      "Epoch [2/2], Step [1512/3732], Loss: 3.4510, accuracy: 12.5644%\n",
      "Epoch [2/2], Step [1513/3732], Loss: 2.6200, accuracy: 12.5691%\n",
      "Epoch [2/2], Step [1514/3732], Loss: 3.1164, accuracy: 12.5691%\n",
      "Epoch [2/2], Step [1515/3732], Loss: 2.8074, accuracy: 12.5691%\n",
      "Epoch [2/2], Step [1516/3732], Loss: 2.7344, accuracy: 12.5715%\n",
      "Epoch [2/2], Step [1517/3732], Loss: 3.0917, accuracy: 12.5762%\n",
      "Epoch [2/2], Step [1518/3732], Loss: 2.8660, accuracy: 12.5738%\n",
      "Epoch [2/2], Step [1519/3732], Loss: 3.3912, accuracy: 12.5762%\n",
      "Epoch [2/2], Step [1520/3732], Loss: 2.8129, accuracy: 12.5762%\n",
      "Epoch [2/2], Step [1521/3732], Loss: 2.8092, accuracy: 12.5785%\n",
      "Epoch [2/2], Step [1522/3732], Loss: 3.5396, accuracy: 12.5785%\n",
      "Epoch [2/2], Step [1523/3732], Loss: 3.3570, accuracy: 12.5785%\n",
      "Epoch [2/2], Step [1524/3732], Loss: 3.1605, accuracy: 12.5761%\n",
      "Epoch [2/2], Step [1525/3732], Loss: 2.9627, accuracy: 12.5761%\n",
      "Epoch [2/2], Step [1526/3732], Loss: 2.9978, accuracy: 12.5761%\n",
      "Epoch [2/2], Step [1527/3732], Loss: 2.4918, accuracy: 12.5808%\n",
      "Epoch [2/2], Step [1528/3732], Loss: 3.5721, accuracy: 12.5784%\n",
      "Epoch [2/2], Step [1529/3732], Loss: 3.8569, accuracy: 12.5784%\n",
      "Epoch [2/2], Step [1530/3732], Loss: 1.9715, accuracy: 12.5831%\n",
      "Epoch [2/2], Step [1531/3732], Loss: 2.9014, accuracy: 12.5831%\n",
      "Epoch [2/2], Step [1532/3732], Loss: 3.7109, accuracy: 12.5807%\n",
      "Epoch [2/2], Step [1533/3732], Loss: 3.5708, accuracy: 12.5807%\n",
      "Epoch [2/2], Step [1534/3732], Loss: 3.0824, accuracy: 12.5855%\n",
      "Epoch [2/2], Step [1535/3732], Loss: 3.0247, accuracy: 12.5878%\n",
      "Epoch [2/2], Step [1536/3732], Loss: 3.2491, accuracy: 12.5878%\n",
      "Epoch [2/2], Step [1537/3732], Loss: 2.5801, accuracy: 12.5901%\n",
      "Epoch [2/2], Step [1538/3732], Loss: 2.9454, accuracy: 12.5901%\n",
      "Epoch [2/2], Step [1539/3732], Loss: 3.5953, accuracy: 12.5877%\n",
      "Epoch [2/2], Step [1540/3732], Loss: 2.8491, accuracy: 12.5925%\n",
      "Epoch [2/2], Step [1541/3732], Loss: 2.6820, accuracy: 12.5948%\n",
      "Epoch [2/2], Step [1542/3732], Loss: 4.4329, accuracy: 12.5924%\n",
      "Epoch [2/2], Step [1543/3732], Loss: 3.9725, accuracy: 12.5900%\n",
      "Epoch [2/2], Step [1544/3732], Loss: 2.9612, accuracy: 12.5924%\n",
      "Epoch [2/2], Step [1545/3732], Loss: 3.1310, accuracy: 12.5948%\n",
      "Epoch [2/2], Step [1546/3732], Loss: 2.7884, accuracy: 12.5947%\n",
      "Epoch [2/2], Step [1547/3732], Loss: 2.9021, accuracy: 12.6018%\n",
      "Epoch [2/2], Step [1548/3732], Loss: 3.4288, accuracy: 12.6042%\n",
      "Epoch [2/2], Step [1549/3732], Loss: 2.8960, accuracy: 12.6041%\n",
      "Epoch [2/2], Step [1550/3732], Loss: 2.7216, accuracy: 12.6065%\n",
      "Epoch [2/2], Step [1551/3732], Loss: 2.5290, accuracy: 12.6112%\n",
      "Epoch [2/2], Step [1552/3732], Loss: 2.8326, accuracy: 12.6088%\n",
      "Epoch [2/2], Step [1553/3732], Loss: 2.8883, accuracy: 12.6088%\n",
      "Epoch [2/2], Step [1554/3732], Loss: 2.9511, accuracy: 12.6135%\n",
      "Epoch [2/2], Step [1555/3732], Loss: 2.7456, accuracy: 12.6182%\n",
      "Epoch [2/2], Step [1556/3732], Loss: 2.7756, accuracy: 12.6182%\n",
      "Epoch [2/2], Step [1557/3732], Loss: 2.6095, accuracy: 12.6229%\n",
      "Epoch [2/2], Step [1558/3732], Loss: 3.7079, accuracy: 12.6205%\n",
      "Epoch [2/2], Step [1559/3732], Loss: 3.5013, accuracy: 12.6205%\n",
      "Epoch [2/2], Step [1560/3732], Loss: 3.2829, accuracy: 12.6228%\n",
      "Epoch [2/2], Step [1561/3732], Loss: 3.8436, accuracy: 12.6228%\n",
      "Epoch [2/2], Step [1562/3732], Loss: 2.8122, accuracy: 12.6275%\n",
      "Epoch [2/2], Step [1563/3732], Loss: 2.7343, accuracy: 12.6298%\n",
      "Epoch [2/2], Step [1564/3732], Loss: 2.8901, accuracy: 12.6298%\n",
      "Epoch [2/2], Step [1565/3732], Loss: 3.4005, accuracy: 12.6298%\n",
      "Epoch [2/2], Step [1566/3732], Loss: 2.8582, accuracy: 12.6274%\n",
      "Epoch [2/2], Step [1567/3732], Loss: 4.2755, accuracy: 12.6250%\n",
      "Epoch [2/2], Step [1568/3732], Loss: 1.9989, accuracy: 12.6344%\n",
      "Epoch [2/2], Step [1569/3732], Loss: 3.1130, accuracy: 12.6368%\n",
      "Epoch [2/2], Step [1570/3732], Loss: 2.9964, accuracy: 12.6391%\n",
      "Epoch [2/2], Step [1571/3732], Loss: 2.9193, accuracy: 12.6414%\n",
      "Epoch [2/2], Step [1572/3732], Loss: 3.0022, accuracy: 12.6438%\n",
      "Epoch [2/2], Step [1573/3732], Loss: 3.1938, accuracy: 12.6437%\n",
      "Epoch [2/2], Step [1574/3732], Loss: 2.7614, accuracy: 12.6413%\n",
      "Epoch [2/2], Step [1575/3732], Loss: 3.3952, accuracy: 12.6390%\n",
      "Epoch [2/2], Step [1576/3732], Loss: 2.6797, accuracy: 12.6413%\n",
      "Epoch [2/2], Step [1577/3732], Loss: 2.9118, accuracy: 12.6436%\n",
      "Epoch [2/2], Step [1578/3732], Loss: 2.8888, accuracy: 12.6436%\n",
      "Epoch [2/2], Step [1579/3732], Loss: 2.9615, accuracy: 12.6436%\n",
      "Epoch [2/2], Step [1580/3732], Loss: 3.0297, accuracy: 12.6459%\n",
      "Epoch [2/2], Step [1581/3732], Loss: 2.7509, accuracy: 12.6459%\n",
      "Epoch [2/2], Step [1582/3732], Loss: 3.1173, accuracy: 12.6435%\n",
      "Epoch [2/2], Step [1583/3732], Loss: 3.3443, accuracy: 12.6435%\n",
      "Epoch [2/2], Step [1584/3732], Loss: 3.4662, accuracy: 12.6434%\n",
      "Epoch [2/2], Step [1585/3732], Loss: 2.7200, accuracy: 12.6481%\n",
      "Epoch [2/2], Step [1586/3732], Loss: 3.1064, accuracy: 12.6551%\n",
      "Epoch [2/2], Step [1587/3732], Loss: 2.7828, accuracy: 12.6575%\n",
      "Epoch [2/2], Step [1588/3732], Loss: 3.4480, accuracy: 12.6551%\n",
      "Epoch [2/2], Step [1589/3732], Loss: 2.7331, accuracy: 12.6574%\n",
      "Epoch [2/2], Step [1590/3732], Loss: 2.9171, accuracy: 12.6597%\n",
      "Epoch [2/2], Step [1591/3732], Loss: 2.9273, accuracy: 12.6620%\n",
      "Epoch [2/2], Step [1592/3732], Loss: 2.7752, accuracy: 12.6690%\n",
      "Epoch [2/2], Step [1593/3732], Loss: 4.2969, accuracy: 12.6714%\n",
      "Epoch [2/2], Step [1594/3732], Loss: 3.0672, accuracy: 12.6737%\n",
      "Epoch [2/2], Step [1595/3732], Loss: 3.0942, accuracy: 12.6760%\n",
      "Epoch [2/2], Step [1596/3732], Loss: 3.1132, accuracy: 12.6783%\n",
      "Epoch [2/2], Step [1597/3732], Loss: 3.6130, accuracy: 12.6759%\n",
      "Epoch [2/2], Step [1598/3732], Loss: 3.4504, accuracy: 12.6759%\n",
      "Epoch [2/2], Step [1599/3732], Loss: 2.6799, accuracy: 12.6759%\n",
      "Epoch [2/2], Step [1600/3732], Loss: 2.9265, accuracy: 12.6782%\n",
      "Epoch [2/2], Step [1601/3732], Loss: 3.1697, accuracy: 12.6758%\n",
      "Epoch [2/2], Step [1602/3732], Loss: 2.9889, accuracy: 12.6781%\n",
      "Epoch [2/2], Step [1603/3732], Loss: 3.0576, accuracy: 12.6757%\n",
      "Epoch [2/2], Step [1604/3732], Loss: 4.1216, accuracy: 12.6734%\n",
      "Epoch [2/2], Step [1605/3732], Loss: 3.2094, accuracy: 12.6710%\n",
      "Epoch [2/2], Step [1606/3732], Loss: 3.4770, accuracy: 12.6709%\n",
      "Epoch [2/2], Step [1607/3732], Loss: 2.7569, accuracy: 12.6756%\n",
      "Epoch [2/2], Step [1608/3732], Loss: 3.4465, accuracy: 12.6732%\n",
      "Epoch [2/2], Step [1609/3732], Loss: 2.3585, accuracy: 12.6779%\n",
      "Epoch [2/2], Step [1610/3732], Loss: 3.7028, accuracy: 12.6755%\n",
      "Epoch [2/2], Step [1611/3732], Loss: 3.5571, accuracy: 12.6778%\n",
      "Epoch [2/2], Step [1612/3732], Loss: 3.3590, accuracy: 12.6754%\n",
      "Epoch [2/2], Step [1613/3732], Loss: 3.4476, accuracy: 12.6731%\n",
      "Epoch [2/2], Step [1614/3732], Loss: 3.2855, accuracy: 12.6730%\n",
      "Epoch [2/2], Step [1615/3732], Loss: 2.8946, accuracy: 12.6777%\n",
      "Epoch [2/2], Step [1616/3732], Loss: 3.0542, accuracy: 12.6776%\n",
      "Epoch [2/2], Step [1617/3732], Loss: 2.7105, accuracy: 12.6799%\n",
      "Epoch [2/2], Step [1618/3732], Loss: 3.5174, accuracy: 12.6799%\n",
      "Epoch [2/2], Step [1619/3732], Loss: 3.7833, accuracy: 12.6799%\n",
      "Epoch [2/2], Step [1620/3732], Loss: 3.1981, accuracy: 12.6822%\n",
      "Epoch [2/2], Step [1621/3732], Loss: 3.0240, accuracy: 12.6845%\n",
      "Epoch [2/2], Step [1622/3732], Loss: 2.7694, accuracy: 12.6844%\n",
      "Epoch [2/2], Step [1623/3732], Loss: 2.5113, accuracy: 12.6867%\n",
      "Epoch [2/2], Step [1624/3732], Loss: 2.1900, accuracy: 12.6937%\n",
      "Epoch [2/2], Step [1625/3732], Loss: 3.0367, accuracy: 12.6960%\n",
      "Epoch [2/2], Step [1626/3732], Loss: 3.1010, accuracy: 12.7006%\n",
      "Epoch [2/2], Step [1627/3732], Loss: 2.8893, accuracy: 12.7029%\n",
      "Epoch [2/2], Step [1628/3732], Loss: 3.2044, accuracy: 12.7029%\n",
      "Epoch [2/2], Step [1629/3732], Loss: 2.4190, accuracy: 12.7075%\n",
      "Epoch [2/2], Step [1630/3732], Loss: 3.0362, accuracy: 12.7098%\n",
      "Epoch [2/2], Step [1631/3732], Loss: 2.8526, accuracy: 12.7144%\n",
      "Epoch [2/2], Step [1632/3732], Loss: 3.3299, accuracy: 12.7121%\n",
      "Epoch [2/2], Step [1633/3732], Loss: 3.7329, accuracy: 12.7144%\n",
      "Epoch [2/2], Step [1634/3732], Loss: 2.6553, accuracy: 12.7143%\n",
      "Epoch [2/2], Step [1635/3732], Loss: 3.0078, accuracy: 12.7119%\n",
      "Epoch [2/2], Step [1636/3732], Loss: 3.1066, accuracy: 12.7119%\n",
      "Epoch [2/2], Step [1637/3732], Loss: 3.2682, accuracy: 12.7119%\n",
      "Epoch [2/2], Step [1638/3732], Loss: 2.0585, accuracy: 12.7188%\n",
      "Epoch [2/2], Step [1639/3732], Loss: 3.2091, accuracy: 12.7188%\n",
      "Epoch [2/2], Step [1640/3732], Loss: 2.7019, accuracy: 12.7211%\n",
      "Epoch [2/2], Step [1641/3732], Loss: 3.3825, accuracy: 12.7233%\n",
      "Epoch [2/2], Step [1642/3732], Loss: 3.0729, accuracy: 12.7256%\n",
      "Epoch [2/2], Step [1643/3732], Loss: 3.8532, accuracy: 12.7256%\n",
      "Epoch [2/2], Step [1644/3732], Loss: 3.1162, accuracy: 12.7232%\n",
      "Epoch [2/2], Step [1645/3732], Loss: 2.9987, accuracy: 12.7232%\n",
      "Epoch [2/2], Step [1646/3732], Loss: 2.2528, accuracy: 12.7301%\n",
      "Epoch [2/2], Step [1647/3732], Loss: 2.9722, accuracy: 12.7324%\n",
      "Epoch [2/2], Step [1648/3732], Loss: 2.3811, accuracy: 12.7347%\n",
      "Epoch [2/2], Step [1649/3732], Loss: 2.5286, accuracy: 12.7346%\n",
      "Epoch [2/2], Step [1650/3732], Loss: 3.1378, accuracy: 12.7369%\n",
      "Epoch [2/2], Step [1651/3732], Loss: 4.2518, accuracy: 12.7369%\n",
      "Epoch [2/2], Step [1652/3732], Loss: 3.2693, accuracy: 12.7368%\n",
      "Epoch [2/2], Step [1653/3732], Loss: 3.3831, accuracy: 12.7391%\n",
      "Epoch [2/2], Step [1654/3732], Loss: 2.7521, accuracy: 12.7437%\n",
      "Epoch [2/2], Step [1655/3732], Loss: 2.9496, accuracy: 12.7436%\n",
      "Epoch [2/2], Step [1656/3732], Loss: 3.1545, accuracy: 12.7459%\n",
      "Epoch [2/2], Step [1657/3732], Loss: 2.8144, accuracy: 12.7482%\n",
      "Epoch [2/2], Step [1658/3732], Loss: 2.5180, accuracy: 12.7551%\n",
      "Epoch [2/2], Step [1659/3732], Loss: 2.8441, accuracy: 12.7574%\n",
      "Epoch [2/2], Step [1660/3732], Loss: 3.5478, accuracy: 12.7550%\n",
      "Epoch [2/2], Step [1661/3732], Loss: 2.4427, accuracy: 12.7596%\n",
      "Epoch [2/2], Step [1662/3732], Loss: 2.3676, accuracy: 12.7665%\n",
      "Epoch [2/2], Step [1663/3732], Loss: 3.1661, accuracy: 12.7665%\n",
      "Epoch [2/2], Step [1664/3732], Loss: 2.9128, accuracy: 12.7687%\n",
      "Epoch [2/2], Step [1665/3732], Loss: 2.5901, accuracy: 12.7710%\n",
      "Epoch [2/2], Step [1666/3732], Loss: 3.3554, accuracy: 12.7686%\n",
      "Epoch [2/2], Step [1667/3732], Loss: 2.9354, accuracy: 12.7709%\n",
      "Epoch [2/2], Step [1668/3732], Loss: 3.6634, accuracy: 12.7731%\n",
      "Epoch [2/2], Step [1669/3732], Loss: 2.7488, accuracy: 12.7800%\n",
      "Epoch [2/2], Step [1670/3732], Loss: 3.0104, accuracy: 12.7800%\n",
      "Epoch [2/2], Step [1671/3732], Loss: 1.9577, accuracy: 12.7869%\n",
      "Epoch [2/2], Step [1672/3732], Loss: 3.0240, accuracy: 12.7868%\n",
      "Epoch [2/2], Step [1673/3732], Loss: 3.0272, accuracy: 12.7891%\n",
      "Epoch [2/2], Step [1674/3732], Loss: 2.7206, accuracy: 12.7890%\n",
      "Epoch [2/2], Step [1675/3732], Loss: 3.5850, accuracy: 12.7913%\n",
      "Epoch [2/2], Step [1676/3732], Loss: 2.4829, accuracy: 12.7935%\n",
      "Epoch [2/2], Step [1677/3732], Loss: 2.6668, accuracy: 12.7935%\n",
      "Epoch [2/2], Step [1678/3732], Loss: 2.5595, accuracy: 12.7981%\n",
      "Epoch [2/2], Step [1679/3732], Loss: 2.6070, accuracy: 12.8003%\n",
      "Epoch [2/2], Step [1680/3732], Loss: 2.7731, accuracy: 12.8026%\n",
      "Epoch [2/2], Step [1681/3732], Loss: 2.5799, accuracy: 12.8048%\n",
      "Epoch [2/2], Step [1682/3732], Loss: 3.0985, accuracy: 12.8071%\n",
      "Epoch [2/2], Step [1683/3732], Loss: 3.4584, accuracy: 12.8047%\n",
      "Epoch [2/2], Step [1684/3732], Loss: 3.4911, accuracy: 12.8047%\n",
      "Epoch [2/2], Step [1685/3732], Loss: 3.0222, accuracy: 12.8046%\n",
      "Epoch [2/2], Step [1686/3732], Loss: 4.1775, accuracy: 12.8045%\n",
      "Epoch [2/2], Step [1687/3732], Loss: 2.7945, accuracy: 12.8068%\n",
      "Epoch [2/2], Step [1688/3732], Loss: 3.2451, accuracy: 12.8067%\n",
      "Epoch [2/2], Step [1689/3732], Loss: 3.2205, accuracy: 12.8067%\n",
      "Epoch [2/2], Step [1690/3732], Loss: 2.9653, accuracy: 12.8089%\n",
      "Epoch [2/2], Step [1691/3732], Loss: 3.5201, accuracy: 12.8066%\n",
      "Epoch [2/2], Step [1692/3732], Loss: 2.8407, accuracy: 12.8088%\n",
      "Epoch [2/2], Step [1693/3732], Loss: 4.2687, accuracy: 12.8065%\n",
      "Epoch [2/2], Step [1694/3732], Loss: 2.9628, accuracy: 12.8041%\n",
      "Epoch [2/2], Step [1695/3732], Loss: 3.5718, accuracy: 12.8040%\n",
      "Epoch [2/2], Step [1696/3732], Loss: 3.4288, accuracy: 12.8040%\n",
      "Epoch [2/2], Step [1697/3732], Loss: 3.0213, accuracy: 12.8062%\n",
      "Epoch [2/2], Step [1698/3732], Loss: 3.3428, accuracy: 12.8062%\n",
      "Epoch [2/2], Step [1699/3732], Loss: 3.0346, accuracy: 12.8084%\n",
      "Epoch [2/2], Step [1700/3732], Loss: 3.5113, accuracy: 12.8061%\n",
      "Epoch [2/2], Step [1701/3732], Loss: 3.2966, accuracy: 12.8083%\n",
      "Epoch [2/2], Step [1702/3732], Loss: 2.7602, accuracy: 12.8128%\n",
      "Epoch [2/2], Step [1703/3732], Loss: 2.7948, accuracy: 12.8174%\n",
      "Epoch [2/2], Step [1704/3732], Loss: 3.1206, accuracy: 12.8150%\n",
      "Epoch [2/2], Step [1705/3732], Loss: 2.9994, accuracy: 12.8150%\n",
      "Epoch [2/2], Step [1706/3732], Loss: 3.6537, accuracy: 12.8149%\n",
      "Epoch [2/2], Step [1707/3732], Loss: 3.1739, accuracy: 12.8172%\n",
      "Epoch [2/2], Step [1708/3732], Loss: 2.4800, accuracy: 12.8217%\n",
      "Epoch [2/2], Step [1709/3732], Loss: 2.9328, accuracy: 12.8216%\n",
      "Epoch [2/2], Step [1710/3732], Loss: 2.8715, accuracy: 12.8239%\n",
      "Epoch [2/2], Step [1711/3732], Loss: 3.4029, accuracy: 12.8215%\n",
      "Epoch [2/2], Step [1712/3732], Loss: 2.9623, accuracy: 12.8238%\n",
      "Epoch [2/2], Step [1713/3732], Loss: 2.5317, accuracy: 12.8306%\n",
      "Epoch [2/2], Step [1714/3732], Loss: 3.1977, accuracy: 12.8328%\n",
      "Epoch [2/2], Step [1715/3732], Loss: 3.7042, accuracy: 12.8328%\n",
      "Epoch [2/2], Step [1716/3732], Loss: 3.1613, accuracy: 12.8350%\n",
      "Epoch [2/2], Step [1717/3732], Loss: 2.5975, accuracy: 12.8349%\n",
      "Epoch [2/2], Step [1718/3732], Loss: 3.3230, accuracy: 12.8372%\n",
      "Epoch [2/2], Step [1719/3732], Loss: 3.3438, accuracy: 12.8371%\n",
      "Epoch [2/2], Step [1720/3732], Loss: 3.0274, accuracy: 12.8347%\n",
      "Epoch [2/2], Step [1721/3732], Loss: 2.5192, accuracy: 12.8393%\n",
      "Epoch [2/2], Step [1722/3732], Loss: 3.3416, accuracy: 12.8415%\n",
      "Epoch [2/2], Step [1723/3732], Loss: 2.9984, accuracy: 12.8391%\n",
      "Epoch [2/2], Step [1724/3732], Loss: 2.7496, accuracy: 12.8368%\n",
      "Epoch [2/2], Step [1725/3732], Loss: 3.4237, accuracy: 12.8367%\n",
      "Epoch [2/2], Step [1726/3732], Loss: 2.9306, accuracy: 12.8367%\n",
      "Epoch [2/2], Step [1727/3732], Loss: 2.1884, accuracy: 12.8412%\n",
      "Epoch [2/2], Step [1728/3732], Loss: 3.3625, accuracy: 12.8388%\n",
      "Epoch [2/2], Step [1729/3732], Loss: 2.4777, accuracy: 12.8433%\n",
      "Epoch [2/2], Step [1730/3732], Loss: 2.2151, accuracy: 12.8456%\n",
      "Epoch [2/2], Step [1731/3732], Loss: 3.4149, accuracy: 12.8455%\n",
      "Epoch [2/2], Step [1732/3732], Loss: 3.4090, accuracy: 12.8432%\n",
      "Epoch [2/2], Step [1733/3732], Loss: 3.3364, accuracy: 12.8408%\n",
      "Epoch [2/2], Step [1734/3732], Loss: 2.7508, accuracy: 12.8430%\n",
      "Epoch [2/2], Step [1735/3732], Loss: 3.5487, accuracy: 12.8430%\n",
      "Epoch [2/2], Step [1736/3732], Loss: 2.4258, accuracy: 12.8475%\n",
      "Epoch [2/2], Step [1737/3732], Loss: 3.3440, accuracy: 12.8451%\n",
      "Epoch [2/2], Step [1738/3732], Loss: 3.2380, accuracy: 12.8428%\n",
      "Epoch [2/2], Step [1739/3732], Loss: 3.2156, accuracy: 12.8427%\n",
      "Epoch [2/2], Step [1740/3732], Loss: 3.6015, accuracy: 12.8404%\n",
      "Epoch [2/2], Step [1741/3732], Loss: 2.9570, accuracy: 12.8426%\n",
      "Epoch [2/2], Step [1742/3732], Loss: 2.3949, accuracy: 12.8471%\n",
      "Epoch [2/2], Step [1743/3732], Loss: 2.5533, accuracy: 12.8493%\n",
      "Epoch [2/2], Step [1744/3732], Loss: 2.8018, accuracy: 12.8515%\n",
      "Epoch [2/2], Step [1745/3732], Loss: 2.7805, accuracy: 12.8538%\n",
      "Epoch [2/2], Step [1746/3732], Loss: 3.5212, accuracy: 12.8537%\n",
      "Epoch [2/2], Step [1747/3732], Loss: 3.4993, accuracy: 12.8513%\n",
      "Epoch [2/2], Step [1748/3732], Loss: 3.6147, accuracy: 12.8513%\n",
      "Epoch [2/2], Step [1749/3732], Loss: 2.9720, accuracy: 12.8535%\n",
      "Epoch [2/2], Step [1750/3732], Loss: 2.8829, accuracy: 12.8534%\n",
      "Epoch [2/2], Step [1751/3732], Loss: 3.1619, accuracy: 12.8556%\n",
      "Epoch [2/2], Step [1752/3732], Loss: 2.4730, accuracy: 12.8601%\n",
      "Epoch [2/2], Step [1753/3732], Loss: 3.1394, accuracy: 12.8646%\n",
      "Epoch [2/2], Step [1754/3732], Loss: 2.3930, accuracy: 12.8668%\n",
      "Epoch [2/2], Step [1755/3732], Loss: 3.3252, accuracy: 12.8668%\n",
      "Epoch [2/2], Step [1756/3732], Loss: 2.8720, accuracy: 12.8667%\n",
      "Epoch [2/2], Step [1757/3732], Loss: 3.1970, accuracy: 12.8666%\n",
      "Epoch [2/2], Step [1758/3732], Loss: 2.7769, accuracy: 12.8689%\n",
      "Epoch [2/2], Step [1759/3732], Loss: 3.8607, accuracy: 12.8688%\n",
      "Epoch [2/2], Step [1760/3732], Loss: 3.1583, accuracy: 12.8733%\n",
      "Epoch [2/2], Step [1761/3732], Loss: 3.0513, accuracy: 12.8709%\n",
      "Epoch [2/2], Step [1762/3732], Loss: 2.9294, accuracy: 12.8754%\n",
      "Epoch [2/2], Step [1763/3732], Loss: 3.3218, accuracy: 12.8753%\n",
      "Epoch [2/2], Step [1764/3732], Loss: 3.4946, accuracy: 12.8753%\n",
      "Epoch [2/2], Step [1765/3732], Loss: 2.9639, accuracy: 12.8775%\n",
      "Epoch [2/2], Step [1766/3732], Loss: 3.6571, accuracy: 12.8751%\n",
      "Epoch [2/2], Step [1767/3732], Loss: 3.4984, accuracy: 12.8751%\n",
      "Epoch [2/2], Step [1768/3732], Loss: 3.1489, accuracy: 12.8727%\n",
      "Epoch [2/2], Step [1769/3732], Loss: 2.9732, accuracy: 12.8749%\n",
      "Epoch [2/2], Step [1770/3732], Loss: 2.7883, accuracy: 12.8749%\n",
      "Epoch [2/2], Step [1771/3732], Loss: 2.8227, accuracy: 12.8748%\n",
      "Epoch [2/2], Step [1772/3732], Loss: 3.1098, accuracy: 12.8770%\n",
      "Epoch [2/2], Step [1773/3732], Loss: 3.1253, accuracy: 12.8769%\n",
      "Epoch [2/2], Step [1774/3732], Loss: 3.0226, accuracy: 12.8769%\n",
      "Epoch [2/2], Step [1775/3732], Loss: 3.0172, accuracy: 12.8791%\n",
      "Epoch [2/2], Step [1776/3732], Loss: 2.2618, accuracy: 12.8835%\n",
      "Epoch [2/2], Step [1777/3732], Loss: 2.7989, accuracy: 12.8880%\n",
      "Epoch [2/2], Step [1778/3732], Loss: 3.6890, accuracy: 12.8879%\n",
      "Epoch [2/2], Step [1779/3732], Loss: 3.0231, accuracy: 12.8924%\n",
      "Epoch [2/2], Step [1780/3732], Loss: 2.9773, accuracy: 12.8901%\n",
      "Epoch [2/2], Step [1781/3732], Loss: 2.6213, accuracy: 12.8968%\n",
      "Epoch [2/2], Step [1782/3732], Loss: 2.9157, accuracy: 12.8967%\n",
      "Epoch [2/2], Step [1783/3732], Loss: 2.5334, accuracy: 12.9012%\n",
      "Epoch [2/2], Step [1784/3732], Loss: 3.4149, accuracy: 12.9011%\n",
      "Epoch [2/2], Step [1785/3732], Loss: 3.2521, accuracy: 12.8988%\n",
      "Epoch [2/2], Step [1786/3732], Loss: 2.9556, accuracy: 12.9010%\n",
      "Epoch [2/2], Step [1787/3732], Loss: 3.8455, accuracy: 12.9009%\n",
      "Epoch [2/2], Step [1788/3732], Loss: 3.1121, accuracy: 12.8986%\n",
      "Epoch [2/2], Step [1789/3732], Loss: 2.9042, accuracy: 12.8985%\n",
      "Epoch [2/2], Step [1790/3732], Loss: 3.4687, accuracy: 12.8961%\n",
      "Epoch [2/2], Step [1791/3732], Loss: 1.5767, accuracy: 12.9051%\n",
      "Epoch [2/2], Step [1792/3732], Loss: 2.7563, accuracy: 12.9028%\n",
      "Epoch [2/2], Step [1793/3732], Loss: 3.8852, accuracy: 12.9005%\n",
      "Epoch [2/2], Step [1794/3732], Loss: 3.6544, accuracy: 12.8981%\n",
      "Epoch [2/2], Step [1795/3732], Loss: 3.4421, accuracy: 12.8980%\n",
      "Epoch [2/2], Step [1796/3732], Loss: 3.9544, accuracy: 12.8957%\n",
      "Epoch [2/2], Step [1797/3732], Loss: 3.2338, accuracy: 12.8934%\n",
      "Epoch [2/2], Step [1798/3732], Loss: 2.9678, accuracy: 12.8978%\n",
      "Epoch [2/2], Step [1799/3732], Loss: 2.3689, accuracy: 12.9023%\n",
      "Epoch [2/2], Step [1800/3732], Loss: 3.3271, accuracy: 12.8999%\n",
      "Epoch [2/2], Step [1801/3732], Loss: 2.7162, accuracy: 12.9021%\n",
      "Epoch [2/2], Step [1802/3732], Loss: 3.0817, accuracy: 12.9021%\n",
      "Epoch [2/2], Step [1803/3732], Loss: 2.8515, accuracy: 12.9042%\n",
      "Epoch [2/2], Step [1804/3732], Loss: 3.5993, accuracy: 12.9019%\n",
      "Epoch [2/2], Step [1805/3732], Loss: 2.9398, accuracy: 12.9041%\n",
      "Epoch [2/2], Step [1806/3732], Loss: 3.0145, accuracy: 12.9063%\n",
      "Epoch [2/2], Step [1807/3732], Loss: 3.4279, accuracy: 12.9062%\n",
      "Epoch [2/2], Step [1808/3732], Loss: 3.3630, accuracy: 12.9061%\n",
      "Epoch [2/2], Step [1809/3732], Loss: 3.3300, accuracy: 12.9038%\n",
      "Epoch [2/2], Step [1810/3732], Loss: 3.1531, accuracy: 12.9037%\n",
      "Epoch [2/2], Step [1811/3732], Loss: 3.5362, accuracy: 12.9037%\n",
      "Epoch [2/2], Step [1812/3732], Loss: 2.5398, accuracy: 12.9058%\n",
      "Epoch [2/2], Step [1813/3732], Loss: 3.5730, accuracy: 12.9080%\n",
      "Epoch [2/2], Step [1814/3732], Loss: 3.7926, accuracy: 12.9080%\n",
      "Epoch [2/2], Step [1815/3732], Loss: 3.3196, accuracy: 12.9056%\n",
      "Epoch [2/2], Step [1816/3732], Loss: 2.9196, accuracy: 12.9078%\n",
      "Epoch [2/2], Step [1817/3732], Loss: 2.2227, accuracy: 12.9100%\n",
      "Epoch [2/2], Step [1818/3732], Loss: 2.7926, accuracy: 12.9099%\n",
      "Epoch [2/2], Step [1819/3732], Loss: 4.0469, accuracy: 12.9076%\n",
      "Epoch [2/2], Step [1820/3732], Loss: 3.0323, accuracy: 12.9120%\n",
      "Epoch [2/2], Step [1821/3732], Loss: 2.6342, accuracy: 12.9142%\n",
      "Epoch [2/2], Step [1822/3732], Loss: 2.9558, accuracy: 12.9164%\n",
      "Epoch [2/2], Step [1823/3732], Loss: 2.7898, accuracy: 12.9185%\n",
      "Epoch [2/2], Step [1824/3732], Loss: 3.7191, accuracy: 12.9207%\n",
      "Epoch [2/2], Step [1825/3732], Loss: 4.1979, accuracy: 12.9184%\n",
      "Epoch [2/2], Step [1826/3732], Loss: 4.2915, accuracy: 12.9161%\n",
      "Epoch [2/2], Step [1827/3732], Loss: 3.2892, accuracy: 12.9160%\n",
      "Epoch [2/2], Step [1828/3732], Loss: 3.0964, accuracy: 12.9182%\n",
      "Epoch [2/2], Step [1829/3732], Loss: 2.7360, accuracy: 12.9203%\n",
      "Epoch [2/2], Step [1830/3732], Loss: 3.2850, accuracy: 12.9203%\n",
      "Epoch [2/2], Step [1831/3732], Loss: 2.5051, accuracy: 12.9224%\n",
      "Epoch [2/2], Step [1832/3732], Loss: 2.6009, accuracy: 12.9291%\n",
      "Epoch [2/2], Step [1833/3732], Loss: 2.9403, accuracy: 12.9335%\n",
      "Epoch [2/2], Step [1834/3732], Loss: 2.7566, accuracy: 12.9357%\n",
      "Epoch [2/2], Step [1835/3732], Loss: 2.8401, accuracy: 12.9378%\n",
      "Epoch [2/2], Step [1836/3732], Loss: 3.0101, accuracy: 12.9378%\n",
      "Epoch [2/2], Step [1837/3732], Loss: 3.4473, accuracy: 12.9377%\n",
      "Epoch [2/2], Step [1838/3732], Loss: 2.3502, accuracy: 12.9399%\n",
      "Epoch [2/2], Step [1839/3732], Loss: 2.3479, accuracy: 12.9443%\n",
      "Epoch [2/2], Step [1840/3732], Loss: 4.6737, accuracy: 12.9442%\n",
      "Epoch [2/2], Step [1841/3732], Loss: 2.6847, accuracy: 12.9463%\n",
      "Epoch [2/2], Step [1842/3732], Loss: 3.3542, accuracy: 12.9463%\n",
      "Epoch [2/2], Step [1843/3732], Loss: 3.7620, accuracy: 12.9462%\n",
      "Epoch [2/2], Step [1844/3732], Loss: 3.2886, accuracy: 12.9461%\n",
      "Epoch [2/2], Step [1845/3732], Loss: 2.5380, accuracy: 12.9483%\n",
      "Epoch [2/2], Step [1846/3732], Loss: 3.2483, accuracy: 12.9482%\n",
      "Epoch [2/2], Step [1847/3732], Loss: 3.6465, accuracy: 12.9459%\n",
      "Epoch [2/2], Step [1848/3732], Loss: 3.3523, accuracy: 12.9503%\n",
      "Epoch [2/2], Step [1849/3732], Loss: 2.9345, accuracy: 12.9524%\n",
      "Epoch [2/2], Step [1850/3732], Loss: 2.9543, accuracy: 12.9523%\n",
      "Epoch [2/2], Step [1851/3732], Loss: 3.1486, accuracy: 12.9500%\n",
      "Epoch [2/2], Step [1852/3732], Loss: 3.1388, accuracy: 12.9499%\n",
      "Epoch [2/2], Step [1853/3732], Loss: 2.8316, accuracy: 12.9521%\n",
      "Epoch [2/2], Step [1854/3732], Loss: 2.4196, accuracy: 12.9543%\n",
      "Epoch [2/2], Step [1855/3732], Loss: 3.3073, accuracy: 12.9542%\n",
      "Epoch [2/2], Step [1856/3732], Loss: 3.0558, accuracy: 12.9541%\n",
      "Epoch [2/2], Step [1857/3732], Loss: 1.9501, accuracy: 12.9585%\n",
      "Epoch [2/2], Step [1858/3732], Loss: 2.7019, accuracy: 12.9606%\n",
      "Epoch [2/2], Step [1859/3732], Loss: 2.8489, accuracy: 12.9628%\n",
      "Epoch [2/2], Step [1860/3732], Loss: 3.0524, accuracy: 12.9649%\n",
      "Epoch [2/2], Step [1861/3732], Loss: 2.6596, accuracy: 12.9671%\n",
      "Epoch [2/2], Step [1862/3732], Loss: 3.1540, accuracy: 12.9670%\n",
      "Epoch [2/2], Step [1863/3732], Loss: 2.7876, accuracy: 12.9647%\n",
      "Epoch [2/2], Step [1864/3732], Loss: 3.8372, accuracy: 12.9669%\n",
      "Epoch [2/2], Step [1865/3732], Loss: 2.8228, accuracy: 12.9712%\n",
      "Epoch [2/2], Step [1866/3732], Loss: 3.9954, accuracy: 12.9712%\n",
      "Epoch [2/2], Step [1867/3732], Loss: 3.6795, accuracy: 12.9711%\n",
      "Epoch [2/2], Step [1868/3732], Loss: 4.1126, accuracy: 12.9710%\n",
      "Epoch [2/2], Step [1869/3732], Loss: 3.2723, accuracy: 12.9709%\n",
      "Epoch [2/2], Step [1870/3732], Loss: 2.9179, accuracy: 12.9708%\n",
      "Epoch [2/2], Step [1871/3732], Loss: 2.8389, accuracy: 12.9730%\n",
      "Epoch [2/2], Step [1872/3732], Loss: 3.2892, accuracy: 12.9729%\n",
      "Epoch [2/2], Step [1873/3732], Loss: 2.8078, accuracy: 12.9728%\n",
      "Epoch [2/2], Step [1874/3732], Loss: 3.2309, accuracy: 12.9749%\n",
      "Epoch [2/2], Step [1875/3732], Loss: 3.5218, accuracy: 12.9749%\n",
      "Epoch [2/2], Step [1876/3732], Loss: 3.5213, accuracy: 12.9725%\n",
      "Epoch [2/2], Step [1877/3732], Loss: 2.2234, accuracy: 12.9791%\n",
      "Epoch [2/2], Step [1878/3732], Loss: 2.9809, accuracy: 12.9835%\n",
      "Epoch [2/2], Step [1879/3732], Loss: 2.9832, accuracy: 12.9834%\n",
      "Epoch [2/2], Step [1880/3732], Loss: 2.9220, accuracy: 12.9878%\n",
      "Epoch [2/2], Step [1881/3732], Loss: 2.7023, accuracy: 12.9944%\n",
      "Epoch [2/2], Step [1882/3732], Loss: 2.9329, accuracy: 12.9943%\n",
      "Epoch [2/2], Step [1883/3732], Loss: 3.5130, accuracy: 12.9942%\n",
      "Epoch [2/2], Step [1884/3732], Loss: 2.8221, accuracy: 12.9941%\n",
      "Epoch [2/2], Step [1885/3732], Loss: 3.0366, accuracy: 12.9940%\n",
      "Epoch [2/2], Step [1886/3732], Loss: 3.0781, accuracy: 12.9939%\n",
      "Epoch [2/2], Step [1887/3732], Loss: 3.3477, accuracy: 12.9961%\n",
      "Epoch [2/2], Step [1888/3732], Loss: 3.3940, accuracy: 12.9960%\n",
      "Epoch [2/2], Step [1889/3732], Loss: 2.9272, accuracy: 13.0004%\n",
      "Epoch [2/2], Step [1890/3732], Loss: 2.9060, accuracy: 13.0047%\n",
      "Epoch [2/2], Step [1891/3732], Loss: 2.8717, accuracy: 13.0068%\n",
      "Epoch [2/2], Step [1892/3732], Loss: 3.2834, accuracy: 13.0068%\n",
      "Epoch [2/2], Step [1893/3732], Loss: 2.5326, accuracy: 13.0089%\n",
      "Epoch [2/2], Step [1894/3732], Loss: 2.2054, accuracy: 13.0155%\n",
      "Epoch [2/2], Step [1895/3732], Loss: 3.4960, accuracy: 13.0176%\n",
      "Epoch [2/2], Step [1896/3732], Loss: 2.4042, accuracy: 13.0219%\n",
      "Epoch [2/2], Step [1897/3732], Loss: 2.7236, accuracy: 13.0241%\n",
      "Epoch [2/2], Step [1898/3732], Loss: 2.7369, accuracy: 13.0262%\n",
      "Epoch [2/2], Step [1899/3732], Loss: 3.5564, accuracy: 13.0261%\n",
      "Epoch [2/2], Step [1900/3732], Loss: 3.0968, accuracy: 13.0327%\n",
      "Epoch [2/2], Step [1901/3732], Loss: 3.3917, accuracy: 13.0326%\n",
      "Epoch [2/2], Step [1902/3732], Loss: 2.3648, accuracy: 13.0369%\n",
      "Epoch [2/2], Step [1903/3732], Loss: 2.8101, accuracy: 13.0390%\n",
      "Epoch [2/2], Step [1904/3732], Loss: 2.8619, accuracy: 13.0412%\n",
      "Epoch [2/2], Step [1905/3732], Loss: 3.0543, accuracy: 13.0433%\n",
      "Epoch [2/2], Step [1906/3732], Loss: 2.2660, accuracy: 13.0476%\n",
      "Epoch [2/2], Step [1907/3732], Loss: 3.8298, accuracy: 13.0475%\n",
      "Epoch [2/2], Step [1908/3732], Loss: 3.6269, accuracy: 13.0452%\n",
      "Epoch [2/2], Step [1909/3732], Loss: 2.9965, accuracy: 13.0473%\n",
      "Epoch [2/2], Step [1910/3732], Loss: 2.8625, accuracy: 13.0450%\n",
      "Epoch [2/2], Step [1911/3732], Loss: 2.6156, accuracy: 13.0449%\n",
      "Epoch [2/2], Step [1912/3732], Loss: 3.4524, accuracy: 13.0470%\n",
      "Epoch [2/2], Step [1913/3732], Loss: 2.6611, accuracy: 13.0492%\n",
      "Epoch [2/2], Step [1914/3732], Loss: 2.4712, accuracy: 13.0513%\n",
      "Epoch [2/2], Step [1915/3732], Loss: 3.6074, accuracy: 13.0512%\n",
      "Epoch [2/2], Step [1916/3732], Loss: 2.8230, accuracy: 13.0533%\n",
      "Epoch [2/2], Step [1917/3732], Loss: 2.8680, accuracy: 13.0532%\n",
      "Epoch [2/2], Step [1918/3732], Loss: 2.1963, accuracy: 13.0575%\n",
      "Epoch [2/2], Step [1919/3732], Loss: 3.1761, accuracy: 13.0618%\n",
      "Epoch [2/2], Step [1920/3732], Loss: 3.5107, accuracy: 13.0617%\n",
      "Epoch [2/2], Step [1921/3732], Loss: 2.7334, accuracy: 13.0639%\n",
      "Epoch [2/2], Step [1922/3732], Loss: 3.0201, accuracy: 13.0638%\n",
      "Epoch [2/2], Step [1923/3732], Loss: 3.3571, accuracy: 13.0637%\n",
      "Epoch [2/2], Step [1924/3732], Loss: 2.7877, accuracy: 13.0680%\n",
      "Epoch [2/2], Step [1925/3732], Loss: 3.0914, accuracy: 13.0679%\n",
      "Epoch [2/2], Step [1926/3732], Loss: 2.1072, accuracy: 13.0744%\n",
      "Epoch [2/2], Step [1927/3732], Loss: 3.2426, accuracy: 13.0743%\n",
      "Epoch [2/2], Step [1928/3732], Loss: 3.8909, accuracy: 13.0742%\n",
      "Epoch [2/2], Step [1929/3732], Loss: 2.9612, accuracy: 13.0763%\n",
      "Epoch [2/2], Step [1930/3732], Loss: 2.7349, accuracy: 13.0762%\n",
      "Epoch [2/2], Step [1931/3732], Loss: 2.8969, accuracy: 13.0761%\n",
      "Epoch [2/2], Step [1932/3732], Loss: 3.3880, accuracy: 13.0738%\n",
      "Epoch [2/2], Step [1933/3732], Loss: 3.0003, accuracy: 13.0759%\n",
      "Epoch [2/2], Step [1934/3732], Loss: 2.7481, accuracy: 13.0780%\n",
      "Epoch [2/2], Step [1935/3732], Loss: 3.5358, accuracy: 13.0801%\n",
      "Epoch [2/2], Step [1936/3732], Loss: 2.6109, accuracy: 13.0822%\n",
      "Epoch [2/2], Step [1937/3732], Loss: 2.9107, accuracy: 13.0821%\n",
      "Epoch [2/2], Step [1938/3732], Loss: 2.2321, accuracy: 13.0864%\n",
      "Epoch [2/2], Step [1939/3732], Loss: 3.2134, accuracy: 13.0863%\n",
      "Epoch [2/2], Step [1940/3732], Loss: 3.7582, accuracy: 13.0862%\n",
      "Epoch [2/2], Step [1941/3732], Loss: 3.2741, accuracy: 13.0905%\n",
      "Epoch [2/2], Step [1942/3732], Loss: 3.0136, accuracy: 13.0926%\n",
      "Epoch [2/2], Step [1943/3732], Loss: 3.0286, accuracy: 13.0947%\n",
      "Epoch [2/2], Step [1944/3732], Loss: 3.3246, accuracy: 13.0924%\n",
      "Epoch [2/2], Step [1945/3732], Loss: 3.0475, accuracy: 13.0967%\n",
      "Epoch [2/2], Step [1946/3732], Loss: 2.7146, accuracy: 13.1010%\n",
      "Epoch [2/2], Step [1947/3732], Loss: 3.0024, accuracy: 13.1031%\n",
      "Epoch [2/2], Step [1948/3732], Loss: 3.1608, accuracy: 13.1052%\n",
      "Epoch [2/2], Step [1949/3732], Loss: 2.9556, accuracy: 13.1095%\n",
      "Epoch [2/2], Step [1950/3732], Loss: 3.3540, accuracy: 13.1116%\n",
      "Epoch [2/2], Step [1951/3732], Loss: 3.0672, accuracy: 13.1137%\n",
      "Epoch [2/2], Step [1952/3732], Loss: 3.5403, accuracy: 13.1136%\n",
      "Epoch [2/2], Step [1953/3732], Loss: 3.1446, accuracy: 13.1157%\n",
      "Epoch [2/2], Step [1954/3732], Loss: 3.0209, accuracy: 13.1155%\n",
      "Epoch [2/2], Step [1955/3732], Loss: 3.1421, accuracy: 13.1176%\n",
      "Epoch [2/2], Step [1956/3732], Loss: 2.5770, accuracy: 13.1219%\n",
      "Epoch [2/2], Step [1957/3732], Loss: 3.8274, accuracy: 13.1196%\n",
      "Epoch [2/2], Step [1958/3732], Loss: 2.8892, accuracy: 13.1195%\n",
      "Epoch [2/2], Step [1959/3732], Loss: 2.8812, accuracy: 13.1172%\n",
      "Epoch [2/2], Step [1960/3732], Loss: 2.7639, accuracy: 13.1193%\n",
      "Epoch [2/2], Step [1961/3732], Loss: 2.8980, accuracy: 13.1192%\n",
      "Epoch [2/2], Step [1962/3732], Loss: 2.8791, accuracy: 13.1213%\n",
      "Epoch [2/2], Step [1963/3732], Loss: 2.3794, accuracy: 13.1234%\n",
      "Epoch [2/2], Step [1964/3732], Loss: 2.7774, accuracy: 13.1232%\n",
      "Epoch [2/2], Step [1965/3732], Loss: 3.6401, accuracy: 13.1231%\n",
      "Epoch [2/2], Step [1966/3732], Loss: 2.8748, accuracy: 13.1252%\n",
      "Epoch [2/2], Step [1967/3732], Loss: 2.7292, accuracy: 13.1273%\n",
      "Epoch [2/2], Step [1968/3732], Loss: 2.6123, accuracy: 13.1316%\n",
      "Epoch [2/2], Step [1969/3732], Loss: 3.2036, accuracy: 13.1293%\n",
      "Epoch [2/2], Step [1970/3732], Loss: 2.9742, accuracy: 13.1270%\n",
      "Epoch [2/2], Step [1971/3732], Loss: 3.9792, accuracy: 13.1247%\n",
      "Epoch [2/2], Step [1972/3732], Loss: 3.5770, accuracy: 13.1246%\n",
      "Epoch [2/2], Step [1973/3732], Loss: 2.1378, accuracy: 13.1310%\n",
      "Epoch [2/2], Step [1974/3732], Loss: 3.0401, accuracy: 13.1309%\n",
      "Epoch [2/2], Step [1975/3732], Loss: 2.6757, accuracy: 13.1352%\n",
      "Epoch [2/2], Step [1976/3732], Loss: 3.4786, accuracy: 13.1351%\n",
      "Epoch [2/2], Step [1977/3732], Loss: 3.4388, accuracy: 13.1350%\n",
      "Epoch [2/2], Step [1978/3732], Loss: 3.0416, accuracy: 13.1392%\n",
      "Epoch [2/2], Step [1979/3732], Loss: 3.1957, accuracy: 13.1413%\n",
      "Epoch [2/2], Step [1980/3732], Loss: 2.6760, accuracy: 13.1412%\n",
      "Epoch [2/2], Step [1981/3732], Loss: 2.9750, accuracy: 13.1433%\n",
      "Epoch [2/2], Step [1982/3732], Loss: 3.2317, accuracy: 13.1432%\n",
      "Epoch [2/2], Step [1983/3732], Loss: 3.1825, accuracy: 13.1452%\n",
      "Epoch [2/2], Step [1984/3732], Loss: 3.3054, accuracy: 13.1451%\n",
      "Epoch [2/2], Step [1985/3732], Loss: 3.5512, accuracy: 13.1472%\n",
      "Epoch [2/2], Step [1986/3732], Loss: 3.2886, accuracy: 13.1471%\n",
      "Epoch [2/2], Step [1987/3732], Loss: 2.8102, accuracy: 13.1470%\n",
      "Epoch [2/2], Step [1988/3732], Loss: 3.3006, accuracy: 13.1469%\n",
      "Epoch [2/2], Step [1989/3732], Loss: 2.6982, accuracy: 13.1489%\n",
      "Epoch [2/2], Step [1990/3732], Loss: 3.1308, accuracy: 13.1510%\n",
      "Epoch [2/2], Step [1991/3732], Loss: 3.1784, accuracy: 13.1509%\n",
      "Epoch [2/2], Step [1992/3732], Loss: 3.0668, accuracy: 13.1530%\n",
      "Epoch [2/2], Step [1993/3732], Loss: 2.7431, accuracy: 13.1594%\n",
      "Epoch [2/2], Step [1994/3732], Loss: 3.9549, accuracy: 13.1571%\n",
      "Epoch [2/2], Step [1995/3732], Loss: 3.6176, accuracy: 13.1548%\n",
      "Epoch [2/2], Step [1996/3732], Loss: 3.5203, accuracy: 13.1547%\n",
      "Epoch [2/2], Step [1997/3732], Loss: 3.2786, accuracy: 13.1546%\n",
      "Epoch [2/2], Step [1998/3732], Loss: 3.2144, accuracy: 13.1566%\n",
      "Epoch [2/2], Step [1999/3732], Loss: 2.8673, accuracy: 13.1565%\n",
      "Epoch [2/2], Step [2000/3732], Loss: 3.6212, accuracy: 13.1542%\n",
      "Epoch [2/2], Step [2001/3732], Loss: 2.6908, accuracy: 13.1541%\n",
      "Epoch [2/2], Step [2002/3732], Loss: 3.1921, accuracy: 13.1540%\n",
      "Epoch [2/2], Step [2003/3732], Loss: 2.8664, accuracy: 13.1539%\n",
      "Epoch [2/2], Step [2004/3732], Loss: 2.7696, accuracy: 13.1581%\n",
      "Epoch [2/2], Step [2005/3732], Loss: 3.0779, accuracy: 13.1602%\n",
      "Epoch [2/2], Step [2006/3732], Loss: 2.8095, accuracy: 13.1644%\n",
      "Epoch [2/2], Step [2007/3732], Loss: 2.5710, accuracy: 13.1687%\n",
      "Epoch [2/2], Step [2008/3732], Loss: 2.7418, accuracy: 13.1729%\n",
      "Epoch [2/2], Step [2009/3732], Loss: 3.8458, accuracy: 13.1728%\n",
      "Epoch [2/2], Step [2010/3732], Loss: 2.5609, accuracy: 13.1770%\n",
      "Epoch [2/2], Step [2011/3732], Loss: 2.5178, accuracy: 13.1813%\n",
      "Epoch [2/2], Step [2012/3732], Loss: 3.4949, accuracy: 13.1811%\n",
      "Epoch [2/2], Step [2013/3732], Loss: 3.2669, accuracy: 13.1810%\n",
      "Epoch [2/2], Step [2014/3732], Loss: 3.3593, accuracy: 13.1787%\n",
      "Epoch [2/2], Step [2015/3732], Loss: 3.7626, accuracy: 13.1786%\n",
      "Epoch [2/2], Step [2016/3732], Loss: 3.4502, accuracy: 13.1785%\n",
      "Epoch [2/2], Step [2017/3732], Loss: 3.5237, accuracy: 13.1762%\n",
      "Epoch [2/2], Step [2018/3732], Loss: 3.5039, accuracy: 13.1761%\n",
      "Epoch [2/2], Step [2019/3732], Loss: 3.3480, accuracy: 13.1760%\n",
      "Epoch [2/2], Step [2020/3732], Loss: 3.6839, accuracy: 13.1737%\n",
      "Epoch [2/2], Step [2021/3732], Loss: 2.7101, accuracy: 13.1779%\n",
      "Epoch [2/2], Step [2022/3732], Loss: 2.3657, accuracy: 13.1843%\n",
      "Epoch [2/2], Step [2023/3732], Loss: 3.0242, accuracy: 13.1885%\n",
      "Epoch [2/2], Step [2024/3732], Loss: 4.1293, accuracy: 13.1862%\n",
      "Epoch [2/2], Step [2025/3732], Loss: 3.1152, accuracy: 13.1839%\n",
      "Epoch [2/2], Step [2026/3732], Loss: 3.2058, accuracy: 13.1838%\n",
      "Epoch [2/2], Step [2027/3732], Loss: 3.0070, accuracy: 13.1815%\n",
      "Epoch [2/2], Step [2028/3732], Loss: 3.2541, accuracy: 13.1858%\n",
      "Epoch [2/2], Step [2029/3732], Loss: 2.8252, accuracy: 13.1900%\n",
      "Epoch [2/2], Step [2030/3732], Loss: 3.1485, accuracy: 13.1899%\n",
      "Epoch [2/2], Step [2031/3732], Loss: 3.1487, accuracy: 13.1941%\n",
      "Epoch [2/2], Step [2032/3732], Loss: 2.9112, accuracy: 13.1918%\n",
      "Epoch [2/2], Step [2033/3732], Loss: 2.8636, accuracy: 13.1938%\n",
      "Epoch [2/2], Step [2034/3732], Loss: 3.0236, accuracy: 13.1959%\n",
      "Epoch [2/2], Step [2035/3732], Loss: 3.5991, accuracy: 13.1958%\n",
      "Epoch [2/2], Step [2036/3732], Loss: 2.9390, accuracy: 13.1935%\n",
      "Epoch [2/2], Step [2037/3732], Loss: 2.9267, accuracy: 13.1977%\n",
      "Epoch [2/2], Step [2038/3732], Loss: 2.6392, accuracy: 13.1997%\n",
      "Epoch [2/2], Step [2039/3732], Loss: 2.9785, accuracy: 13.2040%\n",
      "Epoch [2/2], Step [2040/3732], Loss: 2.8619, accuracy: 13.2060%\n",
      "Epoch [2/2], Step [2041/3732], Loss: 2.5814, accuracy: 13.2080%\n",
      "Epoch [2/2], Step [2042/3732], Loss: 3.2845, accuracy: 13.2057%\n",
      "Epoch [2/2], Step [2043/3732], Loss: 2.7874, accuracy: 13.2056%\n",
      "Epoch [2/2], Step [2044/3732], Loss: 2.8131, accuracy: 13.2077%\n",
      "Epoch [2/2], Step [2045/3732], Loss: 2.9976, accuracy: 13.2097%\n",
      "Epoch [2/2], Step [2046/3732], Loss: 3.2591, accuracy: 13.2139%\n",
      "Epoch [2/2], Step [2047/3732], Loss: 2.8486, accuracy: 13.2138%\n",
      "Epoch [2/2], Step [2048/3732], Loss: 2.8671, accuracy: 13.2137%\n",
      "Epoch [2/2], Step [2049/3732], Loss: 3.5612, accuracy: 13.2114%\n",
      "Epoch [2/2], Step [2050/3732], Loss: 2.6335, accuracy: 13.2134%\n",
      "Epoch [2/2], Step [2051/3732], Loss: 2.0889, accuracy: 13.2198%\n",
      "Epoch [2/2], Step [2052/3732], Loss: 2.9176, accuracy: 13.2218%\n",
      "Epoch [2/2], Step [2053/3732], Loss: 2.8765, accuracy: 13.2260%\n",
      "Epoch [2/2], Step [2054/3732], Loss: 2.8937, accuracy: 13.2324%\n",
      "Epoch [2/2], Step [2055/3732], Loss: 3.5124, accuracy: 13.2322%\n",
      "Epoch [2/2], Step [2056/3732], Loss: 2.7653, accuracy: 13.2343%\n",
      "Epoch [2/2], Step [2057/3732], Loss: 2.8553, accuracy: 13.2363%\n",
      "Epoch [2/2], Step [2058/3732], Loss: 3.0670, accuracy: 13.2362%\n",
      "Epoch [2/2], Step [2059/3732], Loss: 3.8052, accuracy: 13.2361%\n",
      "Epoch [2/2], Step [2060/3732], Loss: 3.7140, accuracy: 13.2359%\n",
      "Epoch [2/2], Step [2061/3732], Loss: 3.8395, accuracy: 13.2336%\n",
      "Epoch [2/2], Step [2062/3732], Loss: 2.7179, accuracy: 13.2335%\n",
      "Epoch [2/2], Step [2063/3732], Loss: 2.8727, accuracy: 13.2355%\n",
      "Epoch [2/2], Step [2064/3732], Loss: 2.4269, accuracy: 13.2376%\n",
      "Epoch [2/2], Step [2065/3732], Loss: 3.4234, accuracy: 13.2418%\n",
      "Epoch [2/2], Step [2066/3732], Loss: 2.6231, accuracy: 13.2438%\n",
      "Epoch [2/2], Step [2067/3732], Loss: 3.0780, accuracy: 13.2458%\n",
      "Epoch [2/2], Step [2068/3732], Loss: 2.1745, accuracy: 13.2478%\n",
      "Epoch [2/2], Step [2069/3732], Loss: 2.7527, accuracy: 13.2520%\n",
      "Epoch [2/2], Step [2070/3732], Loss: 4.0535, accuracy: 13.2497%\n",
      "Epoch [2/2], Step [2071/3732], Loss: 2.8597, accuracy: 13.2496%\n",
      "Epoch [2/2], Step [2072/3732], Loss: 3.6250, accuracy: 13.2495%\n",
      "Epoch [2/2], Step [2073/3732], Loss: 3.1973, accuracy: 13.2494%\n",
      "Epoch [2/2], Step [2074/3732], Loss: 3.0046, accuracy: 13.2514%\n",
      "Epoch [2/2], Step [2075/3732], Loss: 2.4827, accuracy: 13.2556%\n",
      "Epoch [2/2], Step [2076/3732], Loss: 3.0070, accuracy: 13.2533%\n",
      "Epoch [2/2], Step [2077/3732], Loss: 2.9030, accuracy: 13.2553%\n",
      "Epoch [2/2], Step [2078/3732], Loss: 3.1239, accuracy: 13.2552%\n",
      "Epoch [2/2], Step [2079/3732], Loss: 2.6351, accuracy: 13.2550%\n",
      "Epoch [2/2], Step [2080/3732], Loss: 3.2545, accuracy: 13.2549%\n",
      "Epoch [2/2], Step [2081/3732], Loss: 3.0229, accuracy: 13.2591%\n",
      "Epoch [2/2], Step [2082/3732], Loss: 2.8000, accuracy: 13.2611%\n",
      "Epoch [2/2], Step [2083/3732], Loss: 2.5712, accuracy: 13.2653%\n",
      "Epoch [2/2], Step [2084/3732], Loss: 3.3202, accuracy: 13.2651%\n",
      "Epoch [2/2], Step [2085/3732], Loss: 2.7793, accuracy: 13.2693%\n",
      "Epoch [2/2], Step [2086/3732], Loss: 3.4613, accuracy: 13.2692%\n",
      "Epoch [2/2], Step [2087/3732], Loss: 2.7233, accuracy: 13.2712%\n",
      "Epoch [2/2], Step [2088/3732], Loss: 3.0395, accuracy: 13.2689%\n",
      "Epoch [2/2], Step [2089/3732], Loss: 2.7503, accuracy: 13.2709%\n",
      "Epoch [2/2], Step [2090/3732], Loss: 3.1556, accuracy: 13.2708%\n",
      "Epoch [2/2], Step [2091/3732], Loss: 3.7523, accuracy: 13.2685%\n",
      "Epoch [2/2], Step [2092/3732], Loss: 2.5091, accuracy: 13.2727%\n",
      "Epoch [2/2], Step [2093/3732], Loss: 3.8343, accuracy: 13.2747%\n",
      "Epoch [2/2], Step [2094/3732], Loss: 2.6335, accuracy: 13.2767%\n",
      "Epoch [2/2], Step [2095/3732], Loss: 3.6509, accuracy: 13.2808%\n",
      "Epoch [2/2], Step [2096/3732], Loss: 2.9910, accuracy: 13.2829%\n",
      "Epoch [2/2], Step [2097/3732], Loss: 3.1675, accuracy: 13.2827%\n",
      "Epoch [2/2], Step [2098/3732], Loss: 2.8150, accuracy: 13.2869%\n",
      "Epoch [2/2], Step [2099/3732], Loss: 3.2079, accuracy: 13.2889%\n",
      "Epoch [2/2], Step [2100/3732], Loss: 3.5760, accuracy: 13.2888%\n",
      "Epoch [2/2], Step [2101/3732], Loss: 2.9364, accuracy: 13.2886%\n",
      "Epoch [2/2], Step [2102/3732], Loss: 3.4055, accuracy: 13.2863%\n",
      "Epoch [2/2], Step [2103/3732], Loss: 2.9675, accuracy: 13.2905%\n",
      "Epoch [2/2], Step [2104/3732], Loss: 2.5397, accuracy: 13.2925%\n",
      "Epoch [2/2], Step [2105/3732], Loss: 3.1746, accuracy: 13.2924%\n",
      "Epoch [2/2], Step [2106/3732], Loss: 2.9020, accuracy: 13.2901%\n",
      "Epoch [2/2], Step [2107/3732], Loss: 2.7628, accuracy: 13.2921%\n",
      "Epoch [2/2], Step [2108/3732], Loss: 2.6459, accuracy: 13.2941%\n",
      "Epoch [2/2], Step [2109/3732], Loss: 3.0967, accuracy: 13.2940%\n",
      "Epoch [2/2], Step [2110/3732], Loss: 3.0567, accuracy: 13.2938%\n",
      "Epoch [2/2], Step [2111/3732], Loss: 3.2790, accuracy: 13.2915%\n",
      "Epoch [2/2], Step [2112/3732], Loss: 3.3601, accuracy: 13.2935%\n",
      "Epoch [2/2], Step [2113/3732], Loss: 3.0508, accuracy: 13.2977%\n",
      "Epoch [2/2], Step [2114/3732], Loss: 2.5756, accuracy: 13.2997%\n",
      "Epoch [2/2], Step [2115/3732], Loss: 3.6540, accuracy: 13.2996%\n",
      "Epoch [2/2], Step [2116/3732], Loss: 3.3174, accuracy: 13.2994%\n",
      "Epoch [2/2], Step [2117/3732], Loss: 3.7373, accuracy: 13.3014%\n",
      "Epoch [2/2], Step [2118/3732], Loss: 2.5695, accuracy: 13.3077%\n",
      "Epoch [2/2], Step [2119/3732], Loss: 3.1187, accuracy: 13.3076%\n",
      "Epoch [2/2], Step [2120/3732], Loss: 2.6507, accuracy: 13.3096%\n",
      "Epoch [2/2], Step [2121/3732], Loss: 3.4790, accuracy: 13.3073%\n",
      "Epoch [2/2], Step [2122/3732], Loss: 3.0510, accuracy: 13.3093%\n",
      "Epoch [2/2], Step [2123/3732], Loss: 2.5898, accuracy: 13.3134%\n",
      "Epoch [2/2], Step [2124/3732], Loss: 2.7482, accuracy: 13.3133%\n",
      "Epoch [2/2], Step [2125/3732], Loss: 3.5572, accuracy: 13.3131%\n",
      "Epoch [2/2], Step [2126/3732], Loss: 2.9325, accuracy: 13.3130%\n",
      "Epoch [2/2], Step [2127/3732], Loss: 4.2037, accuracy: 13.3129%\n",
      "Epoch [2/2], Step [2128/3732], Loss: 3.3502, accuracy: 13.3127%\n",
      "Epoch [2/2], Step [2129/3732], Loss: 3.5197, accuracy: 13.3147%\n",
      "Epoch [2/2], Step [2130/3732], Loss: 2.2529, accuracy: 13.3167%\n",
      "Epoch [2/2], Step [2131/3732], Loss: 3.1503, accuracy: 13.3187%\n",
      "Epoch [2/2], Step [2132/3732], Loss: 3.6303, accuracy: 13.3164%\n",
      "Epoch [2/2], Step [2133/3732], Loss: 3.1671, accuracy: 13.3184%\n",
      "Epoch [2/2], Step [2134/3732], Loss: 3.1843, accuracy: 13.3183%\n",
      "Epoch [2/2], Step [2135/3732], Loss: 2.7231, accuracy: 13.3245%\n",
      "Epoch [2/2], Step [2136/3732], Loss: 3.3276, accuracy: 13.3223%\n",
      "Epoch [2/2], Step [2137/3732], Loss: 2.6226, accuracy: 13.3264%\n",
      "Epoch [2/2], Step [2138/3732], Loss: 3.1851, accuracy: 13.3284%\n",
      "Epoch [2/2], Step [2139/3732], Loss: 3.4426, accuracy: 13.3282%\n",
      "Epoch [2/2], Step [2140/3732], Loss: 3.6509, accuracy: 13.3260%\n",
      "Epoch [2/2], Step [2141/3732], Loss: 3.2124, accuracy: 13.3279%\n",
      "Epoch [2/2], Step [2142/3732], Loss: 3.1221, accuracy: 13.3299%\n",
      "Epoch [2/2], Step [2143/3732], Loss: 2.3128, accuracy: 13.3362%\n",
      "Epoch [2/2], Step [2144/3732], Loss: 3.3215, accuracy: 13.3339%\n",
      "Epoch [2/2], Step [2145/3732], Loss: 3.3610, accuracy: 13.3316%\n",
      "Epoch [2/2], Step [2146/3732], Loss: 3.1283, accuracy: 13.3315%\n",
      "Epoch [2/2], Step [2147/3732], Loss: 3.1105, accuracy: 13.3313%\n",
      "Epoch [2/2], Step [2148/3732], Loss: 4.0118, accuracy: 13.3291%\n",
      "Epoch [2/2], Step [2149/3732], Loss: 2.3961, accuracy: 13.3311%\n",
      "Epoch [2/2], Step [2150/3732], Loss: 3.2122, accuracy: 13.3309%\n",
      "Epoch [2/2], Step [2151/3732], Loss: 2.8819, accuracy: 13.3329%\n",
      "Epoch [2/2], Step [2152/3732], Loss: 2.4999, accuracy: 13.3328%\n",
      "Epoch [2/2], Step [2153/3732], Loss: 4.0420, accuracy: 13.3305%\n",
      "Epoch [2/2], Step [2154/3732], Loss: 3.5110, accuracy: 13.3304%\n",
      "Epoch [2/2], Step [2155/3732], Loss: 3.5906, accuracy: 13.3281%\n",
      "Epoch [2/2], Step [2156/3732], Loss: 2.9272, accuracy: 13.3301%\n",
      "Epoch [2/2], Step [2157/3732], Loss: 3.6398, accuracy: 13.3278%\n",
      "Epoch [2/2], Step [2158/3732], Loss: 2.9498, accuracy: 13.3319%\n",
      "Epoch [2/2], Step [2159/3732], Loss: 3.3962, accuracy: 13.3318%\n",
      "Epoch [2/2], Step [2160/3732], Loss: 2.5205, accuracy: 13.3380%\n",
      "Epoch [2/2], Step [2161/3732], Loss: 2.8172, accuracy: 13.3400%\n",
      "Epoch [2/2], Step [2162/3732], Loss: 3.4848, accuracy: 13.3398%\n",
      "Epoch [2/2], Step [2163/3732], Loss: 3.2693, accuracy: 13.3376%\n",
      "Epoch [2/2], Step [2164/3732], Loss: 3.7303, accuracy: 13.3396%\n",
      "Epoch [2/2], Step [2165/3732], Loss: 2.4166, accuracy: 13.3458%\n",
      "Epoch [2/2], Step [2166/3732], Loss: 2.2952, accuracy: 13.3499%\n",
      "Epoch [2/2], Step [2167/3732], Loss: 4.1525, accuracy: 13.3476%\n",
      "Epoch [2/2], Step [2168/3732], Loss: 3.8563, accuracy: 13.3475%\n",
      "Epoch [2/2], Step [2169/3732], Loss: 2.2977, accuracy: 13.3537%\n",
      "Epoch [2/2], Step [2170/3732], Loss: 2.8706, accuracy: 13.3535%\n",
      "Epoch [2/2], Step [2171/3732], Loss: 2.4943, accuracy: 13.3576%\n",
      "Epoch [2/2], Step [2172/3732], Loss: 3.6277, accuracy: 13.3554%\n",
      "Epoch [2/2], Step [2173/3732], Loss: 4.2602, accuracy: 13.3531%\n",
      "Epoch [2/2], Step [2174/3732], Loss: 3.3668, accuracy: 13.3529%\n",
      "Epoch [2/2], Step [2175/3732], Loss: 2.6010, accuracy: 13.3528%\n",
      "Epoch [2/2], Step [2176/3732], Loss: 3.3366, accuracy: 13.3527%\n",
      "Epoch [2/2], Step [2177/3732], Loss: 3.0346, accuracy: 13.3546%\n",
      "Epoch [2/2], Step [2178/3732], Loss: 3.0707, accuracy: 13.3587%\n",
      "Epoch [2/2], Step [2179/3732], Loss: 3.6780, accuracy: 13.3586%\n",
      "Epoch [2/2], Step [2180/3732], Loss: 2.6731, accuracy: 13.3605%\n",
      "Epoch [2/2], Step [2181/3732], Loss: 3.2555, accuracy: 13.3604%\n",
      "Epoch [2/2], Step [2182/3732], Loss: 2.8904, accuracy: 13.3645%\n",
      "Epoch [2/2], Step [2183/3732], Loss: 3.4174, accuracy: 13.3686%\n",
      "Epoch [2/2], Step [2184/3732], Loss: 3.1922, accuracy: 13.3684%\n",
      "Epoch [2/2], Step [2185/3732], Loss: 2.4280, accuracy: 13.3725%\n",
      "Epoch [2/2], Step [2186/3732], Loss: 2.9267, accuracy: 13.3723%\n",
      "Epoch [2/2], Step [2187/3732], Loss: 3.3847, accuracy: 13.3722%\n",
      "Epoch [2/2], Step [2188/3732], Loss: 3.0880, accuracy: 13.3720%\n",
      "Epoch [2/2], Step [2189/3732], Loss: 3.3731, accuracy: 13.3698%\n",
      "Epoch [2/2], Step [2190/3732], Loss: 3.4306, accuracy: 13.3696%\n",
      "Epoch [2/2], Step [2191/3732], Loss: 2.8247, accuracy: 13.3674%\n",
      "Epoch [2/2], Step [2192/3732], Loss: 3.6647, accuracy: 13.3672%\n",
      "Epoch [2/2], Step [2193/3732], Loss: 2.8857, accuracy: 13.3713%\n",
      "Epoch [2/2], Step [2194/3732], Loss: 2.7117, accuracy: 13.3733%\n",
      "Epoch [2/2], Step [2195/3732], Loss: 2.9910, accuracy: 13.3752%\n",
      "Epoch [2/2], Step [2196/3732], Loss: 3.3083, accuracy: 13.3730%\n",
      "Epoch [2/2], Step [2197/3732], Loss: 2.9493, accuracy: 13.3770%\n",
      "Epoch [2/2], Step [2198/3732], Loss: 3.4085, accuracy: 13.3790%\n",
      "Epoch [2/2], Step [2199/3732], Loss: 3.8954, accuracy: 13.3767%\n",
      "Epoch [2/2], Step [2200/3732], Loss: 3.4514, accuracy: 13.3745%\n",
      "Epoch [2/2], Step [2201/3732], Loss: 2.9150, accuracy: 13.3786%\n",
      "Epoch [2/2], Step [2202/3732], Loss: 2.9442, accuracy: 13.3784%\n",
      "Epoch [2/2], Step [2203/3732], Loss: 2.8748, accuracy: 13.3804%\n",
      "Epoch [2/2], Step [2204/3732], Loss: 3.9111, accuracy: 13.3802%\n",
      "Epoch [2/2], Step [2205/3732], Loss: 3.3486, accuracy: 13.3801%\n",
      "Epoch [2/2], Step [2206/3732], Loss: 2.8127, accuracy: 13.3841%\n",
      "Epoch [2/2], Step [2207/3732], Loss: 2.7135, accuracy: 13.3840%\n",
      "Epoch [2/2], Step [2208/3732], Loss: 2.4911, accuracy: 13.3838%\n",
      "Epoch [2/2], Step [2209/3732], Loss: 2.9515, accuracy: 13.3879%\n",
      "Epoch [2/2], Step [2210/3732], Loss: 2.9982, accuracy: 13.3899%\n",
      "Epoch [2/2], Step [2211/3732], Loss: 3.2804, accuracy: 13.3918%\n",
      "Epoch [2/2], Step [2212/3732], Loss: 3.1178, accuracy: 13.3938%\n",
      "Epoch [2/2], Step [2213/3732], Loss: 2.3997, accuracy: 13.3978%\n",
      "Epoch [2/2], Step [2214/3732], Loss: 2.2773, accuracy: 13.4019%\n",
      "Epoch [2/2], Step [2215/3732], Loss: 3.4458, accuracy: 13.3996%\n",
      "Epoch [2/2], Step [2216/3732], Loss: 2.8660, accuracy: 13.3995%\n",
      "Epoch [2/2], Step [2217/3732], Loss: 2.6421, accuracy: 13.4014%\n",
      "Epoch [2/2], Step [2218/3732], Loss: 3.3663, accuracy: 13.4055%\n",
      "Epoch [2/2], Step [2219/3732], Loss: 3.6434, accuracy: 13.4032%\n",
      "Epoch [2/2], Step [2220/3732], Loss: 2.1079, accuracy: 13.4094%\n",
      "Epoch [2/2], Step [2221/3732], Loss: 3.2263, accuracy: 13.4071%\n",
      "Epoch [2/2], Step [2222/3732], Loss: 3.1541, accuracy: 13.4070%\n",
      "Epoch [2/2], Step [2223/3732], Loss: 2.5442, accuracy: 13.4089%\n",
      "Epoch [2/2], Step [2224/3732], Loss: 2.2100, accuracy: 13.4150%\n",
      "Epoch [2/2], Step [2225/3732], Loss: 2.7275, accuracy: 13.4191%\n",
      "Epoch [2/2], Step [2226/3732], Loss: 2.8411, accuracy: 13.4210%\n",
      "Epoch [2/2], Step [2227/3732], Loss: 3.2342, accuracy: 13.4230%\n",
      "Epoch [2/2], Step [2228/3732], Loss: 4.9170, accuracy: 13.4207%\n",
      "Epoch [2/2], Step [2229/3732], Loss: 3.1946, accuracy: 13.4227%\n",
      "Epoch [2/2], Step [2230/3732], Loss: 3.8034, accuracy: 13.4246%\n",
      "Epoch [2/2], Step [2231/3732], Loss: 2.7193, accuracy: 13.4245%\n",
      "Epoch [2/2], Step [2232/3732], Loss: 2.8798, accuracy: 13.4243%\n",
      "Epoch [2/2], Step [2233/3732], Loss: 2.9455, accuracy: 13.4262%\n",
      "Epoch [2/2], Step [2234/3732], Loss: 2.5589, accuracy: 13.4303%\n",
      "Epoch [2/2], Step [2235/3732], Loss: 3.6344, accuracy: 13.4280%\n",
      "Epoch [2/2], Step [2236/3732], Loss: 3.1154, accuracy: 13.4279%\n",
      "Epoch [2/2], Step [2237/3732], Loss: 2.2166, accuracy: 13.4340%\n",
      "Epoch [2/2], Step [2238/3732], Loss: 3.0429, accuracy: 13.4317%\n",
      "Epoch [2/2], Step [2239/3732], Loss: 2.8483, accuracy: 13.4337%\n",
      "Epoch [2/2], Step [2240/3732], Loss: 3.7920, accuracy: 13.4314%\n",
      "Epoch [2/2], Step [2241/3732], Loss: 2.9841, accuracy: 13.4313%\n",
      "Epoch [2/2], Step [2242/3732], Loss: 3.0515, accuracy: 13.4332%\n",
      "Epoch [2/2], Step [2243/3732], Loss: 2.5013, accuracy: 13.4372%\n",
      "Epoch [2/2], Step [2244/3732], Loss: 3.4833, accuracy: 13.4371%\n",
      "Epoch [2/2], Step [2245/3732], Loss: 3.1323, accuracy: 13.4369%\n",
      "Epoch [2/2], Step [2246/3732], Loss: 4.9187, accuracy: 13.4368%\n",
      "Epoch [2/2], Step [2247/3732], Loss: 3.0333, accuracy: 13.4387%\n",
      "Epoch [2/2], Step [2248/3732], Loss: 3.2950, accuracy: 13.4406%\n",
      "Epoch [2/2], Step [2249/3732], Loss: 2.7824, accuracy: 13.4426%\n",
      "Epoch [2/2], Step [2250/3732], Loss: 2.9360, accuracy: 13.4466%\n",
      "Epoch [2/2], Step [2251/3732], Loss: 2.8048, accuracy: 13.4485%\n",
      "Epoch [2/2], Step [2252/3732], Loss: 2.8770, accuracy: 13.4505%\n",
      "Epoch [2/2], Step [2253/3732], Loss: 2.8327, accuracy: 13.4503%\n",
      "Epoch [2/2], Step [2254/3732], Loss: 2.0224, accuracy: 13.4543%\n",
      "Epoch [2/2], Step [2255/3732], Loss: 3.2312, accuracy: 13.4583%\n",
      "Epoch [2/2], Step [2256/3732], Loss: 3.1338, accuracy: 13.4603%\n",
      "Epoch [2/2], Step [2257/3732], Loss: 2.8158, accuracy: 13.4622%\n",
      "Epoch [2/2], Step [2258/3732], Loss: 2.3631, accuracy: 13.4641%\n",
      "Epoch [2/2], Step [2259/3732], Loss: 2.8901, accuracy: 13.4639%\n",
      "Epoch [2/2], Step [2260/3732], Loss: 3.5256, accuracy: 13.4617%\n",
      "Epoch [2/2], Step [2261/3732], Loss: 3.0525, accuracy: 13.4615%\n",
      "Epoch [2/2], Step [2262/3732], Loss: 2.9849, accuracy: 13.4593%\n",
      "Epoch [2/2], Step [2263/3732], Loss: 2.6390, accuracy: 13.4633%\n",
      "Epoch [2/2], Step [2264/3732], Loss: 2.8243, accuracy: 13.4673%\n",
      "Epoch [2/2], Step [2265/3732], Loss: 3.3655, accuracy: 13.4672%\n",
      "Epoch [2/2], Step [2266/3732], Loss: 2.6265, accuracy: 13.4712%\n",
      "Epoch [2/2], Step [2267/3732], Loss: 2.8436, accuracy: 13.4710%\n",
      "Epoch [2/2], Step [2268/3732], Loss: 2.8917, accuracy: 13.4687%\n",
      "Epoch [2/2], Step [2269/3732], Loss: 2.5635, accuracy: 13.4728%\n",
      "Epoch [2/2], Step [2270/3732], Loss: 2.4517, accuracy: 13.4768%\n",
      "Epoch [2/2], Step [2271/3732], Loss: 3.3309, accuracy: 13.4766%\n",
      "Epoch [2/2], Step [2272/3732], Loss: 3.1403, accuracy: 13.4744%\n",
      "Epoch [2/2], Step [2273/3732], Loss: 2.6652, accuracy: 13.4763%\n",
      "Epoch [2/2], Step [2274/3732], Loss: 3.5354, accuracy: 13.4782%\n",
      "Epoch [2/2], Step [2275/3732], Loss: 2.5940, accuracy: 13.4801%\n",
      "Epoch [2/2], Step [2276/3732], Loss: 3.0558, accuracy: 13.4799%\n",
      "Epoch [2/2], Step [2277/3732], Loss: 3.0681, accuracy: 13.4798%\n",
      "Epoch [2/2], Step [2278/3732], Loss: 2.8407, accuracy: 13.4838%\n",
      "Epoch [2/2], Step [2279/3732], Loss: 3.7654, accuracy: 13.4836%\n",
      "Epoch [2/2], Step [2280/3732], Loss: 2.5412, accuracy: 13.4897%\n",
      "Epoch [2/2], Step [2281/3732], Loss: 3.8356, accuracy: 13.4874%\n",
      "Epoch [2/2], Step [2282/3732], Loss: 2.6736, accuracy: 13.4873%\n",
      "Epoch [2/2], Step [2283/3732], Loss: 2.2338, accuracy: 13.4892%\n",
      "Epoch [2/2], Step [2284/3732], Loss: 3.0162, accuracy: 13.4911%\n",
      "Epoch [2/2], Step [2285/3732], Loss: 2.7628, accuracy: 13.4909%\n",
      "Epoch [2/2], Step [2286/3732], Loss: 2.5729, accuracy: 13.4908%\n",
      "Epoch [2/2], Step [2287/3732], Loss: 3.3503, accuracy: 13.4927%\n",
      "Epoch [2/2], Step [2288/3732], Loss: 2.6321, accuracy: 13.4925%\n",
      "Epoch [2/2], Step [2289/3732], Loss: 3.1084, accuracy: 13.4944%\n",
      "Epoch [2/2], Step [2290/3732], Loss: 2.2194, accuracy: 13.5005%\n",
      "Epoch [2/2], Step [2291/3732], Loss: 3.7269, accuracy: 13.4983%\n",
      "Epoch [2/2], Step [2292/3732], Loss: 3.2612, accuracy: 13.4960%\n",
      "Epoch [2/2], Step [2293/3732], Loss: 2.7907, accuracy: 13.4979%\n",
      "Epoch [2/2], Step [2294/3732], Loss: 3.3603, accuracy: 13.4978%\n",
      "Epoch [2/2], Step [2295/3732], Loss: 3.0417, accuracy: 13.4976%\n",
      "Epoch [2/2], Step [2296/3732], Loss: 2.2709, accuracy: 13.4995%\n",
      "Epoch [2/2], Step [2297/3732], Loss: 3.8093, accuracy: 13.4993%\n",
      "Epoch [2/2], Step [2298/3732], Loss: 3.0995, accuracy: 13.4971%\n",
      "Epoch [2/2], Step [2299/3732], Loss: 3.2768, accuracy: 13.5011%\n",
      "Epoch [2/2], Step [2300/3732], Loss: 3.5280, accuracy: 13.5030%\n",
      "Epoch [2/2], Step [2301/3732], Loss: 2.9292, accuracy: 13.5049%\n",
      "Epoch [2/2], Step [2302/3732], Loss: 3.9099, accuracy: 13.5068%\n",
      "Epoch [2/2], Step [2303/3732], Loss: 2.8675, accuracy: 13.5108%\n",
      "Epoch [2/2], Step [2304/3732], Loss: 3.6504, accuracy: 13.5085%\n",
      "Epoch [2/2], Step [2305/3732], Loss: 2.9024, accuracy: 13.5084%\n",
      "Epoch [2/2], Step [2306/3732], Loss: 3.1522, accuracy: 13.5082%\n",
      "Epoch [2/2], Step [2307/3732], Loss: 2.5010, accuracy: 13.5080%\n",
      "Epoch [2/2], Step [2308/3732], Loss: 3.3311, accuracy: 13.5058%\n",
      "Epoch [2/2], Step [2309/3732], Loss: 3.2807, accuracy: 13.5036%\n",
      "Epoch [2/2], Step [2310/3732], Loss: 3.1052, accuracy: 13.5055%\n",
      "Epoch [2/2], Step [2311/3732], Loss: 3.4384, accuracy: 13.5053%\n",
      "Epoch [2/2], Step [2312/3732], Loss: 2.6518, accuracy: 13.5072%\n",
      "Epoch [2/2], Step [2313/3732], Loss: 2.4834, accuracy: 13.5091%\n",
      "Epoch [2/2], Step [2314/3732], Loss: 3.2796, accuracy: 13.5110%\n",
      "Epoch [2/2], Step [2315/3732], Loss: 2.4503, accuracy: 13.5150%\n",
      "Epoch [2/2], Step [2316/3732], Loss: 3.0419, accuracy: 13.5169%\n",
      "Epoch [2/2], Step [2317/3732], Loss: 3.4288, accuracy: 13.5188%\n",
      "Epoch [2/2], Step [2318/3732], Loss: 2.9724, accuracy: 13.5186%\n",
      "Epoch [2/2], Step [2319/3732], Loss: 2.9844, accuracy: 13.5184%\n",
      "Epoch [2/2], Step [2320/3732], Loss: 3.1828, accuracy: 13.5162%\n",
      "Epoch [2/2], Step [2321/3732], Loss: 3.1661, accuracy: 13.5160%\n",
      "Epoch [2/2], Step [2322/3732], Loss: 2.7982, accuracy: 13.5200%\n",
      "Epoch [2/2], Step [2323/3732], Loss: 2.8035, accuracy: 13.5239%\n",
      "Epoch [2/2], Step [2324/3732], Loss: 3.1259, accuracy: 13.5258%\n",
      "Epoch [2/2], Step [2325/3732], Loss: 2.8976, accuracy: 13.5277%\n",
      "Epoch [2/2], Step [2326/3732], Loss: 2.8032, accuracy: 13.5276%\n",
      "Epoch [2/2], Step [2327/3732], Loss: 2.7876, accuracy: 13.5274%\n",
      "Epoch [2/2], Step [2328/3732], Loss: 3.5220, accuracy: 13.5272%\n",
      "Epoch [2/2], Step [2329/3732], Loss: 3.6282, accuracy: 13.5271%\n",
      "Epoch [2/2], Step [2330/3732], Loss: 2.1150, accuracy: 13.5351%\n",
      "Epoch [2/2], Step [2331/3732], Loss: 3.2835, accuracy: 13.5350%\n",
      "Epoch [2/2], Step [2332/3732], Loss: 3.6248, accuracy: 13.5327%\n",
      "Epoch [2/2], Step [2333/3732], Loss: 2.5945, accuracy: 13.5326%\n",
      "Epoch [2/2], Step [2334/3732], Loss: 3.1261, accuracy: 13.5324%\n",
      "Epoch [2/2], Step [2335/3732], Loss: 3.3318, accuracy: 13.5343%\n",
      "Epoch [2/2], Step [2336/3732], Loss: 2.9501, accuracy: 13.5321%\n",
      "Epoch [2/2], Step [2337/3732], Loss: 2.6116, accuracy: 13.5339%\n",
      "Epoch [2/2], Step [2338/3732], Loss: 3.5776, accuracy: 13.5317%\n",
      "Epoch [2/2], Step [2339/3732], Loss: 3.6079, accuracy: 13.5295%\n",
      "Epoch [2/2], Step [2340/3732], Loss: 2.4627, accuracy: 13.5334%\n",
      "Epoch [2/2], Step [2341/3732], Loss: 3.5314, accuracy: 13.5333%\n",
      "Epoch [2/2], Step [2342/3732], Loss: 2.9125, accuracy: 13.5351%\n",
      "Epoch [2/2], Step [2343/3732], Loss: 3.9516, accuracy: 13.5350%\n",
      "Epoch [2/2], Step [2344/3732], Loss: 2.6970, accuracy: 13.5369%\n",
      "Epoch [2/2], Step [2345/3732], Loss: 2.6673, accuracy: 13.5388%\n",
      "Epoch [2/2], Step [2346/3732], Loss: 3.7066, accuracy: 13.5365%\n",
      "Epoch [2/2], Step [2347/3732], Loss: 2.5702, accuracy: 13.5405%\n",
      "Epoch [2/2], Step [2348/3732], Loss: 3.2402, accuracy: 13.5403%\n",
      "Epoch [2/2], Step [2349/3732], Loss: 3.6774, accuracy: 13.5401%\n",
      "Epoch [2/2], Step [2350/3732], Loss: 3.2477, accuracy: 13.5420%\n",
      "Epoch [2/2], Step [2351/3732], Loss: 2.8346, accuracy: 13.5418%\n",
      "Epoch [2/2], Step [2352/3732], Loss: 3.0127, accuracy: 13.5417%\n",
      "Epoch [2/2], Step [2353/3732], Loss: 2.9967, accuracy: 13.5435%\n",
      "Epoch [2/2], Step [2354/3732], Loss: 2.6699, accuracy: 13.5454%\n",
      "Epoch [2/2], Step [2355/3732], Loss: 2.5640, accuracy: 13.5494%\n",
      "Epoch [2/2], Step [2356/3732], Loss: 3.1557, accuracy: 13.5492%\n",
      "Epoch [2/2], Step [2357/3732], Loss: 3.2212, accuracy: 13.5490%\n",
      "Epoch [2/2], Step [2358/3732], Loss: 3.0597, accuracy: 13.5509%\n",
      "Epoch [2/2], Step [2359/3732], Loss: 3.1613, accuracy: 13.5528%\n",
      "Epoch [2/2], Step [2360/3732], Loss: 3.0045, accuracy: 13.5547%\n",
      "Epoch [2/2], Step [2361/3732], Loss: 3.3405, accuracy: 13.5545%\n",
      "Epoch [2/2], Step [2362/3732], Loss: 4.3067, accuracy: 13.5523%\n",
      "Epoch [2/2], Step [2363/3732], Loss: 2.2954, accuracy: 13.5582%\n",
      "Epoch [2/2], Step [2364/3732], Loss: 2.7810, accuracy: 13.5601%\n",
      "Epoch [2/2], Step [2365/3732], Loss: 2.7464, accuracy: 13.5620%\n",
      "Epoch [2/2], Step [2366/3732], Loss: 2.6093, accuracy: 13.5659%\n",
      "Epoch [2/2], Step [2367/3732], Loss: 2.9466, accuracy: 13.5637%\n",
      "Epoch [2/2], Step [2368/3732], Loss: 2.7769, accuracy: 13.5656%\n",
      "Epoch [2/2], Step [2369/3732], Loss: 3.2317, accuracy: 13.5674%\n",
      "Epoch [2/2], Step [2370/3732], Loss: 3.0802, accuracy: 13.5673%\n",
      "Epoch [2/2], Step [2371/3732], Loss: 3.2091, accuracy: 13.5671%\n",
      "Epoch [2/2], Step [2372/3732], Loss: 3.5855, accuracy: 13.5690%\n",
      "Epoch [2/2], Step [2373/3732], Loss: 3.1264, accuracy: 13.5708%\n",
      "Epoch [2/2], Step [2374/3732], Loss: 2.8582, accuracy: 13.5727%\n",
      "Epoch [2/2], Step [2375/3732], Loss: 2.7825, accuracy: 13.5766%\n",
      "Epoch [2/2], Step [2376/3732], Loss: 2.0672, accuracy: 13.5826%\n",
      "Epoch [2/2], Step [2377/3732], Loss: 2.3537, accuracy: 13.5886%\n",
      "Epoch [2/2], Step [2378/3732], Loss: 3.3587, accuracy: 13.5863%\n",
      "Epoch [2/2], Step [2379/3732], Loss: 2.3879, accuracy: 13.5882%\n",
      "Epoch [2/2], Step [2380/3732], Loss: 2.7040, accuracy: 13.5921%\n",
      "Epoch [2/2], Step [2381/3732], Loss: 2.9340, accuracy: 13.5940%\n",
      "Epoch [2/2], Step [2382/3732], Loss: 2.9660, accuracy: 13.5938%\n",
      "Epoch [2/2], Step [2383/3732], Loss: 3.3359, accuracy: 13.5936%\n",
      "Epoch [2/2], Step [2384/3732], Loss: 2.7756, accuracy: 13.5914%\n",
      "Epoch [2/2], Step [2385/3732], Loss: 2.9453, accuracy: 13.5933%\n",
      "Epoch [2/2], Step [2386/3732], Loss: 3.4580, accuracy: 13.5910%\n",
      "Epoch [2/2], Step [2387/3732], Loss: 2.7125, accuracy: 13.5950%\n",
      "Epoch [2/2], Step [2388/3732], Loss: 3.3153, accuracy: 13.5948%\n",
      "Epoch [2/2], Step [2389/3732], Loss: 2.9491, accuracy: 13.5946%\n",
      "Epoch [2/2], Step [2390/3732], Loss: 2.8536, accuracy: 13.5944%\n",
      "Epoch [2/2], Step [2391/3732], Loss: 3.6664, accuracy: 13.5942%\n",
      "Epoch [2/2], Step [2392/3732], Loss: 2.0888, accuracy: 13.6002%\n",
      "Epoch [2/2], Step [2393/3732], Loss: 3.0186, accuracy: 13.6000%\n",
      "Epoch [2/2], Step [2394/3732], Loss: 3.3280, accuracy: 13.6019%\n",
      "Epoch [2/2], Step [2395/3732], Loss: 3.2273, accuracy: 13.6037%\n",
      "Epoch [2/2], Step [2396/3732], Loss: 3.0842, accuracy: 13.6015%\n",
      "Epoch [2/2], Step [2397/3732], Loss: 3.1305, accuracy: 13.5993%\n",
      "Epoch [2/2], Step [2398/3732], Loss: 2.8630, accuracy: 13.6011%\n",
      "Epoch [2/2], Step [2399/3732], Loss: 3.8956, accuracy: 13.5989%\n",
      "Epoch [2/2], Step [2400/3732], Loss: 3.0279, accuracy: 13.6008%\n",
      "Epoch [2/2], Step [2401/3732], Loss: 3.3177, accuracy: 13.6006%\n",
      "Epoch [2/2], Step [2402/3732], Loss: 3.2231, accuracy: 13.6025%\n",
      "Epoch [2/2], Step [2403/3732], Loss: 2.9083, accuracy: 13.6023%\n",
      "Epoch [2/2], Step [2404/3732], Loss: 3.3251, accuracy: 13.6021%\n",
      "Epoch [2/2], Step [2405/3732], Loss: 2.5735, accuracy: 13.6060%\n",
      "Epoch [2/2], Step [2406/3732], Loss: 2.8998, accuracy: 13.6058%\n",
      "Epoch [2/2], Step [2407/3732], Loss: 4.0086, accuracy: 13.6036%\n",
      "Epoch [2/2], Step [2408/3732], Loss: 2.8454, accuracy: 13.6075%\n",
      "Epoch [2/2], Step [2409/3732], Loss: 3.0049, accuracy: 13.6114%\n",
      "Epoch [2/2], Step [2410/3732], Loss: 3.1646, accuracy: 13.6132%\n",
      "Epoch [2/2], Step [2411/3732], Loss: 3.1739, accuracy: 13.6151%\n",
      "Epoch [2/2], Step [2412/3732], Loss: 3.2701, accuracy: 13.6149%\n",
      "Epoch [2/2], Step [2413/3732], Loss: 3.2012, accuracy: 13.6127%\n",
      "Epoch [2/2], Step [2414/3732], Loss: 3.5332, accuracy: 13.6125%\n",
      "Epoch [2/2], Step [2415/3732], Loss: 3.5445, accuracy: 13.6103%\n",
      "Epoch [2/2], Step [2416/3732], Loss: 3.3641, accuracy: 13.6101%\n",
      "Epoch [2/2], Step [2417/3732], Loss: 3.5251, accuracy: 13.6079%\n",
      "Epoch [2/2], Step [2418/3732], Loss: 3.3007, accuracy: 13.6118%\n",
      "Epoch [2/2], Step [2419/3732], Loss: 2.7628, accuracy: 13.6096%\n",
      "Epoch [2/2], Step [2420/3732], Loss: 2.4841, accuracy: 13.6114%\n",
      "Epoch [2/2], Step [2421/3732], Loss: 3.0348, accuracy: 13.6133%\n",
      "Epoch [2/2], Step [2422/3732], Loss: 2.6618, accuracy: 13.6192%\n",
      "Epoch [2/2], Step [2423/3732], Loss: 2.8183, accuracy: 13.6231%\n",
      "Epoch [2/2], Step [2424/3732], Loss: 3.1980, accuracy: 13.6209%\n",
      "Epoch [2/2], Step [2425/3732], Loss: 3.1767, accuracy: 13.6227%\n",
      "Epoch [2/2], Step [2426/3732], Loss: 2.4143, accuracy: 13.6225%\n",
      "Epoch [2/2], Step [2427/3732], Loss: 3.3081, accuracy: 13.6244%\n",
      "Epoch [2/2], Step [2428/3732], Loss: 3.0227, accuracy: 13.6262%\n",
      "Epoch [2/2], Step [2429/3732], Loss: 3.4278, accuracy: 13.6240%\n",
      "Epoch [2/2], Step [2430/3732], Loss: 3.2870, accuracy: 13.6218%\n",
      "Epoch [2/2], Step [2431/3732], Loss: 3.1007, accuracy: 13.6257%\n",
      "Epoch [2/2], Step [2432/3732], Loss: 3.3527, accuracy: 13.6255%\n",
      "Epoch [2/2], Step [2433/3732], Loss: 3.6971, accuracy: 13.6273%\n",
      "Epoch [2/2], Step [2434/3732], Loss: 2.6040, accuracy: 13.6312%\n",
      "Epoch [2/2], Step [2435/3732], Loss: 2.4220, accuracy: 13.6351%\n",
      "Epoch [2/2], Step [2436/3732], Loss: 3.2812, accuracy: 13.6349%\n",
      "Epoch [2/2], Step [2437/3732], Loss: 3.3808, accuracy: 13.6327%\n",
      "Epoch [2/2], Step [2438/3732], Loss: 2.8466, accuracy: 13.6365%\n",
      "Epoch [2/2], Step [2439/3732], Loss: 3.9323, accuracy: 13.6343%\n",
      "Epoch [2/2], Step [2440/3732], Loss: 2.6204, accuracy: 13.6321%\n",
      "Epoch [2/2], Step [2441/3732], Loss: 2.9648, accuracy: 13.6380%\n",
      "Epoch [2/2], Step [2442/3732], Loss: 3.0760, accuracy: 13.6399%\n",
      "Epoch [2/2], Step [2443/3732], Loss: 2.9123, accuracy: 13.6417%\n",
      "Epoch [2/2], Step [2444/3732], Loss: 2.7002, accuracy: 13.6435%\n",
      "Epoch [2/2], Step [2445/3732], Loss: 1.4794, accuracy: 13.6535%\n",
      "Epoch [2/2], Step [2446/3732], Loss: 3.7185, accuracy: 13.6513%\n",
      "Epoch [2/2], Step [2447/3732], Loss: 3.4426, accuracy: 13.6511%\n",
      "Epoch [2/2], Step [2448/3732], Loss: 3.0174, accuracy: 13.6529%\n",
      "Epoch [2/2], Step [2449/3732], Loss: 2.8640, accuracy: 13.6527%\n",
      "Epoch [2/2], Step [2450/3732], Loss: 3.5947, accuracy: 13.6525%\n",
      "Epoch [2/2], Step [2451/3732], Loss: 3.7444, accuracy: 13.6524%\n",
      "Epoch [2/2], Step [2452/3732], Loss: 2.8952, accuracy: 13.6542%\n",
      "Epoch [2/2], Step [2453/3732], Loss: 2.8363, accuracy: 13.6560%\n",
      "Epoch [2/2], Step [2454/3732], Loss: 2.5087, accuracy: 13.6599%\n",
      "Epoch [2/2], Step [2455/3732], Loss: 3.0635, accuracy: 13.6597%\n",
      "Epoch [2/2], Step [2456/3732], Loss: 3.0930, accuracy: 13.6595%\n",
      "Epoch [2/2], Step [2457/3732], Loss: 3.2960, accuracy: 13.6593%\n",
      "Epoch [2/2], Step [2458/3732], Loss: 2.5517, accuracy: 13.6611%\n",
      "Epoch [2/2], Step [2459/3732], Loss: 3.1575, accuracy: 13.6610%\n",
      "Epoch [2/2], Step [2460/3732], Loss: 4.3404, accuracy: 13.6588%\n",
      "Epoch [2/2], Step [2461/3732], Loss: 2.9929, accuracy: 13.6586%\n",
      "Epoch [2/2], Step [2462/3732], Loss: 2.8833, accuracy: 13.6624%\n",
      "Epoch [2/2], Step [2463/3732], Loss: 2.8099, accuracy: 13.6642%\n",
      "Epoch [2/2], Step [2464/3732], Loss: 2.4069, accuracy: 13.6661%\n",
      "Epoch [2/2], Step [2465/3732], Loss: 3.3372, accuracy: 13.6639%\n",
      "Epoch [2/2], Step [2466/3732], Loss: 3.5450, accuracy: 13.6617%\n",
      "Epoch [2/2], Step [2467/3732], Loss: 3.2407, accuracy: 13.6615%\n",
      "Epoch [2/2], Step [2468/3732], Loss: 3.1151, accuracy: 13.6613%\n",
      "Epoch [2/2], Step [2469/3732], Loss: 2.1071, accuracy: 13.6631%\n",
      "Epoch [2/2], Step [2470/3732], Loss: 2.8577, accuracy: 13.6649%\n",
      "Epoch [2/2], Step [2471/3732], Loss: 2.7933, accuracy: 13.6648%\n",
      "Epoch [2/2], Step [2472/3732], Loss: 3.0770, accuracy: 13.6666%\n",
      "Epoch [2/2], Step [2473/3732], Loss: 2.8624, accuracy: 13.6664%\n",
      "Epoch [2/2], Step [2474/3732], Loss: 3.5942, accuracy: 13.6642%\n",
      "Epoch [2/2], Step [2475/3732], Loss: 2.5981, accuracy: 13.6700%\n",
      "Epoch [2/2], Step [2476/3732], Loss: 2.8379, accuracy: 13.6739%\n",
      "Epoch [2/2], Step [2477/3732], Loss: 2.1525, accuracy: 13.6797%\n",
      "Epoch [2/2], Step [2478/3732], Loss: 2.7034, accuracy: 13.6795%\n",
      "Epoch [2/2], Step [2479/3732], Loss: 2.9351, accuracy: 13.6814%\n",
      "Epoch [2/2], Step [2480/3732], Loss: 3.3024, accuracy: 13.6852%\n",
      "Epoch [2/2], Step [2481/3732], Loss: 3.5045, accuracy: 13.6850%\n",
      "Epoch [2/2], Step [2482/3732], Loss: 3.1213, accuracy: 13.6848%\n",
      "Epoch [2/2], Step [2483/3732], Loss: 2.8349, accuracy: 13.6866%\n",
      "Epoch [2/2], Step [2484/3732], Loss: 2.8575, accuracy: 13.6885%\n",
      "Epoch [2/2], Step [2485/3732], Loss: 2.8401, accuracy: 13.6923%\n",
      "Epoch [2/2], Step [2486/3732], Loss: 2.7174, accuracy: 13.6941%\n",
      "Epoch [2/2], Step [2487/3732], Loss: 3.1474, accuracy: 13.6959%\n",
      "Epoch [2/2], Step [2488/3732], Loss: 2.1978, accuracy: 13.6977%\n",
      "Epoch [2/2], Step [2489/3732], Loss: 2.9942, accuracy: 13.6976%\n",
      "Epoch [2/2], Step [2490/3732], Loss: 3.3718, accuracy: 13.6954%\n",
      "Epoch [2/2], Step [2491/3732], Loss: 2.4511, accuracy: 13.6952%\n",
      "Epoch [2/2], Step [2492/3732], Loss: 3.2764, accuracy: 13.6970%\n",
      "Epoch [2/2], Step [2493/3732], Loss: 3.2763, accuracy: 13.6968%\n",
      "Epoch [2/2], Step [2494/3732], Loss: 3.2789, accuracy: 13.6986%\n",
      "Epoch [2/2], Step [2495/3732], Loss: 3.7711, accuracy: 13.6964%\n",
      "Epoch [2/2], Step [2496/3732], Loss: 3.5710, accuracy: 13.6962%\n",
      "Epoch [2/2], Step [2497/3732], Loss: 4.0725, accuracy: 13.6940%\n",
      "Epoch [2/2], Step [2498/3732], Loss: 3.2453, accuracy: 13.6938%\n",
      "Epoch [2/2], Step [2499/3732], Loss: 2.7493, accuracy: 13.6936%\n",
      "Epoch [2/2], Step [2500/3732], Loss: 2.9172, accuracy: 13.6954%\n",
      "Epoch [2/2], Step [2501/3732], Loss: 3.0725, accuracy: 13.6953%\n",
      "Epoch [2/2], Step [2502/3732], Loss: 3.0979, accuracy: 13.6951%\n",
      "Epoch [2/2], Step [2503/3732], Loss: 3.0148, accuracy: 13.6949%\n",
      "Epoch [2/2], Step [2504/3732], Loss: 2.5503, accuracy: 13.6947%\n",
      "Epoch [2/2], Step [2505/3732], Loss: 2.8392, accuracy: 13.6945%\n",
      "Epoch [2/2], Step [2506/3732], Loss: 3.0907, accuracy: 13.6943%\n",
      "Epoch [2/2], Step [2507/3732], Loss: 2.2690, accuracy: 13.7001%\n",
      "Epoch [2/2], Step [2508/3732], Loss: 2.8383, accuracy: 13.7019%\n",
      "Epoch [2/2], Step [2509/3732], Loss: 3.0475, accuracy: 13.7037%\n",
      "Epoch [2/2], Step [2510/3732], Loss: 2.4868, accuracy: 13.7055%\n",
      "Epoch [2/2], Step [2511/3732], Loss: 3.2328, accuracy: 13.7074%\n",
      "Epoch [2/2], Step [2512/3732], Loss: 2.5369, accuracy: 13.7112%\n",
      "Epoch [2/2], Step [2513/3732], Loss: 3.7210, accuracy: 13.7090%\n",
      "Epoch [2/2], Step [2514/3732], Loss: 3.3159, accuracy: 13.7108%\n",
      "Epoch [2/2], Step [2515/3732], Loss: 3.2391, accuracy: 13.7106%\n",
      "Epoch [2/2], Step [2516/3732], Loss: 2.3181, accuracy: 13.7164%\n",
      "Epoch [2/2], Step [2517/3732], Loss: 3.2361, accuracy: 13.7162%\n",
      "Epoch [2/2], Step [2518/3732], Loss: 2.9516, accuracy: 13.7180%\n",
      "Epoch [2/2], Step [2519/3732], Loss: 2.7477, accuracy: 13.7198%\n",
      "Epoch [2/2], Step [2520/3732], Loss: 3.0071, accuracy: 13.7196%\n",
      "Epoch [2/2], Step [2521/3732], Loss: 2.0168, accuracy: 13.7294%\n",
      "Epoch [2/2], Step [2522/3732], Loss: 2.3317, accuracy: 13.7352%\n",
      "Epoch [2/2], Step [2523/3732], Loss: 3.2734, accuracy: 13.7350%\n",
      "Epoch [2/2], Step [2524/3732], Loss: 3.1177, accuracy: 13.7368%\n",
      "Epoch [2/2], Step [2525/3732], Loss: 4.0031, accuracy: 13.7346%\n",
      "Epoch [2/2], Step [2526/3732], Loss: 2.3988, accuracy: 13.7344%\n",
      "Epoch [2/2], Step [2527/3732], Loss: 3.8473, accuracy: 13.7322%\n",
      "Epoch [2/2], Step [2528/3732], Loss: 2.6907, accuracy: 13.7340%\n",
      "Epoch [2/2], Step [2529/3732], Loss: 2.9815, accuracy: 13.7338%\n",
      "Epoch [2/2], Step [2530/3732], Loss: 2.9900, accuracy: 13.7356%\n",
      "Epoch [2/2], Step [2531/3732], Loss: 3.7820, accuracy: 13.7374%\n",
      "Epoch [2/2], Step [2532/3732], Loss: 2.9177, accuracy: 13.7392%\n",
      "Epoch [2/2], Step [2533/3732], Loss: 2.2789, accuracy: 13.7430%\n",
      "Epoch [2/2], Step [2534/3732], Loss: 2.8273, accuracy: 13.7448%\n",
      "Epoch [2/2], Step [2535/3732], Loss: 3.4038, accuracy: 13.7446%\n",
      "Epoch [2/2], Step [2536/3732], Loss: 2.4166, accuracy: 13.7504%\n",
      "Epoch [2/2], Step [2537/3732], Loss: 2.8833, accuracy: 13.7522%\n",
      "Epoch [2/2], Step [2538/3732], Loss: 2.9793, accuracy: 13.7560%\n",
      "Epoch [2/2], Step [2539/3732], Loss: 2.8666, accuracy: 13.7578%\n",
      "Epoch [2/2], Step [2540/3732], Loss: 2.5266, accuracy: 13.7576%\n",
      "Epoch [2/2], Step [2541/3732], Loss: 3.3748, accuracy: 13.7554%\n",
      "Epoch [2/2], Step [2542/3732], Loss: 2.9983, accuracy: 13.7532%\n",
      "Epoch [2/2], Step [2543/3732], Loss: 3.5502, accuracy: 13.7510%\n",
      "Epoch [2/2], Step [2544/3732], Loss: 2.7263, accuracy: 13.7508%\n",
      "Epoch [2/2], Step [2545/3732], Loss: 2.3171, accuracy: 13.7546%\n",
      "Epoch [2/2], Step [2546/3732], Loss: 2.1775, accuracy: 13.7604%\n",
      "Epoch [2/2], Step [2547/3732], Loss: 3.0966, accuracy: 13.7582%\n",
      "Epoch [2/2], Step [2548/3732], Loss: 3.3463, accuracy: 13.7600%\n",
      "Epoch [2/2], Step [2549/3732], Loss: 2.7241, accuracy: 13.7617%\n",
      "Epoch [2/2], Step [2550/3732], Loss: 3.7389, accuracy: 13.7615%\n",
      "Epoch [2/2], Step [2551/3732], Loss: 3.2649, accuracy: 13.7613%\n",
      "Epoch [2/2], Step [2552/3732], Loss: 3.3115, accuracy: 13.7592%\n",
      "Epoch [2/2], Step [2553/3732], Loss: 3.1392, accuracy: 13.7609%\n",
      "Epoch [2/2], Step [2554/3732], Loss: 2.8878, accuracy: 13.7607%\n",
      "Epoch [2/2], Step [2555/3732], Loss: 3.5670, accuracy: 13.7585%\n",
      "Epoch [2/2], Step [2556/3732], Loss: 3.4749, accuracy: 13.7623%\n",
      "Epoch [2/2], Step [2557/3732], Loss: 3.9832, accuracy: 13.7601%\n",
      "Epoch [2/2], Step [2558/3732], Loss: 3.0132, accuracy: 13.7579%\n",
      "Epoch [2/2], Step [2559/3732], Loss: 2.7538, accuracy: 13.7617%\n",
      "Epoch [2/2], Step [2560/3732], Loss: 3.5039, accuracy: 13.7615%\n",
      "Epoch [2/2], Step [2561/3732], Loss: 3.2699, accuracy: 13.7613%\n",
      "Epoch [2/2], Step [2562/3732], Loss: 2.8466, accuracy: 13.7631%\n",
      "Epoch [2/2], Step [2563/3732], Loss: 3.2414, accuracy: 13.7649%\n",
      "Epoch [2/2], Step [2564/3732], Loss: 3.0494, accuracy: 13.7667%\n",
      "Epoch [2/2], Step [2565/3732], Loss: 2.9308, accuracy: 13.7704%\n",
      "Epoch [2/2], Step [2566/3732], Loss: 3.0085, accuracy: 13.7722%\n",
      "Epoch [2/2], Step [2567/3732], Loss: 3.7131, accuracy: 13.7720%\n",
      "Epoch [2/2], Step [2568/3732], Loss: 3.2421, accuracy: 13.7738%\n",
      "Epoch [2/2], Step [2569/3732], Loss: 3.0695, accuracy: 13.7756%\n",
      "Epoch [2/2], Step [2570/3732], Loss: 2.6097, accuracy: 13.7754%\n",
      "Epoch [2/2], Step [2571/3732], Loss: 3.2998, accuracy: 13.7732%\n",
      "Epoch [2/2], Step [2572/3732], Loss: 3.0313, accuracy: 13.7730%\n",
      "Epoch [2/2], Step [2573/3732], Loss: 3.0691, accuracy: 13.7748%\n",
      "Epoch [2/2], Step [2574/3732], Loss: 3.0174, accuracy: 13.7766%\n",
      "Epoch [2/2], Step [2575/3732], Loss: 2.8696, accuracy: 13.7783%\n",
      "Epoch [2/2], Step [2576/3732], Loss: 3.5294, accuracy: 13.7781%\n",
      "Epoch [2/2], Step [2577/3732], Loss: 3.3354, accuracy: 13.7760%\n",
      "Epoch [2/2], Step [2578/3732], Loss: 2.9137, accuracy: 13.7777%\n",
      "Epoch [2/2], Step [2579/3732], Loss: 2.6166, accuracy: 13.7795%\n",
      "Epoch [2/2], Step [2580/3732], Loss: 3.0872, accuracy: 13.7793%\n",
      "Epoch [2/2], Step [2581/3732], Loss: 2.8273, accuracy: 13.7831%\n",
      "Epoch [2/2], Step [2582/3732], Loss: 2.9552, accuracy: 13.7829%\n",
      "Epoch [2/2], Step [2583/3732], Loss: 2.4180, accuracy: 13.7866%\n",
      "Epoch [2/2], Step [2584/3732], Loss: 3.4497, accuracy: 13.7884%\n",
      "Epoch [2/2], Step [2585/3732], Loss: 2.9228, accuracy: 13.7882%\n",
      "Epoch [2/2], Step [2586/3732], Loss: 3.1403, accuracy: 13.7900%\n",
      "Epoch [2/2], Step [2587/3732], Loss: 3.6860, accuracy: 13.7878%\n",
      "Epoch [2/2], Step [2588/3732], Loss: 2.2992, accuracy: 13.7915%\n",
      "Epoch [2/2], Step [2589/3732], Loss: 3.1961, accuracy: 13.7913%\n",
      "Epoch [2/2], Step [2590/3732], Loss: 3.0519, accuracy: 13.7911%\n",
      "Epoch [2/2], Step [2591/3732], Loss: 3.1471, accuracy: 13.7909%\n",
      "Epoch [2/2], Step [2592/3732], Loss: 3.3621, accuracy: 13.7907%\n",
      "Epoch [2/2], Step [2593/3732], Loss: 3.1458, accuracy: 13.7905%\n",
      "Epoch [2/2], Step [2594/3732], Loss: 3.3396, accuracy: 13.7903%\n",
      "Epoch [2/2], Step [2595/3732], Loss: 2.7371, accuracy: 13.7921%\n",
      "Epoch [2/2], Step [2596/3732], Loss: 3.2466, accuracy: 13.7919%\n",
      "Epoch [2/2], Step [2597/3732], Loss: 3.9901, accuracy: 13.7897%\n",
      "Epoch [2/2], Step [2598/3732], Loss: 2.7848, accuracy: 13.7895%\n",
      "Epoch [2/2], Step [2599/3732], Loss: 2.9415, accuracy: 13.7913%\n",
      "Epoch [2/2], Step [2600/3732], Loss: 2.8882, accuracy: 13.7911%\n",
      "Epoch [2/2], Step [2601/3732], Loss: 2.6393, accuracy: 13.7928%\n",
      "Epoch [2/2], Step [2602/3732], Loss: 2.7096, accuracy: 13.7946%\n",
      "Epoch [2/2], Step [2603/3732], Loss: 3.3180, accuracy: 13.7944%\n",
      "Epoch [2/2], Step [2604/3732], Loss: 2.9837, accuracy: 13.7962%\n",
      "Epoch [2/2], Step [2605/3732], Loss: 2.5318, accuracy: 13.7999%\n",
      "Epoch [2/2], Step [2606/3732], Loss: 1.8968, accuracy: 13.8036%\n",
      "Epoch [2/2], Step [2607/3732], Loss: 3.3589, accuracy: 13.8054%\n",
      "Epoch [2/2], Step [2608/3732], Loss: 2.9025, accuracy: 13.8072%\n",
      "Epoch [2/2], Step [2609/3732], Loss: 2.8887, accuracy: 13.8109%\n",
      "Epoch [2/2], Step [2610/3732], Loss: 3.6977, accuracy: 13.8146%\n",
      "Epoch [2/2], Step [2611/3732], Loss: 2.9992, accuracy: 13.8144%\n",
      "Epoch [2/2], Step [2612/3732], Loss: 2.6974, accuracy: 13.8162%\n",
      "Epoch [2/2], Step [2613/3732], Loss: 2.9178, accuracy: 13.8180%\n",
      "Epoch [2/2], Step [2614/3732], Loss: 3.4785, accuracy: 13.8158%\n",
      "Epoch [2/2], Step [2615/3732], Loss: 2.7527, accuracy: 13.8215%\n",
      "Epoch [2/2], Step [2616/3732], Loss: 3.4666, accuracy: 13.8213%\n",
      "Epoch [2/2], Step [2617/3732], Loss: 3.6364, accuracy: 13.8191%\n",
      "Epoch [2/2], Step [2618/3732], Loss: 3.3312, accuracy: 13.8189%\n",
      "Epoch [2/2], Step [2619/3732], Loss: 2.8451, accuracy: 13.8187%\n",
      "Epoch [2/2], Step [2620/3732], Loss: 2.4646, accuracy: 13.8224%\n",
      "Epoch [2/2], Step [2621/3732], Loss: 3.0499, accuracy: 13.8222%\n",
      "Epoch [2/2], Step [2622/3732], Loss: 2.8824, accuracy: 13.8259%\n",
      "Epoch [2/2], Step [2623/3732], Loss: 2.6373, accuracy: 13.8297%\n",
      "Epoch [2/2], Step [2624/3732], Loss: 3.0593, accuracy: 13.8295%\n",
      "Epoch [2/2], Step [2625/3732], Loss: 3.1814, accuracy: 13.8292%\n",
      "Epoch [2/2], Step [2626/3732], Loss: 3.4295, accuracy: 13.8310%\n",
      "Epoch [2/2], Step [2627/3732], Loss: 2.9090, accuracy: 13.8288%\n",
      "Epoch [2/2], Step [2628/3732], Loss: 2.9891, accuracy: 13.8286%\n",
      "Epoch [2/2], Step [2629/3732], Loss: 2.9007, accuracy: 13.8304%\n",
      "Epoch [2/2], Step [2630/3732], Loss: 3.5341, accuracy: 13.8302%\n",
      "Epoch [2/2], Step [2631/3732], Loss: 2.9254, accuracy: 13.8300%\n",
      "Epoch [2/2], Step [2632/3732], Loss: 3.5851, accuracy: 13.8297%\n",
      "Epoch [2/2], Step [2633/3732], Loss: 3.5233, accuracy: 13.8276%\n",
      "Epoch [2/2], Step [2634/3732], Loss: 2.2941, accuracy: 13.8293%\n",
      "Epoch [2/2], Step [2635/3732], Loss: 3.0946, accuracy: 13.8311%\n",
      "Epoch [2/2], Step [2636/3732], Loss: 3.3575, accuracy: 13.8309%\n",
      "Epoch [2/2], Step [2637/3732], Loss: 1.8116, accuracy: 13.8405%\n",
      "Epoch [2/2], Step [2638/3732], Loss: 2.5175, accuracy: 13.8442%\n",
      "Epoch [2/2], Step [2639/3732], Loss: 3.0086, accuracy: 13.8459%\n",
      "Epoch [2/2], Step [2640/3732], Loss: 3.1993, accuracy: 13.8477%\n",
      "Epoch [2/2], Step [2641/3732], Loss: 2.9307, accuracy: 13.8494%\n",
      "Epoch [2/2], Step [2642/3732], Loss: 2.1354, accuracy: 13.8551%\n",
      "Epoch [2/2], Step [2643/3732], Loss: 3.7042, accuracy: 13.8549%\n",
      "Epoch [2/2], Step [2644/3732], Loss: 2.6766, accuracy: 13.8586%\n",
      "Epoch [2/2], Step [2645/3732], Loss: 3.0226, accuracy: 13.8604%\n",
      "Epoch [2/2], Step [2646/3732], Loss: 2.4158, accuracy: 13.8621%\n",
      "Epoch [2/2], Step [2647/3732], Loss: 2.2042, accuracy: 13.8639%\n",
      "Epoch [2/2], Step [2648/3732], Loss: 2.8973, accuracy: 13.8636%\n",
      "Epoch [2/2], Step [2649/3732], Loss: 2.8012, accuracy: 13.8693%\n",
      "Epoch [2/2], Step [2650/3732], Loss: 2.8453, accuracy: 13.8691%\n",
      "Epoch [2/2], Step [2651/3732], Loss: 2.8050, accuracy: 13.8689%\n",
      "Epoch [2/2], Step [2652/3732], Loss: 2.6355, accuracy: 13.8667%\n",
      "Epoch [2/2], Step [2653/3732], Loss: 2.7669, accuracy: 13.8704%\n",
      "Epoch [2/2], Step [2654/3732], Loss: 2.7857, accuracy: 13.8721%\n",
      "Epoch [2/2], Step [2655/3732], Loss: 3.0913, accuracy: 13.8719%\n",
      "Epoch [2/2], Step [2656/3732], Loss: 3.3041, accuracy: 13.8698%\n",
      "Epoch [2/2], Step [2657/3732], Loss: 2.4808, accuracy: 13.8695%\n",
      "Epoch [2/2], Step [2658/3732], Loss: 3.1790, accuracy: 13.8693%\n",
      "Epoch [2/2], Step [2659/3732], Loss: 2.6336, accuracy: 13.8730%\n",
      "Epoch [2/2], Step [2660/3732], Loss: 2.2097, accuracy: 13.8787%\n",
      "Epoch [2/2], Step [2661/3732], Loss: 3.4820, accuracy: 13.8785%\n",
      "Epoch [2/2], Step [2662/3732], Loss: 2.7358, accuracy: 13.8822%\n",
      "Epoch [2/2], Step [2663/3732], Loss: 3.3441, accuracy: 13.8819%\n",
      "Epoch [2/2], Step [2664/3732], Loss: 2.3287, accuracy: 13.8876%\n",
      "Epoch [2/2], Step [2665/3732], Loss: 2.8322, accuracy: 13.8874%\n",
      "Epoch [2/2], Step [2666/3732], Loss: 3.3114, accuracy: 13.8852%\n",
      "Epoch [2/2], Step [2667/3732], Loss: 3.6604, accuracy: 13.8850%\n",
      "Epoch [2/2], Step [2668/3732], Loss: 3.0632, accuracy: 13.8867%\n",
      "Epoch [2/2], Step [2669/3732], Loss: 2.7272, accuracy: 13.8885%\n",
      "Epoch [2/2], Step [2670/3732], Loss: 3.3363, accuracy: 13.8882%\n",
      "Epoch [2/2], Step [2671/3732], Loss: 3.0846, accuracy: 13.8861%\n",
      "Epoch [2/2], Step [2672/3732], Loss: 3.0670, accuracy: 13.8859%\n",
      "Epoch [2/2], Step [2673/3732], Loss: 3.0732, accuracy: 13.8876%\n",
      "Epoch [2/2], Step [2674/3732], Loss: 3.2230, accuracy: 13.8874%\n",
      "Epoch [2/2], Step [2675/3732], Loss: 3.7636, accuracy: 13.8872%\n",
      "Epoch [2/2], Step [2676/3732], Loss: 2.5818, accuracy: 13.8889%\n",
      "Epoch [2/2], Step [2677/3732], Loss: 3.2294, accuracy: 13.8906%\n",
      "Epoch [2/2], Step [2678/3732], Loss: 2.9662, accuracy: 13.8904%\n",
      "Epoch [2/2], Step [2679/3732], Loss: 2.6795, accuracy: 13.8941%\n",
      "Epoch [2/2], Step [2680/3732], Loss: 2.7117, accuracy: 13.8958%\n",
      "Epoch [2/2], Step [2681/3732], Loss: 2.3344, accuracy: 13.9015%\n",
      "Epoch [2/2], Step [2682/3732], Loss: 3.1113, accuracy: 13.8993%\n",
      "Epoch [2/2], Step [2683/3732], Loss: 3.0885, accuracy: 13.9010%\n",
      "Epoch [2/2], Step [2684/3732], Loss: 2.3908, accuracy: 13.9047%\n",
      "Epoch [2/2], Step [2685/3732], Loss: 3.1511, accuracy: 13.9045%\n",
      "Epoch [2/2], Step [2686/3732], Loss: 3.4412, accuracy: 13.9081%\n",
      "Epoch [2/2], Step [2687/3732], Loss: 3.2376, accuracy: 13.9079%\n",
      "Epoch [2/2], Step [2688/3732], Loss: 2.0077, accuracy: 13.9155%\n",
      "Epoch [2/2], Step [2689/3732], Loss: 3.4433, accuracy: 13.9153%\n",
      "Epoch [2/2], Step [2690/3732], Loss: 2.4901, accuracy: 13.9170%\n",
      "Epoch [2/2], Step [2691/3732], Loss: 3.1331, accuracy: 13.9168%\n",
      "Epoch [2/2], Step [2692/3732], Loss: 2.8791, accuracy: 13.9185%\n",
      "Epoch [2/2], Step [2693/3732], Loss: 2.8256, accuracy: 13.9163%\n",
      "Epoch [2/2], Step [2694/3732], Loss: 3.0514, accuracy: 13.9181%\n",
      "Epoch [2/2], Step [2695/3732], Loss: 3.2554, accuracy: 13.9198%\n",
      "Epoch [2/2], Step [2696/3732], Loss: 3.4666, accuracy: 13.9196%\n",
      "Epoch [2/2], Step [2697/3732], Loss: 3.9454, accuracy: 13.9174%\n",
      "Epoch [2/2], Step [2698/3732], Loss: 2.8641, accuracy: 13.9172%\n",
      "Epoch [2/2], Step [2699/3732], Loss: 2.7371, accuracy: 13.9150%\n",
      "Epoch [2/2], Step [2700/3732], Loss: 2.3475, accuracy: 13.9206%\n",
      "Epoch [2/2], Step [2701/3732], Loss: 3.7298, accuracy: 13.9204%\n",
      "Epoch [2/2], Step [2702/3732], Loss: 2.7889, accuracy: 13.9221%\n",
      "Epoch [2/2], Step [2703/3732], Loss: 3.0330, accuracy: 13.9239%\n",
      "Epoch [2/2], Step [2704/3732], Loss: 3.2473, accuracy: 13.9256%\n",
      "Epoch [2/2], Step [2705/3732], Loss: 3.0185, accuracy: 13.9273%\n",
      "Epoch [2/2], Step [2706/3732], Loss: 3.3972, accuracy: 13.9251%\n",
      "Epoch [2/2], Step [2707/3732], Loss: 3.2409, accuracy: 13.9230%\n",
      "Epoch [2/2], Step [2708/3732], Loss: 2.8340, accuracy: 13.9266%\n",
      "Epoch [2/2], Step [2709/3732], Loss: 3.3687, accuracy: 13.9264%\n",
      "Epoch [2/2], Step [2710/3732], Loss: 3.0144, accuracy: 13.9301%\n",
      "Epoch [2/2], Step [2711/3732], Loss: 3.1011, accuracy: 13.9318%\n",
      "Epoch [2/2], Step [2712/3732], Loss: 2.3150, accuracy: 13.9335%\n",
      "Epoch [2/2], Step [2713/3732], Loss: 2.2302, accuracy: 13.9391%\n",
      "Epoch [2/2], Step [2714/3732], Loss: 3.2347, accuracy: 13.9428%\n",
      "Epoch [2/2], Step [2715/3732], Loss: 2.5401, accuracy: 13.9464%\n",
      "Epoch [2/2], Step [2716/3732], Loss: 3.1261, accuracy: 13.9481%\n",
      "Epoch [2/2], Step [2717/3732], Loss: 2.7046, accuracy: 13.9498%\n",
      "Epoch [2/2], Step [2718/3732], Loss: 2.8153, accuracy: 13.9496%\n",
      "Epoch [2/2], Step [2719/3732], Loss: 3.0377, accuracy: 13.9494%\n",
      "Epoch [2/2], Step [2720/3732], Loss: 2.9746, accuracy: 13.9472%\n",
      "Epoch [2/2], Step [2721/3732], Loss: 2.8560, accuracy: 13.9489%\n",
      "Epoch [2/2], Step [2722/3732], Loss: 2.0971, accuracy: 13.9526%\n",
      "Epoch [2/2], Step [2723/3732], Loss: 3.7078, accuracy: 13.9504%\n",
      "Epoch [2/2], Step [2724/3732], Loss: 2.2142, accuracy: 13.9560%\n",
      "Epoch [2/2], Step [2725/3732], Loss: 3.1483, accuracy: 13.9538%\n",
      "Epoch [2/2], Step [2726/3732], Loss: 2.2677, accuracy: 13.9556%\n",
      "Epoch [2/2], Step [2727/3732], Loss: 3.4733, accuracy: 13.9534%\n",
      "Epoch [2/2], Step [2728/3732], Loss: 3.2083, accuracy: 13.9512%\n",
      "Epoch [2/2], Step [2729/3732], Loss: 2.1742, accuracy: 13.9549%\n",
      "Epoch [2/2], Step [2730/3732], Loss: 3.5226, accuracy: 13.9566%\n",
      "Epoch [2/2], Step [2731/3732], Loss: 3.2178, accuracy: 13.9564%\n",
      "Epoch [2/2], Step [2732/3732], Loss: 3.3965, accuracy: 13.9561%\n",
      "Epoch [2/2], Step [2733/3732], Loss: 2.2833, accuracy: 13.9598%\n",
      "Epoch [2/2], Step [2734/3732], Loss: 3.0824, accuracy: 13.9615%\n",
      "Epoch [2/2], Step [2735/3732], Loss: 3.3656, accuracy: 13.9613%\n",
      "Epoch [2/2], Step [2736/3732], Loss: 2.9215, accuracy: 13.9630%\n",
      "Epoch [2/2], Step [2737/3732], Loss: 2.0989, accuracy: 13.9647%\n",
      "Epoch [2/2], Step [2738/3732], Loss: 3.2433, accuracy: 13.9645%\n",
      "Epoch [2/2], Step [2739/3732], Loss: 2.6529, accuracy: 13.9662%\n",
      "Epoch [2/2], Step [2740/3732], Loss: 2.5801, accuracy: 13.9659%\n",
      "Epoch [2/2], Step [2741/3732], Loss: 2.8520, accuracy: 13.9657%\n",
      "Epoch [2/2], Step [2742/3732], Loss: 2.6709, accuracy: 13.9674%\n",
      "Epoch [2/2], Step [2743/3732], Loss: 2.7058, accuracy: 13.9691%\n",
      "Epoch [2/2], Step [2744/3732], Loss: 2.9132, accuracy: 13.9727%\n",
      "Epoch [2/2], Step [2745/3732], Loss: 3.2066, accuracy: 13.9725%\n",
      "Epoch [2/2], Step [2746/3732], Loss: 3.7714, accuracy: 13.9723%\n",
      "Epoch [2/2], Step [2747/3732], Loss: 2.4159, accuracy: 13.9759%\n",
      "Epoch [2/2], Step [2748/3732], Loss: 3.0854, accuracy: 13.9738%\n",
      "Epoch [2/2], Step [2749/3732], Loss: 3.7890, accuracy: 13.9716%\n",
      "Epoch [2/2], Step [2750/3732], Loss: 2.7738, accuracy: 13.9733%\n",
      "Epoch [2/2], Step [2751/3732], Loss: 3.5577, accuracy: 13.9750%\n",
      "Epoch [2/2], Step [2752/3732], Loss: 2.5718, accuracy: 13.9786%\n",
      "Epoch [2/2], Step [2753/3732], Loss: 2.2540, accuracy: 13.9803%\n",
      "Epoch [2/2], Step [2754/3732], Loss: 3.4657, accuracy: 13.9801%\n",
      "Epoch [2/2], Step [2755/3732], Loss: 2.8193, accuracy: 13.9837%\n",
      "Epoch [2/2], Step [2756/3732], Loss: 3.1519, accuracy: 13.9816%\n",
      "Epoch [2/2], Step [2757/3732], Loss: 4.4409, accuracy: 13.9814%\n",
      "Epoch [2/2], Step [2758/3732], Loss: 3.3800, accuracy: 13.9811%\n",
      "Epoch [2/2], Step [2759/3732], Loss: 2.2636, accuracy: 13.9828%\n",
      "Epoch [2/2], Step [2760/3732], Loss: 3.0015, accuracy: 13.9807%\n",
      "Epoch [2/2], Step [2761/3732], Loss: 3.5832, accuracy: 13.9804%\n",
      "Epoch [2/2], Step [2762/3732], Loss: 3.3080, accuracy: 13.9802%\n",
      "Epoch [2/2], Step [2763/3732], Loss: 2.7108, accuracy: 13.9800%\n",
      "Epoch [2/2], Step [2764/3732], Loss: 2.5778, accuracy: 13.9817%\n",
      "Epoch [2/2], Step [2765/3732], Loss: 2.9678, accuracy: 13.9815%\n",
      "Epoch [2/2], Step [2766/3732], Loss: 3.1876, accuracy: 13.9831%\n",
      "Epoch [2/2], Step [2767/3732], Loss: 2.7348, accuracy: 13.9848%\n",
      "Epoch [2/2], Step [2768/3732], Loss: 4.2330, accuracy: 13.9827%\n",
      "Epoch [2/2], Step [2769/3732], Loss: 3.4367, accuracy: 13.9805%\n",
      "Epoch [2/2], Step [2770/3732], Loss: 2.5669, accuracy: 13.9842%\n",
      "Epoch [2/2], Step [2771/3732], Loss: 3.1326, accuracy: 13.9839%\n",
      "Epoch [2/2], Step [2772/3732], Loss: 3.4280, accuracy: 13.9837%\n",
      "Epoch [2/2], Step [2773/3732], Loss: 2.2207, accuracy: 13.9854%\n",
      "Epoch [2/2], Step [2774/3732], Loss: 3.0816, accuracy: 13.9871%\n",
      "Epoch [2/2], Step [2775/3732], Loss: 2.8496, accuracy: 13.9888%\n",
      "Epoch [2/2], Step [2776/3732], Loss: 2.3056, accuracy: 13.9943%\n",
      "Epoch [2/2], Step [2777/3732], Loss: 3.0215, accuracy: 13.9941%\n",
      "Epoch [2/2], Step [2778/3732], Loss: 4.0371, accuracy: 13.9958%\n",
      "Epoch [2/2], Step [2779/3732], Loss: 2.8771, accuracy: 13.9975%\n",
      "Epoch [2/2], Step [2780/3732], Loss: 2.8903, accuracy: 13.9972%\n",
      "Epoch [2/2], Step [2781/3732], Loss: 3.0517, accuracy: 13.9989%\n",
      "Epoch [2/2], Step [2782/3732], Loss: 3.4101, accuracy: 13.9987%\n",
      "Epoch [2/2], Step [2783/3732], Loss: 2.7562, accuracy: 13.9985%\n",
      "Epoch [2/2], Step [2784/3732], Loss: 2.8631, accuracy: 14.0021%\n",
      "Epoch [2/2], Step [2785/3732], Loss: 2.4310, accuracy: 14.0038%\n",
      "Epoch [2/2], Step [2786/3732], Loss: 3.3347, accuracy: 14.0035%\n",
      "Epoch [2/2], Step [2787/3732], Loss: 3.1867, accuracy: 14.0014%\n",
      "Epoch [2/2], Step [2788/3732], Loss: 2.7382, accuracy: 14.0031%\n",
      "Epoch [2/2], Step [2789/3732], Loss: 3.4895, accuracy: 14.0028%\n",
      "Epoch [2/2], Step [2790/3732], Loss: 2.5305, accuracy: 14.0045%\n",
      "Epoch [2/2], Step [2791/3732], Loss: 3.1056, accuracy: 14.0024%\n",
      "Epoch [2/2], Step [2792/3732], Loss: 2.9846, accuracy: 14.0041%\n",
      "Epoch [2/2], Step [2793/3732], Loss: 3.4606, accuracy: 14.0019%\n",
      "Epoch [2/2], Step [2794/3732], Loss: 3.8926, accuracy: 13.9998%\n",
      "Epoch [2/2], Step [2795/3732], Loss: 3.6084, accuracy: 13.9995%\n",
      "Epoch [2/2], Step [2796/3732], Loss: 3.2108, accuracy: 13.9993%\n",
      "Epoch [2/2], Step [2797/3732], Loss: 2.4036, accuracy: 14.0029%\n",
      "Epoch [2/2], Step [2798/3732], Loss: 3.1143, accuracy: 14.0027%\n",
      "Epoch [2/2], Step [2799/3732], Loss: 2.7548, accuracy: 14.0044%\n",
      "Epoch [2/2], Step [2800/3732], Loss: 2.2705, accuracy: 14.0099%\n",
      "Epoch [2/2], Step [2801/3732], Loss: 2.7820, accuracy: 14.0135%\n",
      "Epoch [2/2], Step [2802/3732], Loss: 3.5235, accuracy: 14.0171%\n",
      "Epoch [2/2], Step [2803/3732], Loss: 3.3772, accuracy: 14.0149%\n",
      "Epoch [2/2], Step [2804/3732], Loss: 3.2484, accuracy: 14.0147%\n",
      "Epoch [2/2], Step [2805/3732], Loss: 3.4961, accuracy: 14.0145%\n",
      "Epoch [2/2], Step [2806/3732], Loss: 2.7944, accuracy: 14.0180%\n",
      "Epoch [2/2], Step [2807/3732], Loss: 3.2190, accuracy: 14.0216%\n",
      "Epoch [2/2], Step [2808/3732], Loss: 3.4826, accuracy: 14.0233%\n",
      "Epoch [2/2], Step [2809/3732], Loss: 3.1355, accuracy: 14.0250%\n",
      "Epoch [2/2], Step [2810/3732], Loss: 2.3992, accuracy: 14.0286%\n",
      "Epoch [2/2], Step [2811/3732], Loss: 2.4023, accuracy: 14.0341%\n",
      "Epoch [2/2], Step [2812/3732], Loss: 2.4010, accuracy: 14.0377%\n",
      "Epoch [2/2], Step [2813/3732], Loss: 2.7602, accuracy: 14.0413%\n",
      "Epoch [2/2], Step [2814/3732], Loss: 4.2663, accuracy: 14.0391%\n",
      "Epoch [2/2], Step [2815/3732], Loss: 2.4845, accuracy: 14.0408%\n",
      "Epoch [2/2], Step [2816/3732], Loss: 2.2732, accuracy: 14.0444%\n",
      "Epoch [2/2], Step [2817/3732], Loss: 3.6547, accuracy: 14.0422%\n",
      "Epoch [2/2], Step [2818/3732], Loss: 2.7471, accuracy: 14.0458%\n",
      "Epoch [2/2], Step [2819/3732], Loss: 2.7441, accuracy: 14.0494%\n",
      "Epoch [2/2], Step [2820/3732], Loss: 2.1369, accuracy: 14.0530%\n",
      "Epoch [2/2], Step [2821/3732], Loss: 3.1131, accuracy: 14.0546%\n",
      "Epoch [2/2], Step [2822/3732], Loss: 3.1529, accuracy: 14.0544%\n",
      "Epoch [2/2], Step [2823/3732], Loss: 3.2377, accuracy: 14.0523%\n",
      "Epoch [2/2], Step [2824/3732], Loss: 2.6807, accuracy: 14.0558%\n",
      "Epoch [2/2], Step [2825/3732], Loss: 3.2582, accuracy: 14.0537%\n",
      "Epoch [2/2], Step [2826/3732], Loss: 2.8334, accuracy: 14.0554%\n",
      "Epoch [2/2], Step [2827/3732], Loss: 2.6282, accuracy: 14.0551%\n",
      "Epoch [2/2], Step [2828/3732], Loss: 3.1061, accuracy: 14.0549%\n",
      "Epoch [2/2], Step [2829/3732], Loss: 2.4366, accuracy: 14.0585%\n",
      "Epoch [2/2], Step [2830/3732], Loss: 2.7307, accuracy: 14.0563%\n",
      "Epoch [2/2], Step [2831/3732], Loss: 3.3933, accuracy: 14.0561%\n",
      "Epoch [2/2], Step [2832/3732], Loss: 2.8718, accuracy: 14.0577%\n",
      "Epoch [2/2], Step [2833/3732], Loss: 2.7605, accuracy: 14.0575%\n",
      "Epoch [2/2], Step [2834/3732], Loss: 2.4778, accuracy: 14.0573%\n",
      "Epoch [2/2], Step [2835/3732], Loss: 2.8931, accuracy: 14.0589%\n",
      "Epoch [2/2], Step [2836/3732], Loss: 2.7389, accuracy: 14.0625%\n",
      "Epoch [2/2], Step [2837/3732], Loss: 3.5222, accuracy: 14.0642%\n",
      "Epoch [2/2], Step [2838/3732], Loss: 3.4443, accuracy: 14.0639%\n",
      "Epoch [2/2], Step [2839/3732], Loss: 2.3381, accuracy: 14.0675%\n",
      "Epoch [2/2], Step [2840/3732], Loss: 4.2075, accuracy: 14.0673%\n",
      "Epoch [2/2], Step [2841/3732], Loss: 3.4147, accuracy: 14.0670%\n",
      "Epoch [2/2], Step [2842/3732], Loss: 3.3055, accuracy: 14.0668%\n",
      "Epoch [2/2], Step [2843/3732], Loss: 3.4344, accuracy: 14.0684%\n",
      "Epoch [2/2], Step [2844/3732], Loss: 3.5399, accuracy: 14.0682%\n",
      "Epoch [2/2], Step [2845/3732], Loss: 2.8304, accuracy: 14.0680%\n",
      "Epoch [2/2], Step [2846/3732], Loss: 3.1599, accuracy: 14.0677%\n",
      "Epoch [2/2], Step [2847/3732], Loss: 3.2964, accuracy: 14.0656%\n",
      "Epoch [2/2], Step [2848/3732], Loss: 2.9147, accuracy: 14.0672%\n",
      "Epoch [2/2], Step [2849/3732], Loss: 3.7582, accuracy: 14.0651%\n",
      "Epoch [2/2], Step [2850/3732], Loss: 2.9844, accuracy: 14.0649%\n",
      "Epoch [2/2], Step [2851/3732], Loss: 2.5779, accuracy: 14.0665%\n",
      "Epoch [2/2], Step [2852/3732], Loss: 3.0518, accuracy: 14.0644%\n",
      "Epoch [2/2], Step [2853/3732], Loss: 3.3984, accuracy: 14.0661%\n",
      "Epoch [2/2], Step [2854/3732], Loss: 2.6731, accuracy: 14.0677%\n",
      "Epoch [2/2], Step [2855/3732], Loss: 2.4136, accuracy: 14.0675%\n",
      "Epoch [2/2], Step [2856/3732], Loss: 2.8159, accuracy: 14.0691%\n",
      "Epoch [2/2], Step [2857/3732], Loss: 3.5602, accuracy: 14.0689%\n",
      "Epoch [2/2], Step [2858/3732], Loss: 2.5468, accuracy: 14.0706%\n",
      "Epoch [2/2], Step [2859/3732], Loss: 3.5190, accuracy: 14.0703%\n",
      "Epoch [2/2], Step [2860/3732], Loss: 2.8985, accuracy: 14.0701%\n",
      "Epoch [2/2], Step [2861/3732], Loss: 3.5254, accuracy: 14.0698%\n",
      "Epoch [2/2], Step [2862/3732], Loss: 3.0714, accuracy: 14.0696%\n",
      "Epoch [2/2], Step [2863/3732], Loss: 2.8368, accuracy: 14.0694%\n",
      "Epoch [2/2], Step [2864/3732], Loss: 3.2090, accuracy: 14.0672%\n",
      "Epoch [2/2], Step [2865/3732], Loss: 3.1064, accuracy: 14.0689%\n",
      "Epoch [2/2], Step [2866/3732], Loss: 3.5913, accuracy: 14.0706%\n",
      "Epoch [2/2], Step [2867/3732], Loss: 2.5685, accuracy: 14.0741%\n",
      "Epoch [2/2], Step [2868/3732], Loss: 2.8876, accuracy: 14.0758%\n",
      "Epoch [2/2], Step [2869/3732], Loss: 3.3190, accuracy: 14.0755%\n",
      "Epoch [2/2], Step [2870/3732], Loss: 2.9619, accuracy: 14.0772%\n",
      "Epoch [2/2], Step [2871/3732], Loss: 3.1051, accuracy: 14.0769%\n",
      "Epoch [2/2], Step [2872/3732], Loss: 2.7338, accuracy: 14.0786%\n",
      "Epoch [2/2], Step [2873/3732], Loss: 2.6347, accuracy: 14.0765%\n",
      "Epoch [2/2], Step [2874/3732], Loss: 3.2897, accuracy: 14.0781%\n",
      "Epoch [2/2], Step [2875/3732], Loss: 3.3178, accuracy: 14.0760%\n",
      "Epoch [2/2], Step [2876/3732], Loss: 2.5552, accuracy: 14.0776%\n",
      "Epoch [2/2], Step [2877/3732], Loss: 2.7899, accuracy: 14.0793%\n",
      "Epoch [2/2], Step [2878/3732], Loss: 3.1489, accuracy: 14.0790%\n",
      "Epoch [2/2], Step [2879/3732], Loss: 2.9275, accuracy: 14.0807%\n",
      "Epoch [2/2], Step [2880/3732], Loss: 2.5982, accuracy: 14.0824%\n",
      "Epoch [2/2], Step [2881/3732], Loss: 3.5816, accuracy: 14.0821%\n",
      "Epoch [2/2], Step [2882/3732], Loss: 2.7714, accuracy: 14.0857%\n",
      "Epoch [2/2], Step [2883/3732], Loss: 2.9238, accuracy: 14.0854%\n",
      "Epoch [2/2], Step [2884/3732], Loss: 2.0413, accuracy: 14.0890%\n",
      "Epoch [2/2], Step [2885/3732], Loss: 3.6099, accuracy: 14.0887%\n",
      "Epoch [2/2], Step [2886/3732], Loss: 2.2672, accuracy: 14.0922%\n",
      "Epoch [2/2], Step [2887/3732], Loss: 2.9058, accuracy: 14.0939%\n",
      "Epoch [2/2], Step [2888/3732], Loss: 2.7260, accuracy: 14.0974%\n",
      "Epoch [2/2], Step [2889/3732], Loss: 2.6025, accuracy: 14.0972%\n",
      "Epoch [2/2], Step [2890/3732], Loss: 3.3458, accuracy: 14.0988%\n",
      "Epoch [2/2], Step [2891/3732], Loss: 3.3570, accuracy: 14.0986%\n",
      "Epoch [2/2], Step [2892/3732], Loss: 2.2406, accuracy: 14.1040%\n",
      "Epoch [2/2], Step [2893/3732], Loss: 2.7401, accuracy: 14.1057%\n",
      "Epoch [2/2], Step [2894/3732], Loss: 3.0648, accuracy: 14.1054%\n",
      "Epoch [2/2], Step [2895/3732], Loss: 2.6714, accuracy: 14.1071%\n",
      "Epoch [2/2], Step [2896/3732], Loss: 3.0408, accuracy: 14.1087%\n",
      "Epoch [2/2], Step [2897/3732], Loss: 3.5492, accuracy: 14.1066%\n",
      "Epoch [2/2], Step [2898/3732], Loss: 1.8133, accuracy: 14.1158%\n",
      "Epoch [2/2], Step [2899/3732], Loss: 4.2549, accuracy: 14.1136%\n",
      "Epoch [2/2], Step [2900/3732], Loss: 2.4904, accuracy: 14.1153%\n",
      "Epoch [2/2], Step [2901/3732], Loss: 3.3156, accuracy: 14.1169%\n",
      "Epoch [2/2], Step [2902/3732], Loss: 2.6663, accuracy: 14.1167%\n",
      "Epoch [2/2], Step [2903/3732], Loss: 4.2605, accuracy: 14.1164%\n",
      "Epoch [2/2], Step [2904/3732], Loss: 2.9416, accuracy: 14.1162%\n",
      "Epoch [2/2], Step [2905/3732], Loss: 3.3415, accuracy: 14.1159%\n",
      "Epoch [2/2], Step [2906/3732], Loss: 2.7750, accuracy: 14.1157%\n",
      "Epoch [2/2], Step [2907/3732], Loss: 3.2928, accuracy: 14.1173%\n",
      "Epoch [2/2], Step [2908/3732], Loss: 3.1916, accuracy: 14.1209%\n",
      "Epoch [2/2], Step [2909/3732], Loss: 3.2478, accuracy: 14.1244%\n",
      "Epoch [2/2], Step [2910/3732], Loss: 3.3095, accuracy: 14.1241%\n",
      "Epoch [2/2], Step [2911/3732], Loss: 3.1626, accuracy: 14.1239%\n",
      "Epoch [2/2], Step [2912/3732], Loss: 2.6688, accuracy: 14.1274%\n",
      "Epoch [2/2], Step [2913/3732], Loss: 2.7474, accuracy: 14.1309%\n",
      "Epoch [2/2], Step [2914/3732], Loss: 3.0381, accuracy: 14.1326%\n",
      "Epoch [2/2], Step [2915/3732], Loss: 3.6291, accuracy: 14.1323%\n",
      "Epoch [2/2], Step [2916/3732], Loss: 3.6212, accuracy: 14.1302%\n",
      "Epoch [2/2], Step [2917/3732], Loss: 3.5359, accuracy: 14.1281%\n",
      "Epoch [2/2], Step [2918/3732], Loss: 2.4387, accuracy: 14.1316%\n",
      "Epoch [2/2], Step [2919/3732], Loss: 3.9139, accuracy: 14.1313%\n",
      "Epoch [2/2], Step [2920/3732], Loss: 2.7453, accuracy: 14.1348%\n",
      "Epoch [2/2], Step [2921/3732], Loss: 3.3281, accuracy: 14.1346%\n",
      "Epoch [2/2], Step [2922/3732], Loss: 3.1605, accuracy: 14.1362%\n",
      "Epoch [2/2], Step [2923/3732], Loss: 2.9125, accuracy: 14.1397%\n",
      "Epoch [2/2], Step [2924/3732], Loss: 2.9506, accuracy: 14.1395%\n",
      "Epoch [2/2], Step [2925/3732], Loss: 3.4500, accuracy: 14.1374%\n",
      "Epoch [2/2], Step [2926/3732], Loss: 3.4419, accuracy: 14.1353%\n",
      "Epoch [2/2], Step [2927/3732], Loss: 3.1302, accuracy: 14.1350%\n",
      "Epoch [2/2], Step [2928/3732], Loss: 2.7163, accuracy: 14.1366%\n",
      "Epoch [2/2], Step [2929/3732], Loss: 3.3524, accuracy: 14.1345%\n",
      "Epoch [2/2], Step [2930/3732], Loss: 3.0271, accuracy: 14.1361%\n",
      "Epoch [2/2], Step [2931/3732], Loss: 2.8023, accuracy: 14.1378%\n",
      "Epoch [2/2], Step [2932/3732], Loss: 3.6928, accuracy: 14.1375%\n",
      "Epoch [2/2], Step [2933/3732], Loss: 3.0825, accuracy: 14.1392%\n",
      "Epoch [2/2], Step [2934/3732], Loss: 3.3156, accuracy: 14.1389%\n",
      "Epoch [2/2], Step [2935/3732], Loss: 3.2105, accuracy: 14.1368%\n",
      "Epoch [2/2], Step [2936/3732], Loss: 2.9882, accuracy: 14.1384%\n",
      "Epoch [2/2], Step [2937/3732], Loss: 3.1361, accuracy: 14.1363%\n",
      "Epoch [2/2], Step [2938/3732], Loss: 3.1976, accuracy: 14.1361%\n",
      "Epoch [2/2], Step [2939/3732], Loss: 2.9396, accuracy: 14.1377%\n",
      "Epoch [2/2], Step [2940/3732], Loss: 2.9854, accuracy: 14.1412%\n",
      "Epoch [2/2], Step [2941/3732], Loss: 2.8440, accuracy: 14.1428%\n",
      "Epoch [2/2], Step [2942/3732], Loss: 3.7596, accuracy: 14.1407%\n",
      "Epoch [2/2], Step [2943/3732], Loss: 3.3367, accuracy: 14.1404%\n",
      "Epoch [2/2], Step [2944/3732], Loss: 2.5784, accuracy: 14.1421%\n",
      "Epoch [2/2], Step [2945/3732], Loss: 2.0856, accuracy: 14.1474%\n",
      "Epoch [2/2], Step [2946/3732], Loss: 2.5814, accuracy: 14.1491%\n",
      "Epoch [2/2], Step [2947/3732], Loss: 2.7954, accuracy: 14.1488%\n",
      "Epoch [2/2], Step [2948/3732], Loss: 2.8607, accuracy: 14.1467%\n",
      "Epoch [2/2], Step [2949/3732], Loss: 3.0762, accuracy: 14.1446%\n",
      "Epoch [2/2], Step [2950/3732], Loss: 2.8935, accuracy: 14.1481%\n",
      "Epoch [2/2], Step [2951/3732], Loss: 2.9810, accuracy: 14.1516%\n",
      "Epoch [2/2], Step [2952/3732], Loss: 2.8705, accuracy: 14.1513%\n",
      "Epoch [2/2], Step [2953/3732], Loss: 3.5209, accuracy: 14.1511%\n",
      "Epoch [2/2], Step [2954/3732], Loss: 2.9274, accuracy: 14.1490%\n",
      "Epoch [2/2], Step [2955/3732], Loss: 3.4764, accuracy: 14.1506%\n",
      "Epoch [2/2], Step [2956/3732], Loss: 2.5852, accuracy: 14.1503%\n",
      "Epoch [2/2], Step [2957/3732], Loss: 2.2665, accuracy: 14.1501%\n",
      "Epoch [2/2], Step [2958/3732], Loss: 3.6450, accuracy: 14.1480%\n",
      "Epoch [2/2], Step [2959/3732], Loss: 2.8639, accuracy: 14.1496%\n",
      "Epoch [2/2], Step [2960/3732], Loss: 2.8345, accuracy: 14.1475%\n",
      "Epoch [2/2], Step [2961/3732], Loss: 2.9960, accuracy: 14.1472%\n",
      "Epoch [2/2], Step [2962/3732], Loss: 2.4216, accuracy: 14.1507%\n",
      "Epoch [2/2], Step [2963/3732], Loss: 3.2866, accuracy: 14.1505%\n",
      "Epoch [2/2], Step [2964/3732], Loss: 3.0914, accuracy: 14.1502%\n",
      "Epoch [2/2], Step [2965/3732], Loss: 2.8899, accuracy: 14.1519%\n",
      "Epoch [2/2], Step [2966/3732], Loss: 3.3436, accuracy: 14.1497%\n",
      "Epoch [2/2], Step [2967/3732], Loss: 4.0167, accuracy: 14.1476%\n",
      "Epoch [2/2], Step [2968/3732], Loss: 3.0491, accuracy: 14.1474%\n",
      "Epoch [2/2], Step [2969/3732], Loss: 3.5614, accuracy: 14.1490%\n",
      "Epoch [2/2], Step [2970/3732], Loss: 3.2913, accuracy: 14.1488%\n",
      "Epoch [2/2], Step [2971/3732], Loss: 2.7802, accuracy: 14.1522%\n",
      "Epoch [2/2], Step [2972/3732], Loss: 3.9158, accuracy: 14.1501%\n",
      "Epoch [2/2], Step [2973/3732], Loss: 2.7627, accuracy: 14.1518%\n",
      "Epoch [2/2], Step [2974/3732], Loss: 2.9950, accuracy: 14.1515%\n",
      "Epoch [2/2], Step [2975/3732], Loss: 3.0284, accuracy: 14.1550%\n",
      "Epoch [2/2], Step [2976/3732], Loss: 3.6751, accuracy: 14.1585%\n",
      "Epoch [2/2], Step [2977/3732], Loss: 3.3026, accuracy: 14.1601%\n",
      "Epoch [2/2], Step [2978/3732], Loss: 3.0769, accuracy: 14.1580%\n",
      "Epoch [2/2], Step [2979/3732], Loss: 2.6846, accuracy: 14.1577%\n",
      "Epoch [2/2], Step [2980/3732], Loss: 3.6478, accuracy: 14.1556%\n",
      "Epoch [2/2], Step [2981/3732], Loss: 2.9361, accuracy: 14.1554%\n",
      "Epoch [2/2], Step [2982/3732], Loss: 2.3441, accuracy: 14.1588%\n",
      "Epoch [2/2], Step [2983/3732], Loss: 2.6427, accuracy: 14.1642%\n",
      "Epoch [2/2], Step [2984/3732], Loss: 2.8054, accuracy: 14.1621%\n",
      "Epoch [2/2], Step [2985/3732], Loss: 2.7134, accuracy: 14.1637%\n",
      "Epoch [2/2], Step [2986/3732], Loss: 3.1447, accuracy: 14.1653%\n",
      "Epoch [2/2], Step [2987/3732], Loss: 3.3914, accuracy: 14.1632%\n",
      "Epoch [2/2], Step [2988/3732], Loss: 2.0259, accuracy: 14.1667%\n",
      "Epoch [2/2], Step [2989/3732], Loss: 2.6825, accuracy: 14.1683%\n",
      "Epoch [2/2], Step [2990/3732], Loss: 3.0874, accuracy: 14.1680%\n",
      "Epoch [2/2], Step [2991/3732], Loss: 3.7333, accuracy: 14.1659%\n",
      "Epoch [2/2], Step [2992/3732], Loss: 3.2198, accuracy: 14.1675%\n",
      "Epoch [2/2], Step [2993/3732], Loss: 2.9722, accuracy: 14.1691%\n",
      "Epoch [2/2], Step [2994/3732], Loss: 3.0849, accuracy: 14.1689%\n",
      "Epoch [2/2], Step [2995/3732], Loss: 3.5086, accuracy: 14.1668%\n",
      "Epoch [2/2], Step [2996/3732], Loss: 2.8230, accuracy: 14.1665%\n",
      "Epoch [2/2], Step [2997/3732], Loss: 3.3408, accuracy: 14.1663%\n",
      "Epoch [2/2], Step [2998/3732], Loss: 2.7458, accuracy: 14.1679%\n",
      "Epoch [2/2], Step [2999/3732], Loss: 2.8092, accuracy: 14.1695%\n",
      "Epoch [2/2], Step [3000/3732], Loss: 3.3069, accuracy: 14.1693%\n",
      "Epoch [2/2], Step [3001/3732], Loss: 2.9521, accuracy: 14.1690%\n",
      "Epoch [2/2], Step [3002/3732], Loss: 3.2972, accuracy: 14.1688%\n",
      "Epoch [2/2], Step [3003/3732], Loss: 3.3830, accuracy: 14.1667%\n",
      "Epoch [2/2], Step [3004/3732], Loss: 2.5692, accuracy: 14.1701%\n",
      "Epoch [2/2], Step [3005/3732], Loss: 3.4664, accuracy: 14.1699%\n",
      "Epoch [2/2], Step [3006/3732], Loss: 2.4581, accuracy: 14.1715%\n",
      "Epoch [2/2], Step [3007/3732], Loss: 3.1002, accuracy: 14.1731%\n",
      "Epoch [2/2], Step [3008/3732], Loss: 3.5126, accuracy: 14.1728%\n",
      "Epoch [2/2], Step [3009/3732], Loss: 3.5679, accuracy: 14.1726%\n",
      "Epoch [2/2], Step [3010/3732], Loss: 2.3007, accuracy: 14.1761%\n",
      "Epoch [2/2], Step [3011/3732], Loss: 2.5262, accuracy: 14.1777%\n",
      "Epoch [2/2], Step [3012/3732], Loss: 2.8040, accuracy: 14.1811%\n",
      "Epoch [2/2], Step [3013/3732], Loss: 2.1115, accuracy: 14.1864%\n",
      "Epoch [2/2], Step [3014/3732], Loss: 2.8189, accuracy: 14.1880%\n",
      "Epoch [2/2], Step [3015/3732], Loss: 3.6244, accuracy: 14.1859%\n",
      "Epoch [2/2], Step [3016/3732], Loss: 2.9099, accuracy: 14.1857%\n",
      "Epoch [2/2], Step [3017/3732], Loss: 3.8548, accuracy: 14.1854%\n",
      "Epoch [2/2], Step [3018/3732], Loss: 2.6828, accuracy: 14.1870%\n",
      "Epoch [2/2], Step [3019/3732], Loss: 4.3676, accuracy: 14.1849%\n",
      "Epoch [2/2], Step [3020/3732], Loss: 2.7735, accuracy: 14.1847%\n",
      "Epoch [2/2], Step [3021/3732], Loss: 2.6158, accuracy: 14.1881%\n",
      "Epoch [2/2], Step [3022/3732], Loss: 3.3388, accuracy: 14.1897%\n",
      "Epoch [2/2], Step [3023/3732], Loss: 3.1672, accuracy: 14.1913%\n",
      "Epoch [2/2], Step [3024/3732], Loss: 3.4803, accuracy: 14.1911%\n",
      "Epoch [2/2], Step [3025/3732], Loss: 2.7869, accuracy: 14.1927%\n",
      "Epoch [2/2], Step [3026/3732], Loss: 2.6877, accuracy: 14.1906%\n",
      "Epoch [2/2], Step [3027/3732], Loss: 3.4826, accuracy: 14.1922%\n",
      "Epoch [2/2], Step [3028/3732], Loss: 3.0619, accuracy: 14.1919%\n",
      "Epoch [2/2], Step [3029/3732], Loss: 2.4707, accuracy: 14.1954%\n",
      "Epoch [2/2], Step [3030/3732], Loss: 2.3590, accuracy: 14.2007%\n",
      "Epoch [2/2], Step [3031/3732], Loss: 3.0457, accuracy: 14.2023%\n",
      "Epoch [2/2], Step [3032/3732], Loss: 2.7989, accuracy: 14.2020%\n",
      "Epoch [2/2], Step [3033/3732], Loss: 2.6894, accuracy: 14.2036%\n",
      "Epoch [2/2], Step [3034/3732], Loss: 3.6811, accuracy: 14.2034%\n",
      "Epoch [2/2], Step [3035/3732], Loss: 2.8974, accuracy: 14.2068%\n",
      "Epoch [2/2], Step [3036/3732], Loss: 2.9914, accuracy: 14.2084%\n",
      "Epoch [2/2], Step [3037/3732], Loss: 3.5502, accuracy: 14.2082%\n",
      "Epoch [2/2], Step [3038/3732], Loss: 3.0388, accuracy: 14.2097%\n",
      "Epoch [2/2], Step [3039/3732], Loss: 3.2428, accuracy: 14.2095%\n",
      "Epoch [2/2], Step [3040/3732], Loss: 2.9739, accuracy: 14.2111%\n",
      "Epoch [2/2], Step [3041/3732], Loss: 3.3641, accuracy: 14.2108%\n",
      "Epoch [2/2], Step [3042/3732], Loss: 3.2379, accuracy: 14.2087%\n",
      "Epoch [2/2], Step [3043/3732], Loss: 3.1963, accuracy: 14.2085%\n",
      "Epoch [2/2], Step [3044/3732], Loss: 2.9190, accuracy: 14.2101%\n",
      "Epoch [2/2], Step [3045/3732], Loss: 3.4505, accuracy: 14.2080%\n",
      "Epoch [2/2], Step [3046/3732], Loss: 3.0021, accuracy: 14.2096%\n",
      "Epoch [2/2], Step [3047/3732], Loss: 2.3890, accuracy: 14.2130%\n",
      "Epoch [2/2], Step [3048/3732], Loss: 2.4874, accuracy: 14.2164%\n",
      "Epoch [2/2], Step [3049/3732], Loss: 2.7048, accuracy: 14.2180%\n",
      "Epoch [2/2], Step [3050/3732], Loss: 2.6486, accuracy: 14.2178%\n",
      "Epoch [2/2], Step [3051/3732], Loss: 2.7434, accuracy: 14.2194%\n",
      "Epoch [2/2], Step [3052/3732], Loss: 2.6826, accuracy: 14.2210%\n",
      "Epoch [2/2], Step [3053/3732], Loss: 3.1267, accuracy: 14.2207%\n",
      "Epoch [2/2], Step [3054/3732], Loss: 3.0466, accuracy: 14.2205%\n",
      "Epoch [2/2], Step [3055/3732], Loss: 3.5990, accuracy: 14.2184%\n",
      "Epoch [2/2], Step [3056/3732], Loss: 3.4243, accuracy: 14.2218%\n",
      "Epoch [2/2], Step [3057/3732], Loss: 3.2915, accuracy: 14.2234%\n",
      "Epoch [2/2], Step [3058/3732], Loss: 3.2990, accuracy: 14.2213%\n",
      "Epoch [2/2], Step [3059/3732], Loss: 2.6729, accuracy: 14.2229%\n",
      "Epoch [2/2], Step [3060/3732], Loss: 3.0776, accuracy: 14.2208%\n",
      "Epoch [2/2], Step [3061/3732], Loss: 2.1294, accuracy: 14.2205%\n",
      "Epoch [2/2], Step [3062/3732], Loss: 4.0467, accuracy: 14.2184%\n",
      "Epoch [2/2], Step [3063/3732], Loss: 2.7835, accuracy: 14.2182%\n",
      "Epoch [2/2], Step [3064/3732], Loss: 3.3088, accuracy: 14.2179%\n",
      "Epoch [2/2], Step [3065/3732], Loss: 2.7456, accuracy: 14.2195%\n",
      "Epoch [2/2], Step [3066/3732], Loss: 2.7880, accuracy: 14.2229%\n",
      "Epoch [2/2], Step [3067/3732], Loss: 2.1309, accuracy: 14.2264%\n",
      "Epoch [2/2], Step [3068/3732], Loss: 2.6809, accuracy: 14.2316%\n",
      "Epoch [2/2], Step [3069/3732], Loss: 2.2126, accuracy: 14.2369%\n",
      "Epoch [2/2], Step [3070/3732], Loss: 3.5253, accuracy: 14.2366%\n",
      "Epoch [2/2], Step [3071/3732], Loss: 3.1470, accuracy: 14.2382%\n",
      "Epoch [2/2], Step [3072/3732], Loss: 3.1636, accuracy: 14.2398%\n",
      "Epoch [2/2], Step [3073/3732], Loss: 3.2600, accuracy: 14.2395%\n",
      "Epoch [2/2], Step [3074/3732], Loss: 2.6381, accuracy: 14.2411%\n",
      "Epoch [2/2], Step [3075/3732], Loss: 2.6662, accuracy: 14.2427%\n",
      "Epoch [2/2], Step [3076/3732], Loss: 2.9044, accuracy: 14.2443%\n",
      "Epoch [2/2], Step [3077/3732], Loss: 3.6330, accuracy: 14.2440%\n",
      "Epoch [2/2], Step [3078/3732], Loss: 2.8722, accuracy: 14.2438%\n",
      "Epoch [2/2], Step [3079/3732], Loss: 2.6516, accuracy: 14.2435%\n",
      "Epoch [2/2], Step [3080/3732], Loss: 2.1820, accuracy: 14.2488%\n",
      "Epoch [2/2], Step [3081/3732], Loss: 2.9938, accuracy: 14.2485%\n",
      "Epoch [2/2], Step [3082/3732], Loss: 3.2696, accuracy: 14.2482%\n",
      "Epoch [2/2], Step [3083/3732], Loss: 2.5134, accuracy: 14.2535%\n",
      "Epoch [2/2], Step [3084/3732], Loss: 2.3819, accuracy: 14.2569%\n",
      "Epoch [2/2], Step [3085/3732], Loss: 3.1770, accuracy: 14.2585%\n",
      "Epoch [2/2], Step [3086/3732], Loss: 2.7284, accuracy: 14.2619%\n",
      "Epoch [2/2], Step [3087/3732], Loss: 3.2689, accuracy: 14.2616%\n",
      "Epoch [2/2], Step [3088/3732], Loss: 2.6617, accuracy: 14.2614%\n",
      "Epoch [2/2], Step [3089/3732], Loss: 2.7468, accuracy: 14.2629%\n",
      "Epoch [2/2], Step [3090/3732], Loss: 3.0988, accuracy: 14.2645%\n",
      "Epoch [2/2], Step [3091/3732], Loss: 2.9946, accuracy: 14.2661%\n",
      "Epoch [2/2], Step [3092/3732], Loss: 2.7783, accuracy: 14.2658%\n",
      "Epoch [2/2], Step [3093/3732], Loss: 2.9380, accuracy: 14.2674%\n",
      "Epoch [2/2], Step [3094/3732], Loss: 3.3145, accuracy: 14.2690%\n",
      "Epoch [2/2], Step [3095/3732], Loss: 2.3870, accuracy: 14.2724%\n",
      "Epoch [2/2], Step [3096/3732], Loss: 2.9177, accuracy: 14.2703%\n",
      "Epoch [2/2], Step [3097/3732], Loss: 3.1216, accuracy: 14.2719%\n",
      "Epoch [2/2], Step [3098/3732], Loss: 2.6383, accuracy: 14.2771%\n",
      "Epoch [2/2], Step [3099/3732], Loss: 2.9116, accuracy: 14.2787%\n",
      "Epoch [2/2], Step [3100/3732], Loss: 2.7633, accuracy: 14.2784%\n",
      "Epoch [2/2], Step [3101/3732], Loss: 2.8919, accuracy: 14.2781%\n",
      "Epoch [2/2], Step [3102/3732], Loss: 3.4133, accuracy: 14.2779%\n",
      "Epoch [2/2], Step [3103/3732], Loss: 3.1918, accuracy: 14.2776%\n",
      "Epoch [2/2], Step [3104/3732], Loss: 2.7142, accuracy: 14.2792%\n",
      "Epoch [2/2], Step [3105/3732], Loss: 2.4358, accuracy: 14.2844%\n",
      "Epoch [2/2], Step [3106/3732], Loss: 2.3621, accuracy: 14.2878%\n",
      "Epoch [2/2], Step [3107/3732], Loss: 3.0532, accuracy: 14.2894%\n",
      "Epoch [2/2], Step [3108/3732], Loss: 2.5751, accuracy: 14.2928%\n",
      "Epoch [2/2], Step [3109/3732], Loss: 3.3001, accuracy: 14.2925%\n",
      "Epoch [2/2], Step [3110/3732], Loss: 2.7474, accuracy: 14.2941%\n",
      "Epoch [2/2], Step [3111/3732], Loss: 3.2048, accuracy: 14.2938%\n",
      "Epoch [2/2], Step [3112/3732], Loss: 3.1245, accuracy: 14.2935%\n",
      "Epoch [2/2], Step [3113/3732], Loss: 3.7656, accuracy: 14.2915%\n",
      "Epoch [2/2], Step [3114/3732], Loss: 3.0498, accuracy: 14.2967%\n",
      "Epoch [2/2], Step [3115/3732], Loss: 3.0406, accuracy: 14.2982%\n",
      "Epoch [2/2], Step [3116/3732], Loss: 3.4989, accuracy: 14.2980%\n",
      "Epoch [2/2], Step [3117/3732], Loss: 3.4260, accuracy: 14.3014%\n",
      "Epoch [2/2], Step [3118/3732], Loss: 2.7059, accuracy: 14.3029%\n",
      "Epoch [2/2], Step [3119/3732], Loss: 2.9994, accuracy: 14.3063%\n",
      "Epoch [2/2], Step [3120/3732], Loss: 2.5375, accuracy: 14.3097%\n",
      "Epoch [2/2], Step [3121/3732], Loss: 2.8454, accuracy: 14.3131%\n",
      "Epoch [2/2], Step [3122/3732], Loss: 3.5564, accuracy: 14.3110%\n",
      "Epoch [2/2], Step [3123/3732], Loss: 2.6050, accuracy: 14.3125%\n",
      "Epoch [2/2], Step [3124/3732], Loss: 2.8815, accuracy: 14.3123%\n",
      "Epoch [2/2], Step [3125/3732], Loss: 2.8905, accuracy: 14.3102%\n",
      "Epoch [2/2], Step [3126/3732], Loss: 2.8756, accuracy: 14.3118%\n",
      "Epoch [2/2], Step [3127/3732], Loss: 3.1591, accuracy: 14.3133%\n",
      "Epoch [2/2], Step [3128/3732], Loss: 2.7654, accuracy: 14.3130%\n",
      "Epoch [2/2], Step [3129/3732], Loss: 2.4503, accuracy: 14.3146%\n",
      "Epoch [2/2], Step [3130/3732], Loss: 2.7218, accuracy: 14.3162%\n",
      "Epoch [2/2], Step [3131/3732], Loss: 2.2500, accuracy: 14.3177%\n",
      "Epoch [2/2], Step [3132/3732], Loss: 3.1181, accuracy: 14.3211%\n",
      "Epoch [2/2], Step [3133/3732], Loss: 3.0405, accuracy: 14.3227%\n",
      "Epoch [2/2], Step [3134/3732], Loss: 3.7419, accuracy: 14.3224%\n",
      "Epoch [2/2], Step [3135/3732], Loss: 3.7483, accuracy: 14.3203%\n",
      "Epoch [2/2], Step [3136/3732], Loss: 2.5637, accuracy: 14.3255%\n",
      "Epoch [2/2], Step [3137/3732], Loss: 2.6182, accuracy: 14.3270%\n",
      "Epoch [2/2], Step [3138/3732], Loss: 3.1634, accuracy: 14.3250%\n",
      "Epoch [2/2], Step [3139/3732], Loss: 2.8485, accuracy: 14.3265%\n",
      "Epoch [2/2], Step [3140/3732], Loss: 3.2591, accuracy: 14.3281%\n",
      "Epoch [2/2], Step [3141/3732], Loss: 3.2339, accuracy: 14.3278%\n",
      "Epoch [2/2], Step [3142/3732], Loss: 2.6651, accuracy: 14.3294%\n",
      "Epoch [2/2], Step [3143/3732], Loss: 3.3526, accuracy: 14.3309%\n",
      "Epoch [2/2], Step [3144/3732], Loss: 2.7347, accuracy: 14.3306%\n",
      "Epoch [2/2], Step [3145/3732], Loss: 3.2620, accuracy: 14.3286%\n",
      "Epoch [2/2], Step [3146/3732], Loss: 3.0909, accuracy: 14.3283%\n",
      "Epoch [2/2], Step [3147/3732], Loss: 2.5712, accuracy: 14.3298%\n",
      "Epoch [2/2], Step [3148/3732], Loss: 2.2190, accuracy: 14.3350%\n",
      "Epoch [2/2], Step [3149/3732], Loss: 3.3332, accuracy: 14.3329%\n",
      "Epoch [2/2], Step [3150/3732], Loss: 3.7781, accuracy: 14.3309%\n",
      "Epoch [2/2], Step [3151/3732], Loss: 2.9535, accuracy: 14.3324%\n",
      "Epoch [2/2], Step [3152/3732], Loss: 2.7154, accuracy: 14.3340%\n",
      "Epoch [2/2], Step [3153/3732], Loss: 2.3525, accuracy: 14.3373%\n",
      "Epoch [2/2], Step [3154/3732], Loss: 2.5812, accuracy: 14.3425%\n",
      "Epoch [2/2], Step [3155/3732], Loss: 2.6746, accuracy: 14.3422%\n",
      "Epoch [2/2], Step [3156/3732], Loss: 2.5135, accuracy: 14.3456%\n",
      "Epoch [2/2], Step [3157/3732], Loss: 3.3167, accuracy: 14.3471%\n",
      "Epoch [2/2], Step [3158/3732], Loss: 3.0079, accuracy: 14.3469%\n",
      "Epoch [2/2], Step [3159/3732], Loss: 3.5347, accuracy: 14.3448%\n",
      "Epoch [2/2], Step [3160/3732], Loss: 3.1132, accuracy: 14.3463%\n",
      "Epoch [2/2], Step [3161/3732], Loss: 3.1556, accuracy: 14.3461%\n",
      "Epoch [2/2], Step [3162/3732], Loss: 2.8914, accuracy: 14.3476%\n",
      "Epoch [2/2], Step [3163/3732], Loss: 3.3228, accuracy: 14.3492%\n",
      "Epoch [2/2], Step [3164/3732], Loss: 2.9440, accuracy: 14.3471%\n",
      "Epoch [2/2], Step [3165/3732], Loss: 3.7117, accuracy: 14.3468%\n",
      "Epoch [2/2], Step [3166/3732], Loss: 3.3071, accuracy: 14.3484%\n",
      "Epoch [2/2], Step [3167/3732], Loss: 3.2214, accuracy: 14.3463%\n",
      "Epoch [2/2], Step [3168/3732], Loss: 3.0018, accuracy: 14.3478%\n",
      "Epoch [2/2], Step [3169/3732], Loss: 3.0795, accuracy: 14.3494%\n",
      "Epoch [2/2], Step [3170/3732], Loss: 2.6035, accuracy: 14.3545%\n",
      "Epoch [2/2], Step [3171/3732], Loss: 2.7980, accuracy: 14.3543%\n",
      "Epoch [2/2], Step [3172/3732], Loss: 3.0383, accuracy: 14.3540%\n",
      "Epoch [2/2], Step [3173/3732], Loss: 3.0725, accuracy: 14.3537%\n",
      "Epoch [2/2], Step [3174/3732], Loss: 2.7300, accuracy: 14.3553%\n",
      "Epoch [2/2], Step [3175/3732], Loss: 2.7341, accuracy: 14.3550%\n",
      "Epoch [2/2], Step [3176/3732], Loss: 2.6930, accuracy: 14.3547%\n",
      "Epoch [2/2], Step [3177/3732], Loss: 3.4909, accuracy: 14.3545%\n",
      "Epoch [2/2], Step [3178/3732], Loss: 2.3328, accuracy: 14.3596%\n",
      "Epoch [2/2], Step [3179/3732], Loss: 2.9691, accuracy: 14.3612%\n",
      "Epoch [2/2], Step [3180/3732], Loss: 3.4423, accuracy: 14.3627%\n",
      "Epoch [2/2], Step [3181/3732], Loss: 2.6298, accuracy: 14.3660%\n",
      "Epoch [2/2], Step [3182/3732], Loss: 2.9511, accuracy: 14.3694%\n",
      "Epoch [2/2], Step [3183/3732], Loss: 3.4665, accuracy: 14.3673%\n",
      "Epoch [2/2], Step [3184/3732], Loss: 2.8328, accuracy: 14.3707%\n",
      "Epoch [2/2], Step [3185/3732], Loss: 3.0407, accuracy: 14.3704%\n",
      "Epoch [2/2], Step [3186/3732], Loss: 3.9898, accuracy: 14.3701%\n",
      "Epoch [2/2], Step [3187/3732], Loss: 2.2772, accuracy: 14.3717%\n",
      "Epoch [2/2], Step [3188/3732], Loss: 3.1526, accuracy: 14.3750%\n",
      "Epoch [2/2], Step [3189/3732], Loss: 3.0244, accuracy: 14.3747%\n",
      "Epoch [2/2], Step [3190/3732], Loss: 2.9612, accuracy: 14.3745%\n",
      "Epoch [2/2], Step [3191/3732], Loss: 1.5479, accuracy: 14.3814%\n",
      "Epoch [2/2], Step [3192/3732], Loss: 2.2267, accuracy: 14.3829%\n",
      "Epoch [2/2], Step [3193/3732], Loss: 3.4422, accuracy: 14.3827%\n",
      "Epoch [2/2], Step [3194/3732], Loss: 2.3187, accuracy: 14.3842%\n",
      "Epoch [2/2], Step [3195/3732], Loss: 2.3522, accuracy: 14.3875%\n",
      "Epoch [2/2], Step [3196/3732], Loss: 3.6041, accuracy: 14.3855%\n",
      "Epoch [2/2], Step [3197/3732], Loss: 2.6019, accuracy: 14.3852%\n",
      "Epoch [2/2], Step [3198/3732], Loss: 1.9169, accuracy: 14.3921%\n",
      "Epoch [2/2], Step [3199/3732], Loss: 3.2998, accuracy: 14.3919%\n",
      "Epoch [2/2], Step [3200/3732], Loss: 1.4978, accuracy: 14.3988%\n",
      "Epoch [2/2], Step [3201/3732], Loss: 2.4455, accuracy: 14.4039%\n",
      "Epoch [2/2], Step [3202/3732], Loss: 3.1290, accuracy: 14.4037%\n",
      "Epoch [2/2], Step [3203/3732], Loss: 3.2117, accuracy: 14.4034%\n",
      "Epoch [2/2], Step [3204/3732], Loss: 2.3527, accuracy: 14.4049%\n",
      "Epoch [2/2], Step [3205/3732], Loss: 3.2654, accuracy: 14.4082%\n",
      "Epoch [2/2], Step [3206/3732], Loss: 2.6376, accuracy: 14.4080%\n",
      "Epoch [2/2], Step [3207/3732], Loss: 3.7873, accuracy: 14.4077%\n",
      "Epoch [2/2], Step [3208/3732], Loss: 3.7259, accuracy: 14.4074%\n",
      "Epoch [2/2], Step [3209/3732], Loss: 2.8788, accuracy: 14.4071%\n",
      "Epoch [2/2], Step [3210/3732], Loss: 2.3847, accuracy: 14.4105%\n",
      "Epoch [2/2], Step [3211/3732], Loss: 2.8595, accuracy: 14.4138%\n",
      "Epoch [2/2], Step [3212/3732], Loss: 3.5364, accuracy: 14.4153%\n",
      "Epoch [2/2], Step [3213/3732], Loss: 2.7593, accuracy: 14.4150%\n",
      "Epoch [2/2], Step [3214/3732], Loss: 3.7698, accuracy: 14.4130%\n",
      "Epoch [2/2], Step [3215/3732], Loss: 3.5634, accuracy: 14.4127%\n",
      "Epoch [2/2], Step [3216/3732], Loss: 3.0447, accuracy: 14.4196%\n",
      "Epoch [2/2], Step [3217/3732], Loss: 2.9768, accuracy: 14.4193%\n",
      "Epoch [2/2], Step [3218/3732], Loss: 2.4501, accuracy: 14.4209%\n",
      "Epoch [2/2], Step [3219/3732], Loss: 2.9571, accuracy: 14.4206%\n",
      "Epoch [2/2], Step [3220/3732], Loss: 3.9359, accuracy: 14.4185%\n",
      "Epoch [2/2], Step [3221/3732], Loss: 2.6382, accuracy: 14.4218%\n",
      "Epoch [2/2], Step [3222/3732], Loss: 2.4516, accuracy: 14.4252%\n",
      "Epoch [2/2], Step [3223/3732], Loss: 2.8959, accuracy: 14.4267%\n",
      "Epoch [2/2], Step [3224/3732], Loss: 4.0069, accuracy: 14.4246%\n",
      "Epoch [2/2], Step [3225/3732], Loss: 3.1358, accuracy: 14.4261%\n",
      "Epoch [2/2], Step [3226/3732], Loss: 2.9783, accuracy: 14.4258%\n",
      "Epoch [2/2], Step [3227/3732], Loss: 2.6482, accuracy: 14.4310%\n",
      "Epoch [2/2], Step [3228/3732], Loss: 3.4181, accuracy: 14.4307%\n",
      "Epoch [2/2], Step [3229/3732], Loss: 3.2133, accuracy: 14.4322%\n",
      "Epoch [2/2], Step [3230/3732], Loss: 2.9924, accuracy: 14.4301%\n",
      "Epoch [2/2], Step [3231/3732], Loss: 2.4563, accuracy: 14.4298%\n",
      "Epoch [2/2], Step [3232/3732], Loss: 3.4662, accuracy: 14.4314%\n",
      "Epoch [2/2], Step [3233/3732], Loss: 3.2559, accuracy: 14.4311%\n",
      "Epoch [2/2], Step [3234/3732], Loss: 3.3835, accuracy: 14.4326%\n",
      "Epoch [2/2], Step [3235/3732], Loss: 3.9349, accuracy: 14.4323%\n",
      "Epoch [2/2], Step [3236/3732], Loss: 2.5388, accuracy: 14.4338%\n",
      "Epoch [2/2], Step [3237/3732], Loss: 3.4791, accuracy: 14.4336%\n",
      "Epoch [2/2], Step [3238/3732], Loss: 3.0074, accuracy: 14.4351%\n",
      "Epoch [2/2], Step [3239/3732], Loss: 2.5366, accuracy: 14.4384%\n",
      "Epoch [2/2], Step [3240/3732], Loss: 2.8291, accuracy: 14.4399%\n",
      "Epoch [2/2], Step [3241/3732], Loss: 2.5629, accuracy: 14.4396%\n",
      "Epoch [2/2], Step [3242/3732], Loss: 2.6966, accuracy: 14.4411%\n",
      "Epoch [2/2], Step [3243/3732], Loss: 3.2819, accuracy: 14.4427%\n",
      "Epoch [2/2], Step [3244/3732], Loss: 3.1619, accuracy: 14.4424%\n",
      "Epoch [2/2], Step [3245/3732], Loss: 2.4739, accuracy: 14.4457%\n",
      "Epoch [2/2], Step [3246/3732], Loss: 2.3539, accuracy: 14.4490%\n",
      "Epoch [2/2], Step [3247/3732], Loss: 3.4236, accuracy: 14.4469%\n",
      "Epoch [2/2], Step [3248/3732], Loss: 2.4256, accuracy: 14.4484%\n",
      "Epoch [2/2], Step [3249/3732], Loss: 3.5372, accuracy: 14.4481%\n",
      "Epoch [2/2], Step [3250/3732], Loss: 3.2876, accuracy: 14.4497%\n",
      "Epoch [2/2], Step [3251/3732], Loss: 2.6785, accuracy: 14.4547%\n",
      "Epoch [2/2], Step [3252/3732], Loss: 2.4926, accuracy: 14.4545%\n",
      "Epoch [2/2], Step [3253/3732], Loss: 3.1149, accuracy: 14.4542%\n",
      "Epoch [2/2], Step [3254/3732], Loss: 2.1685, accuracy: 14.4593%\n",
      "Epoch [2/2], Step [3255/3732], Loss: 3.1332, accuracy: 14.4590%\n",
      "Epoch [2/2], Step [3256/3732], Loss: 3.3795, accuracy: 14.4587%\n",
      "Epoch [2/2], Step [3257/3732], Loss: 3.4609, accuracy: 14.4584%\n",
      "Epoch [2/2], Step [3258/3732], Loss: 2.4961, accuracy: 14.4617%\n",
      "Epoch [2/2], Step [3259/3732], Loss: 3.3364, accuracy: 14.4615%\n",
      "Epoch [2/2], Step [3260/3732], Loss: 2.8101, accuracy: 14.4630%\n",
      "Epoch [2/2], Step [3261/3732], Loss: 2.8927, accuracy: 14.4627%\n",
      "Epoch [2/2], Step [3262/3732], Loss: 2.8352, accuracy: 14.4660%\n",
      "Epoch [2/2], Step [3263/3732], Loss: 3.1677, accuracy: 14.4657%\n",
      "Epoch [2/2], Step [3264/3732], Loss: 2.8089, accuracy: 14.4672%\n",
      "Epoch [2/2], Step [3265/3732], Loss: 2.8495, accuracy: 14.4687%\n",
      "Epoch [2/2], Step [3266/3732], Loss: 3.7707, accuracy: 14.4666%\n",
      "Epoch [2/2], Step [3267/3732], Loss: 3.8676, accuracy: 14.4681%\n",
      "Epoch [2/2], Step [3268/3732], Loss: 3.3425, accuracy: 14.4661%\n",
      "Epoch [2/2], Step [3269/3732], Loss: 3.2915, accuracy: 14.4658%\n",
      "Epoch [2/2], Step [3270/3732], Loss: 3.5810, accuracy: 14.4655%\n",
      "Epoch [2/2], Step [3271/3732], Loss: 2.9963, accuracy: 14.4670%\n",
      "Epoch [2/2], Step [3272/3732], Loss: 2.3631, accuracy: 14.4703%\n",
      "Epoch [2/2], Step [3273/3732], Loss: 2.5170, accuracy: 14.4736%\n",
      "Epoch [2/2], Step [3274/3732], Loss: 2.3237, accuracy: 14.4769%\n",
      "Epoch [2/2], Step [3275/3732], Loss: 3.2514, accuracy: 14.4766%\n",
      "Epoch [2/2], Step [3276/3732], Loss: 3.1102, accuracy: 14.4799%\n",
      "Epoch [2/2], Step [3277/3732], Loss: 2.7452, accuracy: 14.4832%\n",
      "Epoch [2/2], Step [3278/3732], Loss: 2.4366, accuracy: 14.4864%\n",
      "Epoch [2/2], Step [3279/3732], Loss: 3.2749, accuracy: 14.4862%\n",
      "Epoch [2/2], Step [3280/3732], Loss: 2.8923, accuracy: 14.4859%\n",
      "Epoch [2/2], Step [3281/3732], Loss: 2.6356, accuracy: 14.4892%\n",
      "Epoch [2/2], Step [3282/3732], Loss: 3.1465, accuracy: 14.4889%\n",
      "Epoch [2/2], Step [3283/3732], Loss: 3.1671, accuracy: 14.4868%\n",
      "Epoch [2/2], Step [3284/3732], Loss: 3.2154, accuracy: 14.4883%\n",
      "Epoch [2/2], Step [3285/3732], Loss: 3.4216, accuracy: 14.4880%\n",
      "Epoch [2/2], Step [3286/3732], Loss: 3.2457, accuracy: 14.4895%\n",
      "Epoch [2/2], Step [3287/3732], Loss: 2.9886, accuracy: 14.4910%\n",
      "Epoch [2/2], Step [3288/3732], Loss: 3.2107, accuracy: 14.4907%\n",
      "Epoch [2/2], Step [3289/3732], Loss: 2.1067, accuracy: 14.4976%\n",
      "Epoch [2/2], Step [3290/3732], Loss: 3.5544, accuracy: 14.4973%\n",
      "Epoch [2/2], Step [3291/3732], Loss: 2.6858, accuracy: 14.4988%\n",
      "Epoch [2/2], Step [3292/3732], Loss: 3.3448, accuracy: 14.5003%\n",
      "Epoch [2/2], Step [3293/3732], Loss: 2.8999, accuracy: 14.5000%\n",
      "Epoch [2/2], Step [3294/3732], Loss: 3.7120, accuracy: 14.4997%\n",
      "Epoch [2/2], Step [3295/3732], Loss: 3.1945, accuracy: 14.4977%\n",
      "Epoch [2/2], Step [3296/3732], Loss: 2.3109, accuracy: 14.5009%\n",
      "Epoch [2/2], Step [3297/3732], Loss: 3.3016, accuracy: 14.5042%\n",
      "Epoch [2/2], Step [3298/3732], Loss: 2.6429, accuracy: 14.5075%\n",
      "Epoch [2/2], Step [3299/3732], Loss: 3.3060, accuracy: 14.5090%\n",
      "Epoch [2/2], Step [3300/3732], Loss: 3.1255, accuracy: 14.5087%\n",
      "Epoch [2/2], Step [3301/3732], Loss: 2.6148, accuracy: 14.5119%\n",
      "Epoch [2/2], Step [3302/3732], Loss: 2.8034, accuracy: 14.5152%\n",
      "Epoch [2/2], Step [3303/3732], Loss: 2.8971, accuracy: 14.5185%\n",
      "Epoch [2/2], Step [3304/3732], Loss: 2.9132, accuracy: 14.5182%\n",
      "Epoch [2/2], Step [3305/3732], Loss: 2.2272, accuracy: 14.5250%\n",
      "Epoch [2/2], Step [3306/3732], Loss: 2.5112, accuracy: 14.5265%\n",
      "Epoch [2/2], Step [3307/3732], Loss: 3.4588, accuracy: 14.5280%\n",
      "Epoch [2/2], Step [3308/3732], Loss: 2.4867, accuracy: 14.5313%\n",
      "Epoch [2/2], Step [3309/3732], Loss: 2.6458, accuracy: 14.5310%\n",
      "Epoch [2/2], Step [3310/3732], Loss: 3.4511, accuracy: 14.5289%\n",
      "Epoch [2/2], Step [3311/3732], Loss: 2.7767, accuracy: 14.5322%\n",
      "Epoch [2/2], Step [3312/3732], Loss: 2.8450, accuracy: 14.5354%\n",
      "Epoch [2/2], Step [3313/3732], Loss: 2.6569, accuracy: 14.5369%\n",
      "Epoch [2/2], Step [3314/3732], Loss: 3.1987, accuracy: 14.5348%\n",
      "Epoch [2/2], Step [3315/3732], Loss: 3.4064, accuracy: 14.5346%\n",
      "Epoch [2/2], Step [3316/3732], Loss: 2.9427, accuracy: 14.5343%\n",
      "Epoch [2/2], Step [3317/3732], Loss: 3.4139, accuracy: 14.5340%\n",
      "Epoch [2/2], Step [3318/3732], Loss: 2.9367, accuracy: 14.5337%\n",
      "Epoch [2/2], Step [3319/3732], Loss: 3.3978, accuracy: 14.5334%\n",
      "Epoch [2/2], Step [3320/3732], Loss: 3.3825, accuracy: 14.5313%\n",
      "Epoch [2/2], Step [3321/3732], Loss: 2.5295, accuracy: 14.5364%\n",
      "Epoch [2/2], Step [3322/3732], Loss: 2.3176, accuracy: 14.5379%\n",
      "Epoch [2/2], Step [3323/3732], Loss: 3.1134, accuracy: 14.5411%\n",
      "Epoch [2/2], Step [3324/3732], Loss: 2.7103, accuracy: 14.5408%\n",
      "Epoch [2/2], Step [3325/3732], Loss: 3.3606, accuracy: 14.5388%\n",
      "Epoch [2/2], Step [3326/3732], Loss: 2.9222, accuracy: 14.5402%\n",
      "Epoch [2/2], Step [3327/3732], Loss: 2.6135, accuracy: 14.5417%\n",
      "Epoch [2/2], Step [3328/3732], Loss: 2.7595, accuracy: 14.5432%\n",
      "Epoch [2/2], Step [3329/3732], Loss: 3.1666, accuracy: 14.5447%\n",
      "Epoch [2/2], Step [3330/3732], Loss: 3.0864, accuracy: 14.5444%\n",
      "Epoch [2/2], Step [3331/3732], Loss: 3.8559, accuracy: 14.5423%\n",
      "Epoch [2/2], Step [3332/3732], Loss: 3.0540, accuracy: 14.5420%\n",
      "Epoch [2/2], Step [3333/3732], Loss: 3.0676, accuracy: 14.5418%\n",
      "Epoch [2/2], Step [3334/3732], Loss: 2.7468, accuracy: 14.5450%\n",
      "Epoch [2/2], Step [3335/3732], Loss: 3.1405, accuracy: 14.5447%\n",
      "Epoch [2/2], Step [3336/3732], Loss: 2.4451, accuracy: 14.5480%\n",
      "Epoch [2/2], Step [3337/3732], Loss: 3.4284, accuracy: 14.5494%\n",
      "Epoch [2/2], Step [3338/3732], Loss: 2.8311, accuracy: 14.5509%\n",
      "Epoch [2/2], Step [3339/3732], Loss: 3.2177, accuracy: 14.5542%\n",
      "Epoch [2/2], Step [3340/3732], Loss: 2.4772, accuracy: 14.5574%\n",
      "Epoch [2/2], Step [3341/3732], Loss: 3.3214, accuracy: 14.5554%\n",
      "Epoch [2/2], Step [3342/3732], Loss: 3.1875, accuracy: 14.5568%\n",
      "Epoch [2/2], Step [3343/3732], Loss: 3.3178, accuracy: 14.5548%\n",
      "Epoch [2/2], Step [3344/3732], Loss: 2.5428, accuracy: 14.5562%\n",
      "Epoch [2/2], Step [3345/3732], Loss: 3.3571, accuracy: 14.5542%\n",
      "Epoch [2/2], Step [3346/3732], Loss: 3.4444, accuracy: 14.5539%\n",
      "Epoch [2/2], Step [3347/3732], Loss: 2.4449, accuracy: 14.5571%\n",
      "Epoch [2/2], Step [3348/3732], Loss: 3.1000, accuracy: 14.5569%\n",
      "Epoch [2/2], Step [3349/3732], Loss: 2.9880, accuracy: 14.5566%\n",
      "Epoch [2/2], Step [3350/3732], Loss: 2.8229, accuracy: 14.5580%\n",
      "Epoch [2/2], Step [3351/3732], Loss: 3.7462, accuracy: 14.5560%\n",
      "Epoch [2/2], Step [3352/3732], Loss: 2.7837, accuracy: 14.5592%\n",
      "Epoch [2/2], Step [3353/3732], Loss: 2.9304, accuracy: 14.5589%\n",
      "Epoch [2/2], Step [3354/3732], Loss: 3.3173, accuracy: 14.5604%\n",
      "Epoch [2/2], Step [3355/3732], Loss: 2.8813, accuracy: 14.5583%\n",
      "Epoch [2/2], Step [3356/3732], Loss: 2.9683, accuracy: 14.5598%\n",
      "Epoch [2/2], Step [3357/3732], Loss: 2.2311, accuracy: 14.5631%\n",
      "Epoch [2/2], Step [3358/3732], Loss: 2.6627, accuracy: 14.5645%\n",
      "Epoch [2/2], Step [3359/3732], Loss: 3.0519, accuracy: 14.5678%\n",
      "Epoch [2/2], Step [3360/3732], Loss: 3.0062, accuracy: 14.5710%\n",
      "Epoch [2/2], Step [3361/3732], Loss: 3.7694, accuracy: 14.5689%\n",
      "Epoch [2/2], Step [3362/3732], Loss: 2.4575, accuracy: 14.5704%\n",
      "Epoch [2/2], Step [3363/3732], Loss: 3.0785, accuracy: 14.5719%\n",
      "Epoch [2/2], Step [3364/3732], Loss: 3.2032, accuracy: 14.5734%\n",
      "Epoch [2/2], Step [3365/3732], Loss: 3.0888, accuracy: 14.5713%\n",
      "Epoch [2/2], Step [3366/3732], Loss: 2.6614, accuracy: 14.5728%\n",
      "Epoch [2/2], Step [3367/3732], Loss: 2.4384, accuracy: 14.5760%\n",
      "Epoch [2/2], Step [3368/3732], Loss: 3.2946, accuracy: 14.5775%\n",
      "Epoch [2/2], Step [3369/3732], Loss: 2.9450, accuracy: 14.5772%\n",
      "Epoch [2/2], Step [3370/3732], Loss: 2.8518, accuracy: 14.5769%\n",
      "Epoch [2/2], Step [3371/3732], Loss: 2.0657, accuracy: 14.5819%\n",
      "Epoch [2/2], Step [3372/3732], Loss: 3.6232, accuracy: 14.5798%\n",
      "Epoch [2/2], Step [3373/3732], Loss: 2.2265, accuracy: 14.5866%\n",
      "Epoch [2/2], Step [3374/3732], Loss: 2.9468, accuracy: 14.5880%\n",
      "Epoch [2/2], Step [3375/3732], Loss: 2.5113, accuracy: 14.5912%\n",
      "Epoch [2/2], Step [3376/3732], Loss: 2.6407, accuracy: 14.5945%\n",
      "Epoch [2/2], Step [3377/3732], Loss: 3.2640, accuracy: 14.5942%\n",
      "Epoch [2/2], Step [3378/3732], Loss: 2.4413, accuracy: 14.5939%\n",
      "Epoch [2/2], Step [3379/3732], Loss: 3.0839, accuracy: 14.5953%\n",
      "Epoch [2/2], Step [3380/3732], Loss: 3.0702, accuracy: 14.5968%\n",
      "Epoch [2/2], Step [3381/3732], Loss: 3.3194, accuracy: 14.5965%\n",
      "Epoch [2/2], Step [3382/3732], Loss: 3.3407, accuracy: 14.5980%\n",
      "Epoch [2/2], Step [3383/3732], Loss: 2.2984, accuracy: 14.6030%\n",
      "Epoch [2/2], Step [3384/3732], Loss: 3.2562, accuracy: 14.6027%\n",
      "Epoch [2/2], Step [3385/3732], Loss: 2.6962, accuracy: 14.6059%\n",
      "Epoch [2/2], Step [3386/3732], Loss: 2.8976, accuracy: 14.6091%\n",
      "Epoch [2/2], Step [3387/3732], Loss: 2.9706, accuracy: 14.6123%\n",
      "Epoch [2/2], Step [3388/3732], Loss: 3.2302, accuracy: 14.6120%\n",
      "Epoch [2/2], Step [3389/3732], Loss: 2.8044, accuracy: 14.6135%\n",
      "Epoch [2/2], Step [3390/3732], Loss: 2.8703, accuracy: 14.6132%\n",
      "Epoch [2/2], Step [3391/3732], Loss: 3.0423, accuracy: 14.6129%\n",
      "Epoch [2/2], Step [3392/3732], Loss: 3.1971, accuracy: 14.6143%\n",
      "Epoch [2/2], Step [3393/3732], Loss: 4.1212, accuracy: 14.6123%\n",
      "Epoch [2/2], Step [3394/3732], Loss: 2.5139, accuracy: 14.6137%\n",
      "Epoch [2/2], Step [3395/3732], Loss: 3.0645, accuracy: 14.6134%\n",
      "Epoch [2/2], Step [3396/3732], Loss: 3.1431, accuracy: 14.6131%\n",
      "Epoch [2/2], Step [3397/3732], Loss: 3.0185, accuracy: 14.6128%\n",
      "Epoch [2/2], Step [3398/3732], Loss: 2.4526, accuracy: 14.6161%\n",
      "Epoch [2/2], Step [3399/3732], Loss: 3.6237, accuracy: 14.6140%\n",
      "Epoch [2/2], Step [3400/3732], Loss: 2.2970, accuracy: 14.6155%\n",
      "Epoch [2/2], Step [3401/3732], Loss: 3.3562, accuracy: 14.6169%\n",
      "Epoch [2/2], Step [3402/3732], Loss: 3.1511, accuracy: 14.6184%\n",
      "Epoch [2/2], Step [3403/3732], Loss: 3.5133, accuracy: 14.6198%\n",
      "Epoch [2/2], Step [3404/3732], Loss: 2.6057, accuracy: 14.6213%\n",
      "Epoch [2/2], Step [3405/3732], Loss: 3.0961, accuracy: 14.6245%\n",
      "Epoch [2/2], Step [3406/3732], Loss: 2.8019, accuracy: 14.6242%\n",
      "Epoch [2/2], Step [3407/3732], Loss: 2.8426, accuracy: 14.6256%\n",
      "Epoch [2/2], Step [3408/3732], Loss: 3.1882, accuracy: 14.6271%\n",
      "Epoch [2/2], Step [3409/3732], Loss: 2.4169, accuracy: 14.6303%\n",
      "Epoch [2/2], Step [3410/3732], Loss: 2.6483, accuracy: 14.6335%\n",
      "Epoch [2/2], Step [3411/3732], Loss: 3.2065, accuracy: 14.6350%\n",
      "Epoch [2/2], Step [3412/3732], Loss: 2.3952, accuracy: 14.6399%\n",
      "Epoch [2/2], Step [3413/3732], Loss: 3.0223, accuracy: 14.6414%\n",
      "Epoch [2/2], Step [3414/3732], Loss: 3.5486, accuracy: 14.6411%\n",
      "Epoch [2/2], Step [3415/3732], Loss: 2.8216, accuracy: 14.6408%\n",
      "Epoch [2/2], Step [3416/3732], Loss: 2.8693, accuracy: 14.6440%\n",
      "Epoch [2/2], Step [3417/3732], Loss: 3.0654, accuracy: 14.6454%\n",
      "Epoch [2/2], Step [3418/3732], Loss: 3.4717, accuracy: 14.6434%\n",
      "Epoch [2/2], Step [3419/3732], Loss: 2.5895, accuracy: 14.6483%\n",
      "Epoch [2/2], Step [3420/3732], Loss: 2.4350, accuracy: 14.6497%\n",
      "Epoch [2/2], Step [3421/3732], Loss: 3.1936, accuracy: 14.6512%\n",
      "Epoch [2/2], Step [3422/3732], Loss: 4.1609, accuracy: 14.6491%\n",
      "Epoch [2/2], Step [3423/3732], Loss: 2.8479, accuracy: 14.6541%\n",
      "Epoch [2/2], Step [3424/3732], Loss: 2.9407, accuracy: 14.6538%\n",
      "Epoch [2/2], Step [3425/3732], Loss: 2.9377, accuracy: 14.6552%\n",
      "Epoch [2/2], Step [3426/3732], Loss: 2.8756, accuracy: 14.6567%\n",
      "Epoch [2/2], Step [3427/3732], Loss: 2.2509, accuracy: 14.6599%\n",
      "Epoch [2/2], Step [3428/3732], Loss: 2.8296, accuracy: 14.6613%\n",
      "Epoch [2/2], Step [3429/3732], Loss: 2.7732, accuracy: 14.6628%\n",
      "Epoch [2/2], Step [3430/3732], Loss: 3.0045, accuracy: 14.6625%\n",
      "Epoch [2/2], Step [3431/3732], Loss: 3.0032, accuracy: 14.6604%\n",
      "Epoch [2/2], Step [3432/3732], Loss: 3.4231, accuracy: 14.6584%\n",
      "Epoch [2/2], Step [3433/3732], Loss: 3.5042, accuracy: 14.6598%\n",
      "Epoch [2/2], Step [3434/3732], Loss: 2.6733, accuracy: 14.6630%\n",
      "Epoch [2/2], Step [3435/3732], Loss: 3.7935, accuracy: 14.6644%\n",
      "Epoch [2/2], Step [3436/3732], Loss: 2.1679, accuracy: 14.6676%\n",
      "Epoch [2/2], Step [3437/3732], Loss: 3.0091, accuracy: 14.6725%\n",
      "Epoch [2/2], Step [3438/3732], Loss: 3.4303, accuracy: 14.6722%\n",
      "Epoch [2/2], Step [3439/3732], Loss: 3.1307, accuracy: 14.6737%\n",
      "Epoch [2/2], Step [3440/3732], Loss: 2.2034, accuracy: 14.6786%\n",
      "Epoch [2/2], Step [3441/3732], Loss: 2.7424, accuracy: 14.6783%\n",
      "Epoch [2/2], Step [3442/3732], Loss: 2.3336, accuracy: 14.6815%\n",
      "Epoch [2/2], Step [3443/3732], Loss: 3.2266, accuracy: 14.6812%\n",
      "Epoch [2/2], Step [3444/3732], Loss: 2.7917, accuracy: 14.6826%\n",
      "Epoch [2/2], Step [3445/3732], Loss: 2.1851, accuracy: 14.6858%\n",
      "Epoch [2/2], Step [3446/3732], Loss: 3.2361, accuracy: 14.6855%\n",
      "Epoch [2/2], Step [3447/3732], Loss: 1.9423, accuracy: 14.6904%\n",
      "Epoch [2/2], Step [3448/3732], Loss: 2.4426, accuracy: 14.6936%\n",
      "Epoch [2/2], Step [3449/3732], Loss: 2.7816, accuracy: 14.6933%\n",
      "Epoch [2/2], Step [3450/3732], Loss: 3.0146, accuracy: 14.6965%\n",
      "Epoch [2/2], Step [3451/3732], Loss: 2.3709, accuracy: 14.7014%\n",
      "Epoch [2/2], Step [3452/3732], Loss: 2.4298, accuracy: 14.7011%\n",
      "Epoch [2/2], Step [3453/3732], Loss: 3.2335, accuracy: 14.7008%\n",
      "Epoch [2/2], Step [3454/3732], Loss: 3.1578, accuracy: 14.7005%\n",
      "Epoch [2/2], Step [3455/3732], Loss: 2.7888, accuracy: 14.7019%\n",
      "Epoch [2/2], Step [3456/3732], Loss: 2.7642, accuracy: 14.7016%\n",
      "Epoch [2/2], Step [3457/3732], Loss: 2.4936, accuracy: 14.7013%\n",
      "Epoch [2/2], Step [3458/3732], Loss: 3.5694, accuracy: 14.7010%\n",
      "Epoch [2/2], Step [3459/3732], Loss: 3.0046, accuracy: 14.7041%\n",
      "Epoch [2/2], Step [3460/3732], Loss: 2.1889, accuracy: 14.7056%\n",
      "Epoch [2/2], Step [3461/3732], Loss: 2.9790, accuracy: 14.7053%\n",
      "Epoch [2/2], Step [3462/3732], Loss: 3.8790, accuracy: 14.7050%\n",
      "Epoch [2/2], Step [3463/3732], Loss: 3.4872, accuracy: 14.7029%\n",
      "Epoch [2/2], Step [3464/3732], Loss: 3.0526, accuracy: 14.7043%\n",
      "Epoch [2/2], Step [3465/3732], Loss: 2.8602, accuracy: 14.7040%\n",
      "Epoch [2/2], Step [3466/3732], Loss: 2.9284, accuracy: 14.7037%\n",
      "Epoch [2/2], Step [3467/3732], Loss: 3.3624, accuracy: 14.7017%\n",
      "Epoch [2/2], Step [3468/3732], Loss: 2.6509, accuracy: 14.7049%\n",
      "Epoch [2/2], Step [3469/3732], Loss: 4.0197, accuracy: 14.7028%\n",
      "Epoch [2/2], Step [3470/3732], Loss: 2.4055, accuracy: 14.7042%\n",
      "Epoch [2/2], Step [3471/3732], Loss: 3.3153, accuracy: 14.7039%\n",
      "Epoch [2/2], Step [3472/3732], Loss: 3.4866, accuracy: 14.7019%\n",
      "Epoch [2/2], Step [3473/3732], Loss: 3.4288, accuracy: 14.7033%\n",
      "Epoch [2/2], Step [3474/3732], Loss: 3.5344, accuracy: 14.7013%\n",
      "Epoch [2/2], Step [3475/3732], Loss: 3.2010, accuracy: 14.6993%\n",
      "Epoch [2/2], Step [3476/3732], Loss: 2.5928, accuracy: 14.7007%\n",
      "Epoch [2/2], Step [3477/3732], Loss: 3.4917, accuracy: 14.7004%\n",
      "Epoch [2/2], Step [3478/3732], Loss: 2.2557, accuracy: 14.7035%\n",
      "Epoch [2/2], Step [3479/3732], Loss: 3.4516, accuracy: 14.7015%\n",
      "Epoch [2/2], Step [3480/3732], Loss: 3.1181, accuracy: 14.7047%\n",
      "Epoch [2/2], Step [3481/3732], Loss: 3.8446, accuracy: 14.7061%\n",
      "Epoch [2/2], Step [3482/3732], Loss: 2.0267, accuracy: 14.7110%\n",
      "Epoch [2/2], Step [3483/3732], Loss: 2.2911, accuracy: 14.7124%\n",
      "Epoch [2/2], Step [3484/3732], Loss: 3.0875, accuracy: 14.7138%\n",
      "Epoch [2/2], Step [3485/3732], Loss: 2.9980, accuracy: 14.7153%\n",
      "Epoch [2/2], Step [3486/3732], Loss: 3.1525, accuracy: 14.7132%\n",
      "Epoch [2/2], Step [3487/3732], Loss: 2.9063, accuracy: 14.7129%\n",
      "Epoch [2/2], Step [3488/3732], Loss: 3.4052, accuracy: 14.7126%\n",
      "Epoch [2/2], Step [3489/3732], Loss: 2.3981, accuracy: 14.7140%\n",
      "Epoch [2/2], Step [3490/3732], Loss: 2.9655, accuracy: 14.7120%\n",
      "Epoch [2/2], Step [3491/3732], Loss: 2.1996, accuracy: 14.7134%\n",
      "Epoch [2/2], Step [3492/3732], Loss: 2.3989, accuracy: 14.7148%\n",
      "Epoch [2/2], Step [3493/3732], Loss: 3.4743, accuracy: 14.7145%\n",
      "Epoch [2/2], Step [3494/3732], Loss: 3.1061, accuracy: 14.7177%\n",
      "Epoch [2/2], Step [3495/3732], Loss: 3.4006, accuracy: 14.7174%\n",
      "Epoch [2/2], Step [3496/3732], Loss: 3.0123, accuracy: 14.7188%\n",
      "Epoch [2/2], Step [3497/3732], Loss: 3.1846, accuracy: 14.7202%\n",
      "Epoch [2/2], Step [3498/3732], Loss: 2.4253, accuracy: 14.7251%\n",
      "Epoch [2/2], Step [3499/3732], Loss: 2.1314, accuracy: 14.7300%\n",
      "Epoch [2/2], Step [3500/3732], Loss: 2.7318, accuracy: 14.7331%\n",
      "Epoch [2/2], Step [3501/3732], Loss: 2.3114, accuracy: 14.7345%\n",
      "Epoch [2/2], Step [3502/3732], Loss: 2.2662, accuracy: 14.7360%\n",
      "Epoch [2/2], Step [3503/3732], Loss: 3.7541, accuracy: 14.7357%\n",
      "Epoch [2/2], Step [3504/3732], Loss: 3.2781, accuracy: 14.7354%\n",
      "Epoch [2/2], Step [3505/3732], Loss: 2.6983, accuracy: 14.7333%\n",
      "Epoch [2/2], Step [3506/3732], Loss: 3.4216, accuracy: 14.7330%\n",
      "Epoch [2/2], Step [3507/3732], Loss: 3.5532, accuracy: 14.7327%\n",
      "Epoch [2/2], Step [3508/3732], Loss: 2.8167, accuracy: 14.7341%\n",
      "Epoch [2/2], Step [3509/3732], Loss: 2.8339, accuracy: 14.7355%\n",
      "Epoch [2/2], Step [3510/3732], Loss: 3.2883, accuracy: 14.7335%\n",
      "Epoch [2/2], Step [3511/3732], Loss: 3.6321, accuracy: 14.7332%\n",
      "Epoch [2/2], Step [3512/3732], Loss: 2.5256, accuracy: 14.7346%\n",
      "Epoch [2/2], Step [3513/3732], Loss: 3.3543, accuracy: 14.7360%\n",
      "Epoch [2/2], Step [3514/3732], Loss: 2.9747, accuracy: 14.7392%\n",
      "Epoch [2/2], Step [3515/3732], Loss: 2.3019, accuracy: 14.7406%\n",
      "Epoch [2/2], Step [3516/3732], Loss: 2.6178, accuracy: 14.7437%\n",
      "Epoch [2/2], Step [3517/3732], Loss: 3.1189, accuracy: 14.7451%\n",
      "Epoch [2/2], Step [3518/3732], Loss: 3.1365, accuracy: 14.7483%\n",
      "Epoch [2/2], Step [3519/3732], Loss: 4.0154, accuracy: 14.7462%\n",
      "Epoch [2/2], Step [3520/3732], Loss: 3.5050, accuracy: 14.7459%\n",
      "Epoch [2/2], Step [3521/3732], Loss: 4.0101, accuracy: 14.7439%\n",
      "Epoch [2/2], Step [3522/3732], Loss: 2.9361, accuracy: 14.7436%\n",
      "Epoch [2/2], Step [3523/3732], Loss: 2.8470, accuracy: 14.7450%\n",
      "Epoch [2/2], Step [3524/3732], Loss: 2.4731, accuracy: 14.7481%\n",
      "Epoch [2/2], Step [3525/3732], Loss: 3.0134, accuracy: 14.7461%\n",
      "Epoch [2/2], Step [3526/3732], Loss: 3.4506, accuracy: 14.7475%\n",
      "Epoch [2/2], Step [3527/3732], Loss: 3.0610, accuracy: 14.7472%\n",
      "Epoch [2/2], Step [3528/3732], Loss: 3.5198, accuracy: 14.7469%\n",
      "Epoch [2/2], Step [3529/3732], Loss: 2.8114, accuracy: 14.7483%\n",
      "Epoch [2/2], Step [3530/3732], Loss: 3.1262, accuracy: 14.7480%\n",
      "Epoch [2/2], Step [3531/3732], Loss: 3.6758, accuracy: 14.7477%\n",
      "Epoch [2/2], Step [3532/3732], Loss: 3.3238, accuracy: 14.7474%\n",
      "Epoch [2/2], Step [3533/3732], Loss: 3.0053, accuracy: 14.7488%\n",
      "Epoch [2/2], Step [3534/3732], Loss: 3.0633, accuracy: 14.7502%\n",
      "Epoch [2/2], Step [3535/3732], Loss: 2.9324, accuracy: 14.7499%\n",
      "Epoch [2/2], Step [3536/3732], Loss: 2.8377, accuracy: 14.7513%\n",
      "Epoch [2/2], Step [3537/3732], Loss: 2.4470, accuracy: 14.7527%\n",
      "Epoch [2/2], Step [3538/3732], Loss: 2.4980, accuracy: 14.7541%\n",
      "Epoch [2/2], Step [3539/3732], Loss: 2.3063, accuracy: 14.7555%\n",
      "Epoch [2/2], Step [3540/3732], Loss: 2.8095, accuracy: 14.7569%\n",
      "Epoch [2/2], Step [3541/3732], Loss: 4.1687, accuracy: 14.7549%\n",
      "Epoch [2/2], Step [3542/3732], Loss: 2.8021, accuracy: 14.7563%\n",
      "Epoch [2/2], Step [3543/3732], Loss: 2.8441, accuracy: 14.7577%\n",
      "Epoch [2/2], Step [3544/3732], Loss: 2.6041, accuracy: 14.7591%\n",
      "Epoch [2/2], Step [3545/3732], Loss: 2.9395, accuracy: 14.7605%\n",
      "Epoch [2/2], Step [3546/3732], Loss: 2.7298, accuracy: 14.7620%\n",
      "Epoch [2/2], Step [3547/3732], Loss: 2.2341, accuracy: 14.7668%\n",
      "Epoch [2/2], Step [3548/3732], Loss: 2.9291, accuracy: 14.7699%\n",
      "Epoch [2/2], Step [3549/3732], Loss: 3.4155, accuracy: 14.7713%\n",
      "Epoch [2/2], Step [3550/3732], Loss: 2.3055, accuracy: 14.7762%\n",
      "Epoch [2/2], Step [3551/3732], Loss: 2.6728, accuracy: 14.7758%\n",
      "Epoch [2/2], Step [3552/3732], Loss: 2.9926, accuracy: 14.7773%\n",
      "Epoch [2/2], Step [3553/3732], Loss: 3.4685, accuracy: 14.7769%\n",
      "Epoch [2/2], Step [3554/3732], Loss: 2.6623, accuracy: 14.7783%\n",
      "Epoch [2/2], Step [3555/3732], Loss: 3.8865, accuracy: 14.7780%\n",
      "Epoch [2/2], Step [3556/3732], Loss: 2.3017, accuracy: 14.7811%\n",
      "Epoch [2/2], Step [3557/3732], Loss: 3.1531, accuracy: 14.7808%\n",
      "Epoch [2/2], Step [3558/3732], Loss: 3.4944, accuracy: 14.7822%\n",
      "Epoch [2/2], Step [3559/3732], Loss: 3.3191, accuracy: 14.7819%\n",
      "Epoch [2/2], Step [3560/3732], Loss: 3.2444, accuracy: 14.7833%\n",
      "Epoch [2/2], Step [3561/3732], Loss: 3.0696, accuracy: 14.7864%\n",
      "Epoch [2/2], Step [3562/3732], Loss: 3.2658, accuracy: 14.7861%\n",
      "Epoch [2/2], Step [3563/3732], Loss: 3.2419, accuracy: 14.7841%\n",
      "Epoch [2/2], Step [3564/3732], Loss: 2.0828, accuracy: 14.7872%\n",
      "Epoch [2/2], Step [3565/3732], Loss: 2.8630, accuracy: 14.7852%\n",
      "Epoch [2/2], Step [3566/3732], Loss: 2.5821, accuracy: 14.7866%\n",
      "Epoch [2/2], Step [3567/3732], Loss: 3.2223, accuracy: 14.7863%\n",
      "Epoch [2/2], Step [3568/3732], Loss: 2.7749, accuracy: 14.7894%\n",
      "Epoch [2/2], Step [3569/3732], Loss: 4.6310, accuracy: 14.7874%\n",
      "Epoch [2/2], Step [3570/3732], Loss: 2.8042, accuracy: 14.7888%\n",
      "Epoch [2/2], Step [3571/3732], Loss: 3.2562, accuracy: 14.7884%\n",
      "Epoch [2/2], Step [3572/3732], Loss: 2.6619, accuracy: 14.7898%\n",
      "Epoch [2/2], Step [3573/3732], Loss: 3.2183, accuracy: 14.7912%\n",
      "Epoch [2/2], Step [3574/3732], Loss: 2.8922, accuracy: 14.7926%\n",
      "Epoch [2/2], Step [3575/3732], Loss: 3.5352, accuracy: 14.7923%\n",
      "Epoch [2/2], Step [3576/3732], Loss: 3.2630, accuracy: 14.7937%\n",
      "Epoch [2/2], Step [3577/3732], Loss: 2.2258, accuracy: 14.7934%\n",
      "Epoch [2/2], Step [3578/3732], Loss: 2.4042, accuracy: 14.7948%\n",
      "Epoch [2/2], Step [3579/3732], Loss: 3.7791, accuracy: 14.7928%\n",
      "Epoch [2/2], Step [3580/3732], Loss: 3.1579, accuracy: 14.7959%\n",
      "Epoch [2/2], Step [3581/3732], Loss: 3.2363, accuracy: 14.7956%\n",
      "Epoch [2/2], Step [3582/3732], Loss: 2.7996, accuracy: 14.7953%\n",
      "Epoch [2/2], Step [3583/3732], Loss: 3.4075, accuracy: 14.7967%\n",
      "Epoch [2/2], Step [3584/3732], Loss: 3.3810, accuracy: 14.7980%\n",
      "Epoch [2/2], Step [3585/3732], Loss: 3.3124, accuracy: 14.7960%\n",
      "Epoch [2/2], Step [3586/3732], Loss: 2.3790, accuracy: 14.7991%\n",
      "Epoch [2/2], Step [3587/3732], Loss: 3.0575, accuracy: 14.8005%\n",
      "Epoch [2/2], Step [3588/3732], Loss: 3.5958, accuracy: 14.8019%\n",
      "Epoch [2/2], Step [3589/3732], Loss: 1.7592, accuracy: 14.8067%\n",
      "Epoch [2/2], Step [3590/3732], Loss: 3.2090, accuracy: 14.8064%\n",
      "Epoch [2/2], Step [3591/3732], Loss: 2.9173, accuracy: 14.8078%\n",
      "Epoch [2/2], Step [3592/3732], Loss: 2.3060, accuracy: 14.8109%\n",
      "Epoch [2/2], Step [3593/3732], Loss: 3.2707, accuracy: 14.8106%\n",
      "Epoch [2/2], Step [3594/3732], Loss: 2.8830, accuracy: 14.8103%\n",
      "Epoch [2/2], Step [3595/3732], Loss: 3.2111, accuracy: 14.8099%\n",
      "Epoch [2/2], Step [3596/3732], Loss: 3.9466, accuracy: 14.8096%\n",
      "Epoch [2/2], Step [3597/3732], Loss: 2.7379, accuracy: 14.8076%\n",
      "Epoch [2/2], Step [3598/3732], Loss: 2.8861, accuracy: 14.8073%\n",
      "Epoch [2/2], Step [3599/3732], Loss: 3.4675, accuracy: 14.8053%\n",
      "Epoch [2/2], Step [3600/3732], Loss: 2.6473, accuracy: 14.8084%\n",
      "Epoch [2/2], Step [3601/3732], Loss: 3.4417, accuracy: 14.8081%\n",
      "Epoch [2/2], Step [3602/3732], Loss: 2.8316, accuracy: 14.8077%\n",
      "Epoch [2/2], Step [3603/3732], Loss: 2.9832, accuracy: 14.8091%\n",
      "Epoch [2/2], Step [3604/3732], Loss: 2.9005, accuracy: 14.8122%\n",
      "Epoch [2/2], Step [3605/3732], Loss: 2.7918, accuracy: 14.8136%\n",
      "Epoch [2/2], Step [3606/3732], Loss: 3.1932, accuracy: 14.8133%\n",
      "Epoch [2/2], Step [3607/3732], Loss: 3.8863, accuracy: 14.8113%\n",
      "Epoch [2/2], Step [3608/3732], Loss: 1.8500, accuracy: 14.8144%\n",
      "Epoch [2/2], Step [3609/3732], Loss: 3.3618, accuracy: 14.8158%\n",
      "Epoch [2/2], Step [3610/3732], Loss: 2.6434, accuracy: 14.8154%\n",
      "Epoch [2/2], Step [3611/3732], Loss: 4.0301, accuracy: 14.8134%\n",
      "Epoch [2/2], Step [3612/3732], Loss: 3.2752, accuracy: 14.8131%\n",
      "Epoch [2/2], Step [3613/3732], Loss: 2.5808, accuracy: 14.8162%\n",
      "Epoch [2/2], Step [3614/3732], Loss: 3.0992, accuracy: 14.8159%\n",
      "Epoch [2/2], Step [3615/3732], Loss: 2.8450, accuracy: 14.8190%\n",
      "Epoch [2/2], Step [3616/3732], Loss: 2.9628, accuracy: 14.8187%\n",
      "Epoch [2/2], Step [3617/3732], Loss: 2.1806, accuracy: 14.8200%\n",
      "Epoch [2/2], Step [3618/3732], Loss: 2.9578, accuracy: 14.8197%\n",
      "Epoch [2/2], Step [3619/3732], Loss: 3.2169, accuracy: 14.8177%\n",
      "Epoch [2/2], Step [3620/3732], Loss: 3.6291, accuracy: 14.8191%\n",
      "Epoch [2/2], Step [3621/3732], Loss: 2.8173, accuracy: 14.8188%\n",
      "Epoch [2/2], Step [3622/3732], Loss: 3.3095, accuracy: 14.8168%\n",
      "Epoch [2/2], Step [3623/3732], Loss: 3.5266, accuracy: 14.8165%\n",
      "Epoch [2/2], Step [3624/3732], Loss: 2.8994, accuracy: 14.8161%\n",
      "Epoch [2/2], Step [3625/3732], Loss: 3.7297, accuracy: 14.8141%\n",
      "Epoch [2/2], Step [3626/3732], Loss: 2.4223, accuracy: 14.8206%\n",
      "Epoch [2/2], Step [3627/3732], Loss: 2.7886, accuracy: 14.8220%\n",
      "Epoch [2/2], Step [3628/3732], Loss: 2.6634, accuracy: 14.8268%\n",
      "Epoch [2/2], Step [3629/3732], Loss: 2.6424, accuracy: 14.8281%\n",
      "Epoch [2/2], Step [3630/3732], Loss: 2.9692, accuracy: 14.8295%\n",
      "Epoch [2/2], Step [3631/3732], Loss: 2.7452, accuracy: 14.8309%\n",
      "Epoch [2/2], Step [3632/3732], Loss: 3.4306, accuracy: 14.8289%\n",
      "Epoch [2/2], Step [3633/3732], Loss: 3.7092, accuracy: 14.8269%\n",
      "Epoch [2/2], Step [3634/3732], Loss: 3.0658, accuracy: 14.8266%\n",
      "Epoch [2/2], Step [3635/3732], Loss: 2.9233, accuracy: 14.8296%\n",
      "Epoch [2/2], Step [3636/3732], Loss: 3.2656, accuracy: 14.8276%\n",
      "Epoch [2/2], Step [3637/3732], Loss: 2.6806, accuracy: 14.8290%\n",
      "Epoch [2/2], Step [3638/3732], Loss: 3.3185, accuracy: 14.8287%\n",
      "Epoch [2/2], Step [3639/3732], Loss: 3.0481, accuracy: 14.8267%\n",
      "Epoch [2/2], Step [3640/3732], Loss: 3.6922, accuracy: 14.8247%\n",
      "Epoch [2/2], Step [3641/3732], Loss: 3.5757, accuracy: 14.8244%\n",
      "Epoch [2/2], Step [3642/3732], Loss: 2.6544, accuracy: 14.8257%\n",
      "Epoch [2/2], Step [3643/3732], Loss: 3.1062, accuracy: 14.8254%\n",
      "Epoch [2/2], Step [3644/3732], Loss: 2.8797, accuracy: 14.8268%\n",
      "Epoch [2/2], Step [3645/3732], Loss: 3.1641, accuracy: 14.8265%\n",
      "Epoch [2/2], Step [3646/3732], Loss: 2.9213, accuracy: 14.8262%\n",
      "Epoch [2/2], Step [3647/3732], Loss: 3.2291, accuracy: 14.8242%\n",
      "Epoch [2/2], Step [3648/3732], Loss: 3.3177, accuracy: 14.8238%\n",
      "Epoch [2/2], Step [3649/3732], Loss: 2.5400, accuracy: 14.8269%\n",
      "Epoch [2/2], Step [3650/3732], Loss: 2.8333, accuracy: 14.8300%\n",
      "Epoch [2/2], Step [3651/3732], Loss: 2.6452, accuracy: 14.8314%\n",
      "Epoch [2/2], Step [3652/3732], Loss: 2.8550, accuracy: 14.8311%\n",
      "Epoch [2/2], Step [3653/3732], Loss: 2.6880, accuracy: 14.8290%\n",
      "Epoch [2/2], Step [3654/3732], Loss: 3.2892, accuracy: 14.8304%\n",
      "Epoch [2/2], Step [3655/3732], Loss: 3.4399, accuracy: 14.8318%\n",
      "Epoch [2/2], Step [3656/3732], Loss: 3.6639, accuracy: 14.8315%\n",
      "Epoch [2/2], Step [3657/3732], Loss: 2.8576, accuracy: 14.8312%\n",
      "Epoch [2/2], Step [3658/3732], Loss: 1.6678, accuracy: 14.8393%\n",
      "Epoch [2/2], Step [3659/3732], Loss: 3.0464, accuracy: 14.8390%\n",
      "Epoch [2/2], Step [3660/3732], Loss: 3.0501, accuracy: 14.8387%\n",
      "Epoch [2/2], Step [3661/3732], Loss: 2.2767, accuracy: 14.8434%\n",
      "Epoch [2/2], Step [3662/3732], Loss: 2.5279, accuracy: 14.8448%\n",
      "Epoch [2/2], Step [3663/3732], Loss: 2.5157, accuracy: 14.8462%\n",
      "Epoch [2/2], Step [3664/3732], Loss: 3.5517, accuracy: 14.8442%\n",
      "Epoch [2/2], Step [3665/3732], Loss: 2.3187, accuracy: 14.8472%\n",
      "Epoch [2/2], Step [3666/3732], Loss: 2.9632, accuracy: 14.8486%\n",
      "Epoch [2/2], Step [3667/3732], Loss: 2.8136, accuracy: 14.8517%\n",
      "Epoch [2/2], Step [3668/3732], Loss: 2.9630, accuracy: 14.8530%\n",
      "Epoch [2/2], Step [3669/3732], Loss: 2.5881, accuracy: 14.8544%\n",
      "Epoch [2/2], Step [3670/3732], Loss: 2.0723, accuracy: 14.8575%\n",
      "Epoch [2/2], Step [3671/3732], Loss: 3.1914, accuracy: 14.8572%\n",
      "Epoch [2/2], Step [3672/3732], Loss: 2.8587, accuracy: 14.8568%\n",
      "Epoch [2/2], Step [3673/3732], Loss: 3.0004, accuracy: 14.8565%\n",
      "Epoch [2/2], Step [3674/3732], Loss: 2.9659, accuracy: 14.8579%\n",
      "Epoch [2/2], Step [3675/3732], Loss: 3.4815, accuracy: 14.8576%\n",
      "Epoch [2/2], Step [3676/3732], Loss: 2.9363, accuracy: 14.8589%\n",
      "Epoch [2/2], Step [3677/3732], Loss: 3.1605, accuracy: 14.8603%\n",
      "Epoch [2/2], Step [3678/3732], Loss: 2.8215, accuracy: 14.8617%\n",
      "Epoch [2/2], Step [3679/3732], Loss: 2.4727, accuracy: 14.8630%\n",
      "Epoch [2/2], Step [3680/3732], Loss: 2.6243, accuracy: 14.8644%\n",
      "Epoch [2/2], Step [3681/3732], Loss: 3.4889, accuracy: 14.8641%\n",
      "Epoch [2/2], Step [3682/3732], Loss: 2.3952, accuracy: 14.8671%\n",
      "Epoch [2/2], Step [3683/3732], Loss: 3.1155, accuracy: 14.8685%\n",
      "Epoch [2/2], Step [3684/3732], Loss: 3.5792, accuracy: 14.8682%\n",
      "Epoch [2/2], Step [3685/3732], Loss: 3.2704, accuracy: 14.8679%\n",
      "Epoch [2/2], Step [3686/3732], Loss: 3.2296, accuracy: 14.8676%\n",
      "Epoch [2/2], Step [3687/3732], Loss: 3.2038, accuracy: 14.8689%\n",
      "Epoch [2/2], Step [3688/3732], Loss: 2.5401, accuracy: 14.8720%\n",
      "Epoch [2/2], Step [3689/3732], Loss: 2.3693, accuracy: 14.8750%\n",
      "Epoch [2/2], Step [3690/3732], Loss: 2.8336, accuracy: 14.8747%\n",
      "Epoch [2/2], Step [3691/3732], Loss: 3.0162, accuracy: 14.8777%\n",
      "Epoch [2/2], Step [3692/3732], Loss: 2.6494, accuracy: 14.8825%\n",
      "Epoch [2/2], Step [3693/3732], Loss: 2.8435, accuracy: 14.8838%\n",
      "Epoch [2/2], Step [3694/3732], Loss: 1.8987, accuracy: 14.8869%\n",
      "Epoch [2/2], Step [3695/3732], Loss: 3.2375, accuracy: 14.8849%\n",
      "Epoch [2/2], Step [3696/3732], Loss: 2.8260, accuracy: 14.8862%\n",
      "Epoch [2/2], Step [3697/3732], Loss: 2.3891, accuracy: 14.8910%\n",
      "Epoch [2/2], Step [3698/3732], Loss: 3.4099, accuracy: 14.8890%\n",
      "Epoch [2/2], Step [3699/3732], Loss: 1.9846, accuracy: 14.8937%\n",
      "Epoch [2/2], Step [3700/3732], Loss: 3.6207, accuracy: 14.8917%\n",
      "Epoch [2/2], Step [3701/3732], Loss: 2.6760, accuracy: 14.8947%\n",
      "Epoch [2/2], Step [3702/3732], Loss: 2.3108, accuracy: 14.8961%\n",
      "Epoch [2/2], Step [3703/3732], Loss: 3.4799, accuracy: 14.8941%\n",
      "Epoch [2/2], Step [3704/3732], Loss: 3.6726, accuracy: 14.8921%\n",
      "Epoch [2/2], Step [3705/3732], Loss: 3.5155, accuracy: 14.8934%\n",
      "Epoch [2/2], Step [3706/3732], Loss: 2.5319, accuracy: 14.8948%\n",
      "Epoch [2/2], Step [3707/3732], Loss: 2.7828, accuracy: 14.8962%\n",
      "Epoch [2/2], Step [3708/3732], Loss: 2.6754, accuracy: 14.8975%\n",
      "Epoch [2/2], Step [3709/3732], Loss: 3.3889, accuracy: 14.8989%\n",
      "Epoch [2/2], Step [3710/3732], Loss: 3.0587, accuracy: 14.8985%\n",
      "Epoch [2/2], Step [3711/3732], Loss: 3.5104, accuracy: 14.8982%\n",
      "Epoch [2/2], Step [3712/3732], Loss: 2.7240, accuracy: 14.9013%\n",
      "Epoch [2/2], Step [3713/3732], Loss: 2.7939, accuracy: 14.9026%\n",
      "Epoch [2/2], Step [3714/3732], Loss: 3.4682, accuracy: 14.9040%\n",
      "Epoch [2/2], Step [3715/3732], Loss: 3.2252, accuracy: 14.9053%\n",
      "Epoch [2/2], Step [3716/3732], Loss: 3.9229, accuracy: 14.9050%\n",
      "Epoch [2/2], Step [3717/3732], Loss: 2.5218, accuracy: 14.9080%\n",
      "Epoch [2/2], Step [3718/3732], Loss: 2.6702, accuracy: 14.9094%\n",
      "Epoch [2/2], Step [3719/3732], Loss: 3.3133, accuracy: 14.9108%\n",
      "Epoch [2/2], Step [3720/3732], Loss: 3.1275, accuracy: 14.9087%\n",
      "Epoch [2/2], Step [3721/3732], Loss: 3.0987, accuracy: 14.9101%\n",
      "Epoch [2/2], Step [3722/3732], Loss: 3.5976, accuracy: 14.9081%\n",
      "Epoch [2/2], Step [3723/3732], Loss: 2.8464, accuracy: 14.9078%\n",
      "Epoch [2/2], Step [3724/3732], Loss: 2.5195, accuracy: 14.9125%\n",
      "Epoch [2/2], Step [3725/3732], Loss: 3.2871, accuracy: 14.9122%\n",
      "Epoch [2/2], Step [3726/3732], Loss: 2.4846, accuracy: 14.9169%\n",
      "Epoch [2/2], Step [3727/3732], Loss: 3.1867, accuracy: 14.9199%\n",
      "Epoch [2/2], Step [3728/3732], Loss: 2.1930, accuracy: 14.9246%\n",
      "Epoch [2/2], Step [3729/3732], Loss: 3.3802, accuracy: 14.9243%\n",
      "Epoch [2/2], Step [3730/3732], Loss: 3.5041, accuracy: 14.9223%\n",
      "Epoch [2/2], Step [3731/3732], Loss: 2.2177, accuracy: 14.9253%\n",
      "Epoch [2/2], Step [3732/3732], Loss: 2.9795, accuracy: 14.9250%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 512)\n",
    "        self.fc2 = nn.Linear(512, 50) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle= True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle= True)\n",
    "\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 2\n",
    "correct = 0\n",
    "total = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #predict\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}, accuracy: {(correct / total)*100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6a3632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 25.0000%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 31.2500%\n",
      "test accuracy: 30.0000%\n",
      "test accuracy: 29.1667%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 21.8750%\n",
      "test accuracy: 22.2222%\n",
      "test accuracy: 22.5000%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 22.9167%\n",
      "test accuracy: 23.0769%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 23.3333%\n",
      "test accuracy: 21.8750%\n",
      "test accuracy: 20.5882%\n",
      "test accuracy: 19.4444%\n",
      "test accuracy: 21.0526%\n",
      "test accuracy: 21.2500%\n",
      "test accuracy: 22.6190%\n",
      "test accuracy: 21.5909%\n",
      "test accuracy: 21.7391%\n",
      "test accuracy: 21.8750%\n",
      "test accuracy: 23.0000%\n",
      "test accuracy: 24.0385%\n",
      "test accuracy: 23.1481%\n",
      "test accuracy: 22.3214%\n",
      "test accuracy: 22.4138%\n",
      "test accuracy: 24.1667%\n",
      "test accuracy: 24.1935%\n",
      "test accuracy: 25.7812%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 24.3590%\n",
      "test accuracy: 25.0000%\n",
      "test accuracy: 24.3902%\n",
      "test accuracy: 23.8095%\n",
      "test accuracy: 24.4186%\n",
      "test accuracy: 23.8636%\n",
      "test accuracy: 23.3333%\n",
      "test accuracy: 22.8261%\n",
      "test accuracy: 22.8723%\n",
      "test accuracy: 22.9167%\n",
      "test accuracy: 22.9592%\n",
      "test accuracy: 23.0000%\n",
      "test accuracy: 22.5490%\n",
      "test accuracy: 22.1154%\n",
      "test accuracy: 22.1698%\n",
      "test accuracy: 21.7593%\n",
      "test accuracy: 21.3636%\n",
      "test accuracy: 21.4286%\n",
      "test accuracy: 21.4912%\n",
      "test accuracy: 21.1207%\n",
      "test accuracy: 20.7627%\n",
      "test accuracy: 20.8333%\n",
      "test accuracy: 21.7213%\n",
      "test accuracy: 21.7742%\n",
      "test accuracy: 22.6190%\n",
      "test accuracy: 23.4375%\n",
      "test accuracy: 23.4615%\n",
      "test accuracy: 23.4848%\n",
      "test accuracy: 23.1343%\n",
      "test accuracy: 23.1618%\n",
      "test accuracy: 22.8261%\n",
      "test accuracy: 22.5000%\n",
      "test accuracy: 22.1831%\n",
      "test accuracy: 22.2222%\n",
      "test accuracy: 21.9178%\n",
      "test accuracy: 21.9595%\n",
      "test accuracy: 21.6667%\n",
      "test accuracy: 21.3816%\n",
      "test accuracy: 21.7532%\n",
      "test accuracy: 21.7949%\n",
      "test accuracy: 21.5190%\n",
      "test accuracy: 21.8750%\n",
      "test accuracy: 21.6049%\n",
      "test accuracy: 21.6463%\n",
      "test accuracy: 21.6867%\n",
      "test accuracy: 21.7262%\n",
      "test accuracy: 21.4706%\n",
      "test accuracy: 21.5116%\n",
      "test accuracy: 21.5517%\n",
      "test accuracy: 21.3068%\n",
      "test accuracy: 21.0674%\n",
      "test accuracy: 21.3889%\n",
      "test accuracy: 21.1538%\n",
      "test accuracy: 21.4674%\n",
      "test accuracy: 21.2366%\n",
      "test accuracy: 21.0106%\n",
      "test accuracy: 21.3158%\n",
      "test accuracy: 21.3542%\n",
      "test accuracy: 21.1340%\n",
      "test accuracy: 20.9184%\n",
      "test accuracy: 20.9596%\n",
      "test accuracy: 21.2500%\n",
      "test accuracy: 21.2871%\n",
      "test accuracy: 21.3235%\n",
      "test accuracy: 21.1165%\n",
      "test accuracy: 21.1538%\n",
      "test accuracy: 21.1905%\n",
      "test accuracy: 21.6981%\n",
      "test accuracy: 21.7290%\n",
      "test accuracy: 21.7593%\n",
      "test accuracy: 21.5596%\n",
      "test accuracy: 21.3636%\n",
      "test accuracy: 21.6216%\n",
      "test accuracy: 21.4286%\n",
      "test accuracy: 21.4602%\n",
      "test accuracy: 21.4912%\n",
      "test accuracy: 21.5217%\n",
      "test accuracy: 21.3362%\n",
      "test accuracy: 21.5812%\n",
      "test accuracy: 21.3983%\n",
      "test accuracy: 21.2185%\n",
      "test accuracy: 21.2500%\n",
      "test accuracy: 21.0744%\n",
      "test accuracy: 20.9016%\n",
      "test accuracy: 20.9350%\n",
      "test accuracy: 20.9677%\n",
      "test accuracy: 21.0000%\n",
      "test accuracy: 21.2302%\n",
      "test accuracy: 21.2598%\n",
      "test accuracy: 21.0938%\n",
      "test accuracy: 21.3178%\n",
      "test accuracy: 21.1538%\n",
      "test accuracy: 21.3740%\n",
      "test accuracy: 21.5909%\n",
      "test accuracy: 21.6165%\n",
      "test accuracy: 21.4552%\n",
      "test accuracy: 21.4815%\n",
      "test accuracy: 21.6912%\n",
      "test accuracy: 21.5328%\n",
      "test accuracy: 21.7391%\n",
      "test accuracy: 21.5827%\n",
      "test accuracy: 21.4286%\n",
      "test accuracy: 21.2766%\n",
      "test accuracy: 21.1268%\n",
      "test accuracy: 21.1538%\n",
      "test accuracy: 21.1806%\n",
      "test accuracy: 21.2069%\n",
      "test accuracy: 21.2329%\n",
      "test accuracy: 21.0884%\n",
      "test accuracy: 21.1149%\n",
      "test accuracy: 20.9732%\n",
      "test accuracy: 21.0000%\n",
      "test accuracy: 20.8609%\n",
      "test accuracy: 20.8882%\n",
      "test accuracy: 20.9150%\n",
      "test accuracy: 20.7792%\n",
      "test accuracy: 20.6452%\n",
      "test accuracy: 20.5128%\n",
      "test accuracy: 20.3822%\n",
      "test accuracy: 20.2532%\n",
      "test accuracy: 20.4403%\n",
      "test accuracy: 20.4688%\n",
      "test accuracy: 20.6522%\n",
      "test accuracy: 20.8333%\n",
      "test accuracy: 20.8589%\n",
      "test accuracy: 20.7317%\n",
      "test accuracy: 20.6061%\n",
      "test accuracy: 20.6325%\n",
      "test accuracy: 20.5090%\n",
      "test accuracy: 20.3869%\n",
      "test accuracy: 20.2663%\n",
      "test accuracy: 20.2941%\n",
      "test accuracy: 20.3216%\n",
      "test accuracy: 20.2035%\n",
      "test accuracy: 20.2312%\n",
      "test accuracy: 20.2586%\n",
      "test accuracy: 20.2857%\n",
      "test accuracy: 20.4545%\n",
      "test accuracy: 20.4802%\n",
      "test accuracy: 20.5056%\n",
      "test accuracy: 20.5307%\n",
      "test accuracy: 20.5556%\n",
      "test accuracy: 20.5801%\n",
      "test accuracy: 20.4670%\n",
      "test accuracy: 20.3552%\n",
      "test accuracy: 20.3804%\n",
      "test accuracy: 20.5405%\n",
      "test accuracy: 20.5645%\n",
      "test accuracy: 20.7219%\n",
      "test accuracy: 20.7447%\n",
      "test accuracy: 20.7672%\n",
      "test accuracy: 20.7895%\n",
      "test accuracy: 20.6806%\n",
      "test accuracy: 20.9635%\n",
      "test accuracy: 21.1140%\n",
      "test accuracy: 21.2629%\n",
      "test accuracy: 21.2821%\n",
      "test accuracy: 21.1735%\n",
      "test accuracy: 21.1929%\n",
      "test accuracy: 21.2121%\n",
      "test accuracy: 21.2312%\n",
      "test accuracy: 21.1250%\n",
      "test accuracy: 21.1443%\n",
      "test accuracy: 21.0396%\n",
      "test accuracy: 21.0591%\n",
      "test accuracy: 20.9559%\n",
      "test accuracy: 21.0976%\n",
      "test accuracy: 21.1165%\n",
      "test accuracy: 21.2560%\n",
      "test accuracy: 21.2740%\n",
      "test accuracy: 21.4115%\n",
      "test accuracy: 21.3095%\n",
      "test accuracy: 21.3270%\n",
      "test accuracy: 21.6981%\n",
      "test accuracy: 21.5962%\n",
      "test accuracy: 21.6121%\n",
      "test accuracy: 21.7442%\n",
      "test accuracy: 21.6435%\n",
      "test accuracy: 21.6590%\n",
      "test accuracy: 21.6743%\n",
      "test accuracy: 21.5753%\n",
      "test accuracy: 21.7045%\n",
      "test accuracy: 21.6063%\n",
      "test accuracy: 21.5090%\n",
      "test accuracy: 21.4126%\n",
      "test accuracy: 21.3170%\n",
      "test accuracy: 21.2222%\n",
      "test accuracy: 21.3496%\n",
      "test accuracy: 21.2555%\n",
      "test accuracy: 21.2719%\n",
      "test accuracy: 21.1790%\n",
      "test accuracy: 21.0870%\n",
      "test accuracy: 20.9957%\n",
      "test accuracy: 21.1207%\n",
      "test accuracy: 21.2446%\n",
      "test accuracy: 21.3675%\n",
      "test accuracy: 21.3830%\n",
      "test accuracy: 21.3983%\n",
      "test accuracy: 21.4135%\n",
      "test accuracy: 21.3235%\n",
      "test accuracy: 21.3389%\n",
      "test accuracy: 21.2500%\n",
      "test accuracy: 21.1618%\n",
      "test accuracy: 21.0744%\n",
      "test accuracy: 21.0905%\n",
      "test accuracy: 21.2090%\n",
      "test accuracy: 21.2245%\n",
      "test accuracy: 21.1382%\n",
      "test accuracy: 21.1538%\n",
      "test accuracy: 21.2702%\n",
      "test accuracy: 21.2851%\n",
      "test accuracy: 21.2000%\n",
      "test accuracy: 21.2151%\n",
      "test accuracy: 21.3294%\n",
      "test accuracy: 21.4427%\n",
      "test accuracy: 21.3583%\n",
      "test accuracy: 21.3725%\n",
      "test accuracy: 21.2891%\n",
      "test accuracy: 21.2062%\n",
      "test accuracy: 21.1240%\n",
      "test accuracy: 21.0425%\n",
      "test accuracy: 21.0577%\n",
      "test accuracy: 21.0728%\n",
      "test accuracy: 21.0878%\n",
      "test accuracy: 21.0076%\n",
      "test accuracy: 20.9280%\n",
      "test accuracy: 20.9434%\n",
      "test accuracy: 21.0526%\n",
      "test accuracy: 21.0674%\n",
      "test accuracy: 20.9888%\n",
      "test accuracy: 20.9108%\n",
      "test accuracy: 21.0185%\n",
      "test accuracy: 21.0332%\n",
      "test accuracy: 21.0478%\n",
      "test accuracy: 20.9707%\n",
      "test accuracy: 20.8942%\n",
      "test accuracy: 21.0000%\n",
      "test accuracy: 20.9239%\n",
      "test accuracy: 20.9386%\n",
      "test accuracy: 20.9532%\n",
      "test accuracy: 20.9677%\n",
      "test accuracy: 20.8929%\n",
      "test accuracy: 20.9075%\n",
      "test accuracy: 20.9220%\n",
      "test accuracy: 20.9364%\n",
      "test accuracy: 20.8627%\n",
      "test accuracy: 20.7895%\n",
      "test accuracy: 20.8042%\n",
      "test accuracy: 20.7317%\n",
      "test accuracy: 20.6597%\n",
      "test accuracy: 20.5882%\n",
      "test accuracy: 20.6034%\n",
      "test accuracy: 20.7045%\n",
      "test accuracy: 20.8048%\n",
      "test accuracy: 20.8191%\n",
      "test accuracy: 20.7483%\n",
      "test accuracy: 20.6780%\n",
      "test accuracy: 20.6081%\n",
      "test accuracy: 20.5387%\n",
      "test accuracy: 20.5537%\n",
      "test accuracy: 20.7358%\n",
      "test accuracy: 20.6667%\n",
      "test accuracy: 20.7641%\n",
      "test accuracy: 20.7781%\n",
      "test accuracy: 20.7921%\n",
      "test accuracy: 20.8882%\n",
      "test accuracy: 20.8197%\n",
      "test accuracy: 20.9150%\n",
      "test accuracy: 21.0098%\n",
      "test accuracy: 20.9416%\n",
      "test accuracy: 20.9547%\n",
      "test accuracy: 20.8871%\n",
      "test accuracy: 20.9003%\n",
      "test accuracy: 20.8333%\n",
      "test accuracy: 20.8466%\n",
      "test accuracy: 20.8599%\n",
      "test accuracy: 20.7937%\n",
      "test accuracy: 20.7278%\n",
      "test accuracy: 20.8202%\n",
      "test accuracy: 20.7547%\n",
      "test accuracy: 20.7680%\n",
      "test accuracy: 20.9375%\n",
      "test accuracy: 20.8723%\n",
      "test accuracy: 20.8075%\n",
      "test accuracy: 20.7430%\n",
      "test accuracy: 20.7562%\n",
      "test accuracy: 20.7692%\n",
      "test accuracy: 20.8589%\n",
      "test accuracy: 20.8716%\n",
      "test accuracy: 20.8841%\n",
      "test accuracy: 20.8967%\n",
      "test accuracy: 20.8333%\n",
      "test accuracy: 20.8459%\n",
      "test accuracy: 20.8584%\n",
      "test accuracy: 20.8709%\n",
      "test accuracy: 20.8832%\n",
      "test accuracy: 20.8955%\n",
      "test accuracy: 20.8333%\n",
      "test accuracy: 20.8457%\n",
      "test accuracy: 20.8580%\n",
      "test accuracy: 20.8702%\n",
      "test accuracy: 20.8088%\n",
      "test accuracy: 20.7478%\n",
      "test accuracy: 20.7602%\n",
      "test accuracy: 20.7726%\n",
      "test accuracy: 20.7122%\n",
      "test accuracy: 20.8696%\n",
      "test accuracy: 20.9538%\n",
      "test accuracy: 20.8934%\n",
      "test accuracy: 20.9052%\n",
      "test accuracy: 20.9169%\n",
      "test accuracy: 20.9286%\n",
      "test accuracy: 20.8689%\n",
      "test accuracy: 21.0227%\n",
      "test accuracy: 21.0340%\n",
      "test accuracy: 21.0452%\n",
      "test accuracy: 21.1268%\n",
      "test accuracy: 21.0674%\n",
      "test accuracy: 21.1485%\n",
      "test accuracy: 21.1592%\n",
      "test accuracy: 21.3092%\n",
      "test accuracy: 21.3194%\n",
      "test accuracy: 21.3296%\n",
      "test accuracy: 21.3398%\n",
      "test accuracy: 21.2810%\n",
      "test accuracy: 21.2912%\n",
      "test accuracy: 21.3014%\n",
      "test accuracy: 21.2432%\n",
      "test accuracy: 21.1853%\n",
      "test accuracy: 21.1957%\n",
      "test accuracy: 21.2737%\n",
      "test accuracy: 21.3514%\n",
      "test accuracy: 21.3612%\n",
      "test accuracy: 21.3038%\n",
      "test accuracy: 21.3807%\n",
      "test accuracy: 21.3904%\n",
      "test accuracy: 21.4000%\n",
      "test accuracy: 21.4096%\n",
      "test accuracy: 21.4191%\n",
      "test accuracy: 21.4947%\n",
      "test accuracy: 21.5040%\n",
      "test accuracy: 21.4474%\n",
      "test accuracy: 21.4567%\n",
      "test accuracy: 21.4660%\n",
      "test accuracy: 21.4752%\n",
      "test accuracy: 21.4193%\n",
      "test accuracy: 21.4286%\n",
      "test accuracy: 21.4378%\n",
      "test accuracy: 21.4470%\n",
      "test accuracy: 21.5206%\n",
      "test accuracy: 21.4653%\n",
      "test accuracy: 21.4103%\n",
      "test accuracy: 21.4194%\n",
      "test accuracy: 21.4923%\n",
      "test accuracy: 21.5013%\n",
      "test accuracy: 21.5102%\n",
      "test accuracy: 21.4557%\n",
      "test accuracy: 21.4015%\n",
      "test accuracy: 21.4106%\n",
      "test accuracy: 21.3568%\n",
      "test accuracy: 21.3659%\n",
      "test accuracy: 21.3125%\n",
      "test accuracy: 21.3840%\n",
      "test accuracy: 21.3308%\n",
      "test accuracy: 21.2779%\n",
      "test accuracy: 21.2871%\n",
      "test accuracy: 21.2963%\n",
      "test accuracy: 21.3054%\n",
      "test accuracy: 21.4373%\n",
      "test accuracy: 21.3848%\n",
      "test accuracy: 21.3936%\n",
      "test accuracy: 21.4024%\n",
      "test accuracy: 21.3504%\n",
      "test accuracy: 21.2985%\n",
      "test accuracy: 21.2470%\n",
      "test accuracy: 21.3768%\n",
      "test accuracy: 21.4458%\n",
      "test accuracy: 21.4543%\n",
      "test accuracy: 21.4628%\n",
      "test accuracy: 21.4713%\n",
      "test accuracy: 21.5394%\n",
      "test accuracy: 21.5476%\n",
      "test accuracy: 21.5558%\n",
      "test accuracy: 21.5047%\n",
      "test accuracy: 21.4539%\n",
      "test accuracy: 21.5212%\n",
      "test accuracy: 21.4706%\n",
      "test accuracy: 21.5376%\n",
      "test accuracy: 21.5457%\n",
      "test accuracy: 21.5537%\n",
      "test accuracy: 21.5618%\n",
      "test accuracy: 21.6279%\n",
      "test accuracy: 21.5777%\n",
      "test accuracy: 21.5278%\n",
      "test accuracy: 21.4781%\n",
      "test accuracy: 21.4862%\n",
      "test accuracy: 21.5517%\n",
      "test accuracy: 21.5596%\n",
      "test accuracy: 21.5675%\n",
      "test accuracy: 21.5753%\n",
      "test accuracy: 21.5262%\n",
      "test accuracy: 21.4773%\n",
      "test accuracy: 21.4286%\n",
      "test accuracy: 21.4367%\n",
      "test accuracy: 21.4447%\n",
      "test accuracy: 21.4527%\n",
      "test accuracy: 21.4607%\n",
      "test accuracy: 21.4686%\n",
      "test accuracy: 21.4206%\n",
      "test accuracy: 21.4286%\n",
      "test accuracy: 21.3808%\n",
      "test accuracy: 21.4444%\n",
      "test accuracy: 21.3969%\n",
      "test accuracy: 21.3496%\n",
      "test accuracy: 21.3576%\n",
      "test accuracy: 21.3106%\n",
      "test accuracy: 21.3736%\n",
      "test accuracy: 21.3268%\n",
      "test accuracy: 21.2801%\n",
      "test accuracy: 21.3428%\n",
      "test accuracy: 21.2963%\n",
      "test accuracy: 21.2500%\n",
      "test accuracy: 21.2581%\n",
      "test accuracy: 21.2121%\n",
      "test accuracy: 21.2203%\n",
      "test accuracy: 21.2284%\n",
      "test accuracy: 21.1828%\n",
      "test accuracy: 21.1910%\n",
      "test accuracy: 21.1456%\n",
      "test accuracy: 21.1004%\n",
      "test accuracy: 21.0554%\n",
      "test accuracy: 21.0638%\n",
      "test accuracy: 21.0722%\n",
      "test accuracy: 21.1864%\n",
      "test accuracy: 21.1945%\n",
      "test accuracy: 21.2553%\n",
      "test accuracy: 21.2105%\n",
      "test accuracy: 21.2185%\n",
      "test accuracy: 21.2264%\n",
      "test accuracy: 21.1820%\n",
      "test accuracy: 21.1378%\n",
      "test accuracy: 21.0938%\n",
      "test accuracy: 21.0499%\n",
      "test accuracy: 21.0581%\n",
      "test accuracy: 21.0145%\n",
      "test accuracy: 21.0227%\n",
      "test accuracy: 21.0309%\n",
      "test accuracy: 20.9877%\n",
      "test accuracy: 20.9959%\n",
      "test accuracy: 21.0553%\n",
      "test accuracy: 21.1145%\n",
      "test accuracy: 21.1224%\n",
      "test accuracy: 21.1813%\n",
      "test accuracy: 21.1890%\n",
      "test accuracy: 21.2475%\n",
      "test accuracy: 21.2551%\n",
      "test accuracy: 21.2121%\n",
      "test accuracy: 21.3206%\n",
      "test accuracy: 21.3783%\n",
      "test accuracy: 21.3855%\n",
      "test accuracy: 21.3928%\n",
      "test accuracy: 21.3500%\n",
      "test accuracy: 21.4072%\n",
      "test accuracy: 21.3645%\n",
      "test accuracy: 21.4215%\n",
      "test accuracy: 21.5278%\n",
      "test accuracy: 21.5347%\n",
      "test accuracy: 21.5909%\n",
      "test accuracy: 21.6469%\n",
      "test accuracy: 21.7028%\n",
      "test accuracy: 21.6601%\n",
      "test accuracy: 21.6667%\n",
      "test accuracy: 21.6732%\n",
      "test accuracy: 21.6797%\n",
      "test accuracy: 21.6862%\n",
      "test accuracy: 21.6926%\n",
      "test accuracy: 21.6990%\n",
      "test accuracy: 21.6570%\n",
      "test accuracy: 21.6151%\n",
      "test accuracy: 21.5734%\n",
      "test accuracy: 21.5800%\n",
      "test accuracy: 21.5865%\n",
      "test accuracy: 21.5931%\n",
      "test accuracy: 21.5517%\n",
      "test accuracy: 21.5105%\n",
      "test accuracy: 21.4695%\n",
      "test accuracy: 21.5238%\n",
      "test accuracy: 21.5304%\n",
      "test accuracy: 21.5370%\n",
      "test accuracy: 21.5909%\n",
      "test accuracy: 21.5974%\n",
      "test accuracy: 21.6981%\n",
      "test accuracy: 21.7043%\n",
      "test accuracy: 21.6635%\n",
      "test accuracy: 21.6229%\n",
      "test accuracy: 21.5824%\n",
      "test accuracy: 21.5421%\n",
      "test accuracy: 21.5019%\n",
      "test accuracy: 21.5549%\n",
      "test accuracy: 21.5613%\n",
      "test accuracy: 21.6141%\n",
      "test accuracy: 21.6204%\n",
      "test accuracy: 21.6266%\n",
      "test accuracy: 21.6328%\n",
      "test accuracy: 21.6390%\n",
      "test accuracy: 21.6452%\n",
      "test accuracy: 21.6972%\n",
      "test accuracy: 21.7033%\n",
      "test accuracy: 21.7093%\n",
      "test accuracy: 21.7153%\n",
      "test accuracy: 21.7668%\n",
      "test accuracy: 21.7273%\n",
      "test accuracy: 21.7332%\n",
      "test accuracy: 21.7844%\n",
      "test accuracy: 21.7902%\n",
      "test accuracy: 21.7509%\n",
      "test accuracy: 21.7568%\n",
      "test accuracy: 21.7176%\n",
      "test accuracy: 21.7684%\n",
      "test accuracy: 21.7294%\n",
      "test accuracy: 21.7352%\n",
      "test accuracy: 21.7411%\n",
      "test accuracy: 21.7023%\n",
      "test accuracy: 21.7527%\n",
      "test accuracy: 21.7140%\n",
      "test accuracy: 21.7199%\n",
      "test accuracy: 21.7257%\n",
      "test accuracy: 21.6873%\n",
      "test accuracy: 21.6490%\n",
      "test accuracy: 21.6109%\n",
      "test accuracy: 21.5729%\n",
      "test accuracy: 21.5789%\n",
      "test accuracy: 21.5849%\n",
      "test accuracy: 21.5909%\n",
      "test accuracy: 21.6405%\n",
      "test accuracy: 21.6463%\n",
      "test accuracy: 21.6522%\n",
      "test accuracy: 21.7014%\n",
      "test accuracy: 21.7071%\n",
      "test accuracy: 21.6696%\n",
      "test accuracy: 21.6321%\n",
      "test accuracy: 21.6810%\n",
      "test accuracy: 21.6437%\n",
      "test accuracy: 21.6065%\n",
      "test accuracy: 21.6552%\n",
      "test accuracy: 21.6610%\n",
      "test accuracy: 21.7094%\n",
      "test accuracy: 21.6724%\n",
      "test accuracy: 21.6354%\n",
      "test accuracy: 21.6837%\n",
      "test accuracy: 21.6893%\n",
      "test accuracy: 21.6949%\n",
      "test accuracy: 21.6582%\n",
      "test accuracy: 21.6639%\n",
      "test accuracy: 21.6695%\n",
      "test accuracy: 21.6330%\n",
      "test accuracy: 21.5966%\n",
      "test accuracy: 21.5604%\n",
      "test accuracy: 21.5243%\n",
      "test accuracy: 21.4883%\n",
      "test accuracy: 21.4942%\n",
      "test accuracy: 21.4583%\n",
      "test accuracy: 21.4226%\n",
      "test accuracy: 21.4701%\n",
      "test accuracy: 21.5174%\n",
      "test accuracy: 21.5646%\n",
      "test accuracy: 21.6116%\n",
      "test accuracy: 21.6172%\n",
      "test accuracy: 21.5815%\n",
      "test accuracy: 21.5461%\n",
      "test accuracy: 21.5107%\n",
      "test accuracy: 21.5164%\n",
      "test accuracy: 21.4812%\n",
      "test accuracy: 21.5686%\n",
      "test accuracy: 21.5334%\n",
      "test accuracy: 21.5391%\n",
      "test accuracy: 21.5041%\n",
      "test accuracy: 21.5909%\n",
      "test accuracy: 21.5964%\n",
      "test accuracy: 21.6019%\n",
      "test accuracy: 21.6074%\n",
      "test accuracy: 21.5726%\n",
      "test accuracy: 21.5781%\n",
      "test accuracy: 21.5836%\n",
      "test accuracy: 21.5891%\n",
      "test accuracy: 21.6747%\n",
      "test accuracy: 21.6400%\n",
      "test accuracy: 21.6054%\n",
      "test accuracy: 21.5710%\n",
      "test accuracy: 21.5764%\n",
      "test accuracy: 21.5421%\n",
      "test accuracy: 21.5079%\n",
      "test accuracy: 21.5135%\n",
      "test accuracy: 21.4794%\n",
      "test accuracy: 21.4455%\n",
      "test accuracy: 21.4117%\n",
      "test accuracy: 21.3780%\n",
      "test accuracy: 21.3836%\n",
      "test accuracy: 21.3893%\n",
      "test accuracy: 21.3950%\n",
      "test accuracy: 21.3615%\n",
      "test accuracy: 21.3672%\n",
      "test accuracy: 21.3729%\n",
      "test accuracy: 21.3396%\n",
      "test accuracy: 21.3064%\n",
      "test accuracy: 21.3509%\n",
      "test accuracy: 21.3953%\n",
      "test accuracy: 21.4009%\n",
      "test accuracy: 21.4451%\n",
      "test accuracy: 21.4892%\n",
      "test accuracy: 21.5331%\n",
      "test accuracy: 21.5000%\n",
      "test accuracy: 21.5054%\n",
      "test accuracy: 21.4724%\n",
      "test accuracy: 21.4395%\n",
      "test accuracy: 21.4067%\n",
      "test accuracy: 21.3740%\n",
      "test accuracy: 21.3796%\n",
      "test accuracy: 21.4231%\n",
      "test accuracy: 21.4666%\n",
      "test accuracy: 21.4719%\n",
      "test accuracy: 21.5530%\n",
      "test accuracy: 21.5204%\n",
      "test accuracy: 21.5257%\n",
      "test accuracy: 21.5686%\n",
      "test accuracy: 21.5361%\n",
      "test accuracy: 21.5414%\n",
      "test accuracy: 21.5090%\n",
      "test accuracy: 21.4768%\n",
      "test accuracy: 21.4820%\n",
      "test accuracy: 21.4499%\n",
      "test accuracy: 21.4925%\n",
      "test accuracy: 21.5350%\n",
      "test accuracy: 21.6146%\n",
      "test accuracy: 21.5825%\n",
      "test accuracy: 21.5875%\n",
      "test accuracy: 21.5556%\n",
      "test accuracy: 21.5976%\n",
      "test accuracy: 21.6027%\n",
      "test accuracy: 21.6077%\n",
      "test accuracy: 21.6495%\n",
      "test accuracy: 21.6544%\n",
      "test accuracy: 21.6593%\n",
      "test accuracy: 21.6642%\n",
      "test accuracy: 21.6325%\n",
      "test accuracy: 21.6374%\n",
      "test accuracy: 21.6058%\n",
      "test accuracy: 21.6108%\n",
      "test accuracy: 21.6521%\n",
      "test accuracy: 21.6570%\n",
      "test accuracy: 21.6255%\n",
      "test accuracy: 21.6667%\n",
      "test accuracy: 21.6353%\n",
      "test accuracy: 21.6040%\n",
      "test accuracy: 21.5729%\n",
      "test accuracy: 21.5778%\n",
      "test accuracy: 21.5827%\n",
      "test accuracy: 21.5517%\n",
      "test accuracy: 21.5567%\n",
      "test accuracy: 21.5616%\n",
      "test accuracy: 21.5665%\n",
      "test accuracy: 21.5714%\n",
      "test accuracy: 21.5407%\n",
      "test accuracy: 21.5100%\n",
      "test accuracy: 21.5149%\n",
      "test accuracy: 21.4844%\n",
      "test accuracy: 21.5248%\n",
      "test accuracy: 21.4943%\n",
      "test accuracy: 21.4993%\n",
      "test accuracy: 21.5042%\n",
      "test accuracy: 21.5092%\n",
      "test accuracy: 21.5493%\n",
      "test accuracy: 21.5190%\n",
      "test accuracy: 21.5239%\n",
      "test accuracy: 21.4937%\n",
      "test accuracy: 21.4986%\n",
      "test accuracy: 21.5035%\n",
      "test accuracy: 21.5084%\n",
      "test accuracy: 21.5132%\n",
      "test accuracy: 21.5181%\n",
      "test accuracy: 21.4882%\n",
      "test accuracy: 21.4583%\n",
      "test accuracy: 21.4979%\n",
      "test accuracy: 21.5374%\n",
      "test accuracy: 21.5422%\n",
      "test accuracy: 21.5470%\n",
      "test accuracy: 21.6207%\n",
      "test accuracy: 21.6253%\n",
      "test accuracy: 21.5956%\n",
      "test accuracy: 21.6003%\n",
      "test accuracy: 21.6049%\n",
      "test accuracy: 21.6096%\n",
      "test accuracy: 21.6484%\n",
      "test accuracy: 21.6872%\n",
      "test accuracy: 21.6576%\n",
      "test accuracy: 21.6281%\n",
      "test accuracy: 21.6327%\n",
      "test accuracy: 21.6033%\n",
      "test accuracy: 21.5739%\n",
      "test accuracy: 21.5786%\n",
      "test accuracy: 21.5832%\n",
      "test accuracy: 21.5541%\n",
      "test accuracy: 21.5250%\n",
      "test accuracy: 21.5633%\n",
      "test accuracy: 21.5343%\n",
      "test accuracy: 21.5390%\n",
      "test accuracy: 21.5436%\n",
      "test accuracy: 21.5483%\n",
      "test accuracy: 21.5529%\n",
      "test accuracy: 21.5575%\n",
      "test accuracy: 21.5287%\n",
      "test accuracy: 21.5000%\n",
      "test accuracy: 21.4714%\n",
      "test accuracy: 21.4761%\n",
      "test accuracy: 21.5139%\n",
      "test accuracy: 21.4854%\n",
      "test accuracy: 21.4570%\n",
      "test accuracy: 21.4616%\n",
      "test accuracy: 21.4333%\n",
      "test accuracy: 21.4380%\n",
      "test accuracy: 21.4427%\n",
      "test accuracy: 21.4474%\n",
      "test accuracy: 21.4192%\n",
      "test accuracy: 21.4239%\n",
      "test accuracy: 21.3958%\n",
      "test accuracy: 21.4005%\n",
      "test accuracy: 21.4379%\n",
      "test accuracy: 21.4099%\n",
      "test accuracy: 21.4146%\n",
      "test accuracy: 21.4193%\n",
      "test accuracy: 21.3914%\n",
      "test accuracy: 21.3636%\n",
      "test accuracy: 21.3684%\n",
      "test accuracy: 21.3407%\n",
      "test accuracy: 21.3131%\n",
      "test accuracy: 21.3501%\n",
      "test accuracy: 21.3871%\n",
      "test accuracy: 21.3918%\n",
      "test accuracy: 21.4286%\n",
      "test accuracy: 21.4332%\n",
      "test accuracy: 21.4377%\n",
      "test accuracy: 21.4423%\n",
      "test accuracy: 21.4469%\n",
      "test accuracy: 21.4514%\n",
      "test accuracy: 21.4240%\n",
      "test accuracy: 21.4923%\n",
      "test accuracy: 21.4650%\n",
      "test accuracy: 21.4377%\n",
      "test accuracy: 21.4740%\n",
      "test accuracy: 21.4467%\n",
      "test accuracy: 21.4512%\n",
      "test accuracy: 21.4241%\n",
      "test accuracy: 21.4602%\n",
      "test accuracy: 21.4331%\n",
      "test accuracy: 21.4061%\n",
      "test accuracy: 21.3791%\n",
      "test accuracy: 21.4151%\n",
      "test accuracy: 21.3882%\n",
      "test accuracy: 21.3927%\n",
      "test accuracy: 21.4912%\n",
      "test accuracy: 21.5269%\n",
      "test accuracy: 21.5312%\n",
      "test accuracy: 21.5356%\n",
      "test accuracy: 21.5399%\n",
      "test accuracy: 21.5131%\n",
      "test accuracy: 21.5796%\n",
      "test accuracy: 21.5839%\n",
      "test accuracy: 21.5571%\n",
      "test accuracy: 21.5613%\n",
      "test accuracy: 21.5347%\n",
      "test accuracy: 21.5389%\n",
      "test accuracy: 21.5123%\n",
      "test accuracy: 21.5783%\n",
      "test accuracy: 21.5517%\n",
      "test accuracy: 21.5252%\n",
      "test accuracy: 21.5602%\n",
      "test accuracy: 21.5951%\n",
      "test accuracy: 21.5686%\n",
      "test accuracy: 21.5422%\n",
      "test accuracy: 21.5465%\n",
      "test accuracy: 21.5507%\n",
      "test accuracy: 21.5549%\n",
      "test accuracy: 21.5895%\n",
      "test accuracy: 21.5937%\n",
      "test accuracy: 21.5978%\n",
      "test accuracy: 21.5716%\n",
      "test accuracy: 21.5758%\n",
      "test accuracy: 21.5799%\n",
      "test accuracy: 21.5538%\n",
      "test accuracy: 21.5278%\n",
      "test accuracy: 21.5018%\n",
      "test accuracy: 21.5361%\n",
      "test accuracy: 21.5102%\n",
      "test accuracy: 21.5144%\n",
      "test accuracy: 21.4886%\n",
      "test accuracy: 21.4928%\n",
      "test accuracy: 21.4970%\n",
      "test accuracy: 21.4713%\n",
      "test accuracy: 21.4755%\n",
      "test accuracy: 21.4797%\n",
      "test accuracy: 21.4839%\n",
      "test accuracy: 21.4881%\n",
      "test accuracy: 21.5220%\n",
      "test accuracy: 21.5261%\n",
      "test accuracy: 21.5599%\n",
      "test accuracy: 21.5936%\n",
      "test accuracy: 21.5976%\n",
      "test accuracy: 21.6017%\n",
      "test accuracy: 21.6352%\n",
      "test accuracy: 21.6097%\n",
      "test accuracy: 21.6431%\n",
      "test accuracy: 21.6471%\n",
      "test accuracy: 21.6804%\n",
      "test accuracy: 21.6549%\n",
      "test accuracy: 21.6295%\n",
      "test accuracy: 21.6628%\n",
      "test accuracy: 21.6667%\n",
      "test accuracy: 21.6414%\n",
      "test accuracy: 21.7036%\n",
      "test accuracy: 21.7366%\n",
      "test accuracy: 21.7113%\n",
      "test accuracy: 21.6860%\n",
      "test accuracy: 21.6609%\n",
      "test accuracy: 21.6647%\n",
      "test accuracy: 21.6686%\n",
      "test accuracy: 21.6435%\n",
      "test accuracy: 21.6763%\n",
      "test accuracy: 21.6513%\n",
      "test accuracy: 21.6263%\n",
      "test accuracy: 21.6590%\n",
      "test accuracy: 21.6341%\n",
      "test accuracy: 21.6667%\n",
      "test accuracy: 21.6992%\n",
      "test accuracy: 21.6743%\n",
      "test accuracy: 21.7068%\n",
      "test accuracy: 21.7105%\n",
      "test accuracy: 21.6857%\n",
      "test accuracy: 21.6610%\n",
      "test accuracy: 21.6363%\n",
      "test accuracy: 21.6401%\n",
      "test accuracy: 21.6439%\n",
      "test accuracy: 21.6761%\n",
      "test accuracy: 21.6515%\n",
      "test accuracy: 21.6270%\n",
      "test accuracy: 21.6308%\n",
      "test accuracy: 21.6629%\n",
      "test accuracy: 21.6384%\n",
      "test accuracy: 21.6422%\n",
      "test accuracy: 21.6460%\n",
      "test accuracy: 21.6498%\n",
      "test accuracy: 21.6535%\n",
      "test accuracy: 21.6573%\n",
      "test accuracy: 21.6330%\n",
      "test accuracy: 21.6087%\n",
      "test accuracy: 21.6405%\n",
      "test accuracy: 21.6163%\n",
      "test accuracy: 21.6201%\n",
      "test accuracy: 21.6239%\n",
      "test accuracy: 21.6276%\n",
      "test accuracy: 21.6592%\n",
      "test accuracy: 21.6352%\n",
      "test accuracy: 21.6389%\n",
      "test accuracy: 21.6426%\n",
      "test accuracy: 21.6463%\n",
      "test accuracy: 21.6501%\n",
      "test accuracy: 21.6814%\n",
      "test accuracy: 21.6575%\n",
      "test accuracy: 21.6887%\n",
      "test accuracy: 21.6648%\n",
      "test accuracy: 21.6410%\n",
      "test accuracy: 21.6172%\n",
      "test accuracy: 21.5934%\n",
      "test accuracy: 21.5971%\n",
      "test accuracy: 21.6009%\n",
      "test accuracy: 21.5772%\n",
      "test accuracy: 21.5810%\n",
      "test accuracy: 21.5574%\n",
      "test accuracy: 21.5338%\n",
      "test accuracy: 21.5376%\n",
      "test accuracy: 21.5686%\n",
      "test accuracy: 21.5724%\n",
      "test accuracy: 21.5761%\n",
      "test accuracy: 21.5527%\n",
      "test accuracy: 21.5293%\n",
      "test accuracy: 21.5060%\n",
      "test accuracy: 21.5097%\n",
      "test accuracy: 21.5135%\n",
      "test accuracy: 21.5443%\n",
      "test accuracy: 21.5750%\n",
      "test accuracy: 21.5517%\n",
      "test accuracy: 21.5285%\n",
      "test accuracy: 21.5054%\n",
      "test accuracy: 21.4823%\n",
      "test accuracy: 21.5129%\n",
      "test accuracy: 21.5434%\n",
      "test accuracy: 21.5203%\n",
      "test accuracy: 21.5508%\n",
      "test accuracy: 21.5545%\n",
      "test accuracy: 21.5582%\n",
      "test accuracy: 21.5885%\n",
      "test accuracy: 21.5655%\n",
      "test accuracy: 21.5426%\n",
      "test accuracy: 21.5462%\n",
      "test accuracy: 21.5499%\n",
      "test accuracy: 21.5536%\n",
      "test accuracy: 21.5572%\n",
      "test accuracy: 21.5344%\n",
      "test accuracy: 21.5116%\n",
      "test accuracy: 21.4889%\n",
      "test accuracy: 21.5190%\n",
      "test accuracy: 21.5227%\n",
      "test accuracy: 21.5000%\n",
      "test accuracy: 21.5300%\n",
      "test accuracy: 21.5599%\n",
      "test accuracy: 21.5635%\n",
      "test accuracy: 21.5409%\n",
      "test accuracy: 21.5183%\n",
      "test accuracy: 21.5220%\n",
      "test accuracy: 21.4995%\n",
      "test accuracy: 21.5031%\n",
      "test accuracy: 21.4807%\n",
      "test accuracy: 21.4844%\n",
      "test accuracy: 21.4880%\n",
      "test accuracy: 21.4657%\n",
      "test accuracy: 21.4694%\n",
      "test accuracy: 21.4990%\n",
      "test accuracy: 21.5026%\n",
      "test accuracy: 21.5062%\n",
      "test accuracy: 21.5098%\n",
      "test accuracy: 21.5393%\n",
      "test accuracy: 21.5170%\n",
      "test accuracy: 21.5206%\n",
      "test accuracy: 21.5242%\n",
      "test accuracy: 21.5278%\n",
      "test accuracy: 21.5313%\n",
      "test accuracy: 21.5092%\n",
      "test accuracy: 21.5128%\n",
      "test accuracy: 21.4908%\n",
      "test accuracy: 21.4688%\n",
      "test accuracy: 21.4468%\n",
      "test accuracy: 21.4249%\n",
      "test accuracy: 21.4286%\n",
      "test accuracy: 21.4067%\n",
      "test accuracy: 21.4104%\n",
      "test accuracy: 21.4140%\n",
      "test accuracy: 21.4177%\n",
      "test accuracy: 21.4467%\n",
      "test accuracy: 21.4249%\n",
      "test accuracy: 21.4032%\n",
      "test accuracy: 21.3816%\n",
      "test accuracy: 21.4105%\n",
      "test accuracy: 21.3889%\n",
      "test accuracy: 21.3673%\n",
      "test accuracy: 21.3458%\n",
      "test accuracy: 21.3494%\n",
      "test accuracy: 21.3280%\n",
      "test accuracy: 21.3065%\n",
      "test accuracy: 21.2851%\n",
      "test accuracy: 21.2889%\n",
      "test accuracy: 21.2675%\n",
      "test accuracy: 21.2963%\n",
      "test accuracy: 21.2750%\n",
      "test accuracy: 21.3037%\n",
      "test accuracy: 21.3074%\n",
      "test accuracy: 21.3111%\n",
      "test accuracy: 21.3147%\n",
      "test accuracy: 21.3433%\n",
      "test accuracy: 21.3469%\n",
      "test accuracy: 21.3505%\n",
      "test accuracy: 21.3294%\n",
      "test accuracy: 21.3578%\n",
      "test accuracy: 21.3614%\n",
      "test accuracy: 21.3650%\n",
      "test accuracy: 21.3686%\n",
      "test accuracy: 21.3722%\n",
      "test accuracy: 21.4250%\n",
      "test accuracy: 21.4039%\n",
      "test accuracy: 21.4321%\n",
      "test accuracy: 21.4602%\n",
      "test accuracy: 21.4391%\n",
      "test accuracy: 21.4426%\n",
      "test accuracy: 21.4216%\n",
      "test accuracy: 21.4006%\n",
      "test accuracy: 21.4041%\n",
      "test accuracy: 21.4076%\n",
      "test accuracy: 21.3867%\n",
      "test accuracy: 21.3902%\n",
      "test accuracy: 21.3938%\n",
      "test accuracy: 21.3973%\n",
      "test accuracy: 21.4008%\n",
      "test accuracy: 21.3800%\n",
      "test accuracy: 21.3592%\n",
      "test accuracy: 21.3628%\n",
      "test accuracy: 21.3663%\n",
      "test accuracy: 21.3698%\n",
      "test accuracy: 21.3733%\n",
      "test accuracy: 21.3527%\n",
      "test accuracy: 21.3320%\n",
      "test accuracy: 21.3356%\n",
      "test accuracy: 21.3391%\n",
      "test accuracy: 21.3186%\n",
      "test accuracy: 21.2981%\n",
      "test accuracy: 21.3016%\n",
      "test accuracy: 21.2812%\n",
      "test accuracy: 21.2848%\n",
      "test accuracy: 21.2883%\n",
      "test accuracy: 21.2679%\n",
      "test accuracy: 21.2715%\n",
      "test accuracy: 21.2751%\n",
      "test accuracy: 21.2786%\n",
      "test accuracy: 21.2822%\n",
      "test accuracy: 21.2857%\n",
      "test accuracy: 21.2892%\n",
      "test accuracy: 21.2928%\n",
      "test accuracy: 21.2726%\n",
      "test accuracy: 21.2524%\n",
      "test accuracy: 21.2322%\n",
      "test accuracy: 21.2358%\n",
      "test accuracy: 21.2394%\n",
      "test accuracy: 21.2665%\n",
      "test accuracy: 21.2465%\n",
      "test accuracy: 21.2264%\n",
      "test accuracy: 21.2300%\n",
      "test accuracy: 21.2571%\n",
      "test accuracy: 21.2371%\n",
      "test accuracy: 21.2876%\n",
      "test accuracy: 21.2676%\n",
      "test accuracy: 21.3180%\n",
      "test accuracy: 21.2980%\n",
      "test accuracy: 21.2781%\n",
      "test accuracy: 21.2816%\n",
      "test accuracy: 21.2850%\n",
      "test accuracy: 21.2652%\n",
      "test accuracy: 21.2687%\n",
      "test accuracy: 21.2721%\n",
      "test accuracy: 21.2523%\n",
      "test accuracy: 21.2558%\n",
      "test accuracy: 21.2361%\n",
      "test accuracy: 21.2163%\n",
      "test accuracy: 21.2199%\n",
      "test accuracy: 21.2002%\n",
      "test accuracy: 21.2269%\n",
      "test accuracy: 21.2072%\n",
      "test accuracy: 21.2338%\n",
      "test accuracy: 21.2373%\n",
      "test accuracy: 21.2177%\n",
      "test accuracy: 21.2442%\n",
      "test accuracy: 21.2937%\n",
      "test accuracy: 21.2971%\n",
      "test accuracy: 21.3006%\n",
      "test accuracy: 21.2810%\n",
      "test accuracy: 21.2844%\n",
      "test accuracy: 21.2878%\n",
      "test accuracy: 21.2683%\n",
      "test accuracy: 21.2717%\n",
      "test accuracy: 21.2523%\n",
      "test accuracy: 21.2329%\n",
      "test accuracy: 21.2591%\n",
      "test accuracy: 21.2397%\n",
      "test accuracy: 21.2432%\n",
      "test accuracy: 21.2238%\n",
      "test accuracy: 21.2500%\n",
      "test accuracy: 21.2307%\n",
      "test accuracy: 21.2114%\n",
      "test accuracy: 21.2375%\n",
      "test accuracy: 21.2183%\n",
      "test accuracy: 21.1991%\n",
      "test accuracy: 21.1799%\n",
      "test accuracy: 21.1834%\n",
      "test accuracy: 21.1868%\n",
      "test accuracy: 21.1677%\n",
      "test accuracy: 21.1712%\n",
      "test accuracy: 21.1746%\n",
      "test accuracy: 21.1781%\n",
      "test accuracy: 21.1815%\n",
      "test accuracy: 21.2298%\n",
      "test accuracy: 21.2108%\n",
      "test accuracy: 21.1918%\n",
      "test accuracy: 21.1728%\n",
      "test accuracy: 21.1538%\n",
      "test accuracy: 21.1349%\n",
      "test accuracy: 21.1161%\n",
      "test accuracy: 21.1195%\n",
      "test accuracy: 21.1230%\n",
      "test accuracy: 21.1264%\n",
      "test accuracy: 21.1077%\n",
      "test accuracy: 21.1333%\n",
      "test accuracy: 21.1368%\n",
      "test accuracy: 21.1846%\n",
      "test accuracy: 21.2101%\n",
      "test accuracy: 21.1913%\n",
      "test accuracy: 21.1947%\n",
      "test accuracy: 21.1981%\n",
      "test accuracy: 21.2235%\n",
      "test accuracy: 21.2048%\n",
      "test accuracy: 21.1861%\n",
      "test accuracy: 21.1894%\n",
      "test accuracy: 21.1708%\n",
      "test accuracy: 21.1741%\n",
      "test accuracy: 21.1775%\n",
      "test accuracy: 21.1589%\n",
      "test accuracy: 21.1623%\n",
      "test accuracy: 21.1656%\n",
      "test accuracy: 21.1690%\n",
      "test accuracy: 21.1942%\n",
      "test accuracy: 21.1757%\n",
      "test accuracy: 21.1572%\n",
      "test accuracy: 21.1387%\n",
      "test accuracy: 21.1639%\n",
      "test accuracy: 21.1672%\n",
      "test accuracy: 21.1706%\n",
      "test accuracy: 21.1739%\n",
      "test accuracy: 21.1555%\n",
      "test accuracy: 21.1372%\n",
      "test accuracy: 21.1405%\n",
      "test accuracy: 21.1438%\n",
      "test accuracy: 21.1472%\n",
      "test accuracy: 21.1505%\n",
      "test accuracy: 21.1538%\n",
      "test accuracy: 21.1572%\n",
      "test accuracy: 21.1605%\n",
      "test accuracy: 21.1422%\n",
      "test accuracy: 21.1240%\n",
      "test accuracy: 21.1274%\n",
      "test accuracy: 21.1092%\n",
      "test accuracy: 21.1340%\n",
      "test accuracy: 21.1373%\n",
      "test accuracy: 21.1621%\n",
      "test accuracy: 21.1654%\n",
      "test accuracy: 21.1473%\n",
      "test accuracy: 21.1719%\n",
      "test accuracy: 21.1966%\n",
      "test accuracy: 21.1785%\n",
      "test accuracy: 21.1604%\n",
      "test accuracy: 21.1637%\n",
      "test accuracy: 21.1457%\n",
      "test accuracy: 21.1277%\n",
      "test accuracy: 21.1097%\n",
      "test accuracy: 21.1130%\n",
      "test accuracy: 21.1375%\n",
      "test accuracy: 21.1196%\n",
      "test accuracy: 21.1229%\n",
      "test accuracy: 21.1050%\n",
      "test accuracy: 21.1083%\n",
      "test accuracy: 21.1538%\n",
      "test accuracy: 21.1571%\n",
      "test accuracy: 21.1392%\n",
      "test accuracy: 21.1425%\n",
      "test accuracy: 21.1668%\n",
      "test accuracy: 21.1700%\n",
      "test accuracy: 21.1522%\n",
      "test accuracy: 21.1345%\n",
      "test accuracy: 21.2007%\n",
      "test accuracy: 21.1829%\n",
      "test accuracy: 21.1651%\n",
      "test accuracy: 21.1893%\n",
      "test accuracy: 21.1925%\n",
      "test accuracy: 21.1957%\n",
      "test accuracy: 21.2197%\n",
      "test accuracy: 21.2229%\n",
      "test accuracy: 21.2052%\n",
      "test accuracy: 21.2083%\n",
      "test accuracy: 21.2115%\n",
      "test accuracy: 21.2146%\n",
      "test accuracy: 21.2386%\n",
      "test accuracy: 21.2417%\n",
      "test accuracy: 21.2241%\n",
      "test accuracy: 21.2479%\n",
      "test accuracy: 21.2717%\n",
      "test accuracy: 21.2541%\n",
      "test accuracy: 21.2366%\n",
      "test accuracy: 21.2397%\n",
      "test accuracy: 21.2221%\n",
      "test accuracy: 21.2046%\n",
      "test accuracy: 21.2077%\n",
      "test accuracy: 21.2109%\n",
      "test accuracy: 21.2140%\n",
      "test accuracy: 21.1965%\n",
      "test accuracy: 21.1997%\n",
      "test accuracy: 21.2233%\n",
      "test accuracy: 21.2264%\n",
      "test accuracy: 21.2295%\n",
      "test accuracy: 21.2326%\n",
      "test accuracy: 21.2357%\n",
      "test accuracy: 21.2183%\n",
      "test accuracy: 21.2010%\n",
      "test accuracy: 21.1837%\n",
      "test accuracy: 21.1868%\n",
      "test accuracy: 21.1899%\n",
      "test accuracy: 21.1930%\n",
      "test accuracy: 21.1758%\n",
      "test accuracy: 21.1585%\n",
      "test accuracy: 21.1820%\n",
      "test accuracy: 21.1648%\n",
      "test accuracy: 21.1476%\n",
      "test accuracy: 21.1507%\n",
      "test accuracy: 21.1538%\n",
      "test accuracy: 21.1570%\n",
      "test accuracy: 21.1399%\n",
      "test accuracy: 21.1228%\n",
      "test accuracy: 21.1663%\n",
      "test accuracy: 21.1492%\n",
      "test accuracy: 21.1724%\n",
      "test accuracy: 21.1755%\n",
      "test accuracy: 21.1786%\n",
      "test accuracy: 21.1616%\n",
      "test accuracy: 21.1446%\n",
      "test accuracy: 21.1878%\n",
      "test accuracy: 21.1909%\n",
      "test accuracy: 21.1939%\n",
      "test accuracy: 21.2170%\n",
      "test accuracy: 21.2000%\n",
      "test accuracy: 21.2030%\n",
      "test accuracy: 21.2061%\n",
      "test accuracy: 21.1891%\n",
      "test accuracy: 21.2121%\n",
      "test accuracy: 21.2351%\n",
      "test accuracy: 21.2580%\n",
      "test accuracy: 21.2411%\n",
      "test accuracy: 21.2242%\n",
      "test accuracy: 21.2073%\n",
      "test accuracy: 21.1905%\n",
      "test accuracy: 21.1935%\n",
      "test accuracy: 21.1965%\n",
      "test accuracy: 21.2193%\n",
      "test accuracy: 21.2421%\n",
      "test accuracy: 21.2648%\n",
      "test accuracy: 21.2875%\n",
      "test accuracy: 21.2707%\n",
      "test accuracy: 21.2737%\n",
      "test accuracy: 21.3160%\n",
      "test accuracy: 21.3386%\n",
      "test accuracy: 21.3218%\n",
      "test accuracy: 21.3443%\n",
      "test accuracy: 21.3472%\n",
      "test accuracy: 21.3893%\n",
      "test accuracy: 21.3922%\n",
      "test accuracy: 21.4146%\n",
      "test accuracy: 21.4174%\n",
      "test accuracy: 21.4202%\n",
      "test accuracy: 21.4425%\n",
      "test accuracy: 21.4258%\n",
      "test accuracy: 21.4481%\n",
      "test accuracy: 21.4509%\n",
      "test accuracy: 21.4536%\n",
      "test accuracy: 21.4369%\n",
      "test accuracy: 21.4397%\n",
      "test accuracy: 21.4425%\n",
      "test accuracy: 21.4258%\n",
      "test accuracy: 21.4286%\n",
      "test accuracy: 21.4313%\n",
      "test accuracy: 21.4341%\n",
      "test accuracy: 21.4175%\n",
      "test accuracy: 21.4203%\n",
      "test accuracy: 21.4424%\n",
      "test accuracy: 21.4258%\n",
      "test accuracy: 21.4093%\n",
      "test accuracy: 21.4120%\n",
      "test accuracy: 21.4341%\n",
      "test accuracy: 21.4176%\n",
      "test accuracy: 21.4011%\n",
      "test accuracy: 21.4038%\n",
      "test accuracy: 21.3874%\n",
      "test accuracy: 21.3902%\n",
      "test accuracy: 21.3738%\n",
      "test accuracy: 21.3765%\n",
      "test accuracy: 21.3793%\n",
      "test accuracy: 21.3629%\n",
      "test accuracy: 21.3466%\n",
      "test accuracy: 21.3685%\n",
      "test accuracy: 21.3713%\n",
      "test accuracy: 21.3550%\n",
      "test accuracy: 21.3577%\n",
      "test accuracy: 21.3605%\n",
      "test accuracy: 21.3633%\n",
      "test accuracy: 21.3470%\n",
      "test accuracy: 21.3498%\n",
      "test accuracy: 21.3906%\n",
      "test accuracy: 21.3933%\n",
      "test accuracy: 21.3771%\n",
      "test accuracy: 21.3609%\n",
      "test accuracy: 21.3636%\n",
      "test accuracy: 21.3664%\n",
      "test accuracy: 21.3691%\n",
      "test accuracy: 21.3719%\n",
      "test accuracy: 21.3557%\n",
      "test accuracy: 21.3585%\n",
      "test accuracy: 21.3612%\n",
      "test accuracy: 21.3640%\n",
      "test accuracy: 21.3667%\n",
      "test accuracy: 21.3506%\n",
      "test accuracy: 21.3346%\n",
      "test accuracy: 21.3186%\n",
      "test accuracy: 21.3026%\n",
      "test accuracy: 21.2866%\n",
      "test accuracy: 21.3081%\n",
      "test accuracy: 21.3483%\n",
      "test accuracy: 21.3323%\n",
      "test accuracy: 21.3164%\n",
      "test accuracy: 21.3004%\n",
      "test accuracy: 21.2845%\n",
      "test accuracy: 21.2687%\n",
      "test accuracy: 21.2714%\n",
      "test accuracy: 21.2742%\n",
      "test accuracy: 21.2956%\n",
      "test accuracy: 21.2984%\n",
      "test accuracy: 21.3197%\n",
      "test accuracy: 21.3039%\n",
      "test accuracy: 21.2880%\n",
      "test accuracy: 21.2723%\n",
      "test accuracy: 21.2750%\n",
      "test accuracy: 21.2778%\n",
      "test accuracy: 21.2620%\n",
      "test accuracy: 21.2463%\n",
      "test accuracy: 21.2491%\n",
      "test accuracy: 21.2518%\n",
      "test accuracy: 21.2362%\n",
      "test accuracy: 21.2389%\n",
      "test accuracy: 21.2417%\n",
      "test accuracy: 21.2445%\n",
      "test accuracy: 21.2288%\n",
      "test accuracy: 21.2132%\n",
      "test accuracy: 21.2160%\n",
      "test accuracy: 21.2004%\n",
      "test accuracy: 21.2032%\n",
      "test accuracy: 21.2060%\n",
      "test accuracy: 21.2088%\n",
      "test accuracy: 21.1933%\n",
      "test accuracy: 21.1960%\n",
      "test accuracy: 21.1806%\n",
      "test accuracy: 21.1833%\n",
      "test accuracy: 21.1861%\n",
      "test accuracy: 21.1707%\n",
      "test accuracy: 21.1735%\n",
      "test accuracy: 21.1945%\n",
      "test accuracy: 21.1972%\n",
      "test accuracy: 21.1818%\n",
      "test accuracy: 21.1846%\n",
      "test accuracy: 21.1692%\n",
      "test accuracy: 21.1720%\n",
      "test accuracy: 21.1566%\n",
      "test accuracy: 21.1775%\n",
      "test accuracy: 21.1622%\n",
      "test accuracy: 21.1469%\n",
      "test accuracy: 21.1497%\n",
      "test accuracy: 21.1344%\n",
      "test accuracy: 21.1191%\n",
      "test accuracy: 21.1039%\n",
      "test accuracy: 21.1067%\n",
      "test accuracy: 21.1095%\n",
      "test accuracy: 21.1123%\n",
      "test accuracy: 21.1151%\n",
      "test accuracy: 21.1179%\n",
      "test accuracy: 21.1027%\n",
      "test accuracy: 21.0876%\n",
      "test accuracy: 21.1263%\n",
      "test accuracy: 21.1111%\n",
      "test accuracy: 21.1139%\n",
      "test accuracy: 21.0988%\n",
      "test accuracy: 21.1195%\n",
      "test accuracy: 21.1044%\n",
      "test accuracy: 21.1071%\n",
      "test accuracy: 21.0921%\n",
      "test accuracy: 21.0949%\n",
      "test accuracy: 21.0976%\n",
      "test accuracy: 21.1004%\n",
      "test accuracy: 21.1032%\n",
      "test accuracy: 21.1238%\n",
      "test accuracy: 21.1087%\n",
      "test accuracy: 21.0938%\n",
      "test accuracy: 21.0788%\n",
      "test accuracy: 21.1348%\n",
      "test accuracy: 21.1552%\n",
      "test accuracy: 21.1756%\n",
      "test accuracy: 21.1607%\n",
      "test accuracy: 21.1634%\n",
      "test accuracy: 21.1661%\n",
      "test accuracy: 21.1511%\n",
      "test accuracy: 21.1362%\n",
      "test accuracy: 21.1213%\n",
      "test accuracy: 21.1240%\n",
      "test accuracy: 21.1620%\n",
      "test accuracy: 21.1471%\n",
      "test accuracy: 21.1322%\n",
      "test accuracy: 21.1174%\n",
      "test accuracy: 21.1376%\n",
      "test accuracy: 21.1228%\n",
      "test accuracy: 21.1080%\n",
      "test accuracy: 21.0932%\n",
      "test accuracy: 21.0959%\n",
      "test accuracy: 21.0987%\n",
      "test accuracy: 21.1189%\n",
      "test accuracy: 21.1391%\n",
      "test accuracy: 21.1418%\n",
      "test accuracy: 21.1270%\n",
      "test accuracy: 21.1123%\n",
      "test accuracy: 21.0976%\n",
      "test accuracy: 21.0829%\n",
      "test accuracy: 21.0856%\n",
      "test accuracy: 21.0883%\n",
      "test accuracy: 21.0910%\n",
      "test accuracy: 21.0938%\n",
      "test accuracy: 21.1138%\n",
      "test accuracy: 21.0992%\n",
      "test accuracy: 21.0845%\n",
      "test accuracy: 21.0873%\n",
      "test accuracy: 21.0900%\n",
      "test accuracy: 21.0927%\n",
      "test accuracy: 21.0781%\n",
      "test accuracy: 21.0635%\n",
      "test accuracy: 21.0490%\n",
      "test accuracy: 21.0345%\n",
      "test accuracy: 21.0717%\n",
      "test accuracy: 21.0572%\n",
      "test accuracy: 21.0427%\n",
      "test accuracy: 21.0282%\n",
      "test accuracy: 21.0309%\n",
      "test accuracy: 21.0165%\n",
      "test accuracy: 21.0021%\n",
      "test accuracy: 20.9877%\n",
      "test accuracy: 20.9904%\n",
      "test accuracy: 21.0103%\n",
      "test accuracy: 20.9959%\n",
      "test accuracy: 20.9815%\n",
      "test accuracy: 20.9843%\n",
      "test accuracy: 20.9870%\n",
      "test accuracy: 20.9898%\n",
      "test accuracy: 20.9754%\n",
      "test accuracy: 21.0123%\n",
      "test accuracy: 20.9980%\n",
      "test accuracy: 21.0007%\n",
      "test accuracy: 20.9864%\n",
      "test accuracy: 20.9891%\n",
      "test accuracy: 20.9749%\n",
      "test accuracy: 20.9606%\n",
      "test accuracy: 20.9634%\n",
      "test accuracy: 20.9492%\n",
      "test accuracy: 20.9350%\n",
      "test accuracy: 20.9377%\n",
      "test accuracy: 20.9574%\n",
      "test accuracy: 20.9432%\n",
      "test accuracy: 20.9797%\n",
      "test accuracy: 20.9656%\n",
      "test accuracy: 20.9514%\n",
      "test accuracy: 20.9373%\n",
      "test accuracy: 20.9232%\n",
      "test accuracy: 20.9596%\n",
      "test accuracy: 20.9623%\n",
      "test accuracy: 20.9482%\n",
      "test accuracy: 20.9509%\n",
      "test accuracy: 20.9704%\n",
      "test accuracy: 20.9732%\n",
      "test accuracy: 20.9759%\n",
      "test accuracy: 20.9786%\n",
      "test accuracy: 20.9645%\n",
      "test accuracy: 20.9672%\n",
      "test accuracy: 20.9532%\n",
      "test accuracy: 20.9559%\n",
      "test accuracy: 20.9419%\n",
      "test accuracy: 20.9613%\n",
      "test accuracy: 20.9807%\n",
      "test accuracy: 20.9667%\n",
      "test accuracy: 20.9694%\n",
      "test accuracy: 20.9554%\n",
      "test accuracy: 20.9581%\n",
      "test accuracy: 20.9441%\n",
      "test accuracy: 20.9635%\n",
      "test accuracy: 20.9661%\n",
      "test accuracy: 20.9854%\n",
      "test accuracy: 20.9881%\n",
      "test accuracy: 20.9742%\n",
      "test accuracy: 20.9934%\n",
      "test accuracy: 20.9795%\n",
      "test accuracy: 20.9656%\n",
      "test accuracy: 20.9683%\n",
      "test accuracy: 20.9544%\n",
      "test accuracy: 20.9571%\n",
      "test accuracy: 20.9433%\n",
      "test accuracy: 20.9295%\n",
      "test accuracy: 20.9321%\n",
      "test accuracy: 20.9348%\n",
      "test accuracy: 20.9375%\n",
      "test accuracy: 20.9237%\n",
      "test accuracy: 20.9100%\n",
      "test accuracy: 20.9291%\n",
      "test accuracy: 20.9154%\n",
      "test accuracy: 20.9016%\n",
      "test accuracy: 20.8879%\n",
      "test accuracy: 20.8906%\n",
      "test accuracy: 20.8933%\n",
      "test accuracy: 20.9287%\n",
      "test accuracy: 20.9477%\n",
      "test accuracy: 20.9504%\n",
      "test accuracy: 20.9530%\n",
      "test accuracy: 20.9720%\n",
      "test accuracy: 20.9746%\n",
      "test accuracy: 20.9772%\n",
      "test accuracy: 20.9635%\n",
      "test accuracy: 20.9499%\n",
      "test accuracy: 20.9363%\n",
      "test accuracy: 20.9227%\n",
      "test accuracy: 20.9253%\n",
      "test accuracy: 20.9280%\n",
      "test accuracy: 20.9144%\n",
      "test accuracy: 20.9008%\n",
      "test accuracy: 20.9035%\n",
      "test accuracy: 20.9061%\n",
      "test accuracy: 20.9250%\n",
      "test accuracy: 20.9276%\n",
      "test accuracy: 20.9141%\n",
      "test accuracy: 20.9006%\n",
      "test accuracy: 20.9194%\n",
      "test accuracy: 20.9220%\n",
      "test accuracy: 20.9085%\n",
      "test accuracy: 20.8950%\n",
      "test accuracy: 20.8816%\n",
      "test accuracy: 20.8842%\n",
      "test accuracy: 20.8708%\n",
      "test accuracy: 20.8735%\n",
      "test accuracy: 20.8761%\n",
      "test accuracy: 20.8788%\n",
      "test accuracy: 20.8654%\n",
      "test accuracy: 20.8520%\n",
      "test accuracy: 20.8547%\n",
      "test accuracy: 20.8573%\n",
      "test accuracy: 20.8600%\n",
      "test accuracy: 20.8626%\n",
      "test accuracy: 20.8653%\n",
      "test accuracy: 20.8679%\n",
      "test accuracy: 20.8705%\n",
      "test accuracy: 20.8572%\n",
      "test accuracy: 20.8439%\n",
      "test accuracy: 20.8625%\n",
      "test accuracy: 20.8651%\n",
      "test accuracy: 20.8678%\n",
      "test accuracy: 20.8545%\n",
      "test accuracy: 20.8413%\n",
      "test accuracy: 20.8280%\n",
      "test accuracy: 20.8148%\n",
      "test accuracy: 20.8333%\n",
      "test accuracy: 20.8360%\n",
      "test accuracy: 20.8228%\n",
      "test accuracy: 20.8254%\n",
      "test accuracy: 20.8439%\n",
      "test accuracy: 20.8465%\n",
      "test accuracy: 20.8965%\n",
      "test accuracy: 20.8833%\n",
      "test accuracy: 20.8701%\n",
      "test accuracy: 20.8727%\n",
      "test accuracy: 20.8753%\n",
      "test accuracy: 20.8779%\n",
      "test accuracy: 20.8962%\n",
      "test accuracy: 20.8831%\n",
      "test accuracy: 20.8700%\n",
      "test accuracy: 20.8726%\n",
      "test accuracy: 20.8595%\n",
      "test accuracy: 20.8464%\n",
      "test accuracy: 20.8490%\n",
      "test accuracy: 20.8673%\n",
      "test accuracy: 20.8542%\n",
      "test accuracy: 20.8412%\n",
      "test accuracy: 20.8281%\n",
      "test accuracy: 20.8307%\n",
      "test accuracy: 20.8177%\n",
      "test accuracy: 20.8203%\n",
      "test accuracy: 20.8074%\n",
      "test accuracy: 20.8100%\n",
      "test accuracy: 20.7970%\n",
      "test accuracy: 20.7841%\n",
      "test accuracy: 20.7867%\n",
      "test accuracy: 20.7738%\n",
      "test accuracy: 20.7764%\n",
      "test accuracy: 20.7635%\n",
      "test accuracy: 20.7661%\n",
      "test accuracy: 20.7533%\n",
      "test accuracy: 20.7714%\n",
      "test accuracy: 20.7895%\n",
      "test accuracy: 20.8075%\n",
      "test accuracy: 20.8101%\n",
      "test accuracy: 20.8282%\n",
      "test accuracy: 20.8308%\n",
      "test accuracy: 20.8333%\n",
      "test accuracy: 20.8359%\n",
      "test accuracy: 20.8385%\n",
      "test accuracy: 20.8410%\n",
      "test accuracy: 20.8436%\n",
      "test accuracy: 20.8462%\n",
      "test accuracy: 20.8487%\n",
      "test accuracy: 20.8513%\n",
      "test accuracy: 20.8538%\n",
      "test accuracy: 20.8410%\n",
      "test accuracy: 20.8589%\n",
      "test accuracy: 20.8614%\n",
      "test accuracy: 20.8640%\n",
      "test accuracy: 20.8665%\n",
      "test accuracy: 20.8690%\n",
      "test accuracy: 20.8716%\n",
      "test accuracy: 20.8741%\n",
      "test accuracy: 20.8766%\n",
      "test accuracy: 20.8639%\n",
      "test accuracy: 20.8511%\n",
      "test accuracy: 20.8537%\n",
      "test accuracy: 20.8410%\n",
      "test accuracy: 20.8283%\n",
      "test accuracy: 20.8156%\n",
      "test accuracy: 20.8485%\n",
      "test accuracy: 20.8815%\n",
      "test accuracy: 20.8991%\n",
      "test accuracy: 20.9168%\n",
      "test accuracy: 20.9041%\n",
      "test accuracy: 20.8914%\n",
      "test accuracy: 20.9091%\n",
      "test accuracy: 20.9116%\n",
      "test accuracy: 20.9140%\n",
      "test accuracy: 20.9316%\n",
      "test accuracy: 20.9190%\n",
      "test accuracy: 20.9215%\n",
      "test accuracy: 20.9088%\n",
      "test accuracy: 20.8962%\n",
      "test accuracy: 20.8836%\n",
      "test accuracy: 20.8710%\n",
      "test accuracy: 20.8735%\n",
      "test accuracy: 20.8760%\n",
      "test accuracy: 20.8634%\n",
      "test accuracy: 20.8659%\n",
      "test accuracy: 20.8684%\n",
      "test accuracy: 20.8859%\n",
      "test accuracy: 20.9184%\n",
      "test accuracy: 20.9208%\n",
      "test accuracy: 20.9083%\n",
      "test accuracy: 20.9107%\n",
      "test accuracy: 20.8982%\n",
      "test accuracy: 20.9007%\n",
      "test accuracy: 20.8882%\n",
      "test accuracy: 20.8906%\n",
      "test accuracy: 20.9080%\n",
      "test accuracy: 20.9254%\n",
      "test accuracy: 20.9278%\n",
      "test accuracy: 20.9153%\n",
      "test accuracy: 20.9327%\n",
      "test accuracy: 20.9202%\n",
      "test accuracy: 20.9375%\n",
      "test accuracy: 20.9399%\n",
      "test accuracy: 20.9572%\n",
      "test accuracy: 20.9745%\n",
      "test accuracy: 21.0065%\n",
      "test accuracy: 20.9941%\n",
      "test accuracy: 20.9816%\n",
      "test accuracy: 20.9692%\n",
      "test accuracy: 20.9716%\n",
      "test accuracy: 20.9591%\n",
      "test accuracy: 20.9615%\n",
      "test accuracy: 20.9639%\n",
      "test accuracy: 20.9663%\n",
      "test accuracy: 20.9835%\n",
      "test accuracy: 20.9711%\n",
      "test accuracy: 20.9735%\n",
      "test accuracy: 20.9906%\n",
      "test accuracy: 20.9782%\n",
      "test accuracy: 20.9658%\n",
      "test accuracy: 20.9682%\n",
      "test accuracy: 20.9559%\n",
      "test accuracy: 20.9583%\n",
      "test accuracy: 20.9606%\n",
      "test accuracy: 20.9483%\n",
      "test accuracy: 20.9654%\n",
      "test accuracy: 20.9531%\n",
      "test accuracy: 20.9701%\n",
      "test accuracy: 20.9725%\n",
      "test accuracy: 20.9748%\n",
      "test accuracy: 20.9626%\n",
      "test accuracy: 20.9503%\n",
      "test accuracy: 20.9380%\n",
      "test accuracy: 20.9258%\n",
      "test accuracy: 20.9282%\n",
      "test accuracy: 20.9452%\n",
      "test accuracy: 20.9329%\n",
      "test accuracy: 20.9353%\n",
      "test accuracy: 20.9522%\n",
      "test accuracy: 20.9546%\n",
      "test accuracy: 20.9424%\n",
      "test accuracy: 20.9302%\n",
      "test accuracy: 20.9181%\n",
      "test accuracy: 20.9350%\n",
      "test accuracy: 20.9228%\n",
      "test accuracy: 20.9107%\n",
      "test accuracy: 20.9275%\n",
      "test accuracy: 20.9154%\n",
      "test accuracy: 20.9033%\n",
      "test accuracy: 20.9057%\n",
      "test accuracy: 20.9370%\n",
      "test accuracy: 20.9393%\n",
      "test accuracy: 20.9561%\n",
      "test accuracy: 20.9584%\n",
      "test accuracy: 20.9752%\n",
      "test accuracy: 20.9631%\n",
      "test accuracy: 20.9654%\n",
      "test accuracy: 20.9821%\n",
      "test accuracy: 20.9845%\n",
      "test accuracy: 21.0012%\n",
      "test accuracy: 21.0035%\n",
      "test accuracy: 21.0057%\n",
      "test accuracy: 21.0080%\n",
      "test accuracy: 21.0247%\n",
      "test accuracy: 21.0270%\n",
      "test accuracy: 21.0292%\n",
      "test accuracy: 21.0315%\n",
      "test accuracy: 21.0195%\n",
      "test accuracy: 21.0504%\n",
      "test accuracy: 21.0526%\n",
      "test accuracy: 21.0549%\n",
      "test accuracy: 21.0571%\n",
      "test accuracy: 21.0737%\n",
      "test accuracy: 21.0759%\n",
      "test accuracy: 21.0782%\n",
      "test accuracy: 21.0804%\n",
      "test accuracy: 21.0826%\n",
      "test accuracy: 21.0706%\n",
      "test accuracy: 21.0586%\n",
      "test accuracy: 21.0751%\n",
      "test accuracy: 21.0915%\n",
      "test accuracy: 21.0795%\n",
      "test accuracy: 21.0818%\n",
      "test accuracy: 21.0840%\n",
      "test accuracy: 21.0862%\n",
      "test accuracy: 21.0743%\n",
      "test accuracy: 21.0765%\n",
      "test accuracy: 21.0787%\n",
      "test accuracy: 21.0668%\n",
      "test accuracy: 21.0690%\n",
      "test accuracy: 21.0571%\n",
      "test accuracy: 21.0452%\n",
      "test accuracy: 21.0757%\n",
      "test accuracy: 21.0638%\n",
      "test accuracy: 21.0519%\n",
      "test accuracy: 21.0400%\n",
      "test accuracy: 21.0282%\n",
      "test accuracy: 21.0304%\n",
      "test accuracy: 21.0186%\n",
      "test accuracy: 21.0067%\n",
      "test accuracy: 20.9949%\n",
      "test accuracy: 21.0253%\n",
      "test accuracy: 21.0135%\n",
      "test accuracy: 21.0157%\n",
      "test accuracy: 21.0179%\n",
      "test accuracy: 21.0202%\n",
      "test accuracy: 21.0224%\n",
      "test accuracy: 21.0106%\n",
      "test accuracy: 21.0129%\n",
      "test accuracy: 21.0291%\n",
      "test accuracy: 21.0173%\n",
      "test accuracy: 21.0475%\n",
      "test accuracy: 21.0357%\n",
      "test accuracy: 21.0240%\n",
      "test accuracy: 21.0123%\n",
      "test accuracy: 21.0145%\n",
      "test accuracy: 21.0306%\n",
      "test accuracy: 21.0468%\n",
      "test accuracy: 21.0351%\n",
      "test accuracy: 21.0234%\n",
      "test accuracy: 21.0256%\n",
      "test accuracy: 21.0278%\n",
      "test accuracy: 21.0300%\n",
      "test accuracy: 21.0322%\n",
      "test accuracy: 21.0205%\n",
      "test accuracy: 21.0089%\n",
      "test accuracy: 21.0111%\n",
      "test accuracy: 21.0133%\n",
      "test accuracy: 21.0017%\n",
      "test accuracy: 20.9900%\n",
      "test accuracy: 20.9923%\n",
      "test accuracy: 21.0083%\n",
      "test accuracy: 20.9967%\n",
      "test accuracy: 20.9989%\n",
      "test accuracy: 21.0287%\n",
      "test accuracy: 21.0171%\n",
      "test accuracy: 21.0331%\n",
      "test accuracy: 21.0352%\n",
      "test accuracy: 21.0649%\n",
      "test accuracy: 21.0809%\n",
      "test accuracy: 21.0693%\n",
      "test accuracy: 21.0714%\n",
      "test accuracy: 21.0599%\n",
      "test accuracy: 21.0483%\n",
      "test accuracy: 21.0505%\n",
      "test accuracy: 21.0526%\n",
      "test accuracy: 21.0411%\n",
      "test accuracy: 21.0296%\n",
      "test accuracy: 21.0454%\n",
      "test accuracy: 21.0339%\n",
      "test accuracy: 21.0361%\n",
      "test accuracy: 21.0246%\n",
      "test accuracy: 21.0404%\n",
      "test accuracy: 21.0562%\n",
      "test accuracy: 21.0447%\n",
      "test accuracy: 21.0333%\n",
      "test accuracy: 21.0354%\n",
      "test accuracy: 21.0240%\n",
      "test accuracy: 21.0261%\n",
      "test accuracy: 21.0147%\n",
      "test accuracy: 21.0169%\n",
      "test accuracy: 21.0054%\n",
      "test accuracy: 21.0076%\n",
      "test accuracy: 21.0098%\n",
      "test accuracy: 20.9984%\n",
      "test accuracy: 21.0005%\n",
      "test accuracy: 20.9892%\n",
      "test accuracy: 20.9778%\n",
      "test accuracy: 20.9800%\n",
      "test accuracy: 20.9821%\n",
      "test accuracy: 20.9843%\n",
      "test accuracy: 20.9730%\n",
      "test accuracy: 20.9751%\n",
      "test accuracy: 20.9638%\n",
      "test accuracy: 20.9525%\n",
      "test accuracy: 20.9412%\n",
      "test accuracy: 20.9704%\n",
      "test accuracy: 20.9725%\n",
      "test accuracy: 20.9612%\n",
      "test accuracy: 20.9634%\n",
      "test accuracy: 20.9521%\n",
      "test accuracy: 20.9543%\n",
      "test accuracy: 20.9430%\n",
      "test accuracy: 20.9318%\n",
      "test accuracy: 20.9340%\n",
      "test accuracy: 20.9362%\n",
      "test accuracy: 20.9383%\n",
      "test accuracy: 20.9405%\n",
      "test accuracy: 20.9377%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _,predicted = torch.max(outputs.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).squeeze().sum().numpy()\n",
    "\n",
    "        print(f\"test accuracy: {(correct / total)*100:.4f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b295d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200704, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=50, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
