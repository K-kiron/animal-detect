{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d60b6b",
   "metadata": {},
   "source": [
    "Lien de téléchargement des données: https://cvml.ista.ac.at/AwA2/        \n",
    "\n",
    "13GB file : https://cvml.ista.ac.at/AwA2/AwA2-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d23f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 # Pour utiliser open_cv, il faut la version de python est 3.7\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision \n",
    "from torchvision.io import read_image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8e4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant. Should be the path to the folder named JPEGImages, containing the 33K images in its subfolders.\n",
    "DATA_FOLDER_PATH = \"E:\\\\3710datas\\\\Animals_with_Attributes2\\\\\"\n",
    "JPEGIMAGES_FOLDER_PATH = \"E:\\\\3710datas\\\\Animals_with_Attributes2\\\\JPEGImages\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60066a3e",
   "metadata": {},
   "source": [
    "# Note : Some labels have a low number of images. \n",
    "\n",
    "## Possible solutions to explore : \n",
    "    Data augmentation : creating new training data by applying random transformations to existing images, such as rotating, cropping, or flipping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade44228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antelope': 1046, 'bat': 383, 'beaver': 193, 'blue+whale': 174, 'bobcat': 630, 'buffalo': 895, 'chihuahua': 567, 'chimpanzee': 728, 'collie': 1028, 'cow': 1338, 'dalmatian': 549, 'deer': 1344, 'dolphin': 946, 'elephant': 1038, 'fox': 664, 'german+shepherd': 1033, 'giant+panda': 874, 'giraffe': 1202, 'gorilla': 872, 'grizzly+bear': 852, 'hamster': 779, 'hippopotamus': 684, 'horse': 1645, 'humpback+whale': 709, 'killer+whale': 291, 'leopard': 720, 'lion': 1019, 'mole': 100, 'moose': 704, 'mouse': 185, 'otter': 758, 'ox': 728, 'persian+cat': 747, 'pig': 713, 'polar+bear': 868, 'rabbit': 1088, 'raccoon': 512, 'rat': 310, 'rhinoceros': 696, 'seal': 988, 'sheep': 1420, 'siamese+cat': 500, 'skunk': 188, 'spider+monkey': 291, 'squirrel': 1200, 'tiger': 877, 'walrus': 215, 'weasel': 272, 'wolf': 589, 'zebra': 1170}\n",
      "{'antelope': 0.028, 'bat': 0.0103, 'beaver': 0.0052, 'blue+whale': 0.0047, 'bobcat': 0.0169, 'buffalo': 0.024, 'chihuahua': 0.0152, 'chimpanzee': 0.0195, 'collie': 0.0275, 'cow': 0.0359, 'dalmatian': 0.0147, 'deer': 0.036, 'dolphin': 0.0253, 'elephant': 0.0278, 'fox': 0.0178, 'german+shepherd': 0.0277, 'giant+panda': 0.0234, 'giraffe': 0.0322, 'gorilla': 0.0234, 'grizzly+bear': 0.0228, 'hamster': 0.0209, 'hippopotamus': 0.0183, 'horse': 0.0441, 'humpback+whale': 0.019, 'killer+whale': 0.0078, 'leopard': 0.0193, 'lion': 0.0273, 'mole': 0.0027, 'moose': 0.0189, 'mouse': 0.005, 'otter': 0.0203, 'ox': 0.0195, 'persian+cat': 0.02, 'pig': 0.0191, 'polar+bear': 0.0233, 'rabbit': 0.0292, 'raccoon': 0.0137, 'rat': 0.0083, 'rhinoceros': 0.0186, 'seal': 0.0265, 'sheep': 0.038, 'siamese+cat': 0.0134, 'skunk': 0.005, 'spider+monkey': 0.0078, 'squirrel': 0.0322, 'tiger': 0.0235, 'walrus': 0.0058, 'weasel': 0.0073, 'wolf': 0.0158, 'zebra': 0.0313}\n"
     ]
    }
   ],
   "source": [
    "def find_num_images_per_label(img_dir = JPEGIMAGES_FOLDER_PATH) -> tuple[dict,dict]: \n",
    "    \"\"\" \n",
    "    USEFUL FOR SAMPLING.\n",
    "    Return a dict with keys as the 50 labels, and values being the number of images in each subdirectory corresponding to label\n",
    "    and a second dict with the relative numbers (proportion) for every label compared to the total number of images (useful for sampling)\"\"\"\n",
    "    labels_dirs = os.listdir(img_dir)\n",
    "    num_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    proportions_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    total_num_images = 0\n",
    "\n",
    "    # Update absolute number of images per label\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        specific_label_path = os.path.join(img_dir, labels_dirs[i])\n",
    "        num_images_label = len(os.listdir(specific_label_path))\n",
    "        total_num_images += num_images_label\n",
    "        num_images_per_label[label] = num_images_label\n",
    "\n",
    "    # Update relative number of images per label (proportion)\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        num_images_label = num_images_per_label[label]\n",
    "        proportion_label = round(num_images_label / total_num_images, 4)\n",
    "        proportions_images_per_label[label] = proportion_label\n",
    "\n",
    "    return num_images_per_label, proportions_images_per_label\n",
    "\n",
    "num_images_per_label, proportions_images_per_label = find_num_images_per_label()\n",
    "print(num_images_per_label)\n",
    "print(proportions_images_per_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3b38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existent annotations.csv file.\n",
      " ---------------------------\n",
      "Sucessfully created annotations.csv file.\n"
     ]
    }
   ],
   "source": [
    "ANNOTATIONS_FILENAME = 'annotations.csv'\n",
    "\n",
    "def create_annotations_csv_file(annotations_filename = ANNOTATIONS_FILENAME, img_dir = JPEGIMAGES_FOLDER_PATH): \n",
    "    \"\"\" \n",
    "    Create a csv annotations_file, annotations.csv, with two columns, in the format : \n",
    "                        path/to/image, label\n",
    "    \n",
    "    The annotation csv is necessary for DataLoader.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels_dirs:list = os.listdir(img_dir)\n",
    "   \n",
    "    if os.path.exists(annotations_filename):\n",
    "        os.remove(annotations_filename)\n",
    "        print(f'Deleted existent {ANNOTATIONS_FILENAME} file.\\n ---------------------------')\n",
    "    \n",
    "    with open(annotations_filename, 'w', newline='') as file :\n",
    "        writer = csv.writer(file, dialect='excel', delimiter=',')\n",
    "\n",
    "        for i, label in enumerate(labels_dirs) : \n",
    "\n",
    "            specific_label_path = os.path.join(img_dir, label)\n",
    "            images_names = os.listdir(specific_label_path)\n",
    "\n",
    "            for j, image_name in enumerate(images_names):\n",
    "                full_path_to_img= os.path.join(specific_label_path, image_name)\n",
    "                full_path_to_img= os.path.join(label, image_name)\n",
    "\n",
    "                row = [full_path_to_img, label]\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f'Sucessfully created {ANNOTATIONS_FILENAME} file.')\n",
    "\n",
    "#\n",
    "create_annotations_csv_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dc7a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antelope': 0, 'grizzly+bear': 1, 'killer+whale': 2, 'beaver': 3, 'dalmatian': 4, 'persian+cat': 5, 'horse': 6, 'german+shepherd': 7, 'blue+whale': 8, 'siamese+cat': 9, 'skunk': 10, 'mole': 11, 'tiger': 12, 'hippopotamus': 13, 'leopard': 14, 'moose': 15, 'spider+monkey': 16, 'humpback+whale': 17, 'elephant': 18, 'gorilla': 19, 'ox': 20, 'fox': 21, 'sheep': 22, 'seal': 23, 'chimpanzee': 24, 'hamster': 25, 'squirrel': 26, 'rhinoceros': 27, 'rabbit': 28, 'bat': 29, 'giraffe': 30, 'wolf': 31, 'chihuahua': 32, 'rat': 33, 'weasel': 34, 'otter': 35, 'buffalo': 36, 'zebra': 37, 'giant+panda': 38, 'deer': 39, 'bobcat': 40, 'pig': 41, 'lion': 42, 'mouse': 43, 'polar+bear': 44, 'collie': 45, 'walrus': 46, 'raccoon': 47, 'cow': 48, 'dolphin': 49}\n"
     ]
    }
   ],
   "source": [
    "# labels_in_number = pd.read_csv(DATA_FOLDER_PATH+\"classes.txt\", delim_whitespace=True,header=None)\n",
    "labels_dict = {}\n",
    "with open(DATA_FOLDER_PATH+\"classes.txt\") as f:\n",
    "    for line in f:\n",
    "        # print(line.split())\n",
    "        (key,val) = line.split()\n",
    "        labels_dict[val] = int(key)-1\n",
    "print(labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08979242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWA2Dataset(Dataset): # Dataset class to serve as input for the DataLoader.\n",
    "    \"\"\" \n",
    "    Dataset class to serve as input for the DataLoader.\n",
    "    Implements all the required methods and more. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotations_file=ANNOTATIONS_FILENAME, img_dir=JPEGIMAGES_FOLDER_PATH, \n",
    "                transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        numbers_infos_dicts: tuple[dict,dict] = find_num_images_per_label(img_dir=JPEGIMAGES_FOLDER_PATH)\n",
    "        self.num_images_per_label = numbers_infos_dicts[0]\n",
    "        self.proportions_images_per_label = numbers_infos_dicts[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # img_path = self.img_labels.iloc[idx, 0]\n",
    "        key = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        # Mapping the labels from string to tensor\n",
    "        label = labels_dict[key]\n",
    "\n",
    "        image = read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a923daaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AWA2Dataset()\n",
    "## TODO : Change transforms. Currently this is not useful.\n",
    "dataset.transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.Grayscale(num_output_channels=3),\n",
    "                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the reaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
    "                    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "train_size =  int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d291b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with DataLoader. Everything works good\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=4, shuffle=True)\n",
    "# dataiter = iter(dataloader)\n",
    "# data = next(dataiter)\n",
    "\n",
    "# images, labels = data \n",
    "# print(labels, images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "802f9705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1/7464], Loss: 3.9169\n",
      "Epoch [1/2], Step [2/7464], Loss: 3.8877\n",
      "Epoch [1/2], Step [3/7464], Loss: 3.9056\n",
      "Epoch [1/2], Step [4/7464], Loss: 3.9185\n",
      "Epoch [1/2], Step [5/7464], Loss: 3.8029\n",
      "Epoch [1/2], Step [6/7464], Loss: 3.9254\n",
      "Epoch [1/2], Step [7/7464], Loss: 3.8979\n",
      "Epoch [1/2], Step [8/7464], Loss: 3.9225\n",
      "Epoch [1/2], Step [9/7464], Loss: 3.8364\n",
      "Epoch [1/2], Step [10/7464], Loss: 3.8989\n",
      "Epoch [1/2], Step [11/7464], Loss: 3.8267\n",
      "Epoch [1/2], Step [12/7464], Loss: 3.7474\n",
      "Epoch [1/2], Step [13/7464], Loss: 4.1727\n",
      "Epoch [1/2], Step [14/7464], Loss: 3.8569\n",
      "Epoch [1/2], Step [15/7464], Loss: 3.8428\n",
      "Epoch [1/2], Step [16/7464], Loss: 3.4560\n",
      "Epoch [1/2], Step [17/7464], Loss: 4.0778\n",
      "Epoch [1/2], Step [18/7464], Loss: 3.6479\n",
      "Epoch [1/2], Step [19/7464], Loss: 3.9229\n",
      "Epoch [1/2], Step [20/7464], Loss: 4.2268\n",
      "Epoch [1/2], Step [21/7464], Loss: 3.6706\n",
      "Epoch [1/2], Step [22/7464], Loss: 4.0509\n",
      "Epoch [1/2], Step [23/7464], Loss: 3.6729\n",
      "Epoch [1/2], Step [24/7464], Loss: 3.9700\n",
      "Epoch [1/2], Step [25/7464], Loss: 3.7125\n",
      "Epoch [1/2], Step [26/7464], Loss: 3.5060\n",
      "Epoch [1/2], Step [27/7464], Loss: 3.6651\n",
      "Epoch [1/2], Step [28/7464], Loss: 4.4846\n",
      "Epoch [1/2], Step [29/7464], Loss: 3.5253\n",
      "Epoch [1/2], Step [30/7464], Loss: 4.0394\n",
      "Epoch [1/2], Step [31/7464], Loss: 4.0864\n",
      "Epoch [1/2], Step [32/7464], Loss: 3.7216\n",
      "Epoch [1/2], Step [33/7464], Loss: 3.7109\n",
      "Epoch [1/2], Step [34/7464], Loss: 3.8081\n",
      "Epoch [1/2], Step [35/7464], Loss: 4.2863\n",
      "Epoch [1/2], Step [36/7464], Loss: 3.8803\n",
      "Epoch [1/2], Step [37/7464], Loss: 4.2590\n",
      "Epoch [1/2], Step [38/7464], Loss: 3.8216\n",
      "Epoch [1/2], Step [39/7464], Loss: 3.8929\n",
      "Epoch [1/2], Step [40/7464], Loss: 3.7366\n",
      "Epoch [1/2], Step [41/7464], Loss: 3.5466\n",
      "Epoch [1/2], Step [42/7464], Loss: 3.9443\n",
      "Epoch [1/2], Step [43/7464], Loss: 3.6209\n",
      "Epoch [1/2], Step [44/7464], Loss: 4.2575\n",
      "Epoch [1/2], Step [45/7464], Loss: 3.8276\n",
      "Epoch [1/2], Step [46/7464], Loss: 3.9210\n",
      "Epoch [1/2], Step [47/7464], Loss: 4.0494\n",
      "Epoch [1/2], Step [48/7464], Loss: 3.9581\n",
      "Epoch [1/2], Step [49/7464], Loss: 3.8580\n",
      "Epoch [1/2], Step [50/7464], Loss: 3.9037\n",
      "Epoch [1/2], Step [51/7464], Loss: 3.7430\n",
      "Epoch [1/2], Step [52/7464], Loss: 3.9156\n",
      "Epoch [1/2], Step [53/7464], Loss: 3.8408\n",
      "Epoch [1/2], Step [54/7464], Loss: 3.5466\n",
      "Epoch [1/2], Step [55/7464], Loss: 3.8269\n",
      "Epoch [1/2], Step [56/7464], Loss: 3.6560\n",
      "Epoch [1/2], Step [57/7464], Loss: 4.0328\n",
      "Epoch [1/2], Step [58/7464], Loss: 3.9988\n",
      "Epoch [1/2], Step [59/7464], Loss: 4.0025\n",
      "Epoch [1/2], Step [60/7464], Loss: 4.0545\n",
      "Epoch [1/2], Step [61/7464], Loss: 3.7762\n",
      "Epoch [1/2], Step [62/7464], Loss: 3.9119\n",
      "Epoch [1/2], Step [63/7464], Loss: 3.9349\n",
      "Epoch [1/2], Step [64/7464], Loss: 3.7434\n",
      "Epoch [1/2], Step [65/7464], Loss: 3.8287\n",
      "Epoch [1/2], Step [66/7464], Loss: 3.8743\n",
      "Epoch [1/2], Step [67/7464], Loss: 3.6067\n",
      "Epoch [1/2], Step [68/7464], Loss: 4.0985\n",
      "Epoch [1/2], Step [69/7464], Loss: 3.9170\n",
      "Epoch [1/2], Step [70/7464], Loss: 4.0729\n",
      "Epoch [1/2], Step [71/7464], Loss: 3.7743\n",
      "Epoch [1/2], Step [72/7464], Loss: 3.8573\n",
      "Epoch [1/2], Step [73/7464], Loss: 4.0412\n",
      "Epoch [1/2], Step [74/7464], Loss: 4.0740\n",
      "Epoch [1/2], Step [75/7464], Loss: 3.7123\n",
      "Epoch [1/2], Step [76/7464], Loss: 3.9257\n",
      "Epoch [1/2], Step [77/7464], Loss: 3.8677\n",
      "Epoch [1/2], Step [78/7464], Loss: 3.8727\n",
      "Epoch [1/2], Step [79/7464], Loss: 3.9916\n",
      "Epoch [1/2], Step [80/7464], Loss: 4.0502\n",
      "Epoch [1/2], Step [81/7464], Loss: 3.8167\n",
      "Epoch [1/2], Step [82/7464], Loss: 4.0102\n",
      "Epoch [1/2], Step [83/7464], Loss: 3.7649\n",
      "Epoch [1/2], Step [84/7464], Loss: 3.7818\n",
      "Epoch [1/2], Step [85/7464], Loss: 3.8690\n",
      "Epoch [1/2], Step [86/7464], Loss: 3.7682\n",
      "Epoch [1/2], Step [87/7464], Loss: 3.8838\n",
      "Epoch [1/2], Step [88/7464], Loss: 3.8356\n",
      "Epoch [1/2], Step [89/7464], Loss: 3.7967\n",
      "Epoch [1/2], Step [90/7464], Loss: 3.8034\n",
      "Epoch [1/2], Step [91/7464], Loss: 3.8375\n",
      "Epoch [1/2], Step [92/7464], Loss: 3.8649\n",
      "Epoch [1/2], Step [93/7464], Loss: 4.0185\n",
      "Epoch [1/2], Step [94/7464], Loss: 3.8399\n",
      "Epoch [1/2], Step [95/7464], Loss: 3.9319\n",
      "Epoch [1/2], Step [96/7464], Loss: 3.5027\n",
      "Epoch [1/2], Step [97/7464], Loss: 3.6789\n",
      "Epoch [1/2], Step [98/7464], Loss: 3.8335\n",
      "Epoch [1/2], Step [99/7464], Loss: 3.7655\n",
      "Epoch [1/2], Step [100/7464], Loss: 3.6124\n",
      "Epoch [1/2], Step [101/7464], Loss: 3.5776\n",
      "Epoch [1/2], Step [102/7464], Loss: 4.3543\n",
      "Epoch [1/2], Step [103/7464], Loss: 3.5171\n",
      "Epoch [1/2], Step [104/7464], Loss: 4.0475\n",
      "Epoch [1/2], Step [105/7464], Loss: 3.4590\n",
      "Epoch [1/2], Step [106/7464], Loss: 3.9450\n",
      "Epoch [1/2], Step [107/7464], Loss: 3.7913\n",
      "Epoch [1/2], Step [108/7464], Loss: 3.7494\n",
      "Epoch [1/2], Step [109/7464], Loss: 4.1790\n",
      "Epoch [1/2], Step [110/7464], Loss: 3.8756\n",
      "Epoch [1/2], Step [111/7464], Loss: 3.5881\n",
      "Epoch [1/2], Step [112/7464], Loss: 3.2167\n",
      "Epoch [1/2], Step [113/7464], Loss: 4.0300\n",
      "Epoch [1/2], Step [114/7464], Loss: 3.5635\n",
      "Epoch [1/2], Step [115/7464], Loss: 4.0371\n",
      "Epoch [1/2], Step [116/7464], Loss: 3.9582\n",
      "Epoch [1/2], Step [117/7464], Loss: 4.0107\n",
      "Epoch [1/2], Step [118/7464], Loss: 3.5834\n",
      "Epoch [1/2], Step [119/7464], Loss: 3.6931\n",
      "Epoch [1/2], Step [120/7464], Loss: 3.7922\n",
      "Epoch [1/2], Step [121/7464], Loss: 4.4382\n",
      "Epoch [1/2], Step [122/7464], Loss: 3.7062\n",
      "Epoch [1/2], Step [123/7464], Loss: 3.6168\n",
      "Epoch [1/2], Step [124/7464], Loss: 4.1683\n",
      "Epoch [1/2], Step [125/7464], Loss: 3.6906\n",
      "Epoch [1/2], Step [126/7464], Loss: 3.9235\n",
      "Epoch [1/2], Step [127/7464], Loss: 3.7632\n",
      "Epoch [1/2], Step [128/7464], Loss: 3.8771\n",
      "Epoch [1/2], Step [129/7464], Loss: 3.8043\n",
      "Epoch [1/2], Step [130/7464], Loss: 3.9821\n",
      "Epoch [1/2], Step [131/7464], Loss: 3.7945\n",
      "Epoch [1/2], Step [132/7464], Loss: 3.8792\n",
      "Epoch [1/2], Step [133/7464], Loss: 4.0441\n",
      "Epoch [1/2], Step [134/7464], Loss: 3.5005\n",
      "Epoch [1/2], Step [135/7464], Loss: 3.9199\n",
      "Epoch [1/2], Step [136/7464], Loss: 3.7392\n",
      "Epoch [1/2], Step [137/7464], Loss: 3.9214\n",
      "Epoch [1/2], Step [138/7464], Loss: 3.7230\n",
      "Epoch [1/2], Step [139/7464], Loss: 3.8748\n",
      "Epoch [1/2], Step [140/7464], Loss: 3.7943\n",
      "Epoch [1/2], Step [141/7464], Loss: 3.6612\n",
      "Epoch [1/2], Step [142/7464], Loss: 3.3940\n",
      "Epoch [1/2], Step [143/7464], Loss: 4.1394\n",
      "Epoch [1/2], Step [144/7464], Loss: 4.0579\n",
      "Epoch [1/2], Step [145/7464], Loss: 3.8920\n",
      "Epoch [1/2], Step [146/7464], Loss: 3.4998\n",
      "Epoch [1/2], Step [147/7464], Loss: 4.1913\n",
      "Epoch [1/2], Step [148/7464], Loss: 3.5272\n",
      "Epoch [1/2], Step [149/7464], Loss: 3.8437\n",
      "Epoch [1/2], Step [150/7464], Loss: 4.0746\n",
      "Epoch [1/2], Step [151/7464], Loss: 3.4851\n",
      "Epoch [1/2], Step [152/7464], Loss: 4.0522\n",
      "Epoch [1/2], Step [153/7464], Loss: 4.2082\n",
      "Epoch [1/2], Step [154/7464], Loss: 3.6784\n",
      "Epoch [1/2], Step [155/7464], Loss: 3.6338\n",
      "Epoch [1/2], Step [156/7464], Loss: 3.3753\n",
      "Epoch [1/2], Step [157/7464], Loss: 3.8513\n",
      "Epoch [1/2], Step [158/7464], Loss: 4.0871\n",
      "Epoch [1/2], Step [159/7464], Loss: 3.6515\n",
      "Epoch [1/2], Step [160/7464], Loss: 4.3927\n",
      "Epoch [1/2], Step [161/7464], Loss: 4.3057\n",
      "Epoch [1/2], Step [162/7464], Loss: 4.0839\n",
      "Epoch [1/2], Step [163/7464], Loss: 3.7297\n",
      "Epoch [1/2], Step [164/7464], Loss: 3.6799\n",
      "Epoch [1/2], Step [165/7464], Loss: 4.0605\n",
      "Epoch [1/2], Step [166/7464], Loss: 4.0110\n",
      "Epoch [1/2], Step [167/7464], Loss: 4.0065\n",
      "Epoch [1/2], Step [168/7464], Loss: 3.7121\n",
      "Epoch [1/2], Step [169/7464], Loss: 3.7360\n",
      "Epoch [1/2], Step [170/7464], Loss: 3.8776\n",
      "Epoch [1/2], Step [171/7464], Loss: 3.9921\n",
      "Epoch [1/2], Step [172/7464], Loss: 3.9404\n",
      "Epoch [1/2], Step [173/7464], Loss: 3.9618\n",
      "Epoch [1/2], Step [174/7464], Loss: 3.8417\n",
      "Epoch [1/2], Step [175/7464], Loss: 3.8222\n",
      "Epoch [1/2], Step [176/7464], Loss: 3.9048\n",
      "Epoch [1/2], Step [177/7464], Loss: 3.8832\n",
      "Epoch [1/2], Step [178/7464], Loss: 3.8197\n",
      "Epoch [1/2], Step [179/7464], Loss: 4.0305\n",
      "Epoch [1/2], Step [180/7464], Loss: 3.8840\n",
      "Epoch [1/2], Step [181/7464], Loss: 3.9117\n",
      "Epoch [1/2], Step [182/7464], Loss: 3.6773\n",
      "Epoch [1/2], Step [183/7464], Loss: 3.8439\n",
      "Epoch [1/2], Step [184/7464], Loss: 3.9269\n",
      "Epoch [1/2], Step [185/7464], Loss: 3.8751\n",
      "Epoch [1/2], Step [186/7464], Loss: 3.9533\n",
      "Epoch [1/2], Step [187/7464], Loss: 3.5340\n",
      "Epoch [1/2], Step [188/7464], Loss: 3.8910\n",
      "Epoch [1/2], Step [189/7464], Loss: 4.0775\n",
      "Epoch [1/2], Step [190/7464], Loss: 3.9562\n",
      "Epoch [1/2], Step [191/7464], Loss: 3.7912\n",
      "Epoch [1/2], Step [192/7464], Loss: 3.6960\n",
      "Epoch [1/2], Step [193/7464], Loss: 3.9293\n",
      "Epoch [1/2], Step [194/7464], Loss: 3.9058\n",
      "Epoch [1/2], Step [195/7464], Loss: 3.5562\n",
      "Epoch [1/2], Step [196/7464], Loss: 3.8407\n",
      "Epoch [1/2], Step [197/7464], Loss: 4.0214\n",
      "Epoch [1/2], Step [198/7464], Loss: 3.6504\n",
      "Epoch [1/2], Step [199/7464], Loss: 3.9084\n",
      "Epoch [1/2], Step [200/7464], Loss: 3.9894\n",
      "Epoch [1/2], Step [201/7464], Loss: 4.0462\n",
      "Epoch [1/2], Step [202/7464], Loss: 4.0370\n",
      "Epoch [1/2], Step [203/7464], Loss: 3.6414\n",
      "Epoch [1/2], Step [204/7464], Loss: 3.8576\n",
      "Epoch [1/2], Step [205/7464], Loss: 4.0057\n",
      "Epoch [1/2], Step [206/7464], Loss: 3.8801\n",
      "Epoch [1/2], Step [207/7464], Loss: 3.6985\n",
      "Epoch [1/2], Step [208/7464], Loss: 3.5316\n",
      "Epoch [1/2], Step [209/7464], Loss: 3.9685\n",
      "Epoch [1/2], Step [210/7464], Loss: 3.7085\n",
      "Epoch [1/2], Step [211/7464], Loss: 3.6515\n",
      "Epoch [1/2], Step [212/7464], Loss: 3.9968\n",
      "Epoch [1/2], Step [213/7464], Loss: 3.8218\n",
      "Epoch [1/2], Step [214/7464], Loss: 3.7250\n",
      "Epoch [1/2], Step [215/7464], Loss: 4.1598\n",
      "Epoch [1/2], Step [216/7464], Loss: 3.8765\n",
      "Epoch [1/2], Step [217/7464], Loss: 4.0076\n",
      "Epoch [1/2], Step [218/7464], Loss: 3.6049\n",
      "Epoch [1/2], Step [219/7464], Loss: 3.7318\n",
      "Epoch [1/2], Step [220/7464], Loss: 3.4791\n",
      "Epoch [1/2], Step [221/7464], Loss: 3.6703\n",
      "Epoch [1/2], Step [222/7464], Loss: 3.7906\n",
      "Epoch [1/2], Step [223/7464], Loss: 3.7870\n",
      "Epoch [1/2], Step [224/7464], Loss: 3.8303\n",
      "Epoch [1/2], Step [225/7464], Loss: 3.9597\n",
      "Epoch [1/2], Step [226/7464], Loss: 3.8136\n",
      "Epoch [1/2], Step [227/7464], Loss: 3.9898\n",
      "Epoch [1/2], Step [228/7464], Loss: 3.7458\n",
      "Epoch [1/2], Step [229/7464], Loss: 4.3717\n",
      "Epoch [1/2], Step [230/7464], Loss: 3.6565\n",
      "Epoch [1/2], Step [231/7464], Loss: 3.5638\n",
      "Epoch [1/2], Step [232/7464], Loss: 3.7882\n",
      "Epoch [1/2], Step [233/7464], Loss: 3.7333\n",
      "Epoch [1/2], Step [234/7464], Loss: 3.6786\n",
      "Epoch [1/2], Step [235/7464], Loss: 3.6087\n",
      "Epoch [1/2], Step [236/7464], Loss: 3.4556\n",
      "Epoch [1/2], Step [237/7464], Loss: 3.9649\n",
      "Epoch [1/2], Step [238/7464], Loss: 3.6050\n",
      "Epoch [1/2], Step [239/7464], Loss: 3.9964\n",
      "Epoch [1/2], Step [240/7464], Loss: 3.8764\n",
      "Epoch [1/2], Step [241/7464], Loss: 3.7554\n",
      "Epoch [1/2], Step [242/7464], Loss: 3.5855\n",
      "Epoch [1/2], Step [243/7464], Loss: 3.6740\n",
      "Epoch [1/2], Step [244/7464], Loss: 4.2180\n",
      "Epoch [1/2], Step [245/7464], Loss: 3.9260\n",
      "Epoch [1/2], Step [246/7464], Loss: 4.1243\n",
      "Epoch [1/2], Step [247/7464], Loss: 4.4050\n",
      "Epoch [1/2], Step [248/7464], Loss: 3.7885\n",
      "Epoch [1/2], Step [249/7464], Loss: 3.9424\n",
      "Epoch [1/2], Step [250/7464], Loss: 3.7037\n",
      "Epoch [1/2], Step [251/7464], Loss: 3.6538\n",
      "Epoch [1/2], Step [252/7464], Loss: 3.8169\n",
      "Epoch [1/2], Step [253/7464], Loss: 3.5677\n",
      "Epoch [1/2], Step [254/7464], Loss: 3.6045\n",
      "Epoch [1/2], Step [255/7464], Loss: 3.8012\n",
      "Epoch [1/2], Step [256/7464], Loss: 3.7079\n",
      "Epoch [1/2], Step [257/7464], Loss: 4.1163\n",
      "Epoch [1/2], Step [258/7464], Loss: 3.6171\n",
      "Epoch [1/2], Step [259/7464], Loss: 3.9838\n",
      "Epoch [1/2], Step [260/7464], Loss: 3.9695\n",
      "Epoch [1/2], Step [261/7464], Loss: 3.5502\n",
      "Epoch [1/2], Step [262/7464], Loss: 3.9316\n",
      "Epoch [1/2], Step [263/7464], Loss: 4.2770\n",
      "Epoch [1/2], Step [264/7464], Loss: 3.7694\n",
      "Epoch [1/2], Step [265/7464], Loss: 3.8651\n",
      "Epoch [1/2], Step [266/7464], Loss: 3.8471\n",
      "Epoch [1/2], Step [267/7464], Loss: 3.9465\n",
      "Epoch [1/2], Step [268/7464], Loss: 3.7454\n",
      "Epoch [1/2], Step [269/7464], Loss: 4.0743\n",
      "Epoch [1/2], Step [270/7464], Loss: 3.7164\n",
      "Epoch [1/2], Step [271/7464], Loss: 3.7923\n",
      "Epoch [1/2], Step [272/7464], Loss: 3.9655\n",
      "Epoch [1/2], Step [273/7464], Loss: 3.9483\n",
      "Epoch [1/2], Step [274/7464], Loss: 3.9661\n",
      "Epoch [1/2], Step [275/7464], Loss: 3.9094\n",
      "Epoch [1/2], Step [276/7464], Loss: 3.9060\n",
      "Epoch [1/2], Step [277/7464], Loss: 4.0870\n",
      "Epoch [1/2], Step [278/7464], Loss: 4.1061\n",
      "Epoch [1/2], Step [279/7464], Loss: 3.9369\n",
      "Epoch [1/2], Step [280/7464], Loss: 3.8444\n",
      "Epoch [1/2], Step [281/7464], Loss: 3.7592\n",
      "Epoch [1/2], Step [282/7464], Loss: 3.8211\n",
      "Epoch [1/2], Step [283/7464], Loss: 3.8487\n",
      "Epoch [1/2], Step [284/7464], Loss: 3.8438\n",
      "Epoch [1/2], Step [285/7464], Loss: 3.8392\n",
      "Epoch [1/2], Step [286/7464], Loss: 3.8780\n",
      "Epoch [1/2], Step [287/7464], Loss: 3.7868\n",
      "Epoch [1/2], Step [288/7464], Loss: 3.9161\n",
      "Epoch [1/2], Step [289/7464], Loss: 3.8601\n",
      "Epoch [1/2], Step [290/7464], Loss: 3.8892\n",
      "Epoch [1/2], Step [291/7464], Loss: 3.8829\n",
      "Epoch [1/2], Step [292/7464], Loss: 3.8889\n",
      "Epoch [1/2], Step [293/7464], Loss: 3.7104\n",
      "Epoch [1/2], Step [294/7464], Loss: 3.8126\n",
      "Epoch [1/2], Step [295/7464], Loss: 3.8591\n",
      "Epoch [1/2], Step [296/7464], Loss: 3.9737\n",
      "Epoch [1/2], Step [297/7464], Loss: 3.9283\n",
      "Epoch [1/2], Step [298/7464], Loss: 3.7352\n",
      "Epoch [1/2], Step [299/7464], Loss: 3.7471\n",
      "Epoch [1/2], Step [300/7464], Loss: 4.0017\n",
      "Epoch [1/2], Step [301/7464], Loss: 3.9488\n",
      "Epoch [1/2], Step [302/7464], Loss: 3.7084\n",
      "Epoch [1/2], Step [303/7464], Loss: 3.8346\n",
      "Epoch [1/2], Step [304/7464], Loss: 3.9305\n",
      "Epoch [1/2], Step [305/7464], Loss: 3.7154\n",
      "Epoch [1/2], Step [306/7464], Loss: 3.7365\n",
      "Epoch [1/2], Step [307/7464], Loss: 3.7781\n",
      "Epoch [1/2], Step [308/7464], Loss: 3.8513\n",
      "Epoch [1/2], Step [309/7464], Loss: 3.7343\n",
      "Epoch [1/2], Step [310/7464], Loss: 4.3799\n",
      "Epoch [1/2], Step [311/7464], Loss: 4.1534\n",
      "Epoch [1/2], Step [312/7464], Loss: 3.7955\n",
      "Epoch [1/2], Step [313/7464], Loss: 3.6455\n",
      "Epoch [1/2], Step [314/7464], Loss: 3.8456\n",
      "Epoch [1/2], Step [315/7464], Loss: 3.9268\n",
      "Epoch [1/2], Step [316/7464], Loss: 3.5357\n",
      "Epoch [1/2], Step [317/7464], Loss: 3.8869\n",
      "Epoch [1/2], Step [318/7464], Loss: 3.4381\n",
      "Epoch [1/2], Step [319/7464], Loss: 3.6243\n",
      "Epoch [1/2], Step [320/7464], Loss: 3.7816\n",
      "Epoch [1/2], Step [321/7464], Loss: 3.9131\n",
      "Epoch [1/2], Step [322/7464], Loss: 3.9170\n",
      "Epoch [1/2], Step [323/7464], Loss: 3.8806\n",
      "Epoch [1/2], Step [324/7464], Loss: 3.8403\n",
      "Epoch [1/2], Step [325/7464], Loss: 3.8881\n",
      "Epoch [1/2], Step [326/7464], Loss: 3.8860\n",
      "Epoch [1/2], Step [327/7464], Loss: 3.9893\n",
      "Epoch [1/2], Step [328/7464], Loss: 3.8680\n",
      "Epoch [1/2], Step [329/7464], Loss: 3.7363\n",
      "Epoch [1/2], Step [330/7464], Loss: 3.8087\n",
      "Epoch [1/2], Step [331/7464], Loss: 3.7516\n",
      "Epoch [1/2], Step [332/7464], Loss: 4.3117\n",
      "Epoch [1/2], Step [333/7464], Loss: 4.0917\n",
      "Epoch [1/2], Step [334/7464], Loss: 3.9872\n",
      "Epoch [1/2], Step [335/7464], Loss: 3.5151\n",
      "Epoch [1/2], Step [336/7464], Loss: 3.6417\n",
      "Epoch [1/2], Step [337/7464], Loss: 3.6461\n",
      "Epoch [1/2], Step [338/7464], Loss: 3.9916\n",
      "Epoch [1/2], Step [339/7464], Loss: 3.6279\n",
      "Epoch [1/2], Step [340/7464], Loss: 3.9032\n",
      "Epoch [1/2], Step [341/7464], Loss: 3.8684\n",
      "Epoch [1/2], Step [342/7464], Loss: 3.6016\n",
      "Epoch [1/2], Step [343/7464], Loss: 3.8198\n",
      "Epoch [1/2], Step [344/7464], Loss: 4.0857\n",
      "Epoch [1/2], Step [345/7464], Loss: 3.6393\n",
      "Epoch [1/2], Step [346/7464], Loss: 3.7393\n",
      "Epoch [1/2], Step [347/7464], Loss: 3.9012\n",
      "Epoch [1/2], Step [348/7464], Loss: 3.9438\n",
      "Epoch [1/2], Step [349/7464], Loss: 3.8064\n",
      "Epoch [1/2], Step [350/7464], Loss: 3.8932\n",
      "Epoch [1/2], Step [351/7464], Loss: 3.7782\n",
      "Epoch [1/2], Step [352/7464], Loss: 3.9215\n",
      "Epoch [1/2], Step [353/7464], Loss: 3.8128\n",
      "Epoch [1/2], Step [354/7464], Loss: 3.7114\n",
      "Epoch [1/2], Step [355/7464], Loss: 3.6793\n",
      "Epoch [1/2], Step [356/7464], Loss: 3.9261\n",
      "Epoch [1/2], Step [357/7464], Loss: 3.9245\n",
      "Epoch [1/2], Step [358/7464], Loss: 3.6151\n",
      "Epoch [1/2], Step [359/7464], Loss: 3.7758\n",
      "Epoch [1/2], Step [360/7464], Loss: 3.6542\n",
      "Epoch [1/2], Step [361/7464], Loss: 3.6071\n",
      "Epoch [1/2], Step [362/7464], Loss: 3.9639\n",
      "Epoch [1/2], Step [363/7464], Loss: 4.0601\n",
      "Epoch [1/2], Step [364/7464], Loss: 3.9908\n",
      "Epoch [1/2], Step [365/7464], Loss: 3.8419\n",
      "Epoch [1/2], Step [366/7464], Loss: 3.7551\n",
      "Epoch [1/2], Step [367/7464], Loss: 3.8547\n",
      "Epoch [1/2], Step [368/7464], Loss: 4.1102\n",
      "Epoch [1/2], Step [369/7464], Loss: 3.6431\n",
      "Epoch [1/2], Step [370/7464], Loss: 3.7282\n",
      "Epoch [1/2], Step [371/7464], Loss: 3.5956\n",
      "Epoch [1/2], Step [372/7464], Loss: 3.7725\n",
      "Epoch [1/2], Step [373/7464], Loss: 3.6357\n",
      "Epoch [1/2], Step [374/7464], Loss: 4.1433\n",
      "Epoch [1/2], Step [375/7464], Loss: 3.6235\n",
      "Epoch [1/2], Step [376/7464], Loss: 3.5562\n",
      "Epoch [1/2], Step [377/7464], Loss: 3.9118\n",
      "Epoch [1/2], Step [378/7464], Loss: 3.7623\n",
      "Epoch [1/2], Step [379/7464], Loss: 3.6070\n",
      "Epoch [1/2], Step [380/7464], Loss: 4.1116\n",
      "Epoch [1/2], Step [381/7464], Loss: 4.0030\n",
      "Epoch [1/2], Step [382/7464], Loss: 3.8853\n",
      "Epoch [1/2], Step [383/7464], Loss: 4.0435\n",
      "Epoch [1/2], Step [384/7464], Loss: 3.8271\n",
      "Epoch [1/2], Step [385/7464], Loss: 3.6658\n",
      "Epoch [1/2], Step [386/7464], Loss: 3.7638\n",
      "Epoch [1/2], Step [387/7464], Loss: 3.9668\n",
      "Epoch [1/2], Step [388/7464], Loss: 3.7832\n",
      "Epoch [1/2], Step [389/7464], Loss: 3.6194\n",
      "Epoch [1/2], Step [390/7464], Loss: 3.8361\n",
      "Epoch [1/2], Step [391/7464], Loss: 3.9008\n",
      "Epoch [1/2], Step [392/7464], Loss: 3.4229\n",
      "Epoch [1/2], Step [393/7464], Loss: 3.6904\n",
      "Epoch [1/2], Step [394/7464], Loss: 3.9070\n",
      "Epoch [1/2], Step [395/7464], Loss: 4.0525\n",
      "Epoch [1/2], Step [396/7464], Loss: 3.9991\n",
      "Epoch [1/2], Step [397/7464], Loss: 3.7782\n",
      "Epoch [1/2], Step [398/7464], Loss: 3.5789\n",
      "Epoch [1/2], Step [399/7464], Loss: 3.7756\n",
      "Epoch [1/2], Step [400/7464], Loss: 3.6105\n",
      "Epoch [1/2], Step [401/7464], Loss: 3.7953\n",
      "Epoch [1/2], Step [402/7464], Loss: 3.7081\n",
      "Epoch [1/2], Step [403/7464], Loss: 3.8185\n",
      "Epoch [1/2], Step [404/7464], Loss: 3.6393\n",
      "Epoch [1/2], Step [405/7464], Loss: 3.6842\n",
      "Epoch [1/2], Step [406/7464], Loss: 4.1440\n",
      "Epoch [1/2], Step [407/7464], Loss: 3.7622\n",
      "Epoch [1/2], Step [408/7464], Loss: 3.5902\n",
      "Epoch [1/2], Step [409/7464], Loss: 3.8991\n",
      "Epoch [1/2], Step [410/7464], Loss: 3.9192\n",
      "Epoch [1/2], Step [411/7464], Loss: 3.5525\n",
      "Epoch [1/2], Step [412/7464], Loss: 3.3985\n",
      "Epoch [1/2], Step [413/7464], Loss: 3.2912\n",
      "Epoch [1/2], Step [414/7464], Loss: 4.0216\n",
      "Epoch [1/2], Step [415/7464], Loss: 4.2733\n",
      "Epoch [1/2], Step [416/7464], Loss: 3.8915\n",
      "Epoch [1/2], Step [417/7464], Loss: 4.3175\n",
      "Epoch [1/2], Step [418/7464], Loss: 3.5603\n",
      "Epoch [1/2], Step [419/7464], Loss: 4.0199\n",
      "Epoch [1/2], Step [420/7464], Loss: 3.7927\n",
      "Epoch [1/2], Step [421/7464], Loss: 3.6395\n",
      "Epoch [1/2], Step [422/7464], Loss: 3.5166\n",
      "Epoch [1/2], Step [423/7464], Loss: 4.2040\n",
      "Epoch [1/2], Step [424/7464], Loss: 3.8923\n",
      "Epoch [1/2], Step [425/7464], Loss: 3.6056\n",
      "Epoch [1/2], Step [426/7464], Loss: 3.5262\n",
      "Epoch [1/2], Step [427/7464], Loss: 3.9250\n",
      "Epoch [1/2], Step [428/7464], Loss: 3.7316\n",
      "Epoch [1/2], Step [429/7464], Loss: 3.5709\n",
      "Epoch [1/2], Step [430/7464], Loss: 3.6640\n",
      "Epoch [1/2], Step [431/7464], Loss: 3.7282\n",
      "Epoch [1/2], Step [432/7464], Loss: 4.0905\n",
      "Epoch [1/2], Step [433/7464], Loss: 3.8161\n",
      "Epoch [1/2], Step [434/7464], Loss: 3.4877\n",
      "Epoch [1/2], Step [435/7464], Loss: 3.9829\n",
      "Epoch [1/2], Step [436/7464], Loss: 3.6573\n",
      "Epoch [1/2], Step [437/7464], Loss: 3.7197\n",
      "Epoch [1/2], Step [438/7464], Loss: 3.9159\n",
      "Epoch [1/2], Step [439/7464], Loss: 3.4973\n",
      "Epoch [1/2], Step [440/7464], Loss: 3.7776\n",
      "Epoch [1/2], Step [441/7464], Loss: 3.9315\n",
      "Epoch [1/2], Step [442/7464], Loss: 3.8668\n",
      "Epoch [1/2], Step [443/7464], Loss: 3.5721\n",
      "Epoch [1/2], Step [444/7464], Loss: 4.1135\n",
      "Epoch [1/2], Step [445/7464], Loss: 3.9130\n",
      "Epoch [1/2], Step [446/7464], Loss: 3.6603\n",
      "Epoch [1/2], Step [447/7464], Loss: 3.9690\n",
      "Epoch [1/2], Step [448/7464], Loss: 3.7949\n",
      "Epoch [1/2], Step [449/7464], Loss: 3.9412\n",
      "Epoch [1/2], Step [450/7464], Loss: 3.6502\n",
      "Epoch [1/2], Step [451/7464], Loss: 3.9942\n",
      "Epoch [1/2], Step [452/7464], Loss: 3.4522\n",
      "Epoch [1/2], Step [453/7464], Loss: 4.0164\n",
      "Epoch [1/2], Step [454/7464], Loss: 3.7621\n",
      "Epoch [1/2], Step [455/7464], Loss: 3.6092\n",
      "Epoch [1/2], Step [456/7464], Loss: 4.0330\n",
      "Epoch [1/2], Step [457/7464], Loss: 3.8959\n",
      "Epoch [1/2], Step [458/7464], Loss: 3.8505\n",
      "Epoch [1/2], Step [459/7464], Loss: 4.1641\n",
      "Epoch [1/2], Step [460/7464], Loss: 3.7464\n",
      "Epoch [1/2], Step [461/7464], Loss: 3.8488\n",
      "Epoch [1/2], Step [462/7464], Loss: 3.7321\n",
      "Epoch [1/2], Step [463/7464], Loss: 3.9905\n",
      "Epoch [1/2], Step [464/7464], Loss: 3.6393\n",
      "Epoch [1/2], Step [465/7464], Loss: 3.7575\n",
      "Epoch [1/2], Step [466/7464], Loss: 3.8130\n",
      "Epoch [1/2], Step [467/7464], Loss: 3.7773\n",
      "Epoch [1/2], Step [468/7464], Loss: 4.2133\n",
      "Epoch [1/2], Step [469/7464], Loss: 3.8715\n",
      "Epoch [1/2], Step [470/7464], Loss: 3.7163\n",
      "Epoch [1/2], Step [471/7464], Loss: 3.6263\n",
      "Epoch [1/2], Step [472/7464], Loss: 3.6797\n",
      "Epoch [1/2], Step [473/7464], Loss: 3.7819\n",
      "Epoch [1/2], Step [474/7464], Loss: 3.7574\n",
      "Epoch [1/2], Step [475/7464], Loss: 3.6124\n",
      "Epoch [1/2], Step [476/7464], Loss: 3.7645\n",
      "Epoch [1/2], Step [477/7464], Loss: 3.8785\n",
      "Epoch [1/2], Step [478/7464], Loss: 3.9029\n",
      "Epoch [1/2], Step [479/7464], Loss: 3.9209\n",
      "Epoch [1/2], Step [480/7464], Loss: 3.7351\n",
      "Epoch [1/2], Step [481/7464], Loss: 3.6611\n",
      "Epoch [1/2], Step [482/7464], Loss: 3.4943\n",
      "Epoch [1/2], Step [483/7464], Loss: 3.2386\n",
      "Epoch [1/2], Step [484/7464], Loss: 3.7601\n",
      "Epoch [1/2], Step [485/7464], Loss: 3.9142\n",
      "Epoch [1/2], Step [486/7464], Loss: 3.5244\n",
      "Epoch [1/2], Step [487/7464], Loss: 3.6357\n",
      "Epoch [1/2], Step [488/7464], Loss: 3.8797\n",
      "Epoch [1/2], Step [489/7464], Loss: 3.7871\n",
      "Epoch [1/2], Step [490/7464], Loss: 3.7510\n",
      "Epoch [1/2], Step [491/7464], Loss: 3.3914\n",
      "Epoch [1/2], Step [492/7464], Loss: 3.3116\n",
      "Epoch [1/2], Step [493/7464], Loss: 3.5436\n",
      "Epoch [1/2], Step [494/7464], Loss: 4.7888\n",
      "Epoch [1/2], Step [495/7464], Loss: 3.3871\n",
      "Epoch [1/2], Step [496/7464], Loss: 3.6975\n",
      "Epoch [1/2], Step [497/7464], Loss: 3.4831\n",
      "Epoch [1/2], Step [498/7464], Loss: 3.7372\n",
      "Epoch [1/2], Step [499/7464], Loss: 4.2295\n",
      "Epoch [1/2], Step [500/7464], Loss: 3.9554\n",
      "Epoch [1/2], Step [501/7464], Loss: 3.8639\n",
      "Epoch [1/2], Step [502/7464], Loss: 4.0767\n",
      "Epoch [1/2], Step [503/7464], Loss: 3.6163\n",
      "Epoch [1/2], Step [504/7464], Loss: 3.4634\n",
      "Epoch [1/2], Step [505/7464], Loss: 3.7329\n",
      "Epoch [1/2], Step [506/7464], Loss: 4.2620\n",
      "Epoch [1/2], Step [507/7464], Loss: 3.7986\n",
      "Epoch [1/2], Step [508/7464], Loss: 3.8624\n",
      "Epoch [1/2], Step [509/7464], Loss: 3.7861\n",
      "Epoch [1/2], Step [510/7464], Loss: 3.9196\n",
      "Epoch [1/2], Step [511/7464], Loss: 3.8092\n",
      "Epoch [1/2], Step [512/7464], Loss: 3.8445\n",
      "Epoch [1/2], Step [513/7464], Loss: 4.1456\n",
      "Epoch [1/2], Step [514/7464], Loss: 3.5962\n",
      "Epoch [1/2], Step [515/7464], Loss: 3.5932\n",
      "Epoch [1/2], Step [516/7464], Loss: 3.9874\n",
      "Epoch [1/2], Step [517/7464], Loss: 4.0830\n",
      "Epoch [1/2], Step [518/7464], Loss: 3.8792\n",
      "Epoch [1/2], Step [519/7464], Loss: 3.7791\n",
      "Epoch [1/2], Step [520/7464], Loss: 3.7389\n",
      "Epoch [1/2], Step [521/7464], Loss: 3.6393\n",
      "Epoch [1/2], Step [522/7464], Loss: 3.7402\n",
      "Epoch [1/2], Step [523/7464], Loss: 3.6337\n",
      "Epoch [1/2], Step [524/7464], Loss: 3.8778\n",
      "Epoch [1/2], Step [525/7464], Loss: 3.6890\n",
      "Epoch [1/2], Step [526/7464], Loss: 3.7675\n",
      "Epoch [1/2], Step [527/7464], Loss: 4.1892\n",
      "Epoch [1/2], Step [528/7464], Loss: 3.5345\n",
      "Epoch [1/2], Step [529/7464], Loss: 3.6965\n",
      "Epoch [1/2], Step [530/7464], Loss: 3.6903\n",
      "Epoch [1/2], Step [531/7464], Loss: 3.8479\n",
      "Epoch [1/2], Step [532/7464], Loss: 3.6447\n",
      "Epoch [1/2], Step [533/7464], Loss: 3.4864\n",
      "Epoch [1/2], Step [534/7464], Loss: 3.5390\n",
      "Epoch [1/2], Step [535/7464], Loss: 3.8258\n",
      "Epoch [1/2], Step [536/7464], Loss: 3.6177\n",
      "Epoch [1/2], Step [537/7464], Loss: 3.5026\n",
      "Epoch [1/2], Step [538/7464], Loss: 4.0497\n",
      "Epoch [1/2], Step [539/7464], Loss: 3.8716\n",
      "Epoch [1/2], Step [540/7464], Loss: 3.9483\n",
      "Epoch [1/2], Step [541/7464], Loss: 3.7553\n",
      "Epoch [1/2], Step [542/7464], Loss: 3.3230\n",
      "Epoch [1/2], Step [543/7464], Loss: 3.5770\n",
      "Epoch [1/2], Step [544/7464], Loss: 3.6973\n",
      "Epoch [1/2], Step [545/7464], Loss: 3.6686\n",
      "Epoch [1/2], Step [546/7464], Loss: 3.6201\n",
      "Epoch [1/2], Step [547/7464], Loss: 3.5983\n",
      "Epoch [1/2], Step [548/7464], Loss: 3.7165\n",
      "Epoch [1/2], Step [549/7464], Loss: 3.9654\n",
      "Epoch [1/2], Step [550/7464], Loss: 3.8497\n",
      "Epoch [1/2], Step [551/7464], Loss: 5.1516\n",
      "Epoch [1/2], Step [552/7464], Loss: 3.3293\n",
      "Epoch [1/2], Step [553/7464], Loss: 3.6554\n",
      "Epoch [1/2], Step [554/7464], Loss: 4.1955\n",
      "Epoch [1/2], Step [555/7464], Loss: 3.8648\n",
      "Epoch [1/2], Step [556/7464], Loss: 3.8432\n",
      "Epoch [1/2], Step [557/7464], Loss: 3.9659\n",
      "Epoch [1/2], Step [558/7464], Loss: 3.6089\n",
      "Epoch [1/2], Step [559/7464], Loss: 3.5382\n",
      "Epoch [1/2], Step [560/7464], Loss: 3.6932\n",
      "Epoch [1/2], Step [561/7464], Loss: 3.9556\n",
      "Epoch [1/2], Step [562/7464], Loss: 3.5100\n",
      "Epoch [1/2], Step [563/7464], Loss: 4.0550\n",
      "Epoch [1/2], Step [564/7464], Loss: 3.9001\n",
      "Epoch [1/2], Step [565/7464], Loss: 3.8939\n",
      "Epoch [1/2], Step [566/7464], Loss: 3.8399\n",
      "Epoch [1/2], Step [567/7464], Loss: 3.8294\n",
      "Epoch [1/2], Step [568/7464], Loss: 4.0009\n",
      "Epoch [1/2], Step [569/7464], Loss: 3.9133\n",
      "Epoch [1/2], Step [570/7464], Loss: 3.9131\n",
      "Epoch [1/2], Step [571/7464], Loss: 3.5290\n",
      "Epoch [1/2], Step [572/7464], Loss: 3.8433\n",
      "Epoch [1/2], Step [573/7464], Loss: 3.7339\n",
      "Epoch [1/2], Step [574/7464], Loss: 4.0556\n",
      "Epoch [1/2], Step [575/7464], Loss: 3.8760\n",
      "Epoch [1/2], Step [576/7464], Loss: 3.8178\n",
      "Epoch [1/2], Step [577/7464], Loss: 3.6834\n",
      "Epoch [1/2], Step [578/7464], Loss: 3.8985\n",
      "Epoch [1/2], Step [579/7464], Loss: 3.7340\n",
      "Epoch [1/2], Step [580/7464], Loss: 3.7751\n",
      "Epoch [1/2], Step [581/7464], Loss: 3.6914\n",
      "Epoch [1/2], Step [582/7464], Loss: 3.8941\n",
      "Epoch [1/2], Step [583/7464], Loss: 3.5572\n",
      "Epoch [1/2], Step [584/7464], Loss: 3.8296\n",
      "Epoch [1/2], Step [585/7464], Loss: 3.7624\n",
      "Epoch [1/2], Step [586/7464], Loss: 3.5552\n",
      "Epoch [1/2], Step [587/7464], Loss: 3.6617\n",
      "Epoch [1/2], Step [588/7464], Loss: 3.6911\n",
      "Epoch [1/2], Step [589/7464], Loss: 3.4417\n",
      "Epoch [1/2], Step [590/7464], Loss: 3.9459\n",
      "Epoch [1/2], Step [591/7464], Loss: 3.5226\n",
      "Epoch [1/2], Step [592/7464], Loss: 3.8120\n",
      "Epoch [1/2], Step [593/7464], Loss: 3.7150\n",
      "Epoch [1/2], Step [594/7464], Loss: 4.1912\n",
      "Epoch [1/2], Step [595/7464], Loss: 3.8709\n",
      "Epoch [1/2], Step [596/7464], Loss: 3.7981\n",
      "Epoch [1/2], Step [597/7464], Loss: 3.4783\n",
      "Epoch [1/2], Step [598/7464], Loss: 3.4439\n",
      "Epoch [1/2], Step [599/7464], Loss: 3.1396\n",
      "Epoch [1/2], Step [600/7464], Loss: 3.9375\n",
      "Epoch [1/2], Step [601/7464], Loss: 4.0651\n",
      "Epoch [1/2], Step [602/7464], Loss: 3.6943\n",
      "Epoch [1/2], Step [603/7464], Loss: 3.3089\n",
      "Epoch [1/2], Step [604/7464], Loss: 4.3126\n",
      "Epoch [1/2], Step [605/7464], Loss: 3.8563\n",
      "Epoch [1/2], Step [606/7464], Loss: 3.8125\n",
      "Epoch [1/2], Step [607/7464], Loss: 3.8861\n",
      "Epoch [1/2], Step [608/7464], Loss: 4.1294\n",
      "Epoch [1/2], Step [609/7464], Loss: 3.8107\n",
      "Epoch [1/2], Step [610/7464], Loss: 3.3235\n",
      "Epoch [1/2], Step [611/7464], Loss: 3.9474\n",
      "Epoch [1/2], Step [612/7464], Loss: 3.6345\n",
      "Epoch [1/2], Step [613/7464], Loss: 4.3209\n",
      "Epoch [1/2], Step [614/7464], Loss: 4.4015\n",
      "Epoch [1/2], Step [615/7464], Loss: 3.6737\n",
      "Epoch [1/2], Step [616/7464], Loss: 3.4828\n",
      "Epoch [1/2], Step [617/7464], Loss: 3.6689\n",
      "Epoch [1/2], Step [618/7464], Loss: 3.8263\n",
      "Epoch [1/2], Step [619/7464], Loss: 3.7137\n",
      "Epoch [1/2], Step [620/7464], Loss: 3.7322\n",
      "Epoch [1/2], Step [621/7464], Loss: 3.6877\n",
      "Epoch [1/2], Step [622/7464], Loss: 3.5985\n",
      "Epoch [1/2], Step [623/7464], Loss: 3.6667\n",
      "Epoch [1/2], Step [624/7464], Loss: 4.3775\n",
      "Epoch [1/2], Step [625/7464], Loss: 3.7662\n",
      "Epoch [1/2], Step [626/7464], Loss: 3.7269\n",
      "Epoch [1/2], Step [627/7464], Loss: 4.0240\n",
      "Epoch [1/2], Step [628/7464], Loss: 3.6116\n",
      "Epoch [1/2], Step [629/7464], Loss: 4.0365\n",
      "Epoch [1/2], Step [630/7464], Loss: 3.7710\n",
      "Epoch [1/2], Step [631/7464], Loss: 3.8756\n",
      "Epoch [1/2], Step [632/7464], Loss: 3.6701\n",
      "Epoch [1/2], Step [633/7464], Loss: 3.9028\n",
      "Epoch [1/2], Step [634/7464], Loss: 3.7592\n",
      "Epoch [1/2], Step [635/7464], Loss: 3.7254\n",
      "Epoch [1/2], Step [636/7464], Loss: 3.7385\n",
      "Epoch [1/2], Step [637/7464], Loss: 3.4990\n",
      "Epoch [1/2], Step [638/7464], Loss: 3.6144\n",
      "Epoch [1/2], Step [639/7464], Loss: 3.8516\n",
      "Epoch [1/2], Step [640/7464], Loss: 3.9241\n",
      "Epoch [1/2], Step [641/7464], Loss: 3.4710\n",
      "Epoch [1/2], Step [642/7464], Loss: 4.4274\n",
      "Epoch [1/2], Step [643/7464], Loss: 3.7042\n",
      "Epoch [1/2], Step [644/7464], Loss: 3.7504\n",
      "Epoch [1/2], Step [645/7464], Loss: 3.7446\n",
      "Epoch [1/2], Step [646/7464], Loss: 3.9299\n",
      "Epoch [1/2], Step [647/7464], Loss: 3.8680\n",
      "Epoch [1/2], Step [648/7464], Loss: 3.5613\n",
      "Epoch [1/2], Step [649/7464], Loss: 3.9411\n",
      "Epoch [1/2], Step [650/7464], Loss: 3.7990\n",
      "Epoch [1/2], Step [651/7464], Loss: 3.6480\n",
      "Epoch [1/2], Step [652/7464], Loss: 3.6441\n",
      "Epoch [1/2], Step [653/7464], Loss: 4.0232\n",
      "Epoch [1/2], Step [654/7464], Loss: 4.0867\n",
      "Epoch [1/2], Step [655/7464], Loss: 3.6654\n",
      "Epoch [1/2], Step [656/7464], Loss: 4.1928\n",
      "Epoch [1/2], Step [657/7464], Loss: 3.6695\n",
      "Epoch [1/2], Step [658/7464], Loss: 3.7728\n",
      "Epoch [1/2], Step [659/7464], Loss: 3.6910\n",
      "Epoch [1/2], Step [660/7464], Loss: 3.9721\n",
      "Epoch [1/2], Step [661/7464], Loss: 3.5859\n",
      "Epoch [1/2], Step [662/7464], Loss: 3.6919\n",
      "Epoch [1/2], Step [663/7464], Loss: 4.0074\n",
      "Epoch [1/2], Step [664/7464], Loss: 3.8140\n",
      "Epoch [1/2], Step [665/7464], Loss: 3.6454\n",
      "Epoch [1/2], Step [666/7464], Loss: 4.0498\n",
      "Epoch [1/2], Step [667/7464], Loss: 3.5821\n",
      "Epoch [1/2], Step [668/7464], Loss: 3.5263\n",
      "Epoch [1/2], Step [669/7464], Loss: 4.5720\n",
      "Epoch [1/2], Step [670/7464], Loss: 4.0272\n",
      "Epoch [1/2], Step [671/7464], Loss: 3.6076\n",
      "Epoch [1/2], Step [672/7464], Loss: 3.8487\n",
      "Epoch [1/2], Step [673/7464], Loss: 4.2590\n",
      "Epoch [1/2], Step [674/7464], Loss: 3.7446\n",
      "Epoch [1/2], Step [675/7464], Loss: 3.8398\n",
      "Epoch [1/2], Step [676/7464], Loss: 3.4965\n",
      "Epoch [1/2], Step [677/7464], Loss: 3.7228\n",
      "Epoch [1/2], Step [678/7464], Loss: 3.7871\n",
      "Epoch [1/2], Step [679/7464], Loss: 3.6483\n",
      "Epoch [1/2], Step [680/7464], Loss: 4.2273\n",
      "Epoch [1/2], Step [681/7464], Loss: 3.5061\n",
      "Epoch [1/2], Step [682/7464], Loss: 3.6850\n",
      "Epoch [1/2], Step [683/7464], Loss: 4.0682\n",
      "Epoch [1/2], Step [684/7464], Loss: 3.9521\n",
      "Epoch [1/2], Step [685/7464], Loss: 3.8346\n",
      "Epoch [1/2], Step [686/7464], Loss: 3.8205\n",
      "Epoch [1/2], Step [687/7464], Loss: 3.7564\n",
      "Epoch [1/2], Step [688/7464], Loss: 3.6690\n",
      "Epoch [1/2], Step [689/7464], Loss: 3.8151\n",
      "Epoch [1/2], Step [690/7464], Loss: 3.9291\n",
      "Epoch [1/2], Step [691/7464], Loss: 3.9325\n",
      "Epoch [1/2], Step [692/7464], Loss: 4.1290\n",
      "Epoch [1/2], Step [693/7464], Loss: 3.3975\n",
      "Epoch [1/2], Step [694/7464], Loss: 3.6856\n",
      "Epoch [1/2], Step [695/7464], Loss: 3.6791\n",
      "Epoch [1/2], Step [696/7464], Loss: 4.0695\n",
      "Epoch [1/2], Step [697/7464], Loss: 3.8059\n",
      "Epoch [1/2], Step [698/7464], Loss: 3.6214\n",
      "Epoch [1/2], Step [699/7464], Loss: 3.7271\n",
      "Epoch [1/2], Step [700/7464], Loss: 3.4825\n",
      "Epoch [1/2], Step [701/7464], Loss: 3.7764\n",
      "Epoch [1/2], Step [702/7464], Loss: 3.7190\n",
      "Epoch [1/2], Step [703/7464], Loss: 3.6862\n",
      "Epoch [1/2], Step [704/7464], Loss: 3.6886\n",
      "Epoch [1/2], Step [705/7464], Loss: 3.4742\n",
      "Epoch [1/2], Step [706/7464], Loss: 3.8692\n",
      "Epoch [1/2], Step [707/7464], Loss: 3.7033\n",
      "Epoch [1/2], Step [708/7464], Loss: 3.7364\n",
      "Epoch [1/2], Step [709/7464], Loss: 3.3483\n",
      "Epoch [1/2], Step [710/7464], Loss: 4.5282\n",
      "Epoch [1/2], Step [711/7464], Loss: 3.3239\n",
      "Epoch [1/2], Step [712/7464], Loss: 3.8974\n",
      "Epoch [1/2], Step [713/7464], Loss: 3.4608\n",
      "Epoch [1/2], Step [714/7464], Loss: 4.0034\n",
      "Epoch [1/2], Step [715/7464], Loss: 3.8155\n",
      "Epoch [1/2], Step [716/7464], Loss: 4.2073\n",
      "Epoch [1/2], Step [717/7464], Loss: 4.0551\n",
      "Epoch [1/2], Step [718/7464], Loss: 3.8928\n",
      "Epoch [1/2], Step [719/7464], Loss: 3.6471\n",
      "Epoch [1/2], Step [720/7464], Loss: 3.5943\n",
      "Epoch [1/2], Step [721/7464], Loss: 3.8073\n",
      "Epoch [1/2], Step [722/7464], Loss: 3.6521\n",
      "Epoch [1/2], Step [723/7464], Loss: 3.8095\n",
      "Epoch [1/2], Step [724/7464], Loss: 4.0138\n",
      "Epoch [1/2], Step [725/7464], Loss: 4.0483\n",
      "Epoch [1/2], Step [726/7464], Loss: 3.8415\n",
      "Epoch [1/2], Step [727/7464], Loss: 3.9430\n",
      "Epoch [1/2], Step [728/7464], Loss: 3.5574\n",
      "Epoch [1/2], Step [729/7464], Loss: 3.7655\n",
      "Epoch [1/2], Step [730/7464], Loss: 4.1682\n",
      "Epoch [1/2], Step [731/7464], Loss: 3.4104\n",
      "Epoch [1/2], Step [732/7464], Loss: 3.4014\n",
      "Epoch [1/2], Step [733/7464], Loss: 3.6571\n",
      "Epoch [1/2], Step [734/7464], Loss: 3.7251\n",
      "Epoch [1/2], Step [735/7464], Loss: 3.9557\n",
      "Epoch [1/2], Step [736/7464], Loss: 3.9585\n",
      "Epoch [1/2], Step [737/7464], Loss: 3.5758\n",
      "Epoch [1/2], Step [738/7464], Loss: 3.9417\n",
      "Epoch [1/2], Step [739/7464], Loss: 3.9176\n",
      "Epoch [1/2], Step [740/7464], Loss: 4.2173\n",
      "Epoch [1/2], Step [741/7464], Loss: 3.7680\n",
      "Epoch [1/2], Step [742/7464], Loss: 3.8558\n",
      "Epoch [1/2], Step [743/7464], Loss: 3.9045\n",
      "Epoch [1/2], Step [744/7464], Loss: 3.7915\n",
      "Epoch [1/2], Step [745/7464], Loss: 3.8608\n",
      "Epoch [1/2], Step [746/7464], Loss: 3.9177\n",
      "Epoch [1/2], Step [747/7464], Loss: 3.4197\n",
      "Epoch [1/2], Step [748/7464], Loss: 3.9978\n",
      "Epoch [1/2], Step [749/7464], Loss: 3.9017\n",
      "Epoch [1/2], Step [750/7464], Loss: 3.5989\n",
      "Epoch [1/2], Step [751/7464], Loss: 3.9028\n",
      "Epoch [1/2], Step [752/7464], Loss: 3.9478\n",
      "Epoch [1/2], Step [753/7464], Loss: 3.9119\n",
      "Epoch [1/2], Step [754/7464], Loss: 3.5873\n",
      "Epoch [1/2], Step [755/7464], Loss: 3.8345\n",
      "Epoch [1/2], Step [756/7464], Loss: 3.5281\n",
      "Epoch [1/2], Step [757/7464], Loss: 3.6897\n",
      "Epoch [1/2], Step [758/7464], Loss: 3.6608\n",
      "Epoch [1/2], Step [759/7464], Loss: 4.0720\n",
      "Epoch [1/2], Step [760/7464], Loss: 3.6664\n",
      "Epoch [1/2], Step [761/7464], Loss: 3.7204\n",
      "Epoch [1/2], Step [762/7464], Loss: 3.5669\n",
      "Epoch [1/2], Step [763/7464], Loss: 3.9967\n",
      "Epoch [1/2], Step [764/7464], Loss: 3.5123\n",
      "Epoch [1/2], Step [765/7464], Loss: 3.8296\n",
      "Epoch [1/2], Step [766/7464], Loss: 3.4044\n",
      "Epoch [1/2], Step [767/7464], Loss: 3.5159\n",
      "Epoch [1/2], Step [768/7464], Loss: 3.9038\n",
      "Epoch [1/2], Step [769/7464], Loss: 3.9363\n",
      "Epoch [1/2], Step [770/7464], Loss: 3.4629\n",
      "Epoch [1/2], Step [771/7464], Loss: 3.6952\n",
      "Epoch [1/2], Step [772/7464], Loss: 4.0769\n",
      "Epoch [1/2], Step [773/7464], Loss: 4.1041\n",
      "Epoch [1/2], Step [774/7464], Loss: 4.0227\n",
      "Epoch [1/2], Step [775/7464], Loss: 4.5563\n",
      "Epoch [1/2], Step [776/7464], Loss: 3.8443\n",
      "Epoch [1/2], Step [777/7464], Loss: 3.6656\n",
      "Epoch [1/2], Step [778/7464], Loss: 3.7029\n",
      "Epoch [1/2], Step [779/7464], Loss: 3.5252\n",
      "Epoch [1/2], Step [780/7464], Loss: 3.5444\n",
      "Epoch [1/2], Step [781/7464], Loss: 4.0072\n",
      "Epoch [1/2], Step [782/7464], Loss: 3.4228\n",
      "Epoch [1/2], Step [783/7464], Loss: 4.0129\n",
      "Epoch [1/2], Step [784/7464], Loss: 4.1766\n",
      "Epoch [1/2], Step [785/7464], Loss: 3.6489\n",
      "Epoch [1/2], Step [786/7464], Loss: 3.9303\n",
      "Epoch [1/2], Step [787/7464], Loss: 3.8602\n",
      "Epoch [1/2], Step [788/7464], Loss: 3.9295\n",
      "Epoch [1/2], Step [789/7464], Loss: 4.2485\n",
      "Epoch [1/2], Step [790/7464], Loss: 3.7117\n",
      "Epoch [1/2], Step [791/7464], Loss: 3.7498\n",
      "Epoch [1/2], Step [792/7464], Loss: 3.6951\n",
      "Epoch [1/2], Step [793/7464], Loss: 3.8995\n",
      "Epoch [1/2], Step [794/7464], Loss: 3.8244\n",
      "Epoch [1/2], Step [795/7464], Loss: 3.8783\n",
      "Epoch [1/2], Step [796/7464], Loss: 4.0751\n",
      "Epoch [1/2], Step [797/7464], Loss: 3.7728\n",
      "Epoch [1/2], Step [798/7464], Loss: 3.7442\n",
      "Epoch [1/2], Step [799/7464], Loss: 3.9135\n",
      "Epoch [1/2], Step [800/7464], Loss: 3.8338\n",
      "Epoch [1/2], Step [801/7464], Loss: 3.8455\n",
      "Epoch [1/2], Step [802/7464], Loss: 3.7981\n",
      "Epoch [1/2], Step [803/7464], Loss: 3.8190\n",
      "Epoch [1/2], Step [804/7464], Loss: 3.7054\n",
      "Epoch [1/2], Step [805/7464], Loss: 3.8547\n",
      "Epoch [1/2], Step [806/7464], Loss: 3.9941\n",
      "Epoch [1/2], Step [807/7464], Loss: 3.8895\n",
      "Epoch [1/2], Step [808/7464], Loss: 3.8943\n",
      "Epoch [1/2], Step [809/7464], Loss: 3.8518\n",
      "Epoch [1/2], Step [810/7464], Loss: 3.6559\n",
      "Epoch [1/2], Step [811/7464], Loss: 3.6792\n",
      "Epoch [1/2], Step [812/7464], Loss: 3.7819\n",
      "Epoch [1/2], Step [813/7464], Loss: 3.9383\n",
      "Epoch [1/2], Step [814/7464], Loss: 3.6637\n",
      "Epoch [1/2], Step [815/7464], Loss: 3.9106\n",
      "Epoch [1/2], Step [816/7464], Loss: 3.8621\n",
      "Epoch [1/2], Step [817/7464], Loss: 3.7896\n",
      "Epoch [1/2], Step [818/7464], Loss: 3.6058\n",
      "Epoch [1/2], Step [819/7464], Loss: 3.8955\n",
      "Epoch [1/2], Step [820/7464], Loss: 3.7038\n",
      "Epoch [1/2], Step [821/7464], Loss: 4.2691\n",
      "Epoch [1/2], Step [822/7464], Loss: 3.7784\n",
      "Epoch [1/2], Step [823/7464], Loss: 3.7421\n",
      "Epoch [1/2], Step [824/7464], Loss: 4.0931\n",
      "Epoch [1/2], Step [825/7464], Loss: 3.8638\n",
      "Epoch [1/2], Step [826/7464], Loss: 3.7436\n",
      "Epoch [1/2], Step [827/7464], Loss: 3.9522\n",
      "Epoch [1/2], Step [828/7464], Loss: 3.8630\n",
      "Epoch [1/2], Step [829/7464], Loss: 3.8458\n",
      "Epoch [1/2], Step [830/7464], Loss: 3.6984\n",
      "Epoch [1/2], Step [831/7464], Loss: 3.6532\n",
      "Epoch [1/2], Step [832/7464], Loss: 4.1499\n",
      "Epoch [1/2], Step [833/7464], Loss: 3.7917\n",
      "Epoch [1/2], Step [834/7464], Loss: 3.8619\n",
      "Epoch [1/2], Step [835/7464], Loss: 3.9085\n",
      "Epoch [1/2], Step [836/7464], Loss: 3.6190\n",
      "Epoch [1/2], Step [837/7464], Loss: 3.7366\n",
      "Epoch [1/2], Step [838/7464], Loss: 4.1623\n",
      "Epoch [1/2], Step [839/7464], Loss: 3.7633\n",
      "Epoch [1/2], Step [840/7464], Loss: 3.7018\n",
      "Epoch [1/2], Step [841/7464], Loss: 3.9006\n",
      "Epoch [1/2], Step [842/7464], Loss: 3.8228\n",
      "Epoch [1/2], Step [843/7464], Loss: 3.8921\n",
      "Epoch [1/2], Step [844/7464], Loss: 3.8430\n",
      "Epoch [1/2], Step [845/7464], Loss: 3.9744\n",
      "Epoch [1/2], Step [846/7464], Loss: 3.6790\n",
      "Epoch [1/2], Step [847/7464], Loss: 3.8819\n",
      "Epoch [1/2], Step [848/7464], Loss: 3.5948\n",
      "Epoch [1/2], Step [849/7464], Loss: 3.6495\n",
      "Epoch [1/2], Step [850/7464], Loss: 3.7374\n",
      "Epoch [1/2], Step [851/7464], Loss: 4.0479\n",
      "Epoch [1/2], Step [852/7464], Loss: 3.5302\n",
      "Epoch [1/2], Step [853/7464], Loss: 3.9011\n",
      "Epoch [1/2], Step [854/7464], Loss: 3.7208\n",
      "Epoch [1/2], Step [855/7464], Loss: 3.8585\n",
      "Epoch [1/2], Step [856/7464], Loss: 3.8230\n",
      "Epoch [1/2], Step [857/7464], Loss: 3.8478\n",
      "Epoch [1/2], Step [858/7464], Loss: 3.5921\n",
      "Epoch [1/2], Step [859/7464], Loss: 3.5969\n",
      "Epoch [1/2], Step [860/7464], Loss: 3.7057\n",
      "Epoch [1/2], Step [861/7464], Loss: 3.8692\n",
      "Epoch [1/2], Step [862/7464], Loss: 3.5181\n",
      "Epoch [1/2], Step [863/7464], Loss: 3.9266\n",
      "Epoch [1/2], Step [864/7464], Loss: 4.0949\n",
      "Epoch [1/2], Step [865/7464], Loss: 4.0013\n",
      "Epoch [1/2], Step [866/7464], Loss: 3.7269\n",
      "Epoch [1/2], Step [867/7464], Loss: 3.7171\n",
      "Epoch [1/2], Step [868/7464], Loss: 3.6380\n",
      "Epoch [1/2], Step [869/7464], Loss: 3.9972\n",
      "Epoch [1/2], Step [870/7464], Loss: 3.8827\n",
      "Epoch [1/2], Step [871/7464], Loss: 4.3473\n",
      "Epoch [1/2], Step [872/7464], Loss: 3.8745\n",
      "Epoch [1/2], Step [873/7464], Loss: 3.3584\n",
      "Epoch [1/2], Step [874/7464], Loss: 3.4894\n",
      "Epoch [1/2], Step [875/7464], Loss: 3.8361\n",
      "Epoch [1/2], Step [876/7464], Loss: 3.7907\n",
      "Epoch [1/2], Step [877/7464], Loss: 3.7391\n",
      "Epoch [1/2], Step [878/7464], Loss: 3.7241\n",
      "Epoch [1/2], Step [879/7464], Loss: 3.4764\n",
      "Epoch [1/2], Step [880/7464], Loss: 3.5570\n",
      "Epoch [1/2], Step [881/7464], Loss: 3.9172\n",
      "Epoch [1/2], Step [882/7464], Loss: 3.5651\n",
      "Epoch [1/2], Step [883/7464], Loss: 3.9101\n",
      "Epoch [1/2], Step [884/7464], Loss: 3.9224\n",
      "Epoch [1/2], Step [885/7464], Loss: 3.3716\n",
      "Epoch [1/2], Step [886/7464], Loss: 3.5659\n",
      "Epoch [1/2], Step [887/7464], Loss: 3.7081\n",
      "Epoch [1/2], Step [888/7464], Loss: 3.5204\n",
      "Epoch [1/2], Step [889/7464], Loss: 3.7406\n",
      "Epoch [1/2], Step [890/7464], Loss: 4.1340\n",
      "Epoch [1/2], Step [891/7464], Loss: 3.8958\n",
      "Epoch [1/2], Step [892/7464], Loss: 3.4350\n",
      "Epoch [1/2], Step [893/7464], Loss: 3.9301\n",
      "Epoch [1/2], Step [894/7464], Loss: 3.5155\n",
      "Epoch [1/2], Step [895/7464], Loss: 3.9087\n",
      "Epoch [1/2], Step [896/7464], Loss: 3.6840\n",
      "Epoch [1/2], Step [897/7464], Loss: 3.7324\n",
      "Epoch [1/2], Step [898/7464], Loss: 3.6973\n",
      "Epoch [1/2], Step [899/7464], Loss: 3.4798\n",
      "Epoch [1/2], Step [900/7464], Loss: 3.5527\n",
      "Epoch [1/2], Step [901/7464], Loss: 3.8386\n",
      "Epoch [1/2], Step [902/7464], Loss: 4.1642\n",
      "Epoch [1/2], Step [903/7464], Loss: 3.5145\n",
      "Epoch [1/2], Step [904/7464], Loss: 4.2063\n",
      "Epoch [1/2], Step [905/7464], Loss: 3.7106\n",
      "Epoch [1/2], Step [906/7464], Loss: 4.2011\n",
      "Epoch [1/2], Step [907/7464], Loss: 3.4201\n",
      "Epoch [1/2], Step [908/7464], Loss: 3.5480\n",
      "Epoch [1/2], Step [909/7464], Loss: 3.8760\n",
      "Epoch [1/2], Step [910/7464], Loss: 3.7169\n",
      "Epoch [1/2], Step [911/7464], Loss: 3.6369\n",
      "Epoch [1/2], Step [912/7464], Loss: 4.0460\n",
      "Epoch [1/2], Step [913/7464], Loss: 3.8204\n",
      "Epoch [1/2], Step [914/7464], Loss: 3.9227\n",
      "Epoch [1/2], Step [915/7464], Loss: 3.5583\n",
      "Epoch [1/2], Step [916/7464], Loss: 3.8201\n",
      "Epoch [1/2], Step [917/7464], Loss: 3.7917\n",
      "Epoch [1/2], Step [918/7464], Loss: 3.8844\n",
      "Epoch [1/2], Step [919/7464], Loss: 3.3933\n",
      "Epoch [1/2], Step [920/7464], Loss: 3.8302\n",
      "Epoch [1/2], Step [921/7464], Loss: 3.7960\n",
      "Epoch [1/2], Step [922/7464], Loss: 3.8752\n",
      "Epoch [1/2], Step [923/7464], Loss: 3.3409\n",
      "Epoch [1/2], Step [924/7464], Loss: 3.7641\n",
      "Epoch [1/2], Step [925/7464], Loss: 3.9267\n",
      "Epoch [1/2], Step [926/7464], Loss: 3.8293\n",
      "Epoch [1/2], Step [927/7464], Loss: 3.2825\n",
      "Epoch [1/2], Step [928/7464], Loss: 3.5756\n",
      "Epoch [1/2], Step [929/7464], Loss: 3.7214\n",
      "Epoch [1/2], Step [930/7464], Loss: 3.7802\n",
      "Epoch [1/2], Step [931/7464], Loss: 3.7744\n",
      "Epoch [1/2], Step [932/7464], Loss: 3.7977\n",
      "Epoch [1/2], Step [933/7464], Loss: 3.7779\n",
      "Epoch [1/2], Step [934/7464], Loss: 3.7968\n",
      "Epoch [1/2], Step [935/7464], Loss: 4.3683\n",
      "Epoch [1/2], Step [936/7464], Loss: 3.2852\n",
      "Epoch [1/2], Step [937/7464], Loss: 3.8591\n",
      "Epoch [1/2], Step [938/7464], Loss: 3.7843\n",
      "Epoch [1/2], Step [939/7464], Loss: 3.9667\n",
      "Epoch [1/2], Step [940/7464], Loss: 3.8427\n",
      "Epoch [1/2], Step [941/7464], Loss: 3.3583\n",
      "Epoch [1/2], Step [942/7464], Loss: 3.4197\n",
      "Epoch [1/2], Step [943/7464], Loss: 3.4689\n",
      "Epoch [1/2], Step [944/7464], Loss: 3.5896\n",
      "Epoch [1/2], Step [945/7464], Loss: 4.0048\n",
      "Epoch [1/2], Step [946/7464], Loss: 3.5004\n",
      "Epoch [1/2], Step [947/7464], Loss: 4.3485\n",
      "Epoch [1/2], Step [948/7464], Loss: 4.2766\n",
      "Epoch [1/2], Step [949/7464], Loss: 3.8589\n",
      "Epoch [1/2], Step [950/7464], Loss: 3.4934\n",
      "Epoch [1/2], Step [951/7464], Loss: 4.1757\n",
      "Epoch [1/2], Step [952/7464], Loss: 3.9969\n",
      "Epoch [1/2], Step [953/7464], Loss: 3.4671\n",
      "Epoch [1/2], Step [954/7464], Loss: 3.8513\n",
      "Epoch [1/2], Step [955/7464], Loss: 4.2991\n",
      "Epoch [1/2], Step [956/7464], Loss: 3.5438\n",
      "Epoch [1/2], Step [957/7464], Loss: 3.5524\n",
      "Epoch [1/2], Step [958/7464], Loss: 3.4745\n",
      "Epoch [1/2], Step [959/7464], Loss: 3.6987\n",
      "Epoch [1/2], Step [960/7464], Loss: 3.6736\n",
      "Epoch [1/2], Step [961/7464], Loss: 3.7333\n",
      "Epoch [1/2], Step [962/7464], Loss: 3.6289\n",
      "Epoch [1/2], Step [963/7464], Loss: 3.9644\n",
      "Epoch [1/2], Step [964/7464], Loss: 3.6998\n",
      "Epoch [1/2], Step [965/7464], Loss: 3.8154\n",
      "Epoch [1/2], Step [966/7464], Loss: 3.6111\n",
      "Epoch [1/2], Step [967/7464], Loss: 3.6320\n",
      "Epoch [1/2], Step [968/7464], Loss: 3.6058\n",
      "Epoch [1/2], Step [969/7464], Loss: 3.7237\n",
      "Epoch [1/2], Step [970/7464], Loss: 3.5403\n",
      "Epoch [1/2], Step [971/7464], Loss: 3.7529\n",
      "Epoch [1/2], Step [972/7464], Loss: 3.5422\n",
      "Epoch [1/2], Step [973/7464], Loss: 3.3458\n",
      "Epoch [1/2], Step [974/7464], Loss: 3.6233\n",
      "Epoch [1/2], Step [975/7464], Loss: 4.1990\n",
      "Epoch [1/2], Step [976/7464], Loss: 3.7147\n",
      "Epoch [1/2], Step [977/7464], Loss: 3.6679\n",
      "Epoch [1/2], Step [978/7464], Loss: 3.4711\n",
      "Epoch [1/2], Step [979/7464], Loss: 4.0324\n",
      "Epoch [1/2], Step [980/7464], Loss: 3.4307\n",
      "Epoch [1/2], Step [981/7464], Loss: 4.1221\n",
      "Epoch [1/2], Step [982/7464], Loss: 3.9201\n",
      "Epoch [1/2], Step [983/7464], Loss: 4.3964\n",
      "Epoch [1/2], Step [984/7464], Loss: 3.5684\n",
      "Epoch [1/2], Step [985/7464], Loss: 3.8759\n",
      "Epoch [1/2], Step [986/7464], Loss: 3.6727\n",
      "Epoch [1/2], Step [987/7464], Loss: 3.5976\n",
      "Epoch [1/2], Step [988/7464], Loss: 3.6442\n",
      "Epoch [1/2], Step [989/7464], Loss: 3.4701\n",
      "Epoch [1/2], Step [990/7464], Loss: 3.6196\n",
      "Epoch [1/2], Step [991/7464], Loss: 3.6640\n",
      "Epoch [1/2], Step [992/7464], Loss: 3.9666\n",
      "Epoch [1/2], Step [993/7464], Loss: 3.3807\n",
      "Epoch [1/2], Step [994/7464], Loss: 3.7971\n",
      "Epoch [1/2], Step [995/7464], Loss: 4.1073\n",
      "Epoch [1/2], Step [996/7464], Loss: 3.6574\n",
      "Epoch [1/2], Step [997/7464], Loss: 3.5214\n",
      "Epoch [1/2], Step [998/7464], Loss: 3.7660\n",
      "Epoch [1/2], Step [999/7464], Loss: 3.9449\n",
      "Epoch [1/2], Step [1000/7464], Loss: 3.2806\n",
      "Epoch [1/2], Step [1001/7464], Loss: 3.6609\n",
      "Epoch [1/2], Step [1002/7464], Loss: 3.6862\n",
      "Epoch [1/2], Step [1003/7464], Loss: 3.9762\n",
      "Epoch [1/2], Step [1004/7464], Loss: 3.5310\n",
      "Epoch [1/2], Step [1005/7464], Loss: 3.8903\n",
      "Epoch [1/2], Step [1006/7464], Loss: 4.0004\n",
      "Epoch [1/2], Step [1007/7464], Loss: 3.8861\n",
      "Epoch [1/2], Step [1008/7464], Loss: 4.0391\n",
      "Epoch [1/2], Step [1009/7464], Loss: 3.7341\n",
      "Epoch [1/2], Step [1010/7464], Loss: 3.4695\n",
      "Epoch [1/2], Step [1011/7464], Loss: 3.8022\n",
      "Epoch [1/2], Step [1012/7464], Loss: 3.8499\n",
      "Epoch [1/2], Step [1013/7464], Loss: 3.3235\n",
      "Epoch [1/2], Step [1014/7464], Loss: 3.6580\n",
      "Epoch [1/2], Step [1015/7464], Loss: 3.5655\n",
      "Epoch [1/2], Step [1016/7464], Loss: 3.2009\n",
      "Epoch [1/2], Step [1017/7464], Loss: 3.8732\n",
      "Epoch [1/2], Step [1018/7464], Loss: 3.5287\n",
      "Epoch [1/2], Step [1019/7464], Loss: 3.5522\n",
      "Epoch [1/2], Step [1020/7464], Loss: 3.3418\n",
      "Epoch [1/2], Step [1021/7464], Loss: 3.9533\n",
      "Epoch [1/2], Step [1022/7464], Loss: 3.5084\n",
      "Epoch [1/2], Step [1023/7464], Loss: 3.9834\n",
      "Epoch [1/2], Step [1024/7464], Loss: 3.9992\n",
      "Epoch [1/2], Step [1025/7464], Loss: 3.3096\n",
      "Epoch [1/2], Step [1026/7464], Loss: 3.6421\n",
      "Epoch [1/2], Step [1027/7464], Loss: 3.2738\n",
      "Epoch [1/2], Step [1028/7464], Loss: 3.9650\n",
      "Epoch [1/2], Step [1029/7464], Loss: 3.1143\n",
      "Epoch [1/2], Step [1030/7464], Loss: 3.7390\n",
      "Epoch [1/2], Step [1031/7464], Loss: 3.8185\n",
      "Epoch [1/2], Step [1032/7464], Loss: 4.3463\n",
      "Epoch [1/2], Step [1033/7464], Loss: 3.2257\n",
      "Epoch [1/2], Step [1034/7464], Loss: 3.3258\n",
      "Epoch [1/2], Step [1035/7464], Loss: 3.5697\n",
      "Epoch [1/2], Step [1036/7464], Loss: 3.7982\n",
      "Epoch [1/2], Step [1037/7464], Loss: 3.2985\n",
      "Epoch [1/2], Step [1038/7464], Loss: 3.6337\n",
      "Epoch [1/2], Step [1039/7464], Loss: 3.8814\n",
      "Epoch [1/2], Step [1040/7464], Loss: 3.1777\n",
      "Epoch [1/2], Step [1041/7464], Loss: 4.0027\n",
      "Epoch [1/2], Step [1042/7464], Loss: 3.8530\n",
      "Epoch [1/2], Step [1043/7464], Loss: 3.2554\n",
      "Epoch [1/2], Step [1044/7464], Loss: 3.5543\n",
      "Epoch [1/2], Step [1045/7464], Loss: 4.0527\n",
      "Epoch [1/2], Step [1046/7464], Loss: 3.4873\n",
      "Epoch [1/2], Step [1047/7464], Loss: 3.8521\n",
      "Epoch [1/2], Step [1048/7464], Loss: 3.6826\n",
      "Epoch [1/2], Step [1049/7464], Loss: 3.2613\n",
      "Epoch [1/2], Step [1050/7464], Loss: 3.6216\n",
      "Epoch [1/2], Step [1051/7464], Loss: 3.9488\n",
      "Epoch [1/2], Step [1052/7464], Loss: 3.4156\n",
      "Epoch [1/2], Step [1053/7464], Loss: 3.4862\n",
      "Epoch [1/2], Step [1054/7464], Loss: 3.4707\n",
      "Epoch [1/2], Step [1055/7464], Loss: 4.3335\n",
      "Epoch [1/2], Step [1056/7464], Loss: 3.5193\n",
      "Epoch [1/2], Step [1057/7464], Loss: 4.0656\n",
      "Epoch [1/2], Step [1058/7464], Loss: 4.6960\n",
      "Epoch [1/2], Step [1059/7464], Loss: 3.7061\n",
      "Epoch [1/2], Step [1060/7464], Loss: 3.5841\n",
      "Epoch [1/2], Step [1061/7464], Loss: 3.6045\n",
      "Epoch [1/2], Step [1062/7464], Loss: 3.4919\n",
      "Epoch [1/2], Step [1063/7464], Loss: 3.6790\n",
      "Epoch [1/2], Step [1064/7464], Loss: 3.7236\n",
      "Epoch [1/2], Step [1065/7464], Loss: 3.8640\n",
      "Epoch [1/2], Step [1066/7464], Loss: 3.8283\n",
      "Epoch [1/2], Step [1067/7464], Loss: 3.7683\n",
      "Epoch [1/2], Step [1068/7464], Loss: 3.7629\n",
      "Epoch [1/2], Step [1069/7464], Loss: 4.1396\n",
      "Epoch [1/2], Step [1070/7464], Loss: 4.1976\n",
      "Epoch [1/2], Step [1071/7464], Loss: 3.7086\n",
      "Epoch [1/2], Step [1072/7464], Loss: 3.8130\n",
      "Epoch [1/2], Step [1073/7464], Loss: 3.7594\n",
      "Epoch [1/2], Step [1074/7464], Loss: 3.9189\n",
      "Epoch [1/2], Step [1075/7464], Loss: 3.9956\n",
      "Epoch [1/2], Step [1076/7464], Loss: 3.9782\n",
      "Epoch [1/2], Step [1077/7464], Loss: 3.4854\n",
      "Epoch [1/2], Step [1078/7464], Loss: 3.7183\n",
      "Epoch [1/2], Step [1079/7464], Loss: 3.4552\n",
      "Epoch [1/2], Step [1080/7464], Loss: 3.5439\n",
      "Epoch [1/2], Step [1081/7464], Loss: 3.8956\n",
      "Epoch [1/2], Step [1082/7464], Loss: 3.6867\n",
      "Epoch [1/2], Step [1083/7464], Loss: 3.4566\n",
      "Epoch [1/2], Step [1084/7464], Loss: 3.9164\n",
      "Epoch [1/2], Step [1085/7464], Loss: 3.9744\n",
      "Epoch [1/2], Step [1086/7464], Loss: 3.8766\n",
      "Epoch [1/2], Step [1087/7464], Loss: 3.7763\n",
      "Epoch [1/2], Step [1088/7464], Loss: 3.5155\n",
      "Epoch [1/2], Step [1089/7464], Loss: 4.0146\n",
      "Epoch [1/2], Step [1090/7464], Loss: 4.0306\n",
      "Epoch [1/2], Step [1091/7464], Loss: 4.0763\n",
      "Epoch [1/2], Step [1092/7464], Loss: 3.7133\n",
      "Epoch [1/2], Step [1093/7464], Loss: 3.8441\n",
      "Epoch [1/2], Step [1094/7464], Loss: 4.1241\n",
      "Epoch [1/2], Step [1095/7464], Loss: 3.8148\n",
      "Epoch [1/2], Step [1096/7464], Loss: 3.4515\n",
      "Epoch [1/2], Step [1097/7464], Loss: 3.8387\n",
      "Epoch [1/2], Step [1098/7464], Loss: 4.0026\n",
      "Epoch [1/2], Step [1099/7464], Loss: 3.7767\n",
      "Epoch [1/2], Step [1100/7464], Loss: 3.9025\n",
      "Epoch [1/2], Step [1101/7464], Loss: 3.8168\n",
      "Epoch [1/2], Step [1102/7464], Loss: 3.5694\n",
      "Epoch [1/2], Step [1103/7464], Loss: 3.6347\n",
      "Epoch [1/2], Step [1104/7464], Loss: 3.7936\n",
      "Epoch [1/2], Step [1105/7464], Loss: 3.7710\n",
      "Epoch [1/2], Step [1106/7464], Loss: 3.8737\n",
      "Epoch [1/2], Step [1107/7464], Loss: 3.7915\n",
      "Epoch [1/2], Step [1108/7464], Loss: 3.9537\n",
      "Epoch [1/2], Step [1109/7464], Loss: 3.5043\n",
      "Epoch [1/2], Step [1110/7464], Loss: 3.5125\n",
      "Epoch [1/2], Step [1111/7464], Loss: 4.1222\n",
      "Epoch [1/2], Step [1112/7464], Loss: 3.7565\n",
      "Epoch [1/2], Step [1113/7464], Loss: 3.5918\n",
      "Epoch [1/2], Step [1114/7464], Loss: 4.0927\n",
      "Epoch [1/2], Step [1115/7464], Loss: 4.0370\n",
      "Epoch [1/2], Step [1116/7464], Loss: 3.8676\n",
      "Epoch [1/2], Step [1117/7464], Loss: 3.6149\n",
      "Epoch [1/2], Step [1118/7464], Loss: 3.7720\n",
      "Epoch [1/2], Step [1119/7464], Loss: 4.5026\n",
      "Epoch [1/2], Step [1120/7464], Loss: 3.2214\n",
      "Epoch [1/2], Step [1121/7464], Loss: 3.8204\n",
      "Epoch [1/2], Step [1122/7464], Loss: 3.5928\n",
      "Epoch [1/2], Step [1123/7464], Loss: 3.6983\n",
      "Epoch [1/2], Step [1124/7464], Loss: 3.7617\n",
      "Epoch [1/2], Step [1125/7464], Loss: 4.0665\n",
      "Epoch [1/2], Step [1126/7464], Loss: 3.5940\n",
      "Epoch [1/2], Step [1127/7464], Loss: 3.7281\n",
      "Epoch [1/2], Step [1128/7464], Loss: 4.0687\n",
      "Epoch [1/2], Step [1129/7464], Loss: 3.9367\n",
      "Epoch [1/2], Step [1130/7464], Loss: 3.8852\n",
      "Epoch [1/2], Step [1131/7464], Loss: 3.7464\n",
      "Epoch [1/2], Step [1132/7464], Loss: 3.7799\n",
      "Epoch [1/2], Step [1133/7464], Loss: 3.5292\n",
      "Epoch [1/2], Step [1134/7464], Loss: 3.6524\n",
      "Epoch [1/2], Step [1135/7464], Loss: 3.9198\n",
      "Epoch [1/2], Step [1136/7464], Loss: 3.8216\n",
      "Epoch [1/2], Step [1137/7464], Loss: 3.5674\n",
      "Epoch [1/2], Step [1138/7464], Loss: 3.8848\n",
      "Epoch [1/2], Step [1139/7464], Loss: 3.6466\n",
      "Epoch [1/2], Step [1140/7464], Loss: 3.8372\n",
      "Epoch [1/2], Step [1141/7464], Loss: 3.6102\n",
      "Epoch [1/2], Step [1142/7464], Loss: 3.8536\n",
      "Epoch [1/2], Step [1143/7464], Loss: 3.8023\n",
      "Epoch [1/2], Step [1144/7464], Loss: 3.3083\n",
      "Epoch [1/2], Step [1145/7464], Loss: 3.7399\n",
      "Epoch [1/2], Step [1146/7464], Loss: 3.4912\n",
      "Epoch [1/2], Step [1147/7464], Loss: 3.7910\n",
      "Epoch [1/2], Step [1148/7464], Loss: 3.6692\n",
      "Epoch [1/2], Step [1149/7464], Loss: 3.8979\n",
      "Epoch [1/2], Step [1150/7464], Loss: 3.8931\n",
      "Epoch [1/2], Step [1151/7464], Loss: 4.1180\n",
      "Epoch [1/2], Step [1152/7464], Loss: 3.6027\n",
      "Epoch [1/2], Step [1153/7464], Loss: 3.2931\n",
      "Epoch [1/2], Step [1154/7464], Loss: 3.4559\n",
      "Epoch [1/2], Step [1155/7464], Loss: 4.1952\n",
      "Epoch [1/2], Step [1156/7464], Loss: 3.6832\n",
      "Epoch [1/2], Step [1157/7464], Loss: 3.0550\n",
      "Epoch [1/2], Step [1158/7464], Loss: 3.3819\n",
      "Epoch [1/2], Step [1159/7464], Loss: 4.0267\n",
      "Epoch [1/2], Step [1160/7464], Loss: 3.3215\n",
      "Epoch [1/2], Step [1161/7464], Loss: 3.8117\n",
      "Epoch [1/2], Step [1162/7464], Loss: 3.6993\n",
      "Epoch [1/2], Step [1163/7464], Loss: 3.9081\n",
      "Epoch [1/2], Step [1164/7464], Loss: 3.6158\n",
      "Epoch [1/2], Step [1165/7464], Loss: 3.9784\n",
      "Epoch [1/2], Step [1166/7464], Loss: 3.6773\n",
      "Epoch [1/2], Step [1167/7464], Loss: 3.9021\n",
      "Epoch [1/2], Step [1168/7464], Loss: 3.2413\n",
      "Epoch [1/2], Step [1169/7464], Loss: 3.1729\n",
      "Epoch [1/2], Step [1170/7464], Loss: 3.0153\n",
      "Epoch [1/2], Step [1171/7464], Loss: 3.3168\n",
      "Epoch [1/2], Step [1172/7464], Loss: 3.0958\n",
      "Epoch [1/2], Step [1173/7464], Loss: 3.6439\n",
      "Epoch [1/2], Step [1174/7464], Loss: 3.7944\n",
      "Epoch [1/2], Step [1175/7464], Loss: 3.9371\n",
      "Epoch [1/2], Step [1176/7464], Loss: 3.8634\n",
      "Epoch [1/2], Step [1177/7464], Loss: 3.3410\n",
      "Epoch [1/2], Step [1178/7464], Loss: 3.4897\n",
      "Epoch [1/2], Step [1179/7464], Loss: 3.9824\n",
      "Epoch [1/2], Step [1180/7464], Loss: 3.6928\n",
      "Epoch [1/2], Step [1181/7464], Loss: 3.4708\n",
      "Epoch [1/2], Step [1182/7464], Loss: 3.5663\n",
      "Epoch [1/2], Step [1183/7464], Loss: 4.1914\n",
      "Epoch [1/2], Step [1184/7464], Loss: 3.5481\n",
      "Epoch [1/2], Step [1185/7464], Loss: 4.0795\n",
      "Epoch [1/2], Step [1186/7464], Loss: 3.8303\n",
      "Epoch [1/2], Step [1187/7464], Loss: 3.6813\n",
      "Epoch [1/2], Step [1188/7464], Loss: 3.5210\n",
      "Epoch [1/2], Step [1189/7464], Loss: 3.8514\n",
      "Epoch [1/2], Step [1190/7464], Loss: 3.5719\n",
      "Epoch [1/2], Step [1191/7464], Loss: 3.8730\n",
      "Epoch [1/2], Step [1192/7464], Loss: 3.7826\n",
      "Epoch [1/2], Step [1193/7464], Loss: 4.1666\n",
      "Epoch [1/2], Step [1194/7464], Loss: 3.8402\n",
      "Epoch [1/2], Step [1195/7464], Loss: 3.3802\n",
      "Epoch [1/2], Step [1196/7464], Loss: 3.8922\n",
      "Epoch [1/2], Step [1197/7464], Loss: 3.4546\n",
      "Epoch [1/2], Step [1198/7464], Loss: 3.6470\n",
      "Epoch [1/2], Step [1199/7464], Loss: 3.6297\n",
      "Epoch [1/2], Step [1200/7464], Loss: 4.0674\n",
      "Epoch [1/2], Step [1201/7464], Loss: 3.6314\n",
      "Epoch [1/2], Step [1202/7464], Loss: 3.1758\n",
      "Epoch [1/2], Step [1203/7464], Loss: 3.7315\n",
      "Epoch [1/2], Step [1204/7464], Loss: 3.7147\n",
      "Epoch [1/2], Step [1205/7464], Loss: 3.7980\n",
      "Epoch [1/2], Step [1206/7464], Loss: 3.6222\n",
      "Epoch [1/2], Step [1207/7464], Loss: 4.2173\n",
      "Epoch [1/2], Step [1208/7464], Loss: 3.8859\n",
      "Epoch [1/2], Step [1209/7464], Loss: 3.7148\n",
      "Epoch [1/2], Step [1210/7464], Loss: 4.1653\n",
      "Epoch [1/2], Step [1211/7464], Loss: 4.2708\n",
      "Epoch [1/2], Step [1212/7464], Loss: 3.3253\n",
      "Epoch [1/2], Step [1213/7464], Loss: 3.6913\n",
      "Epoch [1/2], Step [1214/7464], Loss: 3.9545\n",
      "Epoch [1/2], Step [1215/7464], Loss: 3.4974\n",
      "Epoch [1/2], Step [1216/7464], Loss: 3.6565\n",
      "Epoch [1/2], Step [1217/7464], Loss: 3.3647\n",
      "Epoch [1/2], Step [1218/7464], Loss: 3.6682\n",
      "Epoch [1/2], Step [1219/7464], Loss: 3.3657\n",
      "Epoch [1/2], Step [1220/7464], Loss: 3.4111\n",
      "Epoch [1/2], Step [1221/7464], Loss: 3.6766\n",
      "Epoch [1/2], Step [1222/7464], Loss: 3.4825\n",
      "Epoch [1/2], Step [1223/7464], Loss: 3.9155\n",
      "Epoch [1/2], Step [1224/7464], Loss: 3.9353\n",
      "Epoch [1/2], Step [1225/7464], Loss: 3.5778\n",
      "Epoch [1/2], Step [1226/7464], Loss: 4.3047\n",
      "Epoch [1/2], Step [1227/7464], Loss: 3.6386\n",
      "Epoch [1/2], Step [1228/7464], Loss: 3.3272\n",
      "Epoch [1/2], Step [1229/7464], Loss: 3.5337\n",
      "Epoch [1/2], Step [1230/7464], Loss: 3.6324\n",
      "Epoch [1/2], Step [1231/7464], Loss: 3.5459\n",
      "Epoch [1/2], Step [1232/7464], Loss: 4.2620\n",
      "Epoch [1/2], Step [1233/7464], Loss: 3.9491\n",
      "Epoch [1/2], Step [1234/7464], Loss: 3.2056\n",
      "Epoch [1/2], Step [1235/7464], Loss: 3.8149\n",
      "Epoch [1/2], Step [1236/7464], Loss: 3.7617\n",
      "Epoch [1/2], Step [1237/7464], Loss: 3.6783\n",
      "Epoch [1/2], Step [1238/7464], Loss: 3.6306\n",
      "Epoch [1/2], Step [1239/7464], Loss: 3.4189\n",
      "Epoch [1/2], Step [1240/7464], Loss: 3.5930\n",
      "Epoch [1/2], Step [1241/7464], Loss: 4.0410\n",
      "Epoch [1/2], Step [1242/7464], Loss: 4.0411\n",
      "Epoch [1/2], Step [1243/7464], Loss: 4.1220\n",
      "Epoch [1/2], Step [1244/7464], Loss: 3.8361\n",
      "Epoch [1/2], Step [1245/7464], Loss: 3.9672\n",
      "Epoch [1/2], Step [1246/7464], Loss: 3.8361\n",
      "Epoch [1/2], Step [1247/7464], Loss: 3.8888\n",
      "Epoch [1/2], Step [1248/7464], Loss: 4.0073\n",
      "Epoch [1/2], Step [1249/7464], Loss: 3.7615\n",
      "Epoch [1/2], Step [1250/7464], Loss: 3.7640\n",
      "Epoch [1/2], Step [1251/7464], Loss: 3.9548\n",
      "Epoch [1/2], Step [1252/7464], Loss: 3.8551\n",
      "Epoch [1/2], Step [1253/7464], Loss: 3.5606\n",
      "Epoch [1/2], Step [1254/7464], Loss: 3.8063\n",
      "Epoch [1/2], Step [1255/7464], Loss: 3.3642\n",
      "Epoch [1/2], Step [1256/7464], Loss: 3.8550\n",
      "Epoch [1/2], Step [1257/7464], Loss: 3.7425\n",
      "Epoch [1/2], Step [1258/7464], Loss: 4.0368\n",
      "Epoch [1/2], Step [1259/7464], Loss: 3.7706\n",
      "Epoch [1/2], Step [1260/7464], Loss: 3.3892\n",
      "Epoch [1/2], Step [1261/7464], Loss: 3.3976\n",
      "Epoch [1/2], Step [1262/7464], Loss: 3.7048\n",
      "Epoch [1/2], Step [1263/7464], Loss: 3.8707\n",
      "Epoch [1/2], Step [1264/7464], Loss: 4.0570\n",
      "Epoch [1/2], Step [1265/7464], Loss: 3.6182\n",
      "Epoch [1/2], Step [1266/7464], Loss: 3.8631\n",
      "Epoch [1/2], Step [1267/7464], Loss: 3.5936\n",
      "Epoch [1/2], Step [1268/7464], Loss: 4.1029\n",
      "Epoch [1/2], Step [1269/7464], Loss: 3.8602\n",
      "Epoch [1/2], Step [1270/7464], Loss: 3.6696\n",
      "Epoch [1/2], Step [1271/7464], Loss: 3.8285\n",
      "Epoch [1/2], Step [1272/7464], Loss: 4.0585\n",
      "Epoch [1/2], Step [1273/7464], Loss: 3.6278\n",
      "Epoch [1/2], Step [1274/7464], Loss: 3.6496\n",
      "Epoch [1/2], Step [1275/7464], Loss: 3.4347\n",
      "Epoch [1/2], Step [1276/7464], Loss: 3.2239\n",
      "Epoch [1/2], Step [1277/7464], Loss: 3.7689\n",
      "Epoch [1/2], Step [1278/7464], Loss: 3.4822\n",
      "Epoch [1/2], Step [1279/7464], Loss: 3.3893\n",
      "Epoch [1/2], Step [1280/7464], Loss: 3.6386\n",
      "Epoch [1/2], Step [1281/7464], Loss: 3.5974\n",
      "Epoch [1/2], Step [1282/7464], Loss: 3.2040\n",
      "Epoch [1/2], Step [1283/7464], Loss: 3.6102\n",
      "Epoch [1/2], Step [1284/7464], Loss: 4.6564\n",
      "Epoch [1/2], Step [1285/7464], Loss: 4.0836\n",
      "Epoch [1/2], Step [1286/7464], Loss: 3.2290\n",
      "Epoch [1/2], Step [1287/7464], Loss: 4.1042\n",
      "Epoch [1/2], Step [1288/7464], Loss: 3.5798\n",
      "Epoch [1/2], Step [1289/7464], Loss: 3.5706\n",
      "Epoch [1/2], Step [1290/7464], Loss: 3.6872\n",
      "Epoch [1/2], Step [1291/7464], Loss: 4.2025\n",
      "Epoch [1/2], Step [1292/7464], Loss: 3.7850\n",
      "Epoch [1/2], Step [1293/7464], Loss: 3.7218\n",
      "Epoch [1/2], Step [1294/7464], Loss: 3.3971\n",
      "Epoch [1/2], Step [1295/7464], Loss: 3.6356\n",
      "Epoch [1/2], Step [1296/7464], Loss: 3.4838\n",
      "Epoch [1/2], Step [1297/7464], Loss: 3.3885\n",
      "Epoch [1/2], Step [1298/7464], Loss: 3.3539\n",
      "Epoch [1/2], Step [1299/7464], Loss: 3.7834\n",
      "Epoch [1/2], Step [1300/7464], Loss: 4.0017\n",
      "Epoch [1/2], Step [1301/7464], Loss: 3.6337\n",
      "Epoch [1/2], Step [1302/7464], Loss: 3.4621\n",
      "Epoch [1/2], Step [1303/7464], Loss: 3.6960\n",
      "Epoch [1/2], Step [1304/7464], Loss: 3.6317\n",
      "Epoch [1/2], Step [1305/7464], Loss: 3.9316\n",
      "Epoch [1/2], Step [1306/7464], Loss: 3.6559\n",
      "Epoch [1/2], Step [1307/7464], Loss: 4.3393\n",
      "Epoch [1/2], Step [1308/7464], Loss: 3.4751\n",
      "Epoch [1/2], Step [1309/7464], Loss: 3.7539\n",
      "Epoch [1/2], Step [1310/7464], Loss: 3.8220\n",
      "Epoch [1/2], Step [1311/7464], Loss: 3.4039\n",
      "Epoch [1/2], Step [1312/7464], Loss: 3.5888\n",
      "Epoch [1/2], Step [1313/7464], Loss: 3.5651\n",
      "Epoch [1/2], Step [1314/7464], Loss: 3.4857\n",
      "Epoch [1/2], Step [1315/7464], Loss: 3.9735\n",
      "Epoch [1/2], Step [1316/7464], Loss: 3.7548\n",
      "Epoch [1/2], Step [1317/7464], Loss: 3.5832\n",
      "Epoch [1/2], Step [1318/7464], Loss: 3.9508\n",
      "Epoch [1/2], Step [1319/7464], Loss: 3.8038\n",
      "Epoch [1/2], Step [1320/7464], Loss: 3.6939\n",
      "Epoch [1/2], Step [1321/7464], Loss: 3.3644\n",
      "Epoch [1/2], Step [1322/7464], Loss: 3.7494\n",
      "Epoch [1/2], Step [1323/7464], Loss: 3.8371\n",
      "Epoch [1/2], Step [1324/7464], Loss: 3.4226\n",
      "Epoch [1/2], Step [1325/7464], Loss: 3.9240\n",
      "Epoch [1/2], Step [1326/7464], Loss: 3.7744\n",
      "Epoch [1/2], Step [1327/7464], Loss: 3.6470\n",
      "Epoch [1/2], Step [1328/7464], Loss: 3.5236\n",
      "Epoch [1/2], Step [1329/7464], Loss: 3.5164\n",
      "Epoch [1/2], Step [1330/7464], Loss: 3.6577\n",
      "Epoch [1/2], Step [1331/7464], Loss: 3.6749\n",
      "Epoch [1/2], Step [1332/7464], Loss: 3.9901\n",
      "Epoch [1/2], Step [1333/7464], Loss: 3.8586\n",
      "Epoch [1/2], Step [1334/7464], Loss: 4.0500\n",
      "Epoch [1/2], Step [1335/7464], Loss: 4.1702\n",
      "Epoch [1/2], Step [1336/7464], Loss: 4.0383\n",
      "Epoch [1/2], Step [1337/7464], Loss: 3.5905\n",
      "Epoch [1/2], Step [1338/7464], Loss: 3.8751\n",
      "Epoch [1/2], Step [1339/7464], Loss: 3.7818\n",
      "Epoch [1/2], Step [1340/7464], Loss: 3.8132\n",
      "Epoch [1/2], Step [1341/7464], Loss: 4.0015\n",
      "Epoch [1/2], Step [1342/7464], Loss: 3.3594\n",
      "Epoch [1/2], Step [1343/7464], Loss: 3.5829\n",
      "Epoch [1/2], Step [1344/7464], Loss: 3.5708\n",
      "Epoch [1/2], Step [1345/7464], Loss: 3.1034\n",
      "Epoch [1/2], Step [1346/7464], Loss: 4.0606\n",
      "Epoch [1/2], Step [1347/7464], Loss: 3.8136\n",
      "Epoch [1/2], Step [1348/7464], Loss: 3.5264\n",
      "Epoch [1/2], Step [1349/7464], Loss: 3.7002\n",
      "Epoch [1/2], Step [1350/7464], Loss: 3.5184\n",
      "Epoch [1/2], Step [1351/7464], Loss: 3.5979\n",
      "Epoch [1/2], Step [1352/7464], Loss: 4.2760\n",
      "Epoch [1/2], Step [1353/7464], Loss: 3.9211\n",
      "Epoch [1/2], Step [1354/7464], Loss: 4.2342\n",
      "Epoch [1/2], Step [1355/7464], Loss: 3.9525\n",
      "Epoch [1/2], Step [1356/7464], Loss: 3.7632\n",
      "Epoch [1/2], Step [1357/7464], Loss: 3.8033\n",
      "Epoch [1/2], Step [1358/7464], Loss: 3.9387\n",
      "Epoch [1/2], Step [1359/7464], Loss: 3.3858\n",
      "Epoch [1/2], Step [1360/7464], Loss: 4.1769\n",
      "Epoch [1/2], Step [1361/7464], Loss: 3.6117\n",
      "Epoch [1/2], Step [1362/7464], Loss: 3.3613\n",
      "Epoch [1/2], Step [1363/7464], Loss: 3.8661\n",
      "Epoch [1/2], Step [1364/7464], Loss: 3.6300\n",
      "Epoch [1/2], Step [1365/7464], Loss: 3.7174\n",
      "Epoch [1/2], Step [1366/7464], Loss: 3.6830\n",
      "Epoch [1/2], Step [1367/7464], Loss: 3.8990\n",
      "Epoch [1/2], Step [1368/7464], Loss: 3.9579\n",
      "Epoch [1/2], Step [1369/7464], Loss: 3.4075\n",
      "Epoch [1/2], Step [1370/7464], Loss: 3.4562\n",
      "Epoch [1/2], Step [1371/7464], Loss: 4.1101\n",
      "Epoch [1/2], Step [1372/7464], Loss: 3.7931\n",
      "Epoch [1/2], Step [1373/7464], Loss: 3.6401\n",
      "Epoch [1/2], Step [1374/7464], Loss: 4.3787\n",
      "Epoch [1/2], Step [1375/7464], Loss: 3.2567\n",
      "Epoch [1/2], Step [1376/7464], Loss: 3.8278\n",
      "Epoch [1/2], Step [1377/7464], Loss: 3.3991\n",
      "Epoch [1/2], Step [1378/7464], Loss: 3.9992\n",
      "Epoch [1/2], Step [1379/7464], Loss: 3.8982\n",
      "Epoch [1/2], Step [1380/7464], Loss: 3.6112\n",
      "Epoch [1/2], Step [1381/7464], Loss: 4.0991\n",
      "Epoch [1/2], Step [1382/7464], Loss: 3.6689\n",
      "Epoch [1/2], Step [1383/7464], Loss: 4.2212\n",
      "Epoch [1/2], Step [1384/7464], Loss: 3.5918\n",
      "Epoch [1/2], Step [1385/7464], Loss: 3.8218\n",
      "Epoch [1/2], Step [1386/7464], Loss: 3.5547\n",
      "Epoch [1/2], Step [1387/7464], Loss: 3.3834\n",
      "Epoch [1/2], Step [1388/7464], Loss: 3.5817\n",
      "Epoch [1/2], Step [1389/7464], Loss: 3.8605\n",
      "Epoch [1/2], Step [1390/7464], Loss: 3.8004\n",
      "Epoch [1/2], Step [1391/7464], Loss: 3.3073\n",
      "Epoch [1/2], Step [1392/7464], Loss: 3.8725\n",
      "Epoch [1/2], Step [1393/7464], Loss: 3.5129\n",
      "Epoch [1/2], Step [1394/7464], Loss: 3.0534\n",
      "Epoch [1/2], Step [1395/7464], Loss: 3.2483\n",
      "Epoch [1/2], Step [1396/7464], Loss: 3.6795\n",
      "Epoch [1/2], Step [1397/7464], Loss: 4.1975\n",
      "Epoch [1/2], Step [1398/7464], Loss: 3.5622\n",
      "Epoch [1/2], Step [1399/7464], Loss: 3.0892\n",
      "Epoch [1/2], Step [1400/7464], Loss: 3.5746\n",
      "Epoch [1/2], Step [1401/7464], Loss: 3.7899\n",
      "Epoch [1/2], Step [1402/7464], Loss: 3.4444\n",
      "Epoch [1/2], Step [1403/7464], Loss: 4.0390\n",
      "Epoch [1/2], Step [1404/7464], Loss: 3.1997\n",
      "Epoch [1/2], Step [1405/7464], Loss: 4.0445\n",
      "Epoch [1/2], Step [1406/7464], Loss: 3.7662\n",
      "Epoch [1/2], Step [1407/7464], Loss: 3.4390\n",
      "Epoch [1/2], Step [1408/7464], Loss: 2.4575\n",
      "Epoch [1/2], Step [1409/7464], Loss: 3.6653\n",
      "Epoch [1/2], Step [1410/7464], Loss: 3.6719\n",
      "Epoch [1/2], Step [1411/7464], Loss: 3.7212\n",
      "Epoch [1/2], Step [1412/7464], Loss: 3.4595\n",
      "Epoch [1/2], Step [1413/7464], Loss: 3.7073\n",
      "Epoch [1/2], Step [1414/7464], Loss: 4.0722\n",
      "Epoch [1/2], Step [1415/7464], Loss: 3.4814\n",
      "Epoch [1/2], Step [1416/7464], Loss: 3.1027\n",
      "Epoch [1/2], Step [1417/7464], Loss: 3.3854\n",
      "Epoch [1/2], Step [1418/7464], Loss: 4.2939\n",
      "Epoch [1/2], Step [1419/7464], Loss: 3.4008\n",
      "Epoch [1/2], Step [1420/7464], Loss: 4.1690\n",
      "Epoch [1/2], Step [1421/7464], Loss: 3.1453\n",
      "Epoch [1/2], Step [1422/7464], Loss: 3.5286\n",
      "Epoch [1/2], Step [1423/7464], Loss: 4.0298\n",
      "Epoch [1/2], Step [1424/7464], Loss: 3.6469\n",
      "Epoch [1/2], Step [1425/7464], Loss: 3.4469\n",
      "Epoch [1/2], Step [1426/7464], Loss: 3.2423\n",
      "Epoch [1/2], Step [1427/7464], Loss: 4.2104\n",
      "Epoch [1/2], Step [1428/7464], Loss: 3.6667\n",
      "Epoch [1/2], Step [1429/7464], Loss: 3.7870\n",
      "Epoch [1/2], Step [1430/7464], Loss: 4.3692\n",
      "Epoch [1/2], Step [1431/7464], Loss: 3.4871\n",
      "Epoch [1/2], Step [1432/7464], Loss: 3.3022\n",
      "Epoch [1/2], Step [1433/7464], Loss: 3.6475\n",
      "Epoch [1/2], Step [1434/7464], Loss: 3.9214\n",
      "Epoch [1/2], Step [1435/7464], Loss: 4.4497\n",
      "Epoch [1/2], Step [1436/7464], Loss: 3.1942\n",
      "Epoch [1/2], Step [1437/7464], Loss: 3.8256\n",
      "Epoch [1/2], Step [1438/7464], Loss: 3.9699\n",
      "Epoch [1/2], Step [1439/7464], Loss: 3.5099\n",
      "Epoch [1/2], Step [1440/7464], Loss: 3.7281\n",
      "Epoch [1/2], Step [1441/7464], Loss: 3.4262\n",
      "Epoch [1/2], Step [1442/7464], Loss: 3.9006\n",
      "Epoch [1/2], Step [1443/7464], Loss: 3.3810\n",
      "Epoch [1/2], Step [1444/7464], Loss: 3.6827\n",
      "Epoch [1/2], Step [1445/7464], Loss: 3.4266\n",
      "Epoch [1/2], Step [1446/7464], Loss: 3.5965\n",
      "Epoch [1/2], Step [1447/7464], Loss: 3.6991\n",
      "Epoch [1/2], Step [1448/7464], Loss: 3.4575\n",
      "Epoch [1/2], Step [1449/7464], Loss: 3.7052\n",
      "Epoch [1/2], Step [1450/7464], Loss: 4.0253\n",
      "Epoch [1/2], Step [1451/7464], Loss: 3.8016\n",
      "Epoch [1/2], Step [1452/7464], Loss: 3.3583\n",
      "Epoch [1/2], Step [1453/7464], Loss: 3.2295\n",
      "Epoch [1/2], Step [1454/7464], Loss: 3.3381\n",
      "Epoch [1/2], Step [1455/7464], Loss: 3.8305\n",
      "Epoch [1/2], Step [1456/7464], Loss: 3.7377\n",
      "Epoch [1/2], Step [1457/7464], Loss: 3.5184\n",
      "Epoch [1/2], Step [1458/7464], Loss: 3.4189\n",
      "Epoch [1/2], Step [1459/7464], Loss: 3.9487\n",
      "Epoch [1/2], Step [1460/7464], Loss: 3.6028\n",
      "Epoch [1/2], Step [1461/7464], Loss: 3.6785\n",
      "Epoch [1/2], Step [1462/7464], Loss: 3.6312\n",
      "Epoch [1/2], Step [1463/7464], Loss: 3.5624\n",
      "Epoch [1/2], Step [1464/7464], Loss: 4.2560\n",
      "Epoch [1/2], Step [1465/7464], Loss: 3.5949\n",
      "Epoch [1/2], Step [1466/7464], Loss: 3.9259\n",
      "Epoch [1/2], Step [1467/7464], Loss: 3.6646\n",
      "Epoch [1/2], Step [1468/7464], Loss: 3.4655\n",
      "Epoch [1/2], Step [1469/7464], Loss: 3.4713\n",
      "Epoch [1/2], Step [1470/7464], Loss: 4.6352\n",
      "Epoch [1/2], Step [1471/7464], Loss: 3.8117\n",
      "Epoch [1/2], Step [1472/7464], Loss: 3.4760\n",
      "Epoch [1/2], Step [1473/7464], Loss: 3.8852\n",
      "Epoch [1/2], Step [1474/7464], Loss: 3.4666\n",
      "Epoch [1/2], Step [1475/7464], Loss: 3.3704\n",
      "Epoch [1/2], Step [1476/7464], Loss: 4.0694\n",
      "Epoch [1/2], Step [1477/7464], Loss: 4.0883\n",
      "Epoch [1/2], Step [1478/7464], Loss: 3.5983\n",
      "Epoch [1/2], Step [1479/7464], Loss: 3.6445\n",
      "Epoch [1/2], Step [1480/7464], Loss: 3.7033\n",
      "Epoch [1/2], Step [1481/7464], Loss: 3.8433\n",
      "Epoch [1/2], Step [1482/7464], Loss: 3.5681\n",
      "Epoch [1/2], Step [1483/7464], Loss: 3.8575\n",
      "Epoch [1/2], Step [1484/7464], Loss: 4.4923\n",
      "Epoch [1/2], Step [1485/7464], Loss: 3.3202\n",
      "Epoch [1/2], Step [1486/7464], Loss: 3.5759\n",
      "Epoch [1/2], Step [1487/7464], Loss: 3.7070\n",
      "Epoch [1/2], Step [1488/7464], Loss: 4.3686\n",
      "Epoch [1/2], Step [1489/7464], Loss: 3.4592\n",
      "Epoch [1/2], Step [1490/7464], Loss: 3.6177\n",
      "Epoch [1/2], Step [1491/7464], Loss: 3.9438\n",
      "Epoch [1/2], Step [1492/7464], Loss: 3.8292\n",
      "Epoch [1/2], Step [1493/7464], Loss: 3.5577\n",
      "Epoch [1/2], Step [1494/7464], Loss: 3.8117\n",
      "Epoch [1/2], Step [1495/7464], Loss: 3.2134\n",
      "Epoch [1/2], Step [1496/7464], Loss: 3.4560\n",
      "Epoch [1/2], Step [1497/7464], Loss: 3.7801\n",
      "Epoch [1/2], Step [1498/7464], Loss: 3.3364\n",
      "Epoch [1/2], Step [1499/7464], Loss: 3.7680\n",
      "Epoch [1/2], Step [1500/7464], Loss: 3.5925\n",
      "Epoch [1/2], Step [1501/7464], Loss: 3.5851\n",
      "Epoch [1/2], Step [1502/7464], Loss: 3.5579\n",
      "Epoch [1/2], Step [1503/7464], Loss: 3.8327\n",
      "Epoch [1/2], Step [1504/7464], Loss: 4.1982\n",
      "Epoch [1/2], Step [1505/7464], Loss: 4.0796\n",
      "Epoch [1/2], Step [1506/7464], Loss: 3.8446\n",
      "Epoch [1/2], Step [1507/7464], Loss: 3.7795\n",
      "Epoch [1/2], Step [1508/7464], Loss: 3.5564\n",
      "Epoch [1/2], Step [1509/7464], Loss: 3.4956\n",
      "Epoch [1/2], Step [1510/7464], Loss: 3.8226\n",
      "Epoch [1/2], Step [1511/7464], Loss: 4.4368\n",
      "Epoch [1/2], Step [1512/7464], Loss: 3.6913\n",
      "Epoch [1/2], Step [1513/7464], Loss: 3.8541\n",
      "Epoch [1/2], Step [1514/7464], Loss: 3.8756\n",
      "Epoch [1/2], Step [1515/7464], Loss: 3.6552\n",
      "Epoch [1/2], Step [1516/7464], Loss: 3.9734\n",
      "Epoch [1/2], Step [1517/7464], Loss: 3.6473\n",
      "Epoch [1/2], Step [1518/7464], Loss: 3.6067\n",
      "Epoch [1/2], Step [1519/7464], Loss: 3.6460\n",
      "Epoch [1/2], Step [1520/7464], Loss: 3.9124\n",
      "Epoch [1/2], Step [1521/7464], Loss: 4.1315\n",
      "Epoch [1/2], Step [1522/7464], Loss: 3.5642\n",
      "Epoch [1/2], Step [1523/7464], Loss: 3.4706\n",
      "Epoch [1/2], Step [1524/7464], Loss: 3.7909\n",
      "Epoch [1/2], Step [1525/7464], Loss: 4.0590\n",
      "Epoch [1/2], Step [1526/7464], Loss: 3.7935\n",
      "Epoch [1/2], Step [1527/7464], Loss: 3.4740\n",
      "Epoch [1/2], Step [1528/7464], Loss: 3.6761\n",
      "Epoch [1/2], Step [1529/7464], Loss: 3.6672\n",
      "Epoch [1/2], Step [1530/7464], Loss: 3.2854\n",
      "Epoch [1/2], Step [1531/7464], Loss: 3.6265\n",
      "Epoch [1/2], Step [1532/7464], Loss: 3.7113\n",
      "Epoch [1/2], Step [1533/7464], Loss: 4.0662\n",
      "Epoch [1/2], Step [1534/7464], Loss: 3.6929\n",
      "Epoch [1/2], Step [1535/7464], Loss: 3.6788\n",
      "Epoch [1/2], Step [1536/7464], Loss: 3.6958\n",
      "Epoch [1/2], Step [1537/7464], Loss: 3.6769\n",
      "Epoch [1/2], Step [1538/7464], Loss: 3.8097\n",
      "Epoch [1/2], Step [1539/7464], Loss: 3.7786\n",
      "Epoch [1/2], Step [1540/7464], Loss: 3.7246\n",
      "Epoch [1/2], Step [1541/7464], Loss: 3.6293\n",
      "Epoch [1/2], Step [1542/7464], Loss: 3.7846\n",
      "Epoch [1/2], Step [1543/7464], Loss: 3.7595\n",
      "Epoch [1/2], Step [1544/7464], Loss: 3.8850\n",
      "Epoch [1/2], Step [1545/7464], Loss: 3.9541\n",
      "Epoch [1/2], Step [1546/7464], Loss: 3.5132\n",
      "Epoch [1/2], Step [1547/7464], Loss: 3.5392\n",
      "Epoch [1/2], Step [1548/7464], Loss: 3.2665\n",
      "Epoch [1/2], Step [1549/7464], Loss: 3.7686\n",
      "Epoch [1/2], Step [1550/7464], Loss: 3.3589\n",
      "Epoch [1/2], Step [1551/7464], Loss: 3.1458\n",
      "Epoch [1/2], Step [1552/7464], Loss: 4.2509\n",
      "Epoch [1/2], Step [1553/7464], Loss: 3.6733\n",
      "Epoch [1/2], Step [1554/7464], Loss: 3.4793\n",
      "Epoch [1/2], Step [1555/7464], Loss: 4.1896\n",
      "Epoch [1/2], Step [1556/7464], Loss: 3.3499\n",
      "Epoch [1/2], Step [1557/7464], Loss: 3.4779\n",
      "Epoch [1/2], Step [1558/7464], Loss: 4.3530\n",
      "Epoch [1/2], Step [1559/7464], Loss: 4.0177\n",
      "Epoch [1/2], Step [1560/7464], Loss: 3.3523\n",
      "Epoch [1/2], Step [1561/7464], Loss: 3.5345\n",
      "Epoch [1/2], Step [1562/7464], Loss: 4.1836\n",
      "Epoch [1/2], Step [1563/7464], Loss: 4.0132\n",
      "Epoch [1/2], Step [1564/7464], Loss: 3.7590\n",
      "Epoch [1/2], Step [1565/7464], Loss: 3.5758\n",
      "Epoch [1/2], Step [1566/7464], Loss: 3.6049\n",
      "Epoch [1/2], Step [1567/7464], Loss: 3.4091\n",
      "Epoch [1/2], Step [1568/7464], Loss: 3.5298\n",
      "Epoch [1/2], Step [1569/7464], Loss: 3.9284\n",
      "Epoch [1/2], Step [1570/7464], Loss: 3.3372\n",
      "Epoch [1/2], Step [1571/7464], Loss: 3.4196\n",
      "Epoch [1/2], Step [1572/7464], Loss: 3.7563\n",
      "Epoch [1/2], Step [1573/7464], Loss: 4.2006\n",
      "Epoch [1/2], Step [1574/7464], Loss: 4.1845\n",
      "Epoch [1/2], Step [1575/7464], Loss: 3.4801\n",
      "Epoch [1/2], Step [1576/7464], Loss: 3.9199\n",
      "Epoch [1/2], Step [1577/7464], Loss: 3.5357\n",
      "Epoch [1/2], Step [1578/7464], Loss: 3.7238\n",
      "Epoch [1/2], Step [1579/7464], Loss: 3.7617\n",
      "Epoch [1/2], Step [1580/7464], Loss: 3.8366\n",
      "Epoch [1/2], Step [1581/7464], Loss: 3.8953\n",
      "Epoch [1/2], Step [1582/7464], Loss: 3.8376\n",
      "Epoch [1/2], Step [1583/7464], Loss: 3.4541\n",
      "Epoch [1/2], Step [1584/7464], Loss: 3.5711\n",
      "Epoch [1/2], Step [1585/7464], Loss: 3.4649\n",
      "Epoch [1/2], Step [1586/7464], Loss: 3.5032\n",
      "Epoch [1/2], Step [1587/7464], Loss: 3.5945\n",
      "Epoch [1/2], Step [1588/7464], Loss: 3.2224\n",
      "Epoch [1/2], Step [1589/7464], Loss: 3.0973\n",
      "Epoch [1/2], Step [1590/7464], Loss: 3.4821\n",
      "Epoch [1/2], Step [1591/7464], Loss: 4.1980\n",
      "Epoch [1/2], Step [1592/7464], Loss: 3.1852\n",
      "Epoch [1/2], Step [1593/7464], Loss: 4.0581\n",
      "Epoch [1/2], Step [1594/7464], Loss: 3.2830\n",
      "Epoch [1/2], Step [1595/7464], Loss: 4.1038\n",
      "Epoch [1/2], Step [1596/7464], Loss: 3.3571\n",
      "Epoch [1/2], Step [1597/7464], Loss: 3.4488\n",
      "Epoch [1/2], Step [1598/7464], Loss: 4.4932\n",
      "Epoch [1/2], Step [1599/7464], Loss: 4.4969\n",
      "Epoch [1/2], Step [1600/7464], Loss: 3.3294\n",
      "Epoch [1/2], Step [1601/7464], Loss: 3.7619\n",
      "Epoch [1/2], Step [1602/7464], Loss: 4.4541\n",
      "Epoch [1/2], Step [1603/7464], Loss: 3.4920\n",
      "Epoch [1/2], Step [1604/7464], Loss: 3.2963\n",
      "Epoch [1/2], Step [1605/7464], Loss: 4.1378\n",
      "Epoch [1/2], Step [1606/7464], Loss: 3.5526\n",
      "Epoch [1/2], Step [1607/7464], Loss: 3.4811\n",
      "Epoch [1/2], Step [1608/7464], Loss: 3.9868\n",
      "Epoch [1/2], Step [1609/7464], Loss: 3.5675\n",
      "Epoch [1/2], Step [1610/7464], Loss: 4.1828\n",
      "Epoch [1/2], Step [1611/7464], Loss: 3.7793\n",
      "Epoch [1/2], Step [1612/7464], Loss: 3.9801\n",
      "Epoch [1/2], Step [1613/7464], Loss: 3.3522\n",
      "Epoch [1/2], Step [1614/7464], Loss: 3.2646\n",
      "Epoch [1/2], Step [1615/7464], Loss: 3.8648\n",
      "Epoch [1/2], Step [1616/7464], Loss: 3.3184\n",
      "Epoch [1/2], Step [1617/7464], Loss: 3.8955\n",
      "Epoch [1/2], Step [1618/7464], Loss: 3.6250\n",
      "Epoch [1/2], Step [1619/7464], Loss: 3.6912\n",
      "Epoch [1/2], Step [1620/7464], Loss: 3.5331\n",
      "Epoch [1/2], Step [1621/7464], Loss: 3.1749\n",
      "Epoch [1/2], Step [1622/7464], Loss: 3.4609\n",
      "Epoch [1/2], Step [1623/7464], Loss: 3.6774\n",
      "Epoch [1/2], Step [1624/7464], Loss: 3.2688\n",
      "Epoch [1/2], Step [1625/7464], Loss: 3.6921\n",
      "Epoch [1/2], Step [1626/7464], Loss: 3.3395\n",
      "Epoch [1/2], Step [1627/7464], Loss: 3.0602\n",
      "Epoch [1/2], Step [1628/7464], Loss: 3.5565\n",
      "Epoch [1/2], Step [1629/7464], Loss: 3.7497\n",
      "Epoch [1/2], Step [1630/7464], Loss: 4.6603\n",
      "Epoch [1/2], Step [1631/7464], Loss: 3.7843\n",
      "Epoch [1/2], Step [1632/7464], Loss: 4.2533\n",
      "Epoch [1/2], Step [1633/7464], Loss: 3.7156\n",
      "Epoch [1/2], Step [1634/7464], Loss: 3.7878\n",
      "Epoch [1/2], Step [1635/7464], Loss: 2.9607\n",
      "Epoch [1/2], Step [1636/7464], Loss: 3.6634\n",
      "Epoch [1/2], Step [1637/7464], Loss: 3.5154\n",
      "Epoch [1/2], Step [1638/7464], Loss: 4.3968\n",
      "Epoch [1/2], Step [1639/7464], Loss: 4.0828\n",
      "Epoch [1/2], Step [1640/7464], Loss: 3.7917\n",
      "Epoch [1/2], Step [1641/7464], Loss: 4.1144\n",
      "Epoch [1/2], Step [1642/7464], Loss: 3.6778\n",
      "Epoch [1/2], Step [1643/7464], Loss: 3.3326\n",
      "Epoch [1/2], Step [1644/7464], Loss: 3.1296\n",
      "Epoch [1/2], Step [1645/7464], Loss: 3.3535\n",
      "Epoch [1/2], Step [1646/7464], Loss: 3.8898\n",
      "Epoch [1/2], Step [1647/7464], Loss: 3.5264\n",
      "Epoch [1/2], Step [1648/7464], Loss: 3.9243\n",
      "Epoch [1/2], Step [1649/7464], Loss: 3.3289\n",
      "Epoch [1/2], Step [1650/7464], Loss: 3.6374\n",
      "Epoch [1/2], Step [1651/7464], Loss: 3.7941\n",
      "Epoch [1/2], Step [1652/7464], Loss: 3.3385\n",
      "Epoch [1/2], Step [1653/7464], Loss: 3.7831\n",
      "Epoch [1/2], Step [1654/7464], Loss: 3.7941\n",
      "Epoch [1/2], Step [1655/7464], Loss: 3.1729\n",
      "Epoch [1/2], Step [1656/7464], Loss: 3.7536\n",
      "Epoch [1/2], Step [1657/7464], Loss: 4.0886\n",
      "Epoch [1/2], Step [1658/7464], Loss: 3.8007\n",
      "Epoch [1/2], Step [1659/7464], Loss: 3.7355\n",
      "Epoch [1/2], Step [1660/7464], Loss: 3.4033\n",
      "Epoch [1/2], Step [1661/7464], Loss: 3.3425\n",
      "Epoch [1/2], Step [1662/7464], Loss: 3.4078\n",
      "Epoch [1/2], Step [1663/7464], Loss: 3.3781\n",
      "Epoch [1/2], Step [1664/7464], Loss: 3.8808\n",
      "Epoch [1/2], Step [1665/7464], Loss: 3.4634\n",
      "Epoch [1/2], Step [1666/7464], Loss: 2.9690\n",
      "Epoch [1/2], Step [1667/7464], Loss: 3.8073\n",
      "Epoch [1/2], Step [1668/7464], Loss: 3.6987\n",
      "Epoch [1/2], Step [1669/7464], Loss: 3.3617\n",
      "Epoch [1/2], Step [1670/7464], Loss: 3.7334\n",
      "Epoch [1/2], Step [1671/7464], Loss: 3.7948\n",
      "Epoch [1/2], Step [1672/7464], Loss: 3.9961\n",
      "Epoch [1/2], Step [1673/7464], Loss: 3.8404\n",
      "Epoch [1/2], Step [1674/7464], Loss: 3.5186\n",
      "Epoch [1/2], Step [1675/7464], Loss: 4.2316\n",
      "Epoch [1/2], Step [1676/7464], Loss: 3.5304\n",
      "Epoch [1/2], Step [1677/7464], Loss: 3.7182\n",
      "Epoch [1/2], Step [1678/7464], Loss: 3.9213\n",
      "Epoch [1/2], Step [1679/7464], Loss: 3.1132\n",
      "Epoch [1/2], Step [1680/7464], Loss: 4.2084\n",
      "Epoch [1/2], Step [1681/7464], Loss: 3.9488\n",
      "Epoch [1/2], Step [1682/7464], Loss: 3.6645\n",
      "Epoch [1/2], Step [1683/7464], Loss: 2.5873\n",
      "Epoch [1/2], Step [1684/7464], Loss: 3.6774\n",
      "Epoch [1/2], Step [1685/7464], Loss: 3.4700\n",
      "Epoch [1/2], Step [1686/7464], Loss: 3.5148\n",
      "Epoch [1/2], Step [1687/7464], Loss: 3.0554\n",
      "Epoch [1/2], Step [1688/7464], Loss: 3.7884\n",
      "Epoch [1/2], Step [1689/7464], Loss: 3.7960\n",
      "Epoch [1/2], Step [1690/7464], Loss: 3.4098\n",
      "Epoch [1/2], Step [1691/7464], Loss: 4.2479\n",
      "Epoch [1/2], Step [1692/7464], Loss: 4.1533\n",
      "Epoch [1/2], Step [1693/7464], Loss: 3.5103\n",
      "Epoch [1/2], Step [1694/7464], Loss: 3.6968\n",
      "Epoch [1/2], Step [1695/7464], Loss: 3.9011\n",
      "Epoch [1/2], Step [1696/7464], Loss: 3.8286\n",
      "Epoch [1/2], Step [1697/7464], Loss: 3.5781\n",
      "Epoch [1/2], Step [1698/7464], Loss: 3.5648\n",
      "Epoch [1/2], Step [1699/7464], Loss: 3.8257\n",
      "Epoch [1/2], Step [1700/7464], Loss: 4.1264\n",
      "Epoch [1/2], Step [1701/7464], Loss: 3.6711\n",
      "Epoch [1/2], Step [1702/7464], Loss: 3.7361\n",
      "Epoch [1/2], Step [1703/7464], Loss: 3.3174\n",
      "Epoch [1/2], Step [1704/7464], Loss: 3.6845\n",
      "Epoch [1/2], Step [1705/7464], Loss: 3.9322\n",
      "Epoch [1/2], Step [1706/7464], Loss: 3.3921\n",
      "Epoch [1/2], Step [1707/7464], Loss: 3.5443\n",
      "Epoch [1/2], Step [1708/7464], Loss: 3.5069\n",
      "Epoch [1/2], Step [1709/7464], Loss: 3.7108\n",
      "Epoch [1/2], Step [1710/7464], Loss: 3.4226\n",
      "Epoch [1/2], Step [1711/7464], Loss: 3.8322\n",
      "Epoch [1/2], Step [1712/7464], Loss: 3.7819\n",
      "Epoch [1/2], Step [1713/7464], Loss: 3.6480\n",
      "Epoch [1/2], Step [1714/7464], Loss: 3.6101\n",
      "Epoch [1/2], Step [1715/7464], Loss: 3.8208\n",
      "Epoch [1/2], Step [1716/7464], Loss: 3.9377\n",
      "Epoch [1/2], Step [1717/7464], Loss: 3.4520\n",
      "Epoch [1/2], Step [1718/7464], Loss: 4.1493\n",
      "Epoch [1/2], Step [1719/7464], Loss: 4.4161\n",
      "Epoch [1/2], Step [1720/7464], Loss: 3.2529\n",
      "Epoch [1/2], Step [1721/7464], Loss: 3.7482\n",
      "Epoch [1/2], Step [1722/7464], Loss: 3.7612\n",
      "Epoch [1/2], Step [1723/7464], Loss: 3.6594\n",
      "Epoch [1/2], Step [1724/7464], Loss: 3.7074\n",
      "Epoch [1/2], Step [1725/7464], Loss: 4.0174\n",
      "Epoch [1/2], Step [1726/7464], Loss: 3.4504\n",
      "Epoch [1/2], Step [1727/7464], Loss: 3.6757\n",
      "Epoch [1/2], Step [1728/7464], Loss: 4.0803\n",
      "Epoch [1/2], Step [1729/7464], Loss: 3.2625\n",
      "Epoch [1/2], Step [1730/7464], Loss: 3.6407\n",
      "Epoch [1/2], Step [1731/7464], Loss: 3.9803\n",
      "Epoch [1/2], Step [1732/7464], Loss: 3.4230\n",
      "Epoch [1/2], Step [1733/7464], Loss: 3.6909\n",
      "Epoch [1/2], Step [1734/7464], Loss: 3.2578\n",
      "Epoch [1/2], Step [1735/7464], Loss: 3.6484\n",
      "Epoch [1/2], Step [1736/7464], Loss: 3.2747\n",
      "Epoch [1/2], Step [1737/7464], Loss: 3.2493\n",
      "Epoch [1/2], Step [1738/7464], Loss: 3.5198\n",
      "Epoch [1/2], Step [1739/7464], Loss: 3.0336\n",
      "Epoch [1/2], Step [1740/7464], Loss: 3.7320\n",
      "Epoch [1/2], Step [1741/7464], Loss: 3.5265\n",
      "Epoch [1/2], Step [1742/7464], Loss: 3.6559\n",
      "Epoch [1/2], Step [1743/7464], Loss: 3.1641\n",
      "Epoch [1/2], Step [1744/7464], Loss: 3.1795\n",
      "Epoch [1/2], Step [1745/7464], Loss: 3.6642\n",
      "Epoch [1/2], Step [1746/7464], Loss: 3.5394\n",
      "Epoch [1/2], Step [1747/7464], Loss: 3.5449\n",
      "Epoch [1/2], Step [1748/7464], Loss: 3.1384\n",
      "Epoch [1/2], Step [1749/7464], Loss: 3.6739\n",
      "Epoch [1/2], Step [1750/7464], Loss: 3.5105\n",
      "Epoch [1/2], Step [1751/7464], Loss: 2.3157\n",
      "Epoch [1/2], Step [1752/7464], Loss: 4.1115\n",
      "Epoch [1/2], Step [1753/7464], Loss: 4.4569\n",
      "Epoch [1/2], Step [1754/7464], Loss: 3.7425\n",
      "Epoch [1/2], Step [1755/7464], Loss: 3.6298\n",
      "Epoch [1/2], Step [1756/7464], Loss: 3.9435\n",
      "Epoch [1/2], Step [1757/7464], Loss: 3.6893\n",
      "Epoch [1/2], Step [1758/7464], Loss: 3.9155\n",
      "Epoch [1/2], Step [1759/7464], Loss: 3.7611\n",
      "Epoch [1/2], Step [1760/7464], Loss: 3.6498\n",
      "Epoch [1/2], Step [1761/7464], Loss: 3.8502\n",
      "Epoch [1/2], Step [1762/7464], Loss: 3.4996\n",
      "Epoch [1/2], Step [1763/7464], Loss: 3.4411\n",
      "Epoch [1/2], Step [1764/7464], Loss: 4.1713\n",
      "Epoch [1/2], Step [1765/7464], Loss: 3.7148\n",
      "Epoch [1/2], Step [1766/7464], Loss: 3.8795\n",
      "Epoch [1/2], Step [1767/7464], Loss: 3.9995\n",
      "Epoch [1/2], Step [1768/7464], Loss: 3.3214\n",
      "Epoch [1/2], Step [1769/7464], Loss: 3.7979\n",
      "Epoch [1/2], Step [1770/7464], Loss: 3.9601\n",
      "Epoch [1/2], Step [1771/7464], Loss: 3.6244\n",
      "Epoch [1/2], Step [1772/7464], Loss: 4.0478\n",
      "Epoch [1/2], Step [1773/7464], Loss: 3.9067\n",
      "Epoch [1/2], Step [1774/7464], Loss: 3.5899\n",
      "Epoch [1/2], Step [1775/7464], Loss: 3.8203\n",
      "Epoch [1/2], Step [1776/7464], Loss: 3.6758\n",
      "Epoch [1/2], Step [1777/7464], Loss: 3.7110\n",
      "Epoch [1/2], Step [1778/7464], Loss: 3.2031\n",
      "Epoch [1/2], Step [1779/7464], Loss: 3.4294\n",
      "Epoch [1/2], Step [1780/7464], Loss: 2.9074\n",
      "Epoch [1/2], Step [1781/7464], Loss: 3.8031\n",
      "Epoch [1/2], Step [1782/7464], Loss: 3.2441\n",
      "Epoch [1/2], Step [1783/7464], Loss: 4.2486\n",
      "Epoch [1/2], Step [1784/7464], Loss: 3.4641\n",
      "Epoch [1/2], Step [1785/7464], Loss: 3.0026\n",
      "Epoch [1/2], Step [1786/7464], Loss: 3.9237\n",
      "Epoch [1/2], Step [1787/7464], Loss: 3.7619\n",
      "Epoch [1/2], Step [1788/7464], Loss: 4.3874\n",
      "Epoch [1/2], Step [1789/7464], Loss: 4.2484\n",
      "Epoch [1/2], Step [1790/7464], Loss: 3.9414\n",
      "Epoch [1/2], Step [1791/7464], Loss: 3.0363\n",
      "Epoch [1/2], Step [1792/7464], Loss: 3.5236\n",
      "Epoch [1/2], Step [1793/7464], Loss: 4.2589\n",
      "Epoch [1/2], Step [1794/7464], Loss: 3.2973\n",
      "Epoch [1/2], Step [1795/7464], Loss: 3.4822\n",
      "Epoch [1/2], Step [1796/7464], Loss: 3.6204\n",
      "Epoch [1/2], Step [1797/7464], Loss: 3.8191\n",
      "Epoch [1/2], Step [1798/7464], Loss: 3.6845\n",
      "Epoch [1/2], Step [1799/7464], Loss: 3.2917\n",
      "Epoch [1/2], Step [1800/7464], Loss: 3.8162\n",
      "Epoch [1/2], Step [1801/7464], Loss: 3.6821\n",
      "Epoch [1/2], Step [1802/7464], Loss: 3.5231\n",
      "Epoch [1/2], Step [1803/7464], Loss: 3.3991\n",
      "Epoch [1/2], Step [1804/7464], Loss: 4.3309\n",
      "Epoch [1/2], Step [1805/7464], Loss: 3.8069\n",
      "Epoch [1/2], Step [1806/7464], Loss: 3.4903\n",
      "Epoch [1/2], Step [1807/7464], Loss: 3.8448\n",
      "Epoch [1/2], Step [1808/7464], Loss: 3.2210\n",
      "Epoch [1/2], Step [1809/7464], Loss: 3.6763\n",
      "Epoch [1/2], Step [1810/7464], Loss: 3.5420\n",
      "Epoch [1/2], Step [1811/7464], Loss: 3.2844\n",
      "Epoch [1/2], Step [1812/7464], Loss: 3.4604\n",
      "Epoch [1/2], Step [1813/7464], Loss: 3.7014\n",
      "Epoch [1/2], Step [1814/7464], Loss: 4.0205\n",
      "Epoch [1/2], Step [1815/7464], Loss: 3.3309\n",
      "Epoch [1/2], Step [1816/7464], Loss: 3.4446\n",
      "Epoch [1/2], Step [1817/7464], Loss: 3.7062\n",
      "Epoch [1/2], Step [1818/7464], Loss: 3.5741\n",
      "Epoch [1/2], Step [1819/7464], Loss: 3.7608\n",
      "Epoch [1/2], Step [1820/7464], Loss: 3.9507\n",
      "Epoch [1/2], Step [1821/7464], Loss: 3.8587\n",
      "Epoch [1/2], Step [1822/7464], Loss: 4.0035\n",
      "Epoch [1/2], Step [1823/7464], Loss: 3.8384\n",
      "Epoch [1/2], Step [1824/7464], Loss: 3.7198\n",
      "Epoch [1/2], Step [1825/7464], Loss: 3.9754\n",
      "Epoch [1/2], Step [1826/7464], Loss: 3.4250\n",
      "Epoch [1/2], Step [1827/7464], Loss: 3.7857\n",
      "Epoch [1/2], Step [1828/7464], Loss: 3.7882\n",
      "Epoch [1/2], Step [1829/7464], Loss: 3.9187\n",
      "Epoch [1/2], Step [1830/7464], Loss: 4.0814\n",
      "Epoch [1/2], Step [1831/7464], Loss: 3.0797\n",
      "Epoch [1/2], Step [1832/7464], Loss: 3.3938\n",
      "Epoch [1/2], Step [1833/7464], Loss: 3.4546\n",
      "Epoch [1/2], Step [1834/7464], Loss: 3.4756\n",
      "Epoch [1/2], Step [1835/7464], Loss: 3.8426\n",
      "Epoch [1/2], Step [1836/7464], Loss: 3.5595\n",
      "Epoch [1/2], Step [1837/7464], Loss: 3.7377\n",
      "Epoch [1/2], Step [1838/7464], Loss: 3.9499\n",
      "Epoch [1/2], Step [1839/7464], Loss: 3.8750\n",
      "Epoch [1/2], Step [1840/7464], Loss: 3.4499\n",
      "Epoch [1/2], Step [1841/7464], Loss: 4.2359\n",
      "Epoch [1/2], Step [1842/7464], Loss: 3.3612\n",
      "Epoch [1/2], Step [1843/7464], Loss: 3.5333\n",
      "Epoch [1/2], Step [1844/7464], Loss: 3.8401\n",
      "Epoch [1/2], Step [1845/7464], Loss: 3.8378\n",
      "Epoch [1/2], Step [1846/7464], Loss: 4.1435\n",
      "Epoch [1/2], Step [1847/7464], Loss: 3.0283\n",
      "Epoch [1/2], Step [1848/7464], Loss: 3.7432\n",
      "Epoch [1/2], Step [1849/7464], Loss: 3.9528\n",
      "Epoch [1/2], Step [1850/7464], Loss: 3.4269\n",
      "Epoch [1/2], Step [1851/7464], Loss: 3.6896\n",
      "Epoch [1/2], Step [1852/7464], Loss: 3.6373\n",
      "Epoch [1/2], Step [1853/7464], Loss: 4.0085\n",
      "Epoch [1/2], Step [1854/7464], Loss: 3.6740\n",
      "Epoch [1/2], Step [1855/7464], Loss: 3.4364\n",
      "Epoch [1/2], Step [1856/7464], Loss: 3.4641\n",
      "Epoch [1/2], Step [1857/7464], Loss: 2.9697\n",
      "Epoch [1/2], Step [1858/7464], Loss: 3.9794\n",
      "Epoch [1/2], Step [1859/7464], Loss: 3.6458\n",
      "Epoch [1/2], Step [1860/7464], Loss: 3.3816\n",
      "Epoch [1/2], Step [1861/7464], Loss: 3.5573\n",
      "Epoch [1/2], Step [1862/7464], Loss: 3.4796\n",
      "Epoch [1/2], Step [1863/7464], Loss: 3.5291\n",
      "Epoch [1/2], Step [1864/7464], Loss: 3.9939\n",
      "Epoch [1/2], Step [1865/7464], Loss: 3.8618\n",
      "Epoch [1/2], Step [1866/7464], Loss: 2.7238\n",
      "Epoch [1/2], Step [1867/7464], Loss: 3.9684\n",
      "Epoch [1/2], Step [1868/7464], Loss: 3.5760\n",
      "Epoch [1/2], Step [1869/7464], Loss: 3.9231\n",
      "Epoch [1/2], Step [1870/7464], Loss: 3.0536\n",
      "Epoch [1/2], Step [1871/7464], Loss: 4.1829\n",
      "Epoch [1/2], Step [1872/7464], Loss: 4.2386\n",
      "Epoch [1/2], Step [1873/7464], Loss: 4.5379\n",
      "Epoch [1/2], Step [1874/7464], Loss: 4.0429\n",
      "Epoch [1/2], Step [1875/7464], Loss: 3.5277\n",
      "Epoch [1/2], Step [1876/7464], Loss: 3.6388\n",
      "Epoch [1/2], Step [1877/7464], Loss: 3.5060\n",
      "Epoch [1/2], Step [1878/7464], Loss: 3.7651\n",
      "Epoch [1/2], Step [1879/7464], Loss: 3.8363\n",
      "Epoch [1/2], Step [1880/7464], Loss: 3.6040\n",
      "Epoch [1/2], Step [1881/7464], Loss: 3.4517\n",
      "Epoch [1/2], Step [1882/7464], Loss: 3.2476\n",
      "Epoch [1/2], Step [1883/7464], Loss: 3.4696\n",
      "Epoch [1/2], Step [1884/7464], Loss: 3.7876\n",
      "Epoch [1/2], Step [1885/7464], Loss: 3.7971\n",
      "Epoch [1/2], Step [1886/7464], Loss: 4.0017\n",
      "Epoch [1/2], Step [1887/7464], Loss: 3.6986\n",
      "Epoch [1/2], Step [1888/7464], Loss: 3.9243\n",
      "Epoch [1/2], Step [1889/7464], Loss: 3.5776\n",
      "Epoch [1/2], Step [1890/7464], Loss: 3.5870\n",
      "Epoch [1/2], Step [1891/7464], Loss: 4.0014\n",
      "Epoch [1/2], Step [1892/7464], Loss: 4.3002\n",
      "Epoch [1/2], Step [1893/7464], Loss: 3.3662\n",
      "Epoch [1/2], Step [1894/7464], Loss: 3.7411\n",
      "Epoch [1/2], Step [1895/7464], Loss: 3.6888\n",
      "Epoch [1/2], Step [1896/7464], Loss: 4.1221\n",
      "Epoch [1/2], Step [1897/7464], Loss: 3.6060\n",
      "Epoch [1/2], Step [1898/7464], Loss: 4.5097\n",
      "Epoch [1/2], Step [1899/7464], Loss: 3.3456\n",
      "Epoch [1/2], Step [1900/7464], Loss: 3.6693\n",
      "Epoch [1/2], Step [1901/7464], Loss: 3.6430\n",
      "Epoch [1/2], Step [1902/7464], Loss: 3.8575\n",
      "Epoch [1/2], Step [1903/7464], Loss: 4.1163\n",
      "Epoch [1/2], Step [1904/7464], Loss: 3.8143\n",
      "Epoch [1/2], Step [1905/7464], Loss: 3.8929\n",
      "Epoch [1/2], Step [1906/7464], Loss: 3.4996\n",
      "Epoch [1/2], Step [1907/7464], Loss: 3.8605\n",
      "Epoch [1/2], Step [1908/7464], Loss: 3.8107\n",
      "Epoch [1/2], Step [1909/7464], Loss: 3.5582\n",
      "Epoch [1/2], Step [1910/7464], Loss: 3.9272\n",
      "Epoch [1/2], Step [1911/7464], Loss: 3.6554\n",
      "Epoch [1/2], Step [1912/7464], Loss: 3.9703\n",
      "Epoch [1/2], Step [1913/7464], Loss: 3.2922\n",
      "Epoch [1/2], Step [1914/7464], Loss: 3.4705\n",
      "Epoch [1/2], Step [1915/7464], Loss: 3.5044\n",
      "Epoch [1/2], Step [1916/7464], Loss: 3.5019\n",
      "Epoch [1/2], Step [1917/7464], Loss: 3.7635\n",
      "Epoch [1/2], Step [1918/7464], Loss: 3.9144\n",
      "Epoch [1/2], Step [1919/7464], Loss: 3.2481\n",
      "Epoch [1/2], Step [1920/7464], Loss: 3.4087\n",
      "Epoch [1/2], Step [1921/7464], Loss: 3.7489\n",
      "Epoch [1/2], Step [1922/7464], Loss: 2.9824\n",
      "Epoch [1/2], Step [1923/7464], Loss: 3.9181\n",
      "Epoch [1/2], Step [1924/7464], Loss: 3.7187\n",
      "Epoch [1/2], Step [1925/7464], Loss: 3.9145\n",
      "Epoch [1/2], Step [1926/7464], Loss: 3.4541\n",
      "Epoch [1/2], Step [1927/7464], Loss: 4.0919\n",
      "Epoch [1/2], Step [1928/7464], Loss: 3.5661\n",
      "Epoch [1/2], Step [1929/7464], Loss: 3.7086\n",
      "Epoch [1/2], Step [1930/7464], Loss: 3.1999\n",
      "Epoch [1/2], Step [1931/7464], Loss: 4.0961\n",
      "Epoch [1/2], Step [1932/7464], Loss: 3.8854\n",
      "Epoch [1/2], Step [1933/7464], Loss: 4.1308\n",
      "Epoch [1/2], Step [1934/7464], Loss: 3.3783\n",
      "Epoch [1/2], Step [1935/7464], Loss: 3.8509\n",
      "Epoch [1/2], Step [1936/7464], Loss: 3.8579\n",
      "Epoch [1/2], Step [1937/7464], Loss: 3.5181\n",
      "Epoch [1/2], Step [1938/7464], Loss: 3.4920\n",
      "Epoch [1/2], Step [1939/7464], Loss: 3.4068\n",
      "Epoch [1/2], Step [1940/7464], Loss: 3.9455\n",
      "Epoch [1/2], Step [1941/7464], Loss: 3.5908\n",
      "Epoch [1/2], Step [1942/7464], Loss: 3.4525\n",
      "Epoch [1/2], Step [1943/7464], Loss: 3.4867\n",
      "Epoch [1/2], Step [1944/7464], Loss: 3.7970\n",
      "Epoch [1/2], Step [1945/7464], Loss: 3.5644\n",
      "Epoch [1/2], Step [1946/7464], Loss: 4.0675\n",
      "Epoch [1/2], Step [1947/7464], Loss: 3.4115\n",
      "Epoch [1/2], Step [1948/7464], Loss: 3.9493\n",
      "Epoch [1/2], Step [1949/7464], Loss: 3.7935\n",
      "Epoch [1/2], Step [1950/7464], Loss: 4.0904\n",
      "Epoch [1/2], Step [1951/7464], Loss: 3.4959\n",
      "Epoch [1/2], Step [1952/7464], Loss: 3.9230\n",
      "Epoch [1/2], Step [1953/7464], Loss: 3.6881\n",
      "Epoch [1/2], Step [1954/7464], Loss: 3.5344\n",
      "Epoch [1/2], Step [1955/7464], Loss: 3.2806\n",
      "Epoch [1/2], Step [1956/7464], Loss: 4.0435\n",
      "Epoch [1/2], Step [1957/7464], Loss: 4.0068\n",
      "Epoch [1/2], Step [1958/7464], Loss: 3.5121\n",
      "Epoch [1/2], Step [1959/7464], Loss: 3.6028\n",
      "Epoch [1/2], Step [1960/7464], Loss: 2.9479\n",
      "Epoch [1/2], Step [1961/7464], Loss: 3.7327\n",
      "Epoch [1/2], Step [1962/7464], Loss: 3.4533\n",
      "Epoch [1/2], Step [1963/7464], Loss: 3.6233\n",
      "Epoch [1/2], Step [1964/7464], Loss: 3.2275\n",
      "Epoch [1/2], Step [1965/7464], Loss: 3.8631\n",
      "Epoch [1/2], Step [1966/7464], Loss: 3.5458\n",
      "Epoch [1/2], Step [1967/7464], Loss: 3.3472\n",
      "Epoch [1/2], Step [1968/7464], Loss: 3.4938\n",
      "Epoch [1/2], Step [1969/7464], Loss: 3.2778\n",
      "Epoch [1/2], Step [1970/7464], Loss: 3.4190\n",
      "Epoch [1/2], Step [1971/7464], Loss: 3.3087\n",
      "Epoch [1/2], Step [1972/7464], Loss: 3.8037\n",
      "Epoch [1/2], Step [1973/7464], Loss: 3.6438\n",
      "Epoch [1/2], Step [1974/7464], Loss: 3.4422\n",
      "Epoch [1/2], Step [1975/7464], Loss: 3.0885\n",
      "Epoch [1/2], Step [1976/7464], Loss: 4.0144\n",
      "Epoch [1/2], Step [1977/7464], Loss: 3.2399\n",
      "Epoch [1/2], Step [1978/7464], Loss: 3.2251\n",
      "Epoch [1/2], Step [1979/7464], Loss: 4.1629\n",
      "Epoch [1/2], Step [1980/7464], Loss: 3.9573\n",
      "Epoch [1/2], Step [1981/7464], Loss: 3.2563\n",
      "Epoch [1/2], Step [1982/7464], Loss: 3.7383\n",
      "Epoch [1/2], Step [1983/7464], Loss: 4.0478\n",
      "Epoch [1/2], Step [1984/7464], Loss: 3.6065\n",
      "Epoch [1/2], Step [1985/7464], Loss: 3.2763\n",
      "Epoch [1/2], Step [1986/7464], Loss: 3.6759\n",
      "Epoch [1/2], Step [1987/7464], Loss: 3.7627\n",
      "Epoch [1/2], Step [1988/7464], Loss: 3.8490\n",
      "Epoch [1/2], Step [1989/7464], Loss: 3.8347\n",
      "Epoch [1/2], Step [1990/7464], Loss: 3.5495\n",
      "Epoch [1/2], Step [1991/7464], Loss: 3.2058\n",
      "Epoch [1/2], Step [1992/7464], Loss: 4.2174\n",
      "Epoch [1/2], Step [1993/7464], Loss: 3.8187\n",
      "Epoch [1/2], Step [1994/7464], Loss: 2.8006\n",
      "Epoch [1/2], Step [1995/7464], Loss: 4.0436\n",
      "Epoch [1/2], Step [1996/7464], Loss: 3.6161\n",
      "Epoch [1/2], Step [1997/7464], Loss: 3.8630\n",
      "Epoch [1/2], Step [1998/7464], Loss: 3.2279\n",
      "Epoch [1/2], Step [1999/7464], Loss: 4.2020\n",
      "Epoch [1/2], Step [2000/7464], Loss: 3.2346\n",
      "Epoch [1/2], Step [2001/7464], Loss: 3.8196\n",
      "Epoch [1/2], Step [2002/7464], Loss: 3.9432\n",
      "Epoch [1/2], Step [2003/7464], Loss: 3.5296\n",
      "Epoch [1/2], Step [2004/7464], Loss: 3.7942\n",
      "Epoch [1/2], Step [2005/7464], Loss: 3.7343\n",
      "Epoch [1/2], Step [2006/7464], Loss: 3.3387\n",
      "Epoch [1/2], Step [2007/7464], Loss: 4.0697\n",
      "Epoch [1/2], Step [2008/7464], Loss: 3.7782\n",
      "Epoch [1/2], Step [2009/7464], Loss: 3.1624\n",
      "Epoch [1/2], Step [2010/7464], Loss: 3.6687\n",
      "Epoch [1/2], Step [2011/7464], Loss: 3.5193\n",
      "Epoch [1/2], Step [2012/7464], Loss: 3.6726\n",
      "Epoch [1/2], Step [2013/7464], Loss: 3.6903\n",
      "Epoch [1/2], Step [2014/7464], Loss: 3.4612\n",
      "Epoch [1/2], Step [2015/7464], Loss: 4.4496\n",
      "Epoch [1/2], Step [2016/7464], Loss: 4.0526\n",
      "Epoch [1/2], Step [2017/7464], Loss: 3.7812\n",
      "Epoch [1/2], Step [2018/7464], Loss: 3.0907\n",
      "Epoch [1/2], Step [2019/7464], Loss: 3.4866\n",
      "Epoch [1/2], Step [2020/7464], Loss: 3.4039\n",
      "Epoch [1/2], Step [2021/7464], Loss: 2.9871\n",
      "Epoch [1/2], Step [2022/7464], Loss: 3.1782\n",
      "Epoch [1/2], Step [2023/7464], Loss: 3.6738\n",
      "Epoch [1/2], Step [2024/7464], Loss: 3.7429\n",
      "Epoch [1/2], Step [2025/7464], Loss: 3.5721\n",
      "Epoch [1/2], Step [2026/7464], Loss: 3.3704\n",
      "Epoch [1/2], Step [2027/7464], Loss: 3.5383\n",
      "Epoch [1/2], Step [2028/7464], Loss: 3.8766\n",
      "Epoch [1/2], Step [2029/7464], Loss: 3.6689\n",
      "Epoch [1/2], Step [2030/7464], Loss: 3.6050\n",
      "Epoch [1/2], Step [2031/7464], Loss: 4.3927\n",
      "Epoch [1/2], Step [2032/7464], Loss: 4.4239\n",
      "Epoch [1/2], Step [2033/7464], Loss: 3.5891\n",
      "Epoch [1/2], Step [2034/7464], Loss: 3.7273\n",
      "Epoch [1/2], Step [2035/7464], Loss: 3.4294\n",
      "Epoch [1/2], Step [2036/7464], Loss: 3.9891\n",
      "Epoch [1/2], Step [2037/7464], Loss: 4.2137\n",
      "Epoch [1/2], Step [2038/7464], Loss: 4.3073\n",
      "Epoch [1/2], Step [2039/7464], Loss: 4.1045\n",
      "Epoch [1/2], Step [2040/7464], Loss: 3.7597\n",
      "Epoch [1/2], Step [2041/7464], Loss: 3.6242\n",
      "Epoch [1/2], Step [2042/7464], Loss: 3.7903\n",
      "Epoch [1/2], Step [2043/7464], Loss: 3.6799\n",
      "Epoch [1/2], Step [2044/7464], Loss: 3.9825\n",
      "Epoch [1/2], Step [2045/7464], Loss: 3.2642\n",
      "Epoch [1/2], Step [2046/7464], Loss: 3.8601\n",
      "Epoch [1/2], Step [2047/7464], Loss: 3.8811\n",
      "Epoch [1/2], Step [2048/7464], Loss: 3.7475\n",
      "Epoch [1/2], Step [2049/7464], Loss: 3.4346\n",
      "Epoch [1/2], Step [2050/7464], Loss: 4.0425\n",
      "Epoch [1/2], Step [2051/7464], Loss: 3.9209\n",
      "Epoch [1/2], Step [2052/7464], Loss: 3.5862\n",
      "Epoch [1/2], Step [2053/7464], Loss: 4.0050\n",
      "Epoch [1/2], Step [2054/7464], Loss: 3.4156\n",
      "Epoch [1/2], Step [2055/7464], Loss: 3.7186\n",
      "Epoch [1/2], Step [2056/7464], Loss: 3.4574\n",
      "Epoch [1/2], Step [2057/7464], Loss: 3.5386\n",
      "Epoch [1/2], Step [2058/7464], Loss: 3.7429\n",
      "Epoch [1/2], Step [2059/7464], Loss: 3.3615\n",
      "Epoch [1/2], Step [2060/7464], Loss: 3.5429\n",
      "Epoch [1/2], Step [2061/7464], Loss: 4.1788\n",
      "Epoch [1/2], Step [2062/7464], Loss: 3.9256\n",
      "Epoch [1/2], Step [2063/7464], Loss: 3.6601\n",
      "Epoch [1/2], Step [2064/7464], Loss: 4.2922\n",
      "Epoch [1/2], Step [2065/7464], Loss: 3.6518\n",
      "Epoch [1/2], Step [2066/7464], Loss: 3.5155\n",
      "Epoch [1/2], Step [2067/7464], Loss: 3.8238\n",
      "Epoch [1/2], Step [2068/7464], Loss: 3.9192\n",
      "Epoch [1/2], Step [2069/7464], Loss: 3.4959\n",
      "Epoch [1/2], Step [2070/7464], Loss: 3.3678\n",
      "Epoch [1/2], Step [2071/7464], Loss: 3.4878\n",
      "Epoch [1/2], Step [2072/7464], Loss: 3.6392\n",
      "Epoch [1/2], Step [2073/7464], Loss: 3.4654\n",
      "Epoch [1/2], Step [2074/7464], Loss: 3.6833\n",
      "Epoch [1/2], Step [2075/7464], Loss: 2.9881\n",
      "Epoch [1/2], Step [2076/7464], Loss: 3.7716\n",
      "Epoch [1/2], Step [2077/7464], Loss: 3.1974\n",
      "Epoch [1/2], Step [2078/7464], Loss: 3.1121\n",
      "Epoch [1/2], Step [2079/7464], Loss: 3.8638\n",
      "Epoch [1/2], Step [2080/7464], Loss: 3.4455\n",
      "Epoch [1/2], Step [2081/7464], Loss: 3.6451\n",
      "Epoch [1/2], Step [2082/7464], Loss: 3.6703\n",
      "Epoch [1/2], Step [2083/7464], Loss: 3.7042\n",
      "Epoch [1/2], Step [2084/7464], Loss: 3.5970\n",
      "Epoch [1/2], Step [2085/7464], Loss: 2.9245\n",
      "Epoch [1/2], Step [2086/7464], Loss: 3.6716\n",
      "Epoch [1/2], Step [2087/7464], Loss: 3.7040\n",
      "Epoch [1/2], Step [2088/7464], Loss: 3.9564\n",
      "Epoch [1/2], Step [2089/7464], Loss: 4.3201\n",
      "Epoch [1/2], Step [2090/7464], Loss: 3.7413\n",
      "Epoch [1/2], Step [2091/7464], Loss: 3.6512\n",
      "Epoch [1/2], Step [2092/7464], Loss: 3.0229\n",
      "Epoch [1/2], Step [2093/7464], Loss: 4.3760\n",
      "Epoch [1/2], Step [2094/7464], Loss: 3.8643\n",
      "Epoch [1/2], Step [2095/7464], Loss: 2.9527\n",
      "Epoch [1/2], Step [2096/7464], Loss: 4.4013\n",
      "Epoch [1/2], Step [2097/7464], Loss: 3.6979\n",
      "Epoch [1/2], Step [2098/7464], Loss: 3.4595\n",
      "Epoch [1/2], Step [2099/7464], Loss: 3.4364\n",
      "Epoch [1/2], Step [2100/7464], Loss: 4.3266\n",
      "Epoch [1/2], Step [2101/7464], Loss: 4.0185\n",
      "Epoch [1/2], Step [2102/7464], Loss: 4.4509\n",
      "Epoch [1/2], Step [2103/7464], Loss: 3.8185\n",
      "Epoch [1/2], Step [2104/7464], Loss: 3.4420\n",
      "Epoch [1/2], Step [2105/7464], Loss: 3.2617\n",
      "Epoch [1/2], Step [2106/7464], Loss: 3.5641\n",
      "Epoch [1/2], Step [2107/7464], Loss: 3.5520\n",
      "Epoch [1/2], Step [2108/7464], Loss: 3.3877\n",
      "Epoch [1/2], Step [2109/7464], Loss: 3.9478\n",
      "Epoch [1/2], Step [2110/7464], Loss: 3.8861\n",
      "Epoch [1/2], Step [2111/7464], Loss: 3.6738\n",
      "Epoch [1/2], Step [2112/7464], Loss: 3.8949\n",
      "Epoch [1/2], Step [2113/7464], Loss: 3.6467\n",
      "Epoch [1/2], Step [2114/7464], Loss: 3.5733\n",
      "Epoch [1/2], Step [2115/7464], Loss: 3.9721\n",
      "Epoch [1/2], Step [2116/7464], Loss: 3.4856\n",
      "Epoch [1/2], Step [2117/7464], Loss: 3.6991\n",
      "Epoch [1/2], Step [2118/7464], Loss: 3.5018\n",
      "Epoch [1/2], Step [2119/7464], Loss: 3.9656\n",
      "Epoch [1/2], Step [2120/7464], Loss: 3.6686\n",
      "Epoch [1/2], Step [2121/7464], Loss: 3.6488\n",
      "Epoch [1/2], Step [2122/7464], Loss: 3.8059\n",
      "Epoch [1/2], Step [2123/7464], Loss: 3.6054\n",
      "Epoch [1/2], Step [2124/7464], Loss: 4.1148\n",
      "Epoch [1/2], Step [2125/7464], Loss: 3.6805\n",
      "Epoch [1/2], Step [2126/7464], Loss: 3.7761\n",
      "Epoch [1/2], Step [2127/7464], Loss: 3.8904\n",
      "Epoch [1/2], Step [2128/7464], Loss: 3.8373\n",
      "Epoch [1/2], Step [2129/7464], Loss: 3.1781\n",
      "Epoch [1/2], Step [2130/7464], Loss: 3.8655\n",
      "Epoch [1/2], Step [2131/7464], Loss: 3.4251\n",
      "Epoch [1/2], Step [2132/7464], Loss: 3.7717\n",
      "Epoch [1/2], Step [2133/7464], Loss: 3.5496\n",
      "Epoch [1/2], Step [2134/7464], Loss: 3.2356\n",
      "Epoch [1/2], Step [2135/7464], Loss: 3.6735\n",
      "Epoch [1/2], Step [2136/7464], Loss: 3.7473\n",
      "Epoch [1/2], Step [2137/7464], Loss: 3.5558\n",
      "Epoch [1/2], Step [2138/7464], Loss: 3.1074\n",
      "Epoch [1/2], Step [2139/7464], Loss: 3.5069\n",
      "Epoch [1/2], Step [2140/7464], Loss: 3.6285\n",
      "Epoch [1/2], Step [2141/7464], Loss: 3.1807\n",
      "Epoch [1/2], Step [2142/7464], Loss: 4.0177\n",
      "Epoch [1/2], Step [2143/7464], Loss: 4.0577\n",
      "Epoch [1/2], Step [2144/7464], Loss: 3.3549\n",
      "Epoch [1/2], Step [2145/7464], Loss: 3.2815\n",
      "Epoch [1/2], Step [2146/7464], Loss: 3.1316\n",
      "Epoch [1/2], Step [2147/7464], Loss: 3.7948\n",
      "Epoch [1/2], Step [2148/7464], Loss: 3.2104\n",
      "Epoch [1/2], Step [2149/7464], Loss: 3.6510\n",
      "Epoch [1/2], Step [2150/7464], Loss: 3.7172\n",
      "Epoch [1/2], Step [2151/7464], Loss: 3.5073\n",
      "Epoch [1/2], Step [2152/7464], Loss: 3.1122\n",
      "Epoch [1/2], Step [2153/7464], Loss: 3.5481\n",
      "Epoch [1/2], Step [2154/7464], Loss: 3.5005\n",
      "Epoch [1/2], Step [2155/7464], Loss: 4.2568\n",
      "Epoch [1/2], Step [2156/7464], Loss: 3.5606\n",
      "Epoch [1/2], Step [2157/7464], Loss: 4.5101\n",
      "Epoch [1/2], Step [2158/7464], Loss: 3.9423\n",
      "Epoch [1/2], Step [2159/7464], Loss: 3.6313\n",
      "Epoch [1/2], Step [2160/7464], Loss: 4.0385\n",
      "Epoch [1/2], Step [2161/7464], Loss: 3.8152\n",
      "Epoch [1/2], Step [2162/7464], Loss: 3.5063\n",
      "Epoch [1/2], Step [2163/7464], Loss: 3.3095\n",
      "Epoch [1/2], Step [2164/7464], Loss: 4.3575\n",
      "Epoch [1/2], Step [2165/7464], Loss: 3.9329\n",
      "Epoch [1/2], Step [2166/7464], Loss: 3.5148\n",
      "Epoch [1/2], Step [2167/7464], Loss: 3.9975\n",
      "Epoch [1/2], Step [2168/7464], Loss: 3.3394\n",
      "Epoch [1/2], Step [2169/7464], Loss: 3.8434\n",
      "Epoch [1/2], Step [2170/7464], Loss: 3.2027\n",
      "Epoch [1/2], Step [2171/7464], Loss: 3.4406\n",
      "Epoch [1/2], Step [2172/7464], Loss: 3.3829\n",
      "Epoch [1/2], Step [2173/7464], Loss: 4.1402\n",
      "Epoch [1/2], Step [2174/7464], Loss: 3.5246\n",
      "Epoch [1/2], Step [2175/7464], Loss: 3.1633\n",
      "Epoch [1/2], Step [2176/7464], Loss: 3.8155\n",
      "Epoch [1/2], Step [2177/7464], Loss: 3.0853\n",
      "Epoch [1/2], Step [2178/7464], Loss: 3.5744\n",
      "Epoch [1/2], Step [2179/7464], Loss: 3.5242\n",
      "Epoch [1/2], Step [2180/7464], Loss: 3.6257\n",
      "Epoch [1/2], Step [2181/7464], Loss: 3.9632\n",
      "Epoch [1/2], Step [2182/7464], Loss: 3.4008\n",
      "Epoch [1/2], Step [2183/7464], Loss: 3.4996\n",
      "Epoch [1/2], Step [2184/7464], Loss: 3.6487\n",
      "Epoch [1/2], Step [2185/7464], Loss: 3.5255\n",
      "Epoch [1/2], Step [2186/7464], Loss: 3.7602\n",
      "Epoch [1/2], Step [2187/7464], Loss: 3.7670\n",
      "Epoch [1/2], Step [2188/7464], Loss: 3.4233\n",
      "Epoch [1/2], Step [2189/7464], Loss: 3.3463\n",
      "Epoch [1/2], Step [2190/7464], Loss: 3.2895\n",
      "Epoch [1/2], Step [2191/7464], Loss: 4.1724\n",
      "Epoch [1/2], Step [2192/7464], Loss: 3.3942\n",
      "Epoch [1/2], Step [2193/7464], Loss: 4.0821\n",
      "Epoch [1/2], Step [2194/7464], Loss: 3.5681\n",
      "Epoch [1/2], Step [2195/7464], Loss: 3.8859\n",
      "Epoch [1/2], Step [2196/7464], Loss: 3.8706\n",
      "Epoch [1/2], Step [2197/7464], Loss: 3.6158\n",
      "Epoch [1/2], Step [2198/7464], Loss: 4.0636\n",
      "Epoch [1/2], Step [2199/7464], Loss: 3.8232\n",
      "Epoch [1/2], Step [2200/7464], Loss: 3.6393\n",
      "Epoch [1/2], Step [2201/7464], Loss: 3.6838\n",
      "Epoch [1/2], Step [2202/7464], Loss: 3.2838\n",
      "Epoch [1/2], Step [2203/7464], Loss: 3.4726\n",
      "Epoch [1/2], Step [2204/7464], Loss: 3.9402\n",
      "Epoch [1/2], Step [2205/7464], Loss: 3.8081\n",
      "Epoch [1/2], Step [2206/7464], Loss: 3.5696\n",
      "Epoch [1/2], Step [2207/7464], Loss: 3.4167\n",
      "Epoch [1/2], Step [2208/7464], Loss: 3.5219\n",
      "Epoch [1/2], Step [2209/7464], Loss: 3.3768\n",
      "Epoch [1/2], Step [2210/7464], Loss: 3.0390\n",
      "Epoch [1/2], Step [2211/7464], Loss: 3.3249\n",
      "Epoch [1/2], Step [2212/7464], Loss: 3.9264\n",
      "Epoch [1/2], Step [2213/7464], Loss: 3.6898\n",
      "Epoch [1/2], Step [2214/7464], Loss: 3.6486\n",
      "Epoch [1/2], Step [2215/7464], Loss: 3.7469\n",
      "Epoch [1/2], Step [2216/7464], Loss: 4.2115\n",
      "Epoch [1/2], Step [2217/7464], Loss: 3.7662\n",
      "Epoch [1/2], Step [2218/7464], Loss: 3.2720\n",
      "Epoch [1/2], Step [2219/7464], Loss: 3.7543\n",
      "Epoch [1/2], Step [2220/7464], Loss: 3.4398\n",
      "Epoch [1/2], Step [2221/7464], Loss: 3.7212\n",
      "Epoch [1/2], Step [2222/7464], Loss: 3.5359\n",
      "Epoch [1/2], Step [2223/7464], Loss: 3.7017\n",
      "Epoch [1/2], Step [2224/7464], Loss: 3.5164\n",
      "Epoch [1/2], Step [2225/7464], Loss: 4.2567\n",
      "Epoch [1/2], Step [2226/7464], Loss: 3.8713\n",
      "Epoch [1/2], Step [2227/7464], Loss: 3.8699\n",
      "Epoch [1/2], Step [2228/7464], Loss: 4.4018\n",
      "Epoch [1/2], Step [2229/7464], Loss: 3.4525\n",
      "Epoch [1/2], Step [2230/7464], Loss: 3.2447\n",
      "Epoch [1/2], Step [2231/7464], Loss: 3.4750\n",
      "Epoch [1/2], Step [2232/7464], Loss: 4.1317\n",
      "Epoch [1/2], Step [2233/7464], Loss: 3.8598\n",
      "Epoch [1/2], Step [2234/7464], Loss: 3.4357\n",
      "Epoch [1/2], Step [2235/7464], Loss: 3.3698\n",
      "Epoch [1/2], Step [2236/7464], Loss: 3.7097\n",
      "Epoch [1/2], Step [2237/7464], Loss: 4.0253\n",
      "Epoch [1/2], Step [2238/7464], Loss: 3.1725\n",
      "Epoch [1/2], Step [2239/7464], Loss: 3.8436\n",
      "Epoch [1/2], Step [2240/7464], Loss: 3.5886\n",
      "Epoch [1/2], Step [2241/7464], Loss: 4.0914\n",
      "Epoch [1/2], Step [2242/7464], Loss: 3.7392\n",
      "Epoch [1/2], Step [2243/7464], Loss: 3.7988\n",
      "Epoch [1/2], Step [2244/7464], Loss: 3.5641\n",
      "Epoch [1/2], Step [2245/7464], Loss: 3.4525\n",
      "Epoch [1/2], Step [2246/7464], Loss: 3.3873\n",
      "Epoch [1/2], Step [2247/7464], Loss: 4.1948\n",
      "Epoch [1/2], Step [2248/7464], Loss: 3.7104\n",
      "Epoch [1/2], Step [2249/7464], Loss: 3.4074\n",
      "Epoch [1/2], Step [2250/7464], Loss: 3.4716\n",
      "Epoch [1/2], Step [2251/7464], Loss: 3.9421\n",
      "Epoch [1/2], Step [2252/7464], Loss: 3.4539\n",
      "Epoch [1/2], Step [2253/7464], Loss: 3.5949\n",
      "Epoch [1/2], Step [2254/7464], Loss: 3.9715\n",
      "Epoch [1/2], Step [2255/7464], Loss: 3.0955\n",
      "Epoch [1/2], Step [2256/7464], Loss: 3.6188\n",
      "Epoch [1/2], Step [2257/7464], Loss: 3.1732\n",
      "Epoch [1/2], Step [2258/7464], Loss: 3.5588\n",
      "Epoch [1/2], Step [2259/7464], Loss: 3.1518\n",
      "Epoch [1/2], Step [2260/7464], Loss: 4.0607\n",
      "Epoch [1/2], Step [2261/7464], Loss: 3.3947\n",
      "Epoch [1/2], Step [2262/7464], Loss: 2.5473\n",
      "Epoch [1/2], Step [2263/7464], Loss: 3.2346\n",
      "Epoch [1/2], Step [2264/7464], Loss: 3.3165\n",
      "Epoch [1/2], Step [2265/7464], Loss: 3.3850\n",
      "Epoch [1/2], Step [2266/7464], Loss: 3.4226\n",
      "Epoch [1/2], Step [2267/7464], Loss: 4.1695\n",
      "Epoch [1/2], Step [2268/7464], Loss: 3.4334\n",
      "Epoch [1/2], Step [2269/7464], Loss: 4.1035\n",
      "Epoch [1/2], Step [2270/7464], Loss: 3.5559\n",
      "Epoch [1/2], Step [2271/7464], Loss: 3.2568\n",
      "Epoch [1/2], Step [2272/7464], Loss: 4.2221\n",
      "Epoch [1/2], Step [2273/7464], Loss: 3.8786\n",
      "Epoch [1/2], Step [2274/7464], Loss: 4.1886\n",
      "Epoch [1/2], Step [2275/7464], Loss: 3.7847\n",
      "Epoch [1/2], Step [2276/7464], Loss: 3.6362\n",
      "Epoch [1/2], Step [2277/7464], Loss: 3.6876\n",
      "Epoch [1/2], Step [2278/7464], Loss: 3.4023\n",
      "Epoch [1/2], Step [2279/7464], Loss: 3.6994\n",
      "Epoch [1/2], Step [2280/7464], Loss: 3.4409\n",
      "Epoch [1/2], Step [2281/7464], Loss: 4.2339\n",
      "Epoch [1/2], Step [2282/7464], Loss: 4.1050\n",
      "Epoch [1/2], Step [2283/7464], Loss: 2.8711\n",
      "Epoch [1/2], Step [2284/7464], Loss: 3.8395\n",
      "Epoch [1/2], Step [2285/7464], Loss: 3.5136\n",
      "Epoch [1/2], Step [2286/7464], Loss: 3.7201\n",
      "Epoch [1/2], Step [2287/7464], Loss: 3.3636\n",
      "Epoch [1/2], Step [2288/7464], Loss: 3.7921\n",
      "Epoch [1/2], Step [2289/7464], Loss: 3.9226\n",
      "Epoch [1/2], Step [2290/7464], Loss: 4.2765\n",
      "Epoch [1/2], Step [2291/7464], Loss: 3.3431\n",
      "Epoch [1/2], Step [2292/7464], Loss: 3.6281\n",
      "Epoch [1/2], Step [2293/7464], Loss: 3.2537\n",
      "Epoch [1/2], Step [2294/7464], Loss: 3.3910\n",
      "Epoch [1/2], Step [2295/7464], Loss: 3.9477\n",
      "Epoch [1/2], Step [2296/7464], Loss: 3.2416\n",
      "Epoch [1/2], Step [2297/7464], Loss: 3.9184\n",
      "Epoch [1/2], Step [2298/7464], Loss: 3.3500\n",
      "Epoch [1/2], Step [2299/7464], Loss: 3.3459\n",
      "Epoch [1/2], Step [2300/7464], Loss: 3.4885\n",
      "Epoch [1/2], Step [2301/7464], Loss: 3.4051\n",
      "Epoch [1/2], Step [2302/7464], Loss: 3.6353\n",
      "Epoch [1/2], Step [2303/7464], Loss: 3.6326\n",
      "Epoch [1/2], Step [2304/7464], Loss: 4.1978\n",
      "Epoch [1/2], Step [2305/7464], Loss: 4.0337\n",
      "Epoch [1/2], Step [2306/7464], Loss: 3.8754\n",
      "Epoch [1/2], Step [2307/7464], Loss: 3.6472\n",
      "Epoch [1/2], Step [2308/7464], Loss: 2.8430\n",
      "Epoch [1/2], Step [2309/7464], Loss: 3.7737\n",
      "Epoch [1/2], Step [2310/7464], Loss: 3.3578\n",
      "Epoch [1/2], Step [2311/7464], Loss: 3.6315\n",
      "Epoch [1/2], Step [2312/7464], Loss: 3.8810\n",
      "Epoch [1/2], Step [2313/7464], Loss: 2.9700\n",
      "Epoch [1/2], Step [2314/7464], Loss: 3.5822\n",
      "Epoch [1/2], Step [2315/7464], Loss: 3.9441\n",
      "Epoch [1/2], Step [2316/7464], Loss: 3.5648\n",
      "Epoch [1/2], Step [2317/7464], Loss: 3.4662\n",
      "Epoch [1/2], Step [2318/7464], Loss: 3.9915\n",
      "Epoch [1/2], Step [2319/7464], Loss: 3.7363\n",
      "Epoch [1/2], Step [2320/7464], Loss: 4.0310\n",
      "Epoch [1/2], Step [2321/7464], Loss: 3.4055\n",
      "Epoch [1/2], Step [2322/7464], Loss: 3.1641\n",
      "Epoch [1/2], Step [2323/7464], Loss: 4.3298\n",
      "Epoch [1/2], Step [2324/7464], Loss: 3.7479\n",
      "Epoch [1/2], Step [2325/7464], Loss: 3.0787\n",
      "Epoch [1/2], Step [2326/7464], Loss: 4.1695\n",
      "Epoch [1/2], Step [2327/7464], Loss: 4.6907\n",
      "Epoch [1/2], Step [2328/7464], Loss: 2.8048\n",
      "Epoch [1/2], Step [2329/7464], Loss: 3.6699\n",
      "Epoch [1/2], Step [2330/7464], Loss: 3.6966\n",
      "Epoch [1/2], Step [2331/7464], Loss: 3.7644\n",
      "Epoch [1/2], Step [2332/7464], Loss: 4.1084\n",
      "Epoch [1/2], Step [2333/7464], Loss: 4.4473\n",
      "Epoch [1/2], Step [2334/7464], Loss: 3.8511\n",
      "Epoch [1/2], Step [2335/7464], Loss: 3.8850\n",
      "Epoch [1/2], Step [2336/7464], Loss: 3.4649\n",
      "Epoch [1/2], Step [2337/7464], Loss: 3.5877\n",
      "Epoch [1/2], Step [2338/7464], Loss: 3.9831\n",
      "Epoch [1/2], Step [2339/7464], Loss: 3.5846\n",
      "Epoch [1/2], Step [2340/7464], Loss: 3.5213\n",
      "Epoch [1/2], Step [2341/7464], Loss: 3.5582\n",
      "Epoch [1/2], Step [2342/7464], Loss: 3.5932\n",
      "Epoch [1/2], Step [2343/7464], Loss: 4.3667\n",
      "Epoch [1/2], Step [2344/7464], Loss: 3.9351\n",
      "Epoch [1/2], Step [2345/7464], Loss: 3.4785\n",
      "Epoch [1/2], Step [2346/7464], Loss: 3.6016\n",
      "Epoch [1/2], Step [2347/7464], Loss: 3.2472\n",
      "Epoch [1/2], Step [2348/7464], Loss: 3.6929\n",
      "Epoch [1/2], Step [2349/7464], Loss: 3.4497\n",
      "Epoch [1/2], Step [2350/7464], Loss: 3.6697\n",
      "Epoch [1/2], Step [2351/7464], Loss: 3.6691\n",
      "Epoch [1/2], Step [2352/7464], Loss: 3.7449\n",
      "Epoch [1/2], Step [2353/7464], Loss: 3.8387\n",
      "Epoch [1/2], Step [2354/7464], Loss: 3.8456\n",
      "Epoch [1/2], Step [2355/7464], Loss: 3.7522\n",
      "Epoch [1/2], Step [2356/7464], Loss: 3.0317\n",
      "Epoch [1/2], Step [2357/7464], Loss: 3.0359\n",
      "Epoch [1/2], Step [2358/7464], Loss: 3.1722\n",
      "Epoch [1/2], Step [2359/7464], Loss: 3.5693\n",
      "Epoch [1/2], Step [2360/7464], Loss: 2.8645\n",
      "Epoch [1/2], Step [2361/7464], Loss: 3.9149\n",
      "Epoch [1/2], Step [2362/7464], Loss: 3.8719\n",
      "Epoch [1/2], Step [2363/7464], Loss: 3.7836\n",
      "Epoch [1/2], Step [2364/7464], Loss: 3.5160\n",
      "Epoch [1/2], Step [2365/7464], Loss: 3.9829\n",
      "Epoch [1/2], Step [2366/7464], Loss: 4.6962\n",
      "Epoch [1/2], Step [2367/7464], Loss: 3.6871\n",
      "Epoch [1/2], Step [2368/7464], Loss: 3.8226\n",
      "Epoch [1/2], Step [2369/7464], Loss: 3.7437\n",
      "Epoch [1/2], Step [2370/7464], Loss: 3.9682\n",
      "Epoch [1/2], Step [2371/7464], Loss: 3.5236\n",
      "Epoch [1/2], Step [2372/7464], Loss: 3.7662\n",
      "Epoch [1/2], Step [2373/7464], Loss: 3.9473\n",
      "Epoch [1/2], Step [2374/7464], Loss: 3.4562\n",
      "Epoch [1/2], Step [2375/7464], Loss: 3.6195\n",
      "Epoch [1/2], Step [2376/7464], Loss: 3.9305\n",
      "Epoch [1/2], Step [2377/7464], Loss: 3.6593\n",
      "Epoch [1/2], Step [2378/7464], Loss: 3.5213\n",
      "Epoch [1/2], Step [2379/7464], Loss: 3.6706\n",
      "Epoch [1/2], Step [2380/7464], Loss: 3.8681\n",
      "Epoch [1/2], Step [2381/7464], Loss: 3.7902\n",
      "Epoch [1/2], Step [2382/7464], Loss: 3.7444\n",
      "Epoch [1/2], Step [2383/7464], Loss: 4.1339\n",
      "Epoch [1/2], Step [2384/7464], Loss: 3.2598\n",
      "Epoch [1/2], Step [2385/7464], Loss: 3.8648\n",
      "Epoch [1/2], Step [2386/7464], Loss: 3.4795\n",
      "Epoch [1/2], Step [2387/7464], Loss: 3.8582\n",
      "Epoch [1/2], Step [2388/7464], Loss: 3.6784\n",
      "Epoch [1/2], Step [2389/7464], Loss: 3.6352\n",
      "Epoch [1/2], Step [2390/7464], Loss: 3.9278\n",
      "Epoch [1/2], Step [2391/7464], Loss: 3.3960\n",
      "Epoch [1/2], Step [2392/7464], Loss: 3.6619\n",
      "Epoch [1/2], Step [2393/7464], Loss: 3.5004\n",
      "Epoch [1/2], Step [2394/7464], Loss: 4.0569\n",
      "Epoch [1/2], Step [2395/7464], Loss: 3.1590\n",
      "Epoch [1/2], Step [2396/7464], Loss: 3.4947\n",
      "Epoch [1/2], Step [2397/7464], Loss: 3.5341\n",
      "Epoch [1/2], Step [2398/7464], Loss: 3.8675\n",
      "Epoch [1/2], Step [2399/7464], Loss: 3.8517\n",
      "Epoch [1/2], Step [2400/7464], Loss: 3.7836\n",
      "Epoch [1/2], Step [2401/7464], Loss: 3.9092\n",
      "Epoch [1/2], Step [2402/7464], Loss: 3.7631\n",
      "Epoch [1/2], Step [2403/7464], Loss: 3.3372\n",
      "Epoch [1/2], Step [2404/7464], Loss: 3.5241\n",
      "Epoch [1/2], Step [2405/7464], Loss: 3.2652\n",
      "Epoch [1/2], Step [2406/7464], Loss: 3.5603\n",
      "Epoch [1/2], Step [2407/7464], Loss: 3.5531\n",
      "Epoch [1/2], Step [2408/7464], Loss: 3.6111\n",
      "Epoch [1/2], Step [2409/7464], Loss: 3.5918\n",
      "Epoch [1/2], Step [2410/7464], Loss: 3.6886\n",
      "Epoch [1/2], Step [2411/7464], Loss: 3.0239\n",
      "Epoch [1/2], Step [2412/7464], Loss: 3.4730\n",
      "Epoch [1/2], Step [2413/7464], Loss: 3.3926\n",
      "Epoch [1/2], Step [2414/7464], Loss: 3.3203\n",
      "Epoch [1/2], Step [2415/7464], Loss: 4.2857\n",
      "Epoch [1/2], Step [2416/7464], Loss: 4.0769\n",
      "Epoch [1/2], Step [2417/7464], Loss: 3.1776\n",
      "Epoch [1/2], Step [2418/7464], Loss: 3.7183\n",
      "Epoch [1/2], Step [2419/7464], Loss: 3.9704\n",
      "Epoch [1/2], Step [2420/7464], Loss: 3.8099\n",
      "Epoch [1/2], Step [2421/7464], Loss: 3.9479\n",
      "Epoch [1/2], Step [2422/7464], Loss: 3.7073\n",
      "Epoch [1/2], Step [2423/7464], Loss: 3.7186\n",
      "Epoch [1/2], Step [2424/7464], Loss: 3.5428\n",
      "Epoch [1/2], Step [2425/7464], Loss: 3.6418\n",
      "Epoch [1/2], Step [2426/7464], Loss: 3.6275\n",
      "Epoch [1/2], Step [2427/7464], Loss: 3.5218\n",
      "Epoch [1/2], Step [2428/7464], Loss: 3.4272\n",
      "Epoch [1/2], Step [2429/7464], Loss: 3.4233\n",
      "Epoch [1/2], Step [2430/7464], Loss: 3.4101\n",
      "Epoch [1/2], Step [2431/7464], Loss: 2.6876\n",
      "Epoch [1/2], Step [2432/7464], Loss: 3.8641\n",
      "Epoch [1/2], Step [2433/7464], Loss: 3.7688\n",
      "Epoch [1/2], Step [2434/7464], Loss: 3.5755\n",
      "Epoch [1/2], Step [2435/7464], Loss: 3.2348\n",
      "Epoch [1/2], Step [2436/7464], Loss: 2.7041\n",
      "Epoch [1/2], Step [2437/7464], Loss: 3.5788\n",
      "Epoch [1/2], Step [2438/7464], Loss: 3.1654\n",
      "Epoch [1/2], Step [2439/7464], Loss: 3.6629\n",
      "Epoch [1/2], Step [2440/7464], Loss: 3.4035\n",
      "Epoch [1/2], Step [2441/7464], Loss: 3.8616\n",
      "Epoch [1/2], Step [2442/7464], Loss: 2.9972\n",
      "Epoch [1/2], Step [2443/7464], Loss: 4.1453\n",
      "Epoch [1/2], Step [2444/7464], Loss: 3.4404\n",
      "Epoch [1/2], Step [2445/7464], Loss: 4.3830\n",
      "Epoch [1/2], Step [2446/7464], Loss: 3.8487\n",
      "Epoch [1/2], Step [2447/7464], Loss: 3.6096\n",
      "Epoch [1/2], Step [2448/7464], Loss: 4.1202\n",
      "Epoch [1/2], Step [2449/7464], Loss: 3.9376\n",
      "Epoch [1/2], Step [2450/7464], Loss: 4.1496\n",
      "Epoch [1/2], Step [2451/7464], Loss: 3.4075\n",
      "Epoch [1/2], Step [2452/7464], Loss: 4.1306\n",
      "Epoch [1/2], Step [2453/7464], Loss: 3.3103\n",
      "Epoch [1/2], Step [2454/7464], Loss: 3.4163\n",
      "Epoch [1/2], Step [2455/7464], Loss: 3.7999\n",
      "Epoch [1/2], Step [2456/7464], Loss: 3.8928\n",
      "Epoch [1/2], Step [2457/7464], Loss: 3.3399\n",
      "Epoch [1/2], Step [2458/7464], Loss: 3.2750\n",
      "Epoch [1/2], Step [2459/7464], Loss: 3.6189\n",
      "Epoch [1/2], Step [2460/7464], Loss: 3.3791\n",
      "Epoch [1/2], Step [2461/7464], Loss: 3.6999\n",
      "Epoch [1/2], Step [2462/7464], Loss: 3.4755\n",
      "Epoch [1/2], Step [2463/7464], Loss: 3.3303\n",
      "Epoch [1/2], Step [2464/7464], Loss: 3.9513\n",
      "Epoch [1/2], Step [2465/7464], Loss: 3.5151\n",
      "Epoch [1/2], Step [2466/7464], Loss: 3.5748\n",
      "Epoch [1/2], Step [2467/7464], Loss: 3.3208\n",
      "Epoch [1/2], Step [2468/7464], Loss: 3.6517\n",
      "Epoch [1/2], Step [2469/7464], Loss: 3.7432\n",
      "Epoch [1/2], Step [2470/7464], Loss: 3.6020\n",
      "Epoch [1/2], Step [2471/7464], Loss: 3.6218\n",
      "Epoch [1/2], Step [2472/7464], Loss: 3.5324\n",
      "Epoch [1/2], Step [2473/7464], Loss: 4.0456\n",
      "Epoch [1/2], Step [2474/7464], Loss: 3.5147\n",
      "Epoch [1/2], Step [2475/7464], Loss: 3.6071\n",
      "Epoch [1/2], Step [2476/7464], Loss: 3.2180\n",
      "Epoch [1/2], Step [2477/7464], Loss: 3.7708\n",
      "Epoch [1/2], Step [2478/7464], Loss: 3.3451\n",
      "Epoch [1/2], Step [2479/7464], Loss: 3.1061\n",
      "Epoch [1/2], Step [2480/7464], Loss: 3.3391\n",
      "Epoch [1/2], Step [2481/7464], Loss: 3.2561\n",
      "Epoch [1/2], Step [2482/7464], Loss: 3.8989\n",
      "Epoch [1/2], Step [2483/7464], Loss: 3.6422\n",
      "Epoch [1/2], Step [2484/7464], Loss: 3.8324\n",
      "Epoch [1/2], Step [2485/7464], Loss: 2.8988\n",
      "Epoch [1/2], Step [2486/7464], Loss: 3.7243\n",
      "Epoch [1/2], Step [2487/7464], Loss: 3.8816\n",
      "Epoch [1/2], Step [2488/7464], Loss: 3.5617\n",
      "Epoch [1/2], Step [2489/7464], Loss: 3.4455\n",
      "Epoch [1/2], Step [2490/7464], Loss: 2.9403\n",
      "Epoch [1/2], Step [2491/7464], Loss: 3.4787\n",
      "Epoch [1/2], Step [2492/7464], Loss: 3.8576\n",
      "Epoch [1/2], Step [2493/7464], Loss: 3.8041\n",
      "Epoch [1/2], Step [2494/7464], Loss: 4.6957\n",
      "Epoch [1/2], Step [2495/7464], Loss: 4.0969\n",
      "Epoch [1/2], Step [2496/7464], Loss: 4.1407\n",
      "Epoch [1/2], Step [2497/7464], Loss: 3.7514\n",
      "Epoch [1/2], Step [2498/7464], Loss: 4.0786\n",
      "Epoch [1/2], Step [2499/7464], Loss: 2.9430\n",
      "Epoch [1/2], Step [2500/7464], Loss: 3.7692\n",
      "Epoch [1/2], Step [2501/7464], Loss: 4.2695\n",
      "Epoch [1/2], Step [2502/7464], Loss: 3.3870\n",
      "Epoch [1/2], Step [2503/7464], Loss: 4.3590\n",
      "Epoch [1/2], Step [2504/7464], Loss: 3.9721\n",
      "Epoch [1/2], Step [2505/7464], Loss: 3.8587\n",
      "Epoch [1/2], Step [2506/7464], Loss: 3.6883\n",
      "Epoch [1/2], Step [2507/7464], Loss: 3.7120\n",
      "Epoch [1/2], Step [2508/7464], Loss: 3.2158\n",
      "Epoch [1/2], Step [2509/7464], Loss: 3.7222\n",
      "Epoch [1/2], Step [2510/7464], Loss: 3.6368\n",
      "Epoch [1/2], Step [2511/7464], Loss: 3.6896\n",
      "Epoch [1/2], Step [2512/7464], Loss: 3.7108\n",
      "Epoch [1/2], Step [2513/7464], Loss: 3.3069\n",
      "Epoch [1/2], Step [2514/7464], Loss: 3.7282\n",
      "Epoch [1/2], Step [2515/7464], Loss: 3.5878\n",
      "Epoch [1/2], Step [2516/7464], Loss: 3.1154\n",
      "Epoch [1/2], Step [2517/7464], Loss: 3.4598\n",
      "Epoch [1/2], Step [2518/7464], Loss: 3.5212\n",
      "Epoch [1/2], Step [2519/7464], Loss: 3.5829\n",
      "Epoch [1/2], Step [2520/7464], Loss: 3.8691\n",
      "Epoch [1/2], Step [2521/7464], Loss: 3.6304\n",
      "Epoch [1/2], Step [2522/7464], Loss: 3.9437\n",
      "Epoch [1/2], Step [2523/7464], Loss: 3.7669\n",
      "Epoch [1/2], Step [2524/7464], Loss: 3.6575\n",
      "Epoch [1/2], Step [2525/7464], Loss: 4.1774\n",
      "Epoch [1/2], Step [2526/7464], Loss: 3.7278\n",
      "Epoch [1/2], Step [2527/7464], Loss: 3.6122\n",
      "Epoch [1/2], Step [2528/7464], Loss: 3.4861\n",
      "Epoch [1/2], Step [2529/7464], Loss: 3.3353\n",
      "Epoch [1/2], Step [2530/7464], Loss: 3.6664\n",
      "Epoch [1/2], Step [2531/7464], Loss: 4.0811\n",
      "Epoch [1/2], Step [2532/7464], Loss: 3.7101\n",
      "Epoch [1/2], Step [2533/7464], Loss: 3.6331\n",
      "Epoch [1/2], Step [2534/7464], Loss: 4.0169\n",
      "Epoch [1/2], Step [2535/7464], Loss: 3.7627\n",
      "Epoch [1/2], Step [2536/7464], Loss: 3.3393\n",
      "Epoch [1/2], Step [2537/7464], Loss: 4.0430\n",
      "Epoch [1/2], Step [2538/7464], Loss: 3.8545\n",
      "Epoch [1/2], Step [2539/7464], Loss: 4.1438\n",
      "Epoch [1/2], Step [2540/7464], Loss: 2.9634\n",
      "Epoch [1/2], Step [2541/7464], Loss: 3.6727\n",
      "Epoch [1/2], Step [2542/7464], Loss: 3.4384\n",
      "Epoch [1/2], Step [2543/7464], Loss: 4.1563\n",
      "Epoch [1/2], Step [2544/7464], Loss: 3.6804\n",
      "Epoch [1/2], Step [2545/7464], Loss: 3.6476\n",
      "Epoch [1/2], Step [2546/7464], Loss: 3.8612\n",
      "Epoch [1/2], Step [2547/7464], Loss: 3.5536\n",
      "Epoch [1/2], Step [2548/7464], Loss: 3.3793\n",
      "Epoch [1/2], Step [2549/7464], Loss: 3.3586\n",
      "Epoch [1/2], Step [2550/7464], Loss: 3.8677\n",
      "Epoch [1/2], Step [2551/7464], Loss: 3.7319\n",
      "Epoch [1/2], Step [2552/7464], Loss: 3.9720\n",
      "Epoch [1/2], Step [2553/7464], Loss: 3.5075\n",
      "Epoch [1/2], Step [2554/7464], Loss: 3.9133\n",
      "Epoch [1/2], Step [2555/7464], Loss: 3.0866\n",
      "Epoch [1/2], Step [2556/7464], Loss: 3.6892\n",
      "Epoch [1/2], Step [2557/7464], Loss: 3.5096\n",
      "Epoch [1/2], Step [2558/7464], Loss: 3.3292\n",
      "Epoch [1/2], Step [2559/7464], Loss: 3.5450\n",
      "Epoch [1/2], Step [2560/7464], Loss: 4.2440\n",
      "Epoch [1/2], Step [2561/7464], Loss: 3.5790\n",
      "Epoch [1/2], Step [2562/7464], Loss: 3.1000\n",
      "Epoch [1/2], Step [2563/7464], Loss: 3.7546\n",
      "Epoch [1/2], Step [2564/7464], Loss: 4.1455\n",
      "Epoch [1/2], Step [2565/7464], Loss: 3.4384\n",
      "Epoch [1/2], Step [2566/7464], Loss: 4.0495\n",
      "Epoch [1/2], Step [2567/7464], Loss: 3.4683\n",
      "Epoch [1/2], Step [2568/7464], Loss: 3.4566\n",
      "Epoch [1/2], Step [2569/7464], Loss: 3.4496\n",
      "Epoch [1/2], Step [2570/7464], Loss: 3.9126\n",
      "Epoch [1/2], Step [2571/7464], Loss: 3.5839\n",
      "Epoch [1/2], Step [2572/7464], Loss: 3.8684\n",
      "Epoch [1/2], Step [2573/7464], Loss: 3.9235\n",
      "Epoch [1/2], Step [2574/7464], Loss: 3.6385\n",
      "Epoch [1/2], Step [2575/7464], Loss: 3.2515\n",
      "Epoch [1/2], Step [2576/7464], Loss: 3.7090\n",
      "Epoch [1/2], Step [2577/7464], Loss: 3.4260\n",
      "Epoch [1/2], Step [2578/7464], Loss: 3.5728\n",
      "Epoch [1/2], Step [2579/7464], Loss: 3.6955\n",
      "Epoch [1/2], Step [2580/7464], Loss: 3.2034\n",
      "Epoch [1/2], Step [2581/7464], Loss: 3.0939\n",
      "Epoch [1/2], Step [2582/7464], Loss: 3.3596\n",
      "Epoch [1/2], Step [2583/7464], Loss: 3.6594\n",
      "Epoch [1/2], Step [2584/7464], Loss: 3.5132\n",
      "Epoch [1/2], Step [2585/7464], Loss: 3.1056\n",
      "Epoch [1/2], Step [2586/7464], Loss: 3.7911\n",
      "Epoch [1/2], Step [2587/7464], Loss: 4.0105\n",
      "Epoch [1/2], Step [2588/7464], Loss: 4.7558\n",
      "Epoch [1/2], Step [2589/7464], Loss: 2.9051\n",
      "Epoch [1/2], Step [2590/7464], Loss: 3.4390\n",
      "Epoch [1/2], Step [2591/7464], Loss: 3.6346\n",
      "Epoch [1/2], Step [2592/7464], Loss: 3.3702\n",
      "Epoch [1/2], Step [2593/7464], Loss: 3.2084\n",
      "Epoch [1/2], Step [2594/7464], Loss: 3.5281\n",
      "Epoch [1/2], Step [2595/7464], Loss: 3.1877\n",
      "Epoch [1/2], Step [2596/7464], Loss: 3.4561\n",
      "Epoch [1/2], Step [2597/7464], Loss: 3.8057\n",
      "Epoch [1/2], Step [2598/7464], Loss: 3.7260\n",
      "Epoch [1/2], Step [2599/7464], Loss: 3.4476\n",
      "Epoch [1/2], Step [2600/7464], Loss: 3.8182\n",
      "Epoch [1/2], Step [2601/7464], Loss: 3.1090\n",
      "Epoch [1/2], Step [2602/7464], Loss: 3.5457\n",
      "Epoch [1/2], Step [2603/7464], Loss: 3.1434\n",
      "Epoch [1/2], Step [2604/7464], Loss: 3.4969\n",
      "Epoch [1/2], Step [2605/7464], Loss: 3.8265\n",
      "Epoch [1/2], Step [2606/7464], Loss: 4.1102\n",
      "Epoch [1/2], Step [2607/7464], Loss: 4.3793\n",
      "Epoch [1/2], Step [2608/7464], Loss: 4.2474\n",
      "Epoch [1/2], Step [2609/7464], Loss: 3.4474\n",
      "Epoch [1/2], Step [2610/7464], Loss: 3.0125\n",
      "Epoch [1/2], Step [2611/7464], Loss: 3.7511\n",
      "Epoch [1/2], Step [2612/7464], Loss: 3.1394\n",
      "Epoch [1/2], Step [2613/7464], Loss: 3.9023\n",
      "Epoch [1/2], Step [2614/7464], Loss: 3.8381\n",
      "Epoch [1/2], Step [2615/7464], Loss: 3.3484\n",
      "Epoch [1/2], Step [2616/7464], Loss: 3.6351\n",
      "Epoch [1/2], Step [2617/7464], Loss: 3.4467\n",
      "Epoch [1/2], Step [2618/7464], Loss: 3.8791\n",
      "Epoch [1/2], Step [2619/7464], Loss: 3.9550\n",
      "Epoch [1/2], Step [2620/7464], Loss: 3.0686\n",
      "Epoch [1/2], Step [2621/7464], Loss: 3.3256\n",
      "Epoch [1/2], Step [2622/7464], Loss: 3.5740\n",
      "Epoch [1/2], Step [2623/7464], Loss: 3.2184\n",
      "Epoch [1/2], Step [2624/7464], Loss: 3.9260\n",
      "Epoch [1/2], Step [2625/7464], Loss: 3.5056\n",
      "Epoch [1/2], Step [2626/7464], Loss: 3.6761\n",
      "Epoch [1/2], Step [2627/7464], Loss: 3.6886\n",
      "Epoch [1/2], Step [2628/7464], Loss: 3.7158\n",
      "Epoch [1/2], Step [2629/7464], Loss: 3.7846\n",
      "Epoch [1/2], Step [2630/7464], Loss: 3.9142\n",
      "Epoch [1/2], Step [2631/7464], Loss: 3.7305\n",
      "Epoch [1/2], Step [2632/7464], Loss: 3.2137\n",
      "Epoch [1/2], Step [2633/7464], Loss: 2.5193\n",
      "Epoch [1/2], Step [2634/7464], Loss: 3.6184\n",
      "Epoch [1/2], Step [2635/7464], Loss: 3.2345\n",
      "Epoch [1/2], Step [2636/7464], Loss: 4.6010\n",
      "Epoch [1/2], Step [2637/7464], Loss: 2.9626\n",
      "Epoch [1/2], Step [2638/7464], Loss: 3.2831\n",
      "Epoch [1/2], Step [2639/7464], Loss: 3.9100\n",
      "Epoch [1/2], Step [2640/7464], Loss: 4.1145\n",
      "Epoch [1/2], Step [2641/7464], Loss: 3.3517\n",
      "Epoch [1/2], Step [2642/7464], Loss: 3.4490\n",
      "Epoch [1/2], Step [2643/7464], Loss: 4.1106\n",
      "Epoch [1/2], Step [2644/7464], Loss: 3.9446\n",
      "Epoch [1/2], Step [2645/7464], Loss: 3.5790\n",
      "Epoch [1/2], Step [2646/7464], Loss: 3.2972\n",
      "Epoch [1/2], Step [2647/7464], Loss: 3.0191\n",
      "Epoch [1/2], Step [2648/7464], Loss: 3.0869\n",
      "Epoch [1/2], Step [2649/7464], Loss: 3.1047\n",
      "Epoch [1/2], Step [2650/7464], Loss: 3.7993\n",
      "Epoch [1/2], Step [2651/7464], Loss: 3.7631\n",
      "Epoch [1/2], Step [2652/7464], Loss: 3.6888\n",
      "Epoch [1/2], Step [2653/7464], Loss: 3.2660\n",
      "Epoch [1/2], Step [2654/7464], Loss: 3.4201\n",
      "Epoch [1/2], Step [2655/7464], Loss: 3.7073\n",
      "Epoch [1/2], Step [2656/7464], Loss: 3.5513\n",
      "Epoch [1/2], Step [2657/7464], Loss: 4.2466\n",
      "Epoch [1/2], Step [2658/7464], Loss: 3.1927\n",
      "Epoch [1/2], Step [2659/7464], Loss: 4.0301\n",
      "Epoch [1/2], Step [2660/7464], Loss: 4.0319\n",
      "Epoch [1/2], Step [2661/7464], Loss: 3.4380\n",
      "Epoch [1/2], Step [2662/7464], Loss: 3.5175\n",
      "Epoch [1/2], Step [2663/7464], Loss: 3.7961\n",
      "Epoch [1/2], Step [2664/7464], Loss: 4.0873\n",
      "Epoch [1/2], Step [2665/7464], Loss: 3.9867\n",
      "Epoch [1/2], Step [2666/7464], Loss: 3.5478\n",
      "Epoch [1/2], Step [2667/7464], Loss: 3.6340\n",
      "Epoch [1/2], Step [2668/7464], Loss: 3.8137\n",
      "Epoch [1/2], Step [2669/7464], Loss: 3.4241\n",
      "Epoch [1/2], Step [2670/7464], Loss: 3.9515\n",
      "Epoch [1/2], Step [2671/7464], Loss: 3.3551\n",
      "Epoch [1/2], Step [2672/7464], Loss: 4.1610\n",
      "Epoch [1/2], Step [2673/7464], Loss: 3.6634\n",
      "Epoch [1/2], Step [2674/7464], Loss: 3.6886\n",
      "Epoch [1/2], Step [2675/7464], Loss: 3.4624\n",
      "Epoch [1/2], Step [2676/7464], Loss: 3.7208\n",
      "Epoch [1/2], Step [2677/7464], Loss: 2.9953\n",
      "Epoch [1/2], Step [2678/7464], Loss: 3.9849\n",
      "Epoch [1/2], Step [2679/7464], Loss: 3.5446\n",
      "Epoch [1/2], Step [2680/7464], Loss: 3.7316\n",
      "Epoch [1/2], Step [2681/7464], Loss: 4.0391\n",
      "Epoch [1/2], Step [2682/7464], Loss: 3.5653\n",
      "Epoch [1/2], Step [2683/7464], Loss: 3.5941\n",
      "Epoch [1/2], Step [2684/7464], Loss: 3.8050\n",
      "Epoch [1/2], Step [2685/7464], Loss: 4.3103\n",
      "Epoch [1/2], Step [2686/7464], Loss: 3.3208\n",
      "Epoch [1/2], Step [2687/7464], Loss: 3.8683\n",
      "Epoch [1/2], Step [2688/7464], Loss: 3.8780\n",
      "Epoch [1/2], Step [2689/7464], Loss: 3.1581\n",
      "Epoch [1/2], Step [2690/7464], Loss: 3.3225\n",
      "Epoch [1/2], Step [2691/7464], Loss: 3.4759\n",
      "Epoch [1/2], Step [2692/7464], Loss: 3.3384\n",
      "Epoch [1/2], Step [2693/7464], Loss: 3.4070\n",
      "Epoch [1/2], Step [2694/7464], Loss: 3.7587\n",
      "Epoch [1/2], Step [2695/7464], Loss: 4.1433\n",
      "Epoch [1/2], Step [2696/7464], Loss: 3.0071\n",
      "Epoch [1/2], Step [2697/7464], Loss: 3.4339\n",
      "Epoch [1/2], Step [2698/7464], Loss: 2.8449\n",
      "Epoch [1/2], Step [2699/7464], Loss: 3.8018\n",
      "Epoch [1/2], Step [2700/7464], Loss: 3.0378\n",
      "Epoch [1/2], Step [2701/7464], Loss: 3.4358\n",
      "Epoch [1/2], Step [2702/7464], Loss: 3.7330\n",
      "Epoch [1/2], Step [2703/7464], Loss: 4.4879\n",
      "Epoch [1/2], Step [2704/7464], Loss: 3.9888\n",
      "Epoch [1/2], Step [2705/7464], Loss: 3.3553\n",
      "Epoch [1/2], Step [2706/7464], Loss: 2.8991\n",
      "Epoch [1/2], Step [2707/7464], Loss: 4.0661\n",
      "Epoch [1/2], Step [2708/7464], Loss: 3.7928\n",
      "Epoch [1/2], Step [2709/7464], Loss: 4.0684\n",
      "Epoch [1/2], Step [2710/7464], Loss: 4.0356\n",
      "Epoch [1/2], Step [2711/7464], Loss: 3.0952\n",
      "Epoch [1/2], Step [2712/7464], Loss: 3.1058\n",
      "Epoch [1/2], Step [2713/7464], Loss: 2.9799\n",
      "Epoch [1/2], Step [2714/7464], Loss: 4.0992\n",
      "Epoch [1/2], Step [2715/7464], Loss: 3.5208\n",
      "Epoch [1/2], Step [2716/7464], Loss: 3.2082\n",
      "Epoch [1/2], Step [2717/7464], Loss: 3.6042\n",
      "Epoch [1/2], Step [2718/7464], Loss: 3.7731\n",
      "Epoch [1/2], Step [2719/7464], Loss: 3.6203\n",
      "Epoch [1/2], Step [2720/7464], Loss: 3.7826\n",
      "Epoch [1/2], Step [2721/7464], Loss: 4.0501\n",
      "Epoch [1/2], Step [2722/7464], Loss: 3.5977\n",
      "Epoch [1/2], Step [2723/7464], Loss: 3.2019\n",
      "Epoch [1/2], Step [2724/7464], Loss: 3.1986\n",
      "Epoch [1/2], Step [2725/7464], Loss: 2.9470\n",
      "Epoch [1/2], Step [2726/7464], Loss: 3.8706\n",
      "Epoch [1/2], Step [2727/7464], Loss: 2.8268\n",
      "Epoch [1/2], Step [2728/7464], Loss: 4.0914\n",
      "Epoch [1/2], Step [2729/7464], Loss: 3.9126\n",
      "Epoch [1/2], Step [2730/7464], Loss: 3.8815\n",
      "Epoch [1/2], Step [2731/7464], Loss: 3.7913\n",
      "Epoch [1/2], Step [2732/7464], Loss: 3.4386\n",
      "Epoch [1/2], Step [2733/7464], Loss: 3.7651\n",
      "Epoch [1/2], Step [2734/7464], Loss: 2.8359\n",
      "Epoch [1/2], Step [2735/7464], Loss: 3.5377\n",
      "Epoch [1/2], Step [2736/7464], Loss: 3.2150\n",
      "Epoch [1/2], Step [2737/7464], Loss: 3.3040\n",
      "Epoch [1/2], Step [2738/7464], Loss: 3.6005\n",
      "Epoch [1/2], Step [2739/7464], Loss: 2.8805\n",
      "Epoch [1/2], Step [2740/7464], Loss: 3.5356\n",
      "Epoch [1/2], Step [2741/7464], Loss: 3.3942\n",
      "Epoch [1/2], Step [2742/7464], Loss: 3.4370\n",
      "Epoch [1/2], Step [2743/7464], Loss: 3.5826\n",
      "Epoch [1/2], Step [2744/7464], Loss: 3.8041\n",
      "Epoch [1/2], Step [2745/7464], Loss: 3.0757\n",
      "Epoch [1/2], Step [2746/7464], Loss: 3.2904\n",
      "Epoch [1/2], Step [2747/7464], Loss: 3.9690\n",
      "Epoch [1/2], Step [2748/7464], Loss: 3.1962\n",
      "Epoch [1/2], Step [2749/7464], Loss: 3.6391\n",
      "Epoch [1/2], Step [2750/7464], Loss: 3.6448\n",
      "Epoch [1/2], Step [2751/7464], Loss: 3.6920\n",
      "Epoch [1/2], Step [2752/7464], Loss: 3.1378\n",
      "Epoch [1/2], Step [2753/7464], Loss: 3.6382\n",
      "Epoch [1/2], Step [2754/7464], Loss: 3.1589\n",
      "Epoch [1/2], Step [2755/7464], Loss: 3.4770\n",
      "Epoch [1/2], Step [2756/7464], Loss: 4.4310\n",
      "Epoch [1/2], Step [2757/7464], Loss: 2.6331\n",
      "Epoch [1/2], Step [2758/7464], Loss: 2.8765\n",
      "Epoch [1/2], Step [2759/7464], Loss: 2.9572\n",
      "Epoch [1/2], Step [2760/7464], Loss: 3.0202\n",
      "Epoch [1/2], Step [2761/7464], Loss: 3.6104\n",
      "Epoch [1/2], Step [2762/7464], Loss: 3.5662\n",
      "Epoch [1/2], Step [2763/7464], Loss: 3.9904\n",
      "Epoch [1/2], Step [2764/7464], Loss: 3.6565\n",
      "Epoch [1/2], Step [2765/7464], Loss: 3.8059\n",
      "Epoch [1/2], Step [2766/7464], Loss: 3.7189\n",
      "Epoch [1/2], Step [2767/7464], Loss: 3.9074\n",
      "Epoch [1/2], Step [2768/7464], Loss: 4.0286\n",
      "Epoch [1/2], Step [2769/7464], Loss: 3.6533\n",
      "Epoch [1/2], Step [2770/7464], Loss: 3.8241\n",
      "Epoch [1/2], Step [2771/7464], Loss: 3.5844\n",
      "Epoch [1/2], Step [2772/7464], Loss: 3.5073\n",
      "Epoch [1/2], Step [2773/7464], Loss: 3.8840\n",
      "Epoch [1/2], Step [2774/7464], Loss: 3.3527\n",
      "Epoch [1/2], Step [2775/7464], Loss: 3.7893\n",
      "Epoch [1/2], Step [2776/7464], Loss: 3.9428\n",
      "Epoch [1/2], Step [2777/7464], Loss: 3.8664\n",
      "Epoch [1/2], Step [2778/7464], Loss: 3.5504\n",
      "Epoch [1/2], Step [2779/7464], Loss: 3.9111\n",
      "Epoch [1/2], Step [2780/7464], Loss: 3.6534\n",
      "Epoch [1/2], Step [2781/7464], Loss: 3.1116\n",
      "Epoch [1/2], Step [2782/7464], Loss: 3.7484\n",
      "Epoch [1/2], Step [2783/7464], Loss: 3.9506\n",
      "Epoch [1/2], Step [2784/7464], Loss: 3.6556\n",
      "Epoch [1/2], Step [2785/7464], Loss: 4.0578\n",
      "Epoch [1/2], Step [2786/7464], Loss: 3.8038\n",
      "Epoch [1/2], Step [2787/7464], Loss: 3.5387\n",
      "Epoch [1/2], Step [2788/7464], Loss: 3.9678\n",
      "Epoch [1/2], Step [2789/7464], Loss: 3.5284\n",
      "Epoch [1/2], Step [2790/7464], Loss: 3.4790\n",
      "Epoch [1/2], Step [2791/7464], Loss: 2.9998\n",
      "Epoch [1/2], Step [2792/7464], Loss: 3.8415\n",
      "Epoch [1/2], Step [2793/7464], Loss: 3.8260\n",
      "Epoch [1/2], Step [2794/7464], Loss: 3.3263\n",
      "Epoch [1/2], Step [2795/7464], Loss: 3.1920\n",
      "Epoch [1/2], Step [2796/7464], Loss: 3.8135\n",
      "Epoch [1/2], Step [2797/7464], Loss: 3.2127\n",
      "Epoch [1/2], Step [2798/7464], Loss: 3.5520\n",
      "Epoch [1/2], Step [2799/7464], Loss: 4.0089\n",
      "Epoch [1/2], Step [2800/7464], Loss: 3.3059\n",
      "Epoch [1/2], Step [2801/7464], Loss: 4.0222\n",
      "Epoch [1/2], Step [2802/7464], Loss: 3.3727\n",
      "Epoch [1/2], Step [2803/7464], Loss: 3.9613\n",
      "Epoch [1/2], Step [2804/7464], Loss: 3.5779\n",
      "Epoch [1/2], Step [2805/7464], Loss: 3.1724\n",
      "Epoch [1/2], Step [2806/7464], Loss: 3.6168\n",
      "Epoch [1/2], Step [2807/7464], Loss: 3.2825\n",
      "Epoch [1/2], Step [2808/7464], Loss: 2.9929\n",
      "Epoch [1/2], Step [2809/7464], Loss: 3.1306\n",
      "Epoch [1/2], Step [2810/7464], Loss: 3.0830\n",
      "Epoch [1/2], Step [2811/7464], Loss: 3.9988\n",
      "Epoch [1/2], Step [2812/7464], Loss: 3.9197\n",
      "Epoch [1/2], Step [2813/7464], Loss: 3.3874\n",
      "Epoch [1/2], Step [2814/7464], Loss: 3.6964\n",
      "Epoch [1/2], Step [2815/7464], Loss: 2.9805\n",
      "Epoch [1/2], Step [2816/7464], Loss: 3.2350\n",
      "Epoch [1/2], Step [2817/7464], Loss: 3.7088\n",
      "Epoch [1/2], Step [2818/7464], Loss: 3.5707\n",
      "Epoch [1/2], Step [2819/7464], Loss: 3.4535\n",
      "Epoch [1/2], Step [2820/7464], Loss: 3.2942\n",
      "Epoch [1/2], Step [2821/7464], Loss: 3.6458\n",
      "Epoch [1/2], Step [2822/7464], Loss: 2.7042\n",
      "Epoch [1/2], Step [2823/7464], Loss: 3.3856\n",
      "Epoch [1/2], Step [2824/7464], Loss: 3.8306\n",
      "Epoch [1/2], Step [2825/7464], Loss: 3.3884\n",
      "Epoch [1/2], Step [2826/7464], Loss: 3.7452\n",
      "Epoch [1/2], Step [2827/7464], Loss: 3.5201\n",
      "Epoch [1/2], Step [2828/7464], Loss: 3.4100\n",
      "Epoch [1/2], Step [2829/7464], Loss: 3.6453\n",
      "Epoch [1/2], Step [2830/7464], Loss: 3.5497\n",
      "Epoch [1/2], Step [2831/7464], Loss: 3.6021\n",
      "Epoch [1/2], Step [2832/7464], Loss: 3.3335\n",
      "Epoch [1/2], Step [2833/7464], Loss: 3.6290\n",
      "Epoch [1/2], Step [2834/7464], Loss: 3.5441\n",
      "Epoch [1/2], Step [2835/7464], Loss: 3.8661\n",
      "Epoch [1/2], Step [2836/7464], Loss: 3.8598\n",
      "Epoch [1/2], Step [2837/7464], Loss: 3.8452\n",
      "Epoch [1/2], Step [2838/7464], Loss: 3.3531\n",
      "Epoch [1/2], Step [2839/7464], Loss: 3.0377\n",
      "Epoch [1/2], Step [2840/7464], Loss: 4.2613\n",
      "Epoch [1/2], Step [2841/7464], Loss: 3.3367\n",
      "Epoch [1/2], Step [2842/7464], Loss: 3.5907\n",
      "Epoch [1/2], Step [2843/7464], Loss: 3.7192\n",
      "Epoch [1/2], Step [2844/7464], Loss: 2.9310\n",
      "Epoch [1/2], Step [2845/7464], Loss: 2.8972\n",
      "Epoch [1/2], Step [2846/7464], Loss: 3.6297\n",
      "Epoch [1/2], Step [2847/7464], Loss: 3.3214\n",
      "Epoch [1/2], Step [2848/7464], Loss: 3.8981\n",
      "Epoch [1/2], Step [2849/7464], Loss: 3.5249\n",
      "Epoch [1/2], Step [2850/7464], Loss: 3.6058\n",
      "Epoch [1/2], Step [2851/7464], Loss: 3.0422\n",
      "Epoch [1/2], Step [2852/7464], Loss: 3.1405\n",
      "Epoch [1/2], Step [2853/7464], Loss: 3.6611\n",
      "Epoch [1/2], Step [2854/7464], Loss: 3.6117\n",
      "Epoch [1/2], Step [2855/7464], Loss: 2.3396\n",
      "Epoch [1/2], Step [2856/7464], Loss: 3.1763\n",
      "Epoch [1/2], Step [2857/7464], Loss: 3.6425\n",
      "Epoch [1/2], Step [2858/7464], Loss: 2.6005\n",
      "Epoch [1/2], Step [2859/7464], Loss: 3.4191\n",
      "Epoch [1/2], Step [2860/7464], Loss: 3.5941\n",
      "Epoch [1/2], Step [2861/7464], Loss: 3.8731\n",
      "Epoch [1/2], Step [2862/7464], Loss: 3.5690\n",
      "Epoch [1/2], Step [2863/7464], Loss: 3.3862\n",
      "Epoch [1/2], Step [2864/7464], Loss: 3.5596\n",
      "Epoch [1/2], Step [2865/7464], Loss: 2.7129\n",
      "Epoch [1/2], Step [2866/7464], Loss: 3.2321\n",
      "Epoch [1/2], Step [2867/7464], Loss: 3.0647\n",
      "Epoch [1/2], Step [2868/7464], Loss: 3.5667\n",
      "Epoch [1/2], Step [2869/7464], Loss: 3.2922\n",
      "Epoch [1/2], Step [2870/7464], Loss: 3.6331\n",
      "Epoch [1/2], Step [2871/7464], Loss: 3.7637\n",
      "Epoch [1/2], Step [2872/7464], Loss: 3.1065\n",
      "Epoch [1/2], Step [2873/7464], Loss: 2.5939\n",
      "Epoch [1/2], Step [2874/7464], Loss: 3.2436\n",
      "Epoch [1/2], Step [2875/7464], Loss: 4.3074\n",
      "Epoch [1/2], Step [2876/7464], Loss: 2.9966\n",
      "Epoch [1/2], Step [2877/7464], Loss: 3.5453\n",
      "Epoch [1/2], Step [2878/7464], Loss: 3.6367\n",
      "Epoch [1/2], Step [2879/7464], Loss: 3.4047\n",
      "Epoch [1/2], Step [2880/7464], Loss: 3.1263\n",
      "Epoch [1/2], Step [2881/7464], Loss: 3.6582\n",
      "Epoch [1/2], Step [2882/7464], Loss: 3.8982\n",
      "Epoch [1/2], Step [2883/7464], Loss: 3.3698\n",
      "Epoch [1/2], Step [2884/7464], Loss: 3.9247\n",
      "Epoch [1/2], Step [2885/7464], Loss: 3.1710\n",
      "Epoch [1/2], Step [2886/7464], Loss: 3.5338\n",
      "Epoch [1/2], Step [2887/7464], Loss: 3.2381\n",
      "Epoch [1/2], Step [2888/7464], Loss: 3.8221\n",
      "Epoch [1/2], Step [2889/7464], Loss: 4.5537\n",
      "Epoch [1/2], Step [2890/7464], Loss: 3.8769\n",
      "Epoch [1/2], Step [2891/7464], Loss: 3.8947\n",
      "Epoch [1/2], Step [2892/7464], Loss: 3.3140\n",
      "Epoch [1/2], Step [2893/7464], Loss: 3.7544\n",
      "Epoch [1/2], Step [2894/7464], Loss: 4.2861\n",
      "Epoch [1/2], Step [2895/7464], Loss: 3.6150\n",
      "Epoch [1/2], Step [2896/7464], Loss: 3.6313\n",
      "Epoch [1/2], Step [2897/7464], Loss: 3.3709\n",
      "Epoch [1/2], Step [2898/7464], Loss: 3.6435\n",
      "Epoch [1/2], Step [2899/7464], Loss: 3.1316\n",
      "Epoch [1/2], Step [2900/7464], Loss: 3.2971\n",
      "Epoch [1/2], Step [2901/7464], Loss: 3.3734\n",
      "Epoch [1/2], Step [2902/7464], Loss: 3.9624\n",
      "Epoch [1/2], Step [2903/7464], Loss: 3.5777\n",
      "Epoch [1/2], Step [2904/7464], Loss: 3.5988\n",
      "Epoch [1/2], Step [2905/7464], Loss: 4.0166\n",
      "Epoch [1/2], Step [2906/7464], Loss: 3.1064\n",
      "Epoch [1/2], Step [2907/7464], Loss: 3.4628\n",
      "Epoch [1/2], Step [2908/7464], Loss: 3.6587\n",
      "Epoch [1/2], Step [2909/7464], Loss: 2.9306\n",
      "Epoch [1/2], Step [2910/7464], Loss: 3.7347\n",
      "Epoch [1/2], Step [2911/7464], Loss: 3.9364\n",
      "Epoch [1/2], Step [2912/7464], Loss: 3.3208\n",
      "Epoch [1/2], Step [2913/7464], Loss: 4.4284\n",
      "Epoch [1/2], Step [2914/7464], Loss: 3.6657\n",
      "Epoch [1/2], Step [2915/7464], Loss: 3.3541\n",
      "Epoch [1/2], Step [2916/7464], Loss: 3.5885\n",
      "Epoch [1/2], Step [2917/7464], Loss: 3.3634\n",
      "Epoch [1/2], Step [2918/7464], Loss: 3.7773\n",
      "Epoch [1/2], Step [2919/7464], Loss: 3.3990\n",
      "Epoch [1/2], Step [2920/7464], Loss: 3.6176\n",
      "Epoch [1/2], Step [2921/7464], Loss: 3.7710\n",
      "Epoch [1/2], Step [2922/7464], Loss: 3.4378\n",
      "Epoch [1/2], Step [2923/7464], Loss: 3.6641\n",
      "Epoch [1/2], Step [2924/7464], Loss: 3.5338\n",
      "Epoch [1/2], Step [2925/7464], Loss: 3.2435\n",
      "Epoch [1/2], Step [2926/7464], Loss: 3.2410\n",
      "Epoch [1/2], Step [2927/7464], Loss: 3.3869\n",
      "Epoch [1/2], Step [2928/7464], Loss: 3.5315\n",
      "Epoch [1/2], Step [2929/7464], Loss: 3.8913\n",
      "Epoch [1/2], Step [2930/7464], Loss: 3.4328\n",
      "Epoch [1/2], Step [2931/7464], Loss: 3.3464\n",
      "Epoch [1/2], Step [2932/7464], Loss: 3.3474\n",
      "Epoch [1/2], Step [2933/7464], Loss: 2.8495\n",
      "Epoch [1/2], Step [2934/7464], Loss: 3.8088\n",
      "Epoch [1/2], Step [2935/7464], Loss: 3.7669\n",
      "Epoch [1/2], Step [2936/7464], Loss: 3.4648\n",
      "Epoch [1/2], Step [2937/7464], Loss: 3.7494\n",
      "Epoch [1/2], Step [2938/7464], Loss: 3.3514\n",
      "Epoch [1/2], Step [2939/7464], Loss: 2.8350\n",
      "Epoch [1/2], Step [2940/7464], Loss: 4.7842\n",
      "Epoch [1/2], Step [2941/7464], Loss: 4.2216\n",
      "Epoch [1/2], Step [2942/7464], Loss: 3.5740\n",
      "Epoch [1/2], Step [2943/7464], Loss: 2.7421\n",
      "Epoch [1/2], Step [2944/7464], Loss: 4.1474\n",
      "Epoch [1/2], Step [2945/7464], Loss: 3.5045\n",
      "Epoch [1/2], Step [2946/7464], Loss: 3.3735\n",
      "Epoch [1/2], Step [2947/7464], Loss: 2.1994\n",
      "Epoch [1/2], Step [2948/7464], Loss: 2.9453\n",
      "Epoch [1/2], Step [2949/7464], Loss: 4.0000\n",
      "Epoch [1/2], Step [2950/7464], Loss: 3.4701\n",
      "Epoch [1/2], Step [2951/7464], Loss: 3.2484\n",
      "Epoch [1/2], Step [2952/7464], Loss: 3.0137\n",
      "Epoch [1/2], Step [2953/7464], Loss: 3.7774\n",
      "Epoch [1/2], Step [2954/7464], Loss: 3.1892\n",
      "Epoch [1/2], Step [2955/7464], Loss: 3.5926\n",
      "Epoch [1/2], Step [2956/7464], Loss: 3.7408\n",
      "Epoch [1/2], Step [2957/7464], Loss: 3.7386\n",
      "Epoch [1/2], Step [2958/7464], Loss: 3.2588\n",
      "Epoch [1/2], Step [2959/7464], Loss: 3.7593\n",
      "Epoch [1/2], Step [2960/7464], Loss: 3.2759\n",
      "Epoch [1/2], Step [2961/7464], Loss: 3.6933\n",
      "Epoch [1/2], Step [2962/7464], Loss: 3.2194\n",
      "Epoch [1/2], Step [2963/7464], Loss: 2.9120\n",
      "Epoch [1/2], Step [2964/7464], Loss: 3.6145\n",
      "Epoch [1/2], Step [2965/7464], Loss: 3.2931\n",
      "Epoch [1/2], Step [2966/7464], Loss: 3.3399\n",
      "Epoch [1/2], Step [2967/7464], Loss: 3.6874\n",
      "Epoch [1/2], Step [2968/7464], Loss: 3.5010\n",
      "Epoch [1/2], Step [2969/7464], Loss: 2.8893\n",
      "Epoch [1/2], Step [2970/7464], Loss: 2.9577\n",
      "Epoch [1/2], Step [2971/7464], Loss: 3.1084\n",
      "Epoch [1/2], Step [2972/7464], Loss: 3.5711\n",
      "Epoch [1/2], Step [2973/7464], Loss: 3.9214\n",
      "Epoch [1/2], Step [2974/7464], Loss: 3.7271\n",
      "Epoch [1/2], Step [2975/7464], Loss: 3.5278\n",
      "Epoch [1/2], Step [2976/7464], Loss: 3.8114\n",
      "Epoch [1/2], Step [2977/7464], Loss: 3.4159\n",
      "Epoch [1/2], Step [2978/7464], Loss: 3.1157\n",
      "Epoch [1/2], Step [2979/7464], Loss: 2.4008\n",
      "Epoch [1/2], Step [2980/7464], Loss: 3.2589\n",
      "Epoch [1/2], Step [2981/7464], Loss: 3.4401\n",
      "Epoch [1/2], Step [2982/7464], Loss: 3.1657\n",
      "Epoch [1/2], Step [2983/7464], Loss: 3.1017\n",
      "Epoch [1/2], Step [2984/7464], Loss: 4.2783\n",
      "Epoch [1/2], Step [2985/7464], Loss: 3.0820\n",
      "Epoch [1/2], Step [2986/7464], Loss: 3.1584\n",
      "Epoch [1/2], Step [2987/7464], Loss: 4.1249\n",
      "Epoch [1/2], Step [2988/7464], Loss: 3.5446\n",
      "Epoch [1/2], Step [2989/7464], Loss: 4.1503\n",
      "Epoch [1/2], Step [2990/7464], Loss: 3.0463\n",
      "Epoch [1/2], Step [2991/7464], Loss: 3.7645\n",
      "Epoch [1/2], Step [2992/7464], Loss: 4.1120\n",
      "Epoch [1/2], Step [2993/7464], Loss: 3.4335\n",
      "Epoch [1/2], Step [2994/7464], Loss: 3.3393\n",
      "Epoch [1/2], Step [2995/7464], Loss: 3.3355\n",
      "Epoch [1/2], Step [2996/7464], Loss: 3.7213\n",
      "Epoch [1/2], Step [2997/7464], Loss: 3.6132\n",
      "Epoch [1/2], Step [2998/7464], Loss: 3.0788\n",
      "Epoch [1/2], Step [2999/7464], Loss: 3.7068\n",
      "Epoch [1/2], Step [3000/7464], Loss: 3.2562\n",
      "Epoch [1/2], Step [3001/7464], Loss: 3.0966\n",
      "Epoch [1/2], Step [3002/7464], Loss: 3.5484\n",
      "Epoch [1/2], Step [3003/7464], Loss: 3.7012\n",
      "Epoch [1/2], Step [3004/7464], Loss: 3.6653\n",
      "Epoch [1/2], Step [3005/7464], Loss: 3.4219\n",
      "Epoch [1/2], Step [3006/7464], Loss: 2.6039\n",
      "Epoch [1/2], Step [3007/7464], Loss: 3.6868\n",
      "Epoch [1/2], Step [3008/7464], Loss: 3.9933\n",
      "Epoch [1/2], Step [3009/7464], Loss: 2.5920\n",
      "Epoch [1/2], Step [3010/7464], Loss: 4.0773\n",
      "Epoch [1/2], Step [3011/7464], Loss: 3.3748\n",
      "Epoch [1/2], Step [3012/7464], Loss: 3.1061\n",
      "Epoch [1/2], Step [3013/7464], Loss: 3.3344\n",
      "Epoch [1/2], Step [3014/7464], Loss: 3.6833\n",
      "Epoch [1/2], Step [3015/7464], Loss: 3.2405\n",
      "Epoch [1/2], Step [3016/7464], Loss: 3.2495\n",
      "Epoch [1/2], Step [3017/7464], Loss: 3.4633\n",
      "Epoch [1/2], Step [3018/7464], Loss: 3.8516\n",
      "Epoch [1/2], Step [3019/7464], Loss: 4.0838\n",
      "Epoch [1/2], Step [3020/7464], Loss: 2.6591\n",
      "Epoch [1/2], Step [3021/7464], Loss: 3.4902\n",
      "Epoch [1/2], Step [3022/7464], Loss: 4.2348\n",
      "Epoch [1/2], Step [3023/7464], Loss: 3.9914\n",
      "Epoch [1/2], Step [3024/7464], Loss: 3.9129\n",
      "Epoch [1/2], Step [3025/7464], Loss: 2.8428\n",
      "Epoch [1/2], Step [3026/7464], Loss: 4.1505\n",
      "Epoch [1/2], Step [3027/7464], Loss: 3.8166\n",
      "Epoch [1/2], Step [3028/7464], Loss: 3.9631\n",
      "Epoch [1/2], Step [3029/7464], Loss: 2.9831\n",
      "Epoch [1/2], Step [3030/7464], Loss: 3.5384\n",
      "Epoch [1/2], Step [3031/7464], Loss: 3.8363\n",
      "Epoch [1/2], Step [3032/7464], Loss: 3.2593\n",
      "Epoch [1/2], Step [3033/7464], Loss: 3.3354\n",
      "Epoch [1/2], Step [3034/7464], Loss: 3.8946\n",
      "Epoch [1/2], Step [3035/7464], Loss: 3.6521\n",
      "Epoch [1/2], Step [3036/7464], Loss: 3.4767\n",
      "Epoch [1/2], Step [3037/7464], Loss: 3.2982\n",
      "Epoch [1/2], Step [3038/7464], Loss: 3.8944\n",
      "Epoch [1/2], Step [3039/7464], Loss: 3.0906\n",
      "Epoch [1/2], Step [3040/7464], Loss: 2.9987\n",
      "Epoch [1/2], Step [3041/7464], Loss: 3.3668\n",
      "Epoch [1/2], Step [3042/7464], Loss: 3.4007\n",
      "Epoch [1/2], Step [3043/7464], Loss: 2.6051\n",
      "Epoch [1/2], Step [3044/7464], Loss: 3.4338\n",
      "Epoch [1/2], Step [3045/7464], Loss: 3.9231\n",
      "Epoch [1/2], Step [3046/7464], Loss: 3.4409\n",
      "Epoch [1/2], Step [3047/7464], Loss: 3.6007\n",
      "Epoch [1/2], Step [3048/7464], Loss: 3.3715\n",
      "Epoch [1/2], Step [3049/7464], Loss: 3.9455\n",
      "Epoch [1/2], Step [3050/7464], Loss: 4.5025\n",
      "Epoch [1/2], Step [3051/7464], Loss: 3.8957\n",
      "Epoch [1/2], Step [3052/7464], Loss: 3.9479\n",
      "Epoch [1/2], Step [3053/7464], Loss: 4.0740\n",
      "Epoch [1/2], Step [3054/7464], Loss: 3.6530\n",
      "Epoch [1/2], Step [3055/7464], Loss: 3.3152\n",
      "Epoch [1/2], Step [3056/7464], Loss: 3.7290\n",
      "Epoch [1/2], Step [3057/7464], Loss: 3.5020\n",
      "Epoch [1/2], Step [3058/7464], Loss: 2.9150\n",
      "Epoch [1/2], Step [3059/7464], Loss: 3.8593\n",
      "Epoch [1/2], Step [3060/7464], Loss: 3.4558\n",
      "Epoch [1/2], Step [3061/7464], Loss: 3.6052\n",
      "Epoch [1/2], Step [3062/7464], Loss: 3.1787\n",
      "Epoch [1/2], Step [3063/7464], Loss: 3.5678\n",
      "Epoch [1/2], Step [3064/7464], Loss: 3.4824\n",
      "Epoch [1/2], Step [3065/7464], Loss: 3.7870\n",
      "Epoch [1/2], Step [3066/7464], Loss: 3.6977\n",
      "Epoch [1/2], Step [3067/7464], Loss: 3.2480\n",
      "Epoch [1/2], Step [3068/7464], Loss: 3.2600\n",
      "Epoch [1/2], Step [3069/7464], Loss: 3.5310\n",
      "Epoch [1/2], Step [3070/7464], Loss: 3.8587\n",
      "Epoch [1/2], Step [3071/7464], Loss: 3.7635\n",
      "Epoch [1/2], Step [3072/7464], Loss: 3.5782\n",
      "Epoch [1/2], Step [3073/7464], Loss: 3.0876\n",
      "Epoch [1/2], Step [3074/7464], Loss: 2.9467\n",
      "Epoch [1/2], Step [3075/7464], Loss: 3.2642\n",
      "Epoch [1/2], Step [3076/7464], Loss: 3.9908\n",
      "Epoch [1/2], Step [3077/7464], Loss: 4.0186\n",
      "Epoch [1/2], Step [3078/7464], Loss: 3.1934\n",
      "Epoch [1/2], Step [3079/7464], Loss: 3.8151\n",
      "Epoch [1/2], Step [3080/7464], Loss: 3.9318\n",
      "Epoch [1/2], Step [3081/7464], Loss: 3.2068\n",
      "Epoch [1/2], Step [3082/7464], Loss: 3.5545\n",
      "Epoch [1/2], Step [3083/7464], Loss: 3.7866\n",
      "Epoch [1/2], Step [3084/7464], Loss: 2.8923\n",
      "Epoch [1/2], Step [3085/7464], Loss: 4.0177\n",
      "Epoch [1/2], Step [3086/7464], Loss: 3.7945\n",
      "Epoch [1/2], Step [3087/7464], Loss: 4.3045\n",
      "Epoch [1/2], Step [3088/7464], Loss: 3.6008\n",
      "Epoch [1/2], Step [3089/7464], Loss: 3.5264\n",
      "Epoch [1/2], Step [3090/7464], Loss: 3.3797\n",
      "Epoch [1/2], Step [3091/7464], Loss: 3.3515\n",
      "Epoch [1/2], Step [3092/7464], Loss: 3.2664\n",
      "Epoch [1/2], Step [3093/7464], Loss: 4.4146\n",
      "Epoch [1/2], Step [3094/7464], Loss: 3.9522\n",
      "Epoch [1/2], Step [3095/7464], Loss: 3.5176\n",
      "Epoch [1/2], Step [3096/7464], Loss: 3.5294\n",
      "Epoch [1/2], Step [3097/7464], Loss: 3.6836\n",
      "Epoch [1/2], Step [3098/7464], Loss: 3.1976\n",
      "Epoch [1/2], Step [3099/7464], Loss: 3.8726\n",
      "Epoch [1/2], Step [3100/7464], Loss: 3.5717\n",
      "Epoch [1/2], Step [3101/7464], Loss: 3.8266\n",
      "Epoch [1/2], Step [3102/7464], Loss: 3.3560\n",
      "Epoch [1/2], Step [3103/7464], Loss: 3.6351\n",
      "Epoch [1/2], Step [3104/7464], Loss: 4.0375\n",
      "Epoch [1/2], Step [3105/7464], Loss: 3.3250\n",
      "Epoch [1/2], Step [3106/7464], Loss: 2.6800\n",
      "Epoch [1/2], Step [3107/7464], Loss: 3.9054\n",
      "Epoch [1/2], Step [3108/7464], Loss: 3.5956\n",
      "Epoch [1/2], Step [3109/7464], Loss: 3.7308\n",
      "Epoch [1/2], Step [3110/7464], Loss: 3.9023\n",
      "Epoch [1/2], Step [3111/7464], Loss: 3.9174\n",
      "Epoch [1/2], Step [3112/7464], Loss: 3.5729\n",
      "Epoch [1/2], Step [3113/7464], Loss: 3.1405\n",
      "Epoch [1/2], Step [3114/7464], Loss: 4.2396\n",
      "Epoch [1/2], Step [3115/7464], Loss: 3.6019\n",
      "Epoch [1/2], Step [3116/7464], Loss: 3.8820\n",
      "Epoch [1/2], Step [3117/7464], Loss: 3.2323\n",
      "Epoch [1/2], Step [3118/7464], Loss: 2.5754\n",
      "Epoch [1/2], Step [3119/7464], Loss: 2.9009\n",
      "Epoch [1/2], Step [3120/7464], Loss: 3.5560\n",
      "Epoch [1/2], Step [3121/7464], Loss: 3.3776\n",
      "Epoch [1/2], Step [3122/7464], Loss: 3.4669\n",
      "Epoch [1/2], Step [3123/7464], Loss: 3.7986\n",
      "Epoch [1/2], Step [3124/7464], Loss: 4.1268\n",
      "Epoch [1/2], Step [3125/7464], Loss: 3.8338\n",
      "Epoch [1/2], Step [3126/7464], Loss: 2.6331\n",
      "Epoch [1/2], Step [3127/7464], Loss: 3.3079\n",
      "Epoch [1/2], Step [3128/7464], Loss: 2.8672\n",
      "Epoch [1/2], Step [3129/7464], Loss: 3.1729\n",
      "Epoch [1/2], Step [3130/7464], Loss: 3.9574\n",
      "Epoch [1/2], Step [3131/7464], Loss: 3.1814\n",
      "Epoch [1/2], Step [3132/7464], Loss: 3.4186\n",
      "Epoch [1/2], Step [3133/7464], Loss: 3.5113\n",
      "Epoch [1/2], Step [3134/7464], Loss: 3.5433\n",
      "Epoch [1/2], Step [3135/7464], Loss: 3.8256\n",
      "Epoch [1/2], Step [3136/7464], Loss: 3.6325\n",
      "Epoch [1/2], Step [3137/7464], Loss: 3.4108\n",
      "Epoch [1/2], Step [3138/7464], Loss: 2.9162\n",
      "Epoch [1/2], Step [3139/7464], Loss: 2.2693\n",
      "Epoch [1/2], Step [3140/7464], Loss: 3.4593\n",
      "Epoch [1/2], Step [3141/7464], Loss: 3.6807\n",
      "Epoch [1/2], Step [3142/7464], Loss: 4.0049\n",
      "Epoch [1/2], Step [3143/7464], Loss: 3.8518\n",
      "Epoch [1/2], Step [3144/7464], Loss: 4.0832\n",
      "Epoch [1/2], Step [3145/7464], Loss: 3.5832\n",
      "Epoch [1/2], Step [3146/7464], Loss: 3.2715\n",
      "Epoch [1/2], Step [3147/7464], Loss: 3.2353\n",
      "Epoch [1/2], Step [3148/7464], Loss: 3.9120\n",
      "Epoch [1/2], Step [3149/7464], Loss: 4.0727\n",
      "Epoch [1/2], Step [3150/7464], Loss: 3.3331\n",
      "Epoch [1/2], Step [3151/7464], Loss: 3.1954\n",
      "Epoch [1/2], Step [3152/7464], Loss: 3.2536\n",
      "Epoch [1/2], Step [3153/7464], Loss: 3.9968\n",
      "Epoch [1/2], Step [3154/7464], Loss: 3.3735\n",
      "Epoch [1/2], Step [3155/7464], Loss: 3.5956\n",
      "Epoch [1/2], Step [3156/7464], Loss: 3.7781\n",
      "Epoch [1/2], Step [3157/7464], Loss: 4.0834\n",
      "Epoch [1/2], Step [3158/7464], Loss: 3.3833\n",
      "Epoch [1/2], Step [3159/7464], Loss: 3.8966\n",
      "Epoch [1/2], Step [3160/7464], Loss: 3.8192\n",
      "Epoch [1/2], Step [3161/7464], Loss: 3.8926\n",
      "Epoch [1/2], Step [3162/7464], Loss: 2.9169\n",
      "Epoch [1/2], Step [3163/7464], Loss: 2.8657\n",
      "Epoch [1/2], Step [3164/7464], Loss: 3.2567\n",
      "Epoch [1/2], Step [3165/7464], Loss: 3.7166\n",
      "Epoch [1/2], Step [3166/7464], Loss: 4.1770\n",
      "Epoch [1/2], Step [3167/7464], Loss: 3.4162\n",
      "Epoch [1/2], Step [3168/7464], Loss: 3.6815\n",
      "Epoch [1/2], Step [3169/7464], Loss: 4.0646\n",
      "Epoch [1/2], Step [3170/7464], Loss: 3.3399\n",
      "Epoch [1/2], Step [3171/7464], Loss: 3.5632\n",
      "Epoch [1/2], Step [3172/7464], Loss: 3.2837\n",
      "Epoch [1/2], Step [3173/7464], Loss: 2.9296\n",
      "Epoch [1/2], Step [3174/7464], Loss: 3.7888\n",
      "Epoch [1/2], Step [3175/7464], Loss: 3.3722\n",
      "Epoch [1/2], Step [3176/7464], Loss: 3.5041\n",
      "Epoch [1/2], Step [3177/7464], Loss: 3.1682\n",
      "Epoch [1/2], Step [3178/7464], Loss: 3.6858\n",
      "Epoch [1/2], Step [3179/7464], Loss: 3.5710\n",
      "Epoch [1/2], Step [3180/7464], Loss: 3.6635\n",
      "Epoch [1/2], Step [3181/7464], Loss: 3.5094\n",
      "Epoch [1/2], Step [3182/7464], Loss: 3.1787\n",
      "Epoch [1/2], Step [3183/7464], Loss: 2.9943\n",
      "Epoch [1/2], Step [3184/7464], Loss: 3.0412\n",
      "Epoch [1/2], Step [3185/7464], Loss: 3.4311\n",
      "Epoch [1/2], Step [3186/7464], Loss: 2.9496\n",
      "Epoch [1/2], Step [3187/7464], Loss: 3.3135\n",
      "Epoch [1/2], Step [3188/7464], Loss: 3.4319\n",
      "Epoch [1/2], Step [3189/7464], Loss: 2.7187\n",
      "Epoch [1/2], Step [3190/7464], Loss: 3.2605\n",
      "Epoch [1/2], Step [3191/7464], Loss: 3.1027\n",
      "Epoch [1/2], Step [3192/7464], Loss: 2.8373\n",
      "Epoch [1/2], Step [3193/7464], Loss: 2.7466\n",
      "Epoch [1/2], Step [3194/7464], Loss: 2.8808\n",
      "Epoch [1/2], Step [3195/7464], Loss: 2.8954\n",
      "Epoch [1/2], Step [3196/7464], Loss: 2.6829\n",
      "Epoch [1/2], Step [3197/7464], Loss: 3.2706\n",
      "Epoch [1/2], Step [3198/7464], Loss: 4.0337\n",
      "Epoch [1/2], Step [3199/7464], Loss: 3.2161\n",
      "Epoch [1/2], Step [3200/7464], Loss: 2.7762\n",
      "Epoch [1/2], Step [3201/7464], Loss: 3.2985\n",
      "Epoch [1/2], Step [3202/7464], Loss: 3.7991\n",
      "Epoch [1/2], Step [3203/7464], Loss: 4.1704\n",
      "Epoch [1/2], Step [3204/7464], Loss: 3.6507\n",
      "Epoch [1/2], Step [3205/7464], Loss: 3.1440\n",
      "Epoch [1/2], Step [3206/7464], Loss: 3.0816\n",
      "Epoch [1/2], Step [3207/7464], Loss: 4.5454\n",
      "Epoch [1/2], Step [3208/7464], Loss: 3.2747\n",
      "Epoch [1/2], Step [3209/7464], Loss: 3.5631\n",
      "Epoch [1/2], Step [3210/7464], Loss: 3.3308\n",
      "Epoch [1/2], Step [3211/7464], Loss: 3.0994\n",
      "Epoch [1/2], Step [3212/7464], Loss: 2.8704\n",
      "Epoch [1/2], Step [3213/7464], Loss: 3.6460\n",
      "Epoch [1/2], Step [3214/7464], Loss: 3.7890\n",
      "Epoch [1/2], Step [3215/7464], Loss: 3.3371\n",
      "Epoch [1/2], Step [3216/7464], Loss: 3.2807\n",
      "Epoch [1/2], Step [3217/7464], Loss: 3.6226\n",
      "Epoch [1/2], Step [3218/7464], Loss: 3.1732\n",
      "Epoch [1/2], Step [3219/7464], Loss: 3.2482\n",
      "Epoch [1/2], Step [3220/7464], Loss: 3.3870\n",
      "Epoch [1/2], Step [3221/7464], Loss: 3.5086\n",
      "Epoch [1/2], Step [3222/7464], Loss: 4.3022\n",
      "Epoch [1/2], Step [3223/7464], Loss: 3.0970\n",
      "Epoch [1/2], Step [3224/7464], Loss: 4.2697\n",
      "Epoch [1/2], Step [3225/7464], Loss: 3.1425\n",
      "Epoch [1/2], Step [3226/7464], Loss: 4.1361\n",
      "Epoch [1/2], Step [3227/7464], Loss: 4.1103\n",
      "Epoch [1/2], Step [3228/7464], Loss: 3.6405\n",
      "Epoch [1/2], Step [3229/7464], Loss: 3.1091\n",
      "Epoch [1/2], Step [3230/7464], Loss: 3.2516\n",
      "Epoch [1/2], Step [3231/7464], Loss: 2.8625\n",
      "Epoch [1/2], Step [3232/7464], Loss: 3.0653\n",
      "Epoch [1/2], Step [3233/7464], Loss: 2.9242\n",
      "Epoch [1/2], Step [3234/7464], Loss: 4.3400\n",
      "Epoch [1/2], Step [3235/7464], Loss: 3.1187\n",
      "Epoch [1/2], Step [3236/7464], Loss: 3.0434\n",
      "Epoch [1/2], Step [3237/7464], Loss: 3.6302\n",
      "Epoch [1/2], Step [3238/7464], Loss: 3.5161\n",
      "Epoch [1/2], Step [3239/7464], Loss: 4.5698\n",
      "Epoch [1/2], Step [3240/7464], Loss: 3.1184\n",
      "Epoch [1/2], Step [3241/7464], Loss: 3.1043\n",
      "Epoch [1/2], Step [3242/7464], Loss: 3.8295\n",
      "Epoch [1/2], Step [3243/7464], Loss: 3.2555\n",
      "Epoch [1/2], Step [3244/7464], Loss: 3.1249\n",
      "Epoch [1/2], Step [3245/7464], Loss: 3.3508\n",
      "Epoch [1/2], Step [3246/7464], Loss: 3.3565\n",
      "Epoch [1/2], Step [3247/7464], Loss: 3.1502\n",
      "Epoch [1/2], Step [3248/7464], Loss: 3.4349\n",
      "Epoch [1/2], Step [3249/7464], Loss: 2.4181\n",
      "Epoch [1/2], Step [3250/7464], Loss: 4.3431\n",
      "Epoch [1/2], Step [3251/7464], Loss: 2.9456\n",
      "Epoch [1/2], Step [3252/7464], Loss: 2.7598\n",
      "Epoch [1/2], Step [3253/7464], Loss: 4.6097\n",
      "Epoch [1/2], Step [3254/7464], Loss: 3.6512\n",
      "Epoch [1/2], Step [3255/7464], Loss: 4.2661\n",
      "Epoch [1/2], Step [3256/7464], Loss: 2.8105\n",
      "Epoch [1/2], Step [3257/7464], Loss: 3.4404\n",
      "Epoch [1/2], Step [3258/7464], Loss: 2.7853\n",
      "Epoch [1/2], Step [3259/7464], Loss: 2.8721\n",
      "Epoch [1/2], Step [3260/7464], Loss: 3.6550\n",
      "Epoch [1/2], Step [3261/7464], Loss: 3.8029\n",
      "Epoch [1/2], Step [3262/7464], Loss: 3.5568\n",
      "Epoch [1/2], Step [3263/7464], Loss: 3.9859\n",
      "Epoch [1/2], Step [3264/7464], Loss: 3.2091\n",
      "Epoch [1/2], Step [3265/7464], Loss: 3.5198\n",
      "Epoch [1/2], Step [3266/7464], Loss: 2.8811\n",
      "Epoch [1/2], Step [3267/7464], Loss: 3.2673\n",
      "Epoch [1/2], Step [3268/7464], Loss: 3.6436\n",
      "Epoch [1/2], Step [3269/7464], Loss: 3.6817\n",
      "Epoch [1/2], Step [3270/7464], Loss: 3.7529\n",
      "Epoch [1/2], Step [3271/7464], Loss: 3.8421\n",
      "Epoch [1/2], Step [3272/7464], Loss: 3.7725\n",
      "Epoch [1/2], Step [3273/7464], Loss: 3.5573\n",
      "Epoch [1/2], Step [3274/7464], Loss: 3.5682\n",
      "Epoch [1/2], Step [3275/7464], Loss: 3.3927\n",
      "Epoch [1/2], Step [3276/7464], Loss: 3.7626\n",
      "Epoch [1/2], Step [3277/7464], Loss: 3.1131\n",
      "Epoch [1/2], Step [3278/7464], Loss: 3.5836\n",
      "Epoch [1/2], Step [3279/7464], Loss: 4.4679\n",
      "Epoch [1/2], Step [3280/7464], Loss: 4.1614\n",
      "Epoch [1/2], Step [3281/7464], Loss: 3.5540\n",
      "Epoch [1/2], Step [3282/7464], Loss: 3.3276\n",
      "Epoch [1/2], Step [3283/7464], Loss: 3.6034\n",
      "Epoch [1/2], Step [3284/7464], Loss: 4.0027\n",
      "Epoch [1/2], Step [3285/7464], Loss: 3.3041\n",
      "Epoch [1/2], Step [3286/7464], Loss: 2.4917\n",
      "Epoch [1/2], Step [3287/7464], Loss: 3.5941\n",
      "Epoch [1/2], Step [3288/7464], Loss: 3.0668\n",
      "Epoch [1/2], Step [3289/7464], Loss: 3.2021\n",
      "Epoch [1/2], Step [3290/7464], Loss: 3.2852\n",
      "Epoch [1/2], Step [3291/7464], Loss: 3.7790\n",
      "Epoch [1/2], Step [3292/7464], Loss: 3.3667\n",
      "Epoch [1/2], Step [3293/7464], Loss: 2.7583\n",
      "Epoch [1/2], Step [3294/7464], Loss: 3.8374\n",
      "Epoch [1/2], Step [3295/7464], Loss: 3.2591\n",
      "Epoch [1/2], Step [3296/7464], Loss: 3.7343\n",
      "Epoch [1/2], Step [3297/7464], Loss: 3.0724\n",
      "Epoch [1/2], Step [3298/7464], Loss: 3.8514\n",
      "Epoch [1/2], Step [3299/7464], Loss: 3.5957\n",
      "Epoch [1/2], Step [3300/7464], Loss: 3.6944\n",
      "Epoch [1/2], Step [3301/7464], Loss: 3.6229\n",
      "Epoch [1/2], Step [3302/7464], Loss: 3.0630\n",
      "Epoch [1/2], Step [3303/7464], Loss: 3.1649\n",
      "Epoch [1/2], Step [3304/7464], Loss: 2.6915\n",
      "Epoch [1/2], Step [3305/7464], Loss: 2.8654\n",
      "Epoch [1/2], Step [3306/7464], Loss: 4.2664\n",
      "Epoch [1/2], Step [3307/7464], Loss: 3.1438\n",
      "Epoch [1/2], Step [3308/7464], Loss: 3.6439\n",
      "Epoch [1/2], Step [3309/7464], Loss: 3.6869\n",
      "Epoch [1/2], Step [3310/7464], Loss: 3.3591\n",
      "Epoch [1/2], Step [3311/7464], Loss: 3.1487\n",
      "Epoch [1/2], Step [3312/7464], Loss: 3.6555\n",
      "Epoch [1/2], Step [3313/7464], Loss: 3.9434\n",
      "Epoch [1/2], Step [3314/7464], Loss: 4.0058\n",
      "Epoch [1/2], Step [3315/7464], Loss: 3.7246\n",
      "Epoch [1/2], Step [3316/7464], Loss: 2.8817\n",
      "Epoch [1/2], Step [3317/7464], Loss: 3.3235\n",
      "Epoch [1/2], Step [3318/7464], Loss: 4.4334\n",
      "Epoch [1/2], Step [3319/7464], Loss: 3.5792\n",
      "Epoch [1/2], Step [3320/7464], Loss: 3.6369\n",
      "Epoch [1/2], Step [3321/7464], Loss: 3.5274\n",
      "Epoch [1/2], Step [3322/7464], Loss: 2.9756\n",
      "Epoch [1/2], Step [3323/7464], Loss: 3.0385\n",
      "Epoch [1/2], Step [3324/7464], Loss: 3.4945\n",
      "Epoch [1/2], Step [3325/7464], Loss: 3.2597\n",
      "Epoch [1/2], Step [3326/7464], Loss: 3.5715\n",
      "Epoch [1/2], Step [3327/7464], Loss: 3.7340\n",
      "Epoch [1/2], Step [3328/7464], Loss: 3.5757\n",
      "Epoch [1/2], Step [3329/7464], Loss: 3.7715\n",
      "Epoch [1/2], Step [3330/7464], Loss: 3.0794\n",
      "Epoch [1/2], Step [3331/7464], Loss: 4.4077\n",
      "Epoch [1/2], Step [3332/7464], Loss: 2.9926\n",
      "Epoch [1/2], Step [3333/7464], Loss: 3.7035\n",
      "Epoch [1/2], Step [3334/7464], Loss: 3.1370\n",
      "Epoch [1/2], Step [3335/7464], Loss: 2.8872\n",
      "Epoch [1/2], Step [3336/7464], Loss: 3.9043\n",
      "Epoch [1/2], Step [3337/7464], Loss: 3.6908\n",
      "Epoch [1/2], Step [3338/7464], Loss: 3.3145\n",
      "Epoch [1/2], Step [3339/7464], Loss: 2.9887\n",
      "Epoch [1/2], Step [3340/7464], Loss: 3.4179\n",
      "Epoch [1/2], Step [3341/7464], Loss: 3.4652\n",
      "Epoch [1/2], Step [3342/7464], Loss: 2.6897\n",
      "Epoch [1/2], Step [3343/7464], Loss: 2.3207\n",
      "Epoch [1/2], Step [3344/7464], Loss: 3.9657\n",
      "Epoch [1/2], Step [3345/7464], Loss: 3.5191\n",
      "Epoch [1/2], Step [3346/7464], Loss: 2.7163\n",
      "Epoch [1/2], Step [3347/7464], Loss: 3.7981\n",
      "Epoch [1/2], Step [3348/7464], Loss: 2.6060\n",
      "Epoch [1/2], Step [3349/7464], Loss: 3.1462\n",
      "Epoch [1/2], Step [3350/7464], Loss: 3.2531\n",
      "Epoch [1/2], Step [3351/7464], Loss: 3.0243\n",
      "Epoch [1/2], Step [3352/7464], Loss: 3.8808\n",
      "Epoch [1/2], Step [3353/7464], Loss: 3.3385\n",
      "Epoch [1/2], Step [3354/7464], Loss: 3.2762\n",
      "Epoch [1/2], Step [3355/7464], Loss: 3.0696\n",
      "Epoch [1/2], Step [3356/7464], Loss: 2.5994\n",
      "Epoch [1/2], Step [3357/7464], Loss: 4.6586\n",
      "Epoch [1/2], Step [3358/7464], Loss: 3.5571\n",
      "Epoch [1/2], Step [3359/7464], Loss: 2.6952\n",
      "Epoch [1/2], Step [3360/7464], Loss: 3.4682\n",
      "Epoch [1/2], Step [3361/7464], Loss: 2.8084\n",
      "Epoch [1/2], Step [3362/7464], Loss: 3.2933\n",
      "Epoch [1/2], Step [3363/7464], Loss: 3.2939\n",
      "Epoch [1/2], Step [3364/7464], Loss: 3.0753\n",
      "Epoch [1/2], Step [3365/7464], Loss: 2.9573\n",
      "Epoch [1/2], Step [3366/7464], Loss: 3.7913\n",
      "Epoch [1/2], Step [3367/7464], Loss: 3.2805\n",
      "Epoch [1/2], Step [3368/7464], Loss: 3.9831\n",
      "Epoch [1/2], Step [3369/7464], Loss: 2.6556\n",
      "Epoch [1/2], Step [3370/7464], Loss: 3.3601\n",
      "Epoch [1/2], Step [3371/7464], Loss: 3.4296\n",
      "Epoch [1/2], Step [3372/7464], Loss: 4.2685\n",
      "Epoch [1/2], Step [3373/7464], Loss: 3.4797\n",
      "Epoch [1/2], Step [3374/7464], Loss: 3.1756\n",
      "Epoch [1/2], Step [3375/7464], Loss: 3.9614\n",
      "Epoch [1/2], Step [3376/7464], Loss: 3.5669\n",
      "Epoch [1/2], Step [3377/7464], Loss: 2.8658\n",
      "Epoch [1/2], Step [3378/7464], Loss: 3.7252\n",
      "Epoch [1/2], Step [3379/7464], Loss: 2.4311\n",
      "Epoch [1/2], Step [3380/7464], Loss: 4.0287\n",
      "Epoch [1/2], Step [3381/7464], Loss: 3.7400\n",
      "Epoch [1/2], Step [3382/7464], Loss: 3.7768\n",
      "Epoch [1/2], Step [3383/7464], Loss: 4.3916\n",
      "Epoch [1/2], Step [3384/7464], Loss: 3.5110\n",
      "Epoch [1/2], Step [3385/7464], Loss: 4.2147\n",
      "Epoch [1/2], Step [3386/7464], Loss: 2.6747\n",
      "Epoch [1/2], Step [3387/7464], Loss: 3.0937\n",
      "Epoch [1/2], Step [3388/7464], Loss: 3.9509\n",
      "Epoch [1/2], Step [3389/7464], Loss: 3.2597\n",
      "Epoch [1/2], Step [3390/7464], Loss: 3.6944\n",
      "Epoch [1/2], Step [3391/7464], Loss: 3.1131\n",
      "Epoch [1/2], Step [3392/7464], Loss: 4.1815\n",
      "Epoch [1/2], Step [3393/7464], Loss: 3.7643\n",
      "Epoch [1/2], Step [3394/7464], Loss: 4.2567\n",
      "Epoch [1/2], Step [3395/7464], Loss: 3.4111\n",
      "Epoch [1/2], Step [3396/7464], Loss: 3.4133\n",
      "Epoch [1/2], Step [3397/7464], Loss: 3.4143\n",
      "Epoch [1/2], Step [3398/7464], Loss: 3.5588\n",
      "Epoch [1/2], Step [3399/7464], Loss: 2.6287\n",
      "Epoch [1/2], Step [3400/7464], Loss: 4.0364\n",
      "Epoch [1/2], Step [3401/7464], Loss: 4.3196\n",
      "Epoch [1/2], Step [3402/7464], Loss: 3.3706\n",
      "Epoch [1/2], Step [3403/7464], Loss: 3.0503\n",
      "Epoch [1/2], Step [3404/7464], Loss: 2.9026\n",
      "Epoch [1/2], Step [3405/7464], Loss: 3.7907\n",
      "Epoch [1/2], Step [3406/7464], Loss: 3.1425\n",
      "Epoch [1/2], Step [3407/7464], Loss: 2.5387\n",
      "Epoch [1/2], Step [3408/7464], Loss: 2.8832\n",
      "Epoch [1/2], Step [3409/7464], Loss: 3.1620\n",
      "Epoch [1/2], Step [3410/7464], Loss: 3.7019\n",
      "Epoch [1/2], Step [3411/7464], Loss: 3.4446\n",
      "Epoch [1/2], Step [3412/7464], Loss: 3.3563\n",
      "Epoch [1/2], Step [3413/7464], Loss: 3.2477\n",
      "Epoch [1/2], Step [3414/7464], Loss: 3.6016\n",
      "Epoch [1/2], Step [3415/7464], Loss: 4.0969\n",
      "Epoch [1/2], Step [3416/7464], Loss: 4.1336\n",
      "Epoch [1/2], Step [3417/7464], Loss: 3.5544\n",
      "Epoch [1/2], Step [3418/7464], Loss: 3.1604\n",
      "Epoch [1/2], Step [3419/7464], Loss: 1.9571\n",
      "Epoch [1/2], Step [3420/7464], Loss: 3.8981\n",
      "Epoch [1/2], Step [3421/7464], Loss: 3.4814\n",
      "Epoch [1/2], Step [3422/7464], Loss: 3.6571\n",
      "Epoch [1/2], Step [3423/7464], Loss: 3.1675\n",
      "Epoch [1/2], Step [3424/7464], Loss: 3.3203\n",
      "Epoch [1/2], Step [3425/7464], Loss: 2.9311\n",
      "Epoch [1/2], Step [3426/7464], Loss: 3.6453\n",
      "Epoch [1/2], Step [3427/7464], Loss: 4.7197\n",
      "Epoch [1/2], Step [3428/7464], Loss: 3.6458\n",
      "Epoch [1/2], Step [3429/7464], Loss: 3.4694\n",
      "Epoch [1/2], Step [3430/7464], Loss: 3.4965\n",
      "Epoch [1/2], Step [3431/7464], Loss: 2.8544\n",
      "Epoch [1/2], Step [3432/7464], Loss: 3.2899\n",
      "Epoch [1/2], Step [3433/7464], Loss: 3.2624\n",
      "Epoch [1/2], Step [3434/7464], Loss: 3.4377\n",
      "Epoch [1/2], Step [3435/7464], Loss: 4.0871\n",
      "Epoch [1/2], Step [3436/7464], Loss: 2.9564\n",
      "Epoch [1/2], Step [3437/7464], Loss: 1.9391\n",
      "Epoch [1/2], Step [3438/7464], Loss: 3.5539\n",
      "Epoch [1/2], Step [3439/7464], Loss: 2.7978\n",
      "Epoch [1/2], Step [3440/7464], Loss: 3.7663\n",
      "Epoch [1/2], Step [3441/7464], Loss: 2.8307\n",
      "Epoch [1/2], Step [3442/7464], Loss: 3.5245\n",
      "Epoch [1/2], Step [3443/7464], Loss: 2.8925\n",
      "Epoch [1/2], Step [3444/7464], Loss: 2.8426\n",
      "Epoch [1/2], Step [3445/7464], Loss: 3.8230\n",
      "Epoch [1/2], Step [3446/7464], Loss: 4.6558\n",
      "Epoch [1/2], Step [3447/7464], Loss: 2.6356\n",
      "Epoch [1/2], Step [3448/7464], Loss: 3.6609\n",
      "Epoch [1/2], Step [3449/7464], Loss: 3.4834\n",
      "Epoch [1/2], Step [3450/7464], Loss: 3.2842\n",
      "Epoch [1/2], Step [3451/7464], Loss: 3.8186\n",
      "Epoch [1/2], Step [3452/7464], Loss: 3.2443\n",
      "Epoch [1/2], Step [3453/7464], Loss: 3.5089\n",
      "Epoch [1/2], Step [3454/7464], Loss: 3.3192\n",
      "Epoch [1/2], Step [3455/7464], Loss: 3.2309\n",
      "Epoch [1/2], Step [3456/7464], Loss: 3.5125\n",
      "Epoch [1/2], Step [3457/7464], Loss: 3.5305\n",
      "Epoch [1/2], Step [3458/7464], Loss: 4.0062\n",
      "Epoch [1/2], Step [3459/7464], Loss: 3.3254\n",
      "Epoch [1/2], Step [3460/7464], Loss: 3.5190\n",
      "Epoch [1/2], Step [3461/7464], Loss: 3.3057\n",
      "Epoch [1/2], Step [3462/7464], Loss: 3.7711\n",
      "Epoch [1/2], Step [3463/7464], Loss: 2.6653\n",
      "Epoch [1/2], Step [3464/7464], Loss: 2.9155\n",
      "Epoch [1/2], Step [3465/7464], Loss: 3.2279\n",
      "Epoch [1/2], Step [3466/7464], Loss: 3.1839\n",
      "Epoch [1/2], Step [3467/7464], Loss: 3.7308\n",
      "Epoch [1/2], Step [3468/7464], Loss: 3.0700\n",
      "Epoch [1/2], Step [3469/7464], Loss: 3.8863\n",
      "Epoch [1/2], Step [3470/7464], Loss: 4.0805\n",
      "Epoch [1/2], Step [3471/7464], Loss: 4.3451\n",
      "Epoch [1/2], Step [3472/7464], Loss: 3.4405\n",
      "Epoch [1/2], Step [3473/7464], Loss: 3.1687\n",
      "Epoch [1/2], Step [3474/7464], Loss: 2.3642\n",
      "Epoch [1/2], Step [3475/7464], Loss: 2.8808\n",
      "Epoch [1/2], Step [3476/7464], Loss: 3.2142\n",
      "Epoch [1/2], Step [3477/7464], Loss: 3.3131\n",
      "Epoch [1/2], Step [3478/7464], Loss: 3.5635\n",
      "Epoch [1/2], Step [3479/7464], Loss: 3.5215\n",
      "Epoch [1/2], Step [3480/7464], Loss: 3.5436\n",
      "Epoch [1/2], Step [3481/7464], Loss: 3.9783\n",
      "Epoch [1/2], Step [3482/7464], Loss: 3.6661\n",
      "Epoch [1/2], Step [3483/7464], Loss: 3.2855\n",
      "Epoch [1/2], Step [3484/7464], Loss: 3.6015\n",
      "Epoch [1/2], Step [3485/7464], Loss: 2.7320\n",
      "Epoch [1/2], Step [3486/7464], Loss: 2.9716\n",
      "Epoch [1/2], Step [3487/7464], Loss: 3.1152\n",
      "Epoch [1/2], Step [3488/7464], Loss: 3.5720\n",
      "Epoch [1/2], Step [3489/7464], Loss: 2.6289\n",
      "Epoch [1/2], Step [3490/7464], Loss: 3.3596\n",
      "Epoch [1/2], Step [3491/7464], Loss: 2.6692\n",
      "Epoch [1/2], Step [3492/7464], Loss: 2.8663\n",
      "Epoch [1/2], Step [3493/7464], Loss: 3.5984\n",
      "Epoch [1/2], Step [3494/7464], Loss: 3.0200\n",
      "Epoch [1/2], Step [3495/7464], Loss: 3.7193\n",
      "Epoch [1/2], Step [3496/7464], Loss: 2.6637\n",
      "Epoch [1/2], Step [3497/7464], Loss: 2.4086\n",
      "Epoch [1/2], Step [3498/7464], Loss: 4.2531\n",
      "Epoch [1/2], Step [3499/7464], Loss: 4.0701\n",
      "Epoch [1/2], Step [3500/7464], Loss: 3.9072\n",
      "Epoch [1/2], Step [3501/7464], Loss: 3.3535\n",
      "Epoch [1/2], Step [3502/7464], Loss: 3.5233\n",
      "Epoch [1/2], Step [3503/7464], Loss: 2.7837\n",
      "Epoch [1/2], Step [3504/7464], Loss: 3.7954\n",
      "Epoch [1/2], Step [3505/7464], Loss: 3.3428\n",
      "Epoch [1/2], Step [3506/7464], Loss: 2.4370\n",
      "Epoch [1/2], Step [3507/7464], Loss: 3.5796\n",
      "Epoch [1/2], Step [3508/7464], Loss: 2.9399\n",
      "Epoch [1/2], Step [3509/7464], Loss: 4.5142\n",
      "Epoch [1/2], Step [3510/7464], Loss: 4.1233\n",
      "Epoch [1/2], Step [3511/7464], Loss: 4.1600\n",
      "Epoch [1/2], Step [3512/7464], Loss: 3.6138\n",
      "Epoch [1/2], Step [3513/7464], Loss: 3.1832\n",
      "Epoch [1/2], Step [3514/7464], Loss: 3.4821\n",
      "Epoch [1/2], Step [3515/7464], Loss: 3.8059\n",
      "Epoch [1/2], Step [3516/7464], Loss: 4.3660\n",
      "Epoch [1/2], Step [3517/7464], Loss: 3.9559\n",
      "Epoch [1/2], Step [3518/7464], Loss: 3.5127\n",
      "Epoch [1/2], Step [3519/7464], Loss: 3.5440\n",
      "Epoch [1/2], Step [3520/7464], Loss: 3.3430\n",
      "Epoch [1/2], Step [3521/7464], Loss: 3.7609\n",
      "Epoch [1/2], Step [3522/7464], Loss: 3.4643\n",
      "Epoch [1/2], Step [3523/7464], Loss: 3.4820\n",
      "Epoch [1/2], Step [3524/7464], Loss: 3.0673\n",
      "Epoch [1/2], Step [3525/7464], Loss: 3.4307\n",
      "Epoch [1/2], Step [3526/7464], Loss: 3.9910\n",
      "Epoch [1/2], Step [3527/7464], Loss: 3.5434\n",
      "Epoch [1/2], Step [3528/7464], Loss: 3.9588\n",
      "Epoch [1/2], Step [3529/7464], Loss: 2.9933\n",
      "Epoch [1/2], Step [3530/7464], Loss: 3.3099\n",
      "Epoch [1/2], Step [3531/7464], Loss: 3.1014\n",
      "Epoch [1/2], Step [3532/7464], Loss: 3.7344\n",
      "Epoch [1/2], Step [3533/7464], Loss: 3.1202\n",
      "Epoch [1/2], Step [3534/7464], Loss: 3.1603\n",
      "Epoch [1/2], Step [3535/7464], Loss: 3.8452\n",
      "Epoch [1/2], Step [3536/7464], Loss: 4.3553\n",
      "Epoch [1/2], Step [3537/7464], Loss: 3.5209\n",
      "Epoch [1/2], Step [3538/7464], Loss: 2.8875\n",
      "Epoch [1/2], Step [3539/7464], Loss: 3.3739\n",
      "Epoch [1/2], Step [3540/7464], Loss: 3.2394\n",
      "Epoch [1/2], Step [3541/7464], Loss: 3.0180\n",
      "Epoch [1/2], Step [3542/7464], Loss: 3.0740\n",
      "Epoch [1/2], Step [3543/7464], Loss: 3.7910\n",
      "Epoch [1/2], Step [3544/7464], Loss: 3.4532\n",
      "Epoch [1/2], Step [3545/7464], Loss: 3.7700\n",
      "Epoch [1/2], Step [3546/7464], Loss: 3.5216\n",
      "Epoch [1/2], Step [3547/7464], Loss: 3.3337\n",
      "Epoch [1/2], Step [3548/7464], Loss: 3.4562\n",
      "Epoch [1/2], Step [3549/7464], Loss: 3.7642\n",
      "Epoch [1/2], Step [3550/7464], Loss: 4.2059\n",
      "Epoch [1/2], Step [3551/7464], Loss: 3.5778\n",
      "Epoch [1/2], Step [3552/7464], Loss: 3.6419\n",
      "Epoch [1/2], Step [3553/7464], Loss: 3.1940\n",
      "Epoch [1/2], Step [3554/7464], Loss: 3.6638\n",
      "Epoch [1/2], Step [3555/7464], Loss: 3.5723\n",
      "Epoch [1/2], Step [3556/7464], Loss: 3.1139\n",
      "Epoch [1/2], Step [3557/7464], Loss: 3.2650\n",
      "Epoch [1/2], Step [3558/7464], Loss: 2.7411\n",
      "Epoch [1/2], Step [3559/7464], Loss: 3.7269\n",
      "Epoch [1/2], Step [3560/7464], Loss: 4.0642\n",
      "Epoch [1/2], Step [3561/7464], Loss: 2.8468\n",
      "Epoch [1/2], Step [3562/7464], Loss: 3.7191\n",
      "Epoch [1/2], Step [3563/7464], Loss: 2.9192\n",
      "Epoch [1/2], Step [3564/7464], Loss: 3.8584\n",
      "Epoch [1/2], Step [3565/7464], Loss: 2.8468\n",
      "Epoch [1/2], Step [3566/7464], Loss: 3.6396\n",
      "Epoch [1/2], Step [3567/7464], Loss: 4.2283\n",
      "Epoch [1/2], Step [3568/7464], Loss: 3.9476\n",
      "Epoch [1/2], Step [3569/7464], Loss: 3.1967\n",
      "Epoch [1/2], Step [3570/7464], Loss: 3.0491\n",
      "Epoch [1/2], Step [3571/7464], Loss: 3.0386\n",
      "Epoch [1/2], Step [3572/7464], Loss: 2.5836\n",
      "Epoch [1/2], Step [3573/7464], Loss: 3.4386\n",
      "Epoch [1/2], Step [3574/7464], Loss: 2.5853\n",
      "Epoch [1/2], Step [3575/7464], Loss: 2.5912\n",
      "Epoch [1/2], Step [3576/7464], Loss: 3.6219\n",
      "Epoch [1/2], Step [3577/7464], Loss: 3.7770\n",
      "Epoch [1/2], Step [3578/7464], Loss: 2.9136\n",
      "Epoch [1/2], Step [3579/7464], Loss: 2.3301\n",
      "Epoch [1/2], Step [3580/7464], Loss: 3.0332\n",
      "Epoch [1/2], Step [3581/7464], Loss: 2.3867\n",
      "Epoch [1/2], Step [3582/7464], Loss: 3.3330\n",
      "Epoch [1/2], Step [3583/7464], Loss: 2.8463\n",
      "Epoch [1/2], Step [3584/7464], Loss: 3.3205\n",
      "Epoch [1/2], Step [3585/7464], Loss: 2.6791\n",
      "Epoch [1/2], Step [3586/7464], Loss: 3.5139\n",
      "Epoch [1/2], Step [3587/7464], Loss: 2.8613\n",
      "Epoch [1/2], Step [3588/7464], Loss: 3.4949\n",
      "Epoch [1/2], Step [3589/7464], Loss: 3.0434\n",
      "Epoch [1/2], Step [3590/7464], Loss: 2.8598\n",
      "Epoch [1/2], Step [3591/7464], Loss: 3.0842\n",
      "Epoch [1/2], Step [3592/7464], Loss: 4.1322\n",
      "Epoch [1/2], Step [3593/7464], Loss: 3.8019\n",
      "Epoch [1/2], Step [3594/7464], Loss: 2.5081\n",
      "Epoch [1/2], Step [3595/7464], Loss: 3.5312\n",
      "Epoch [1/2], Step [3596/7464], Loss: 3.1547\n",
      "Epoch [1/2], Step [3597/7464], Loss: 3.4080\n",
      "Epoch [1/2], Step [3598/7464], Loss: 2.6394\n",
      "Epoch [1/2], Step [3599/7464], Loss: 3.6818\n",
      "Epoch [1/2], Step [3600/7464], Loss: 3.1001\n",
      "Epoch [1/2], Step [3601/7464], Loss: 3.0510\n",
      "Epoch [1/2], Step [3602/7464], Loss: 3.3529\n",
      "Epoch [1/2], Step [3603/7464], Loss: 4.4524\n",
      "Epoch [1/2], Step [3604/7464], Loss: 3.5632\n",
      "Epoch [1/2], Step [3605/7464], Loss: 3.3651\n",
      "Epoch [1/2], Step [3606/7464], Loss: 3.4561\n",
      "Epoch [1/2], Step [3607/7464], Loss: 3.3945\n",
      "Epoch [1/2], Step [3608/7464], Loss: 3.5304\n",
      "Epoch [1/2], Step [3609/7464], Loss: 4.0548\n",
      "Epoch [1/2], Step [3610/7464], Loss: 3.7173\n",
      "Epoch [1/2], Step [3611/7464], Loss: 4.5115\n",
      "Epoch [1/2], Step [3612/7464], Loss: 3.1251\n",
      "Epoch [1/2], Step [3613/7464], Loss: 3.2271\n",
      "Epoch [1/2], Step [3614/7464], Loss: 3.1244\n",
      "Epoch [1/2], Step [3615/7464], Loss: 3.3963\n",
      "Epoch [1/2], Step [3616/7464], Loss: 3.3379\n",
      "Epoch [1/2], Step [3617/7464], Loss: 2.9756\n",
      "Epoch [1/2], Step [3618/7464], Loss: 2.8348\n",
      "Epoch [1/2], Step [3619/7464], Loss: 3.5925\n",
      "Epoch [1/2], Step [3620/7464], Loss: 3.9225\n",
      "Epoch [1/2], Step [3621/7464], Loss: 4.6344\n",
      "Epoch [1/2], Step [3622/7464], Loss: 3.4310\n",
      "Epoch [1/2], Step [3623/7464], Loss: 3.6251\n",
      "Epoch [1/2], Step [3624/7464], Loss: 3.6657\n",
      "Epoch [1/2], Step [3625/7464], Loss: 4.2879\n",
      "Epoch [1/2], Step [3626/7464], Loss: 4.6620\n",
      "Epoch [1/2], Step [3627/7464], Loss: 3.7733\n",
      "Epoch [1/2], Step [3628/7464], Loss: 3.3335\n",
      "Epoch [1/2], Step [3629/7464], Loss: 3.4496\n",
      "Epoch [1/2], Step [3630/7464], Loss: 3.2260\n",
      "Epoch [1/2], Step [3631/7464], Loss: 3.4435\n",
      "Epoch [1/2], Step [3632/7464], Loss: 4.1441\n",
      "Epoch [1/2], Step [3633/7464], Loss: 3.0489\n",
      "Epoch [1/2], Step [3634/7464], Loss: 4.1266\n",
      "Epoch [1/2], Step [3635/7464], Loss: 4.1147\n",
      "Epoch [1/2], Step [3636/7464], Loss: 3.2560\n",
      "Epoch [1/2], Step [3637/7464], Loss: 3.6922\n",
      "Epoch [1/2], Step [3638/7464], Loss: 3.7265\n",
      "Epoch [1/2], Step [3639/7464], Loss: 3.6473\n",
      "Epoch [1/2], Step [3640/7464], Loss: 3.1672\n",
      "Epoch [1/2], Step [3641/7464], Loss: 3.5091\n",
      "Epoch [1/2], Step [3642/7464], Loss: 3.1695\n",
      "Epoch [1/2], Step [3643/7464], Loss: 2.9706\n",
      "Epoch [1/2], Step [3644/7464], Loss: 3.9148\n",
      "Epoch [1/2], Step [3645/7464], Loss: 3.5423\n",
      "Epoch [1/2], Step [3646/7464], Loss: 3.7486\n",
      "Epoch [1/2], Step [3647/7464], Loss: 3.6152\n",
      "Epoch [1/2], Step [3648/7464], Loss: 3.7175\n",
      "Epoch [1/2], Step [3649/7464], Loss: 3.3827\n",
      "Epoch [1/2], Step [3650/7464], Loss: 3.3891\n",
      "Epoch [1/2], Step [3651/7464], Loss: 3.4184\n",
      "Epoch [1/2], Step [3652/7464], Loss: 3.5512\n",
      "Epoch [1/2], Step [3653/7464], Loss: 3.0690\n",
      "Epoch [1/2], Step [3654/7464], Loss: 3.0904\n",
      "Epoch [1/2], Step [3655/7464], Loss: 4.3698\n",
      "Epoch [1/2], Step [3656/7464], Loss: 4.2042\n",
      "Epoch [1/2], Step [3657/7464], Loss: 2.7986\n",
      "Epoch [1/2], Step [3658/7464], Loss: 3.6336\n",
      "Epoch [1/2], Step [3659/7464], Loss: 3.4485\n",
      "Epoch [1/2], Step [3660/7464], Loss: 3.9602\n",
      "Epoch [1/2], Step [3661/7464], Loss: 3.4578\n",
      "Epoch [1/2], Step [3662/7464], Loss: 3.4882\n",
      "Epoch [1/2], Step [3663/7464], Loss: 3.4467\n",
      "Epoch [1/2], Step [3664/7464], Loss: 2.8765\n",
      "Epoch [1/2], Step [3665/7464], Loss: 3.1438\n",
      "Epoch [1/2], Step [3666/7464], Loss: 3.5089\n",
      "Epoch [1/2], Step [3667/7464], Loss: 4.0042\n",
      "Epoch [1/2], Step [3668/7464], Loss: 4.0928\n",
      "Epoch [1/2], Step [3669/7464], Loss: 3.4445\n",
      "Epoch [1/2], Step [3670/7464], Loss: 3.5286\n",
      "Epoch [1/2], Step [3671/7464], Loss: 3.4533\n",
      "Epoch [1/2], Step [3672/7464], Loss: 3.7274\n",
      "Epoch [1/2], Step [3673/7464], Loss: 2.9707\n",
      "Epoch [1/2], Step [3674/7464], Loss: 3.0481\n",
      "Epoch [1/2], Step [3675/7464], Loss: 3.0117\n",
      "Epoch [1/2], Step [3676/7464], Loss: 3.0418\n",
      "Epoch [1/2], Step [3677/7464], Loss: 3.2622\n",
      "Epoch [1/2], Step [3678/7464], Loss: 3.2598\n",
      "Epoch [1/2], Step [3679/7464], Loss: 3.5967\n",
      "Epoch [1/2], Step [3680/7464], Loss: 3.1204\n",
      "Epoch [1/2], Step [3681/7464], Loss: 3.3381\n",
      "Epoch [1/2], Step [3682/7464], Loss: 3.5878\n",
      "Epoch [1/2], Step [3683/7464], Loss: 3.2877\n",
      "Epoch [1/2], Step [3684/7464], Loss: 3.8744\n",
      "Epoch [1/2], Step [3685/7464], Loss: 3.9329\n",
      "Epoch [1/2], Step [3686/7464], Loss: 4.0886\n",
      "Epoch [1/2], Step [3687/7464], Loss: 3.4862\n",
      "Epoch [1/2], Step [3688/7464], Loss: 3.4472\n",
      "Epoch [1/2], Step [3689/7464], Loss: 4.0884\n",
      "Epoch [1/2], Step [3690/7464], Loss: 2.1556\n",
      "Epoch [1/2], Step [3691/7464], Loss: 3.9564\n",
      "Epoch [1/2], Step [3692/7464], Loss: 3.4481\n",
      "Epoch [1/2], Step [3693/7464], Loss: 3.5963\n",
      "Epoch [1/2], Step [3694/7464], Loss: 3.2483\n",
      "Epoch [1/2], Step [3695/7464], Loss: 3.6542\n",
      "Epoch [1/2], Step [3696/7464], Loss: 3.8799\n",
      "Epoch [1/2], Step [3697/7464], Loss: 3.3096\n",
      "Epoch [1/2], Step [3698/7464], Loss: 3.0734\n",
      "Epoch [1/2], Step [3699/7464], Loss: 3.4445\n",
      "Epoch [1/2], Step [3700/7464], Loss: 3.4124\n",
      "Epoch [1/2], Step [3701/7464], Loss: 3.1278\n",
      "Epoch [1/2], Step [3702/7464], Loss: 3.7964\n",
      "Epoch [1/2], Step [3703/7464], Loss: 2.8940\n",
      "Epoch [1/2], Step [3704/7464], Loss: 3.1377\n",
      "Epoch [1/2], Step [3705/7464], Loss: 3.8720\n",
      "Epoch [1/2], Step [3706/7464], Loss: 3.1353\n",
      "Epoch [1/2], Step [3707/7464], Loss: 3.0527\n",
      "Epoch [1/2], Step [3708/7464], Loss: 4.2319\n",
      "Epoch [1/2], Step [3709/7464], Loss: 3.0586\n",
      "Epoch [1/2], Step [3710/7464], Loss: 3.4070\n",
      "Epoch [1/2], Step [3711/7464], Loss: 2.9588\n",
      "Epoch [1/2], Step [3712/7464], Loss: 2.9330\n",
      "Epoch [1/2], Step [3713/7464], Loss: 3.4795\n",
      "Epoch [1/2], Step [3714/7464], Loss: 3.1829\n",
      "Epoch [1/2], Step [3715/7464], Loss: 3.1510\n",
      "Epoch [1/2], Step [3716/7464], Loss: 4.1650\n",
      "Epoch [1/2], Step [3717/7464], Loss: 2.1632\n",
      "Epoch [1/2], Step [3718/7464], Loss: 3.7647\n",
      "Epoch [1/2], Step [3719/7464], Loss: 3.3239\n",
      "Epoch [1/2], Step [3720/7464], Loss: 3.8386\n",
      "Epoch [1/2], Step [3721/7464], Loss: 3.2685\n",
      "Epoch [1/2], Step [3722/7464], Loss: 3.0593\n",
      "Epoch [1/2], Step [3723/7464], Loss: 3.2248\n",
      "Epoch [1/2], Step [3724/7464], Loss: 3.3298\n",
      "Epoch [1/2], Step [3725/7464], Loss: 2.9511\n",
      "Epoch [1/2], Step [3726/7464], Loss: 3.0616\n",
      "Epoch [1/2], Step [3727/7464], Loss: 3.5280\n",
      "Epoch [1/2], Step [3728/7464], Loss: 3.9821\n",
      "Epoch [1/2], Step [3729/7464], Loss: 4.3512\n",
      "Epoch [1/2], Step [3730/7464], Loss: 3.2775\n",
      "Epoch [1/2], Step [3731/7464], Loss: 3.0509\n",
      "Epoch [1/2], Step [3732/7464], Loss: 2.7534\n",
      "Epoch [1/2], Step [3733/7464], Loss: 4.6073\n",
      "Epoch [1/2], Step [3734/7464], Loss: 3.9166\n",
      "Epoch [1/2], Step [3735/7464], Loss: 3.7867\n",
      "Epoch [1/2], Step [3736/7464], Loss: 3.4061\n",
      "Epoch [1/2], Step [3737/7464], Loss: 3.9324\n",
      "Epoch [1/2], Step [3738/7464], Loss: 2.7185\n",
      "Epoch [1/2], Step [3739/7464], Loss: 3.4458\n",
      "Epoch [1/2], Step [3740/7464], Loss: 2.8456\n",
      "Epoch [1/2], Step [3741/7464], Loss: 3.1573\n",
      "Epoch [1/2], Step [3742/7464], Loss: 3.2234\n",
      "Epoch [1/2], Step [3743/7464], Loss: 2.9800\n",
      "Epoch [1/2], Step [3744/7464], Loss: 2.4198\n",
      "Epoch [1/2], Step [3745/7464], Loss: 3.8954\n",
      "Epoch [1/2], Step [3746/7464], Loss: 3.7039\n",
      "Epoch [1/2], Step [3747/7464], Loss: 2.7229\n",
      "Epoch [1/2], Step [3748/7464], Loss: 3.1358\n",
      "Epoch [1/2], Step [3749/7464], Loss: 2.7579\n",
      "Epoch [1/2], Step [3750/7464], Loss: 3.2978\n",
      "Epoch [1/2], Step [3751/7464], Loss: 3.0809\n",
      "Epoch [1/2], Step [3752/7464], Loss: 2.9141\n",
      "Epoch [1/2], Step [3753/7464], Loss: 4.3250\n",
      "Epoch [1/2], Step [3754/7464], Loss: 4.0677\n",
      "Epoch [1/2], Step [3755/7464], Loss: 3.5045\n",
      "Epoch [1/2], Step [3756/7464], Loss: 4.1695\n",
      "Epoch [1/2], Step [3757/7464], Loss: 3.7353\n",
      "Epoch [1/2], Step [3758/7464], Loss: 2.7395\n",
      "Epoch [1/2], Step [3759/7464], Loss: 3.9337\n",
      "Epoch [1/2], Step [3760/7464], Loss: 3.4917\n",
      "Epoch [1/2], Step [3761/7464], Loss: 3.2065\n",
      "Epoch [1/2], Step [3762/7464], Loss: 3.2125\n",
      "Epoch [1/2], Step [3763/7464], Loss: 2.8258\n",
      "Epoch [1/2], Step [3764/7464], Loss: 3.5692\n",
      "Epoch [1/2], Step [3765/7464], Loss: 3.7877\n",
      "Epoch [1/2], Step [3766/7464], Loss: 2.5982\n",
      "Epoch [1/2], Step [3767/7464], Loss: 3.1613\n",
      "Epoch [1/2], Step [3768/7464], Loss: 2.9427\n",
      "Epoch [1/2], Step [3769/7464], Loss: 2.7425\n",
      "Epoch [1/2], Step [3770/7464], Loss: 3.4150\n",
      "Epoch [1/2], Step [3771/7464], Loss: 3.7405\n",
      "Epoch [1/2], Step [3772/7464], Loss: 2.6401\n",
      "Epoch [1/2], Step [3773/7464], Loss: 3.3327\n",
      "Epoch [1/2], Step [3774/7464], Loss: 2.8367\n",
      "Epoch [1/2], Step [3775/7464], Loss: 2.7627\n",
      "Epoch [1/2], Step [3776/7464], Loss: 3.9350\n",
      "Epoch [1/2], Step [3777/7464], Loss: 2.5831\n",
      "Epoch [1/2], Step [3778/7464], Loss: 2.2348\n",
      "Epoch [1/2], Step [3779/7464], Loss: 2.9413\n",
      "Epoch [1/2], Step [3780/7464], Loss: 3.7877\n",
      "Epoch [1/2], Step [3781/7464], Loss: 4.4306\n",
      "Epoch [1/2], Step [3782/7464], Loss: 3.2357\n",
      "Epoch [1/2], Step [3783/7464], Loss: 3.6364\n",
      "Epoch [1/2], Step [3784/7464], Loss: 2.9375\n",
      "Epoch [1/2], Step [3785/7464], Loss: 4.0155\n",
      "Epoch [1/2], Step [3786/7464], Loss: 3.3114\n",
      "Epoch [1/2], Step [3787/7464], Loss: 3.3308\n",
      "Epoch [1/2], Step [3788/7464], Loss: 3.3038\n",
      "Epoch [1/2], Step [3789/7464], Loss: 3.9738\n",
      "Epoch [1/2], Step [3790/7464], Loss: 3.2146\n",
      "Epoch [1/2], Step [3791/7464], Loss: 3.5792\n",
      "Epoch [1/2], Step [3792/7464], Loss: 2.8205\n",
      "Epoch [1/2], Step [3793/7464], Loss: 3.0895\n",
      "Epoch [1/2], Step [3794/7464], Loss: 3.6762\n",
      "Epoch [1/2], Step [3795/7464], Loss: 3.3117\n",
      "Epoch [1/2], Step [3796/7464], Loss: 3.3519\n",
      "Epoch [1/2], Step [3797/7464], Loss: 2.5172\n",
      "Epoch [1/2], Step [3798/7464], Loss: 2.5013\n",
      "Epoch [1/2], Step [3799/7464], Loss: 3.2455\n",
      "Epoch [1/2], Step [3800/7464], Loss: 2.4529\n",
      "Epoch [1/2], Step [3801/7464], Loss: 3.7316\n",
      "Epoch [1/2], Step [3802/7464], Loss: 3.2481\n",
      "Epoch [1/2], Step [3803/7464], Loss: 3.5415\n",
      "Epoch [1/2], Step [3804/7464], Loss: 4.3092\n",
      "Epoch [1/2], Step [3805/7464], Loss: 3.8778\n",
      "Epoch [1/2], Step [3806/7464], Loss: 3.2251\n",
      "Epoch [1/2], Step [3807/7464], Loss: 3.2279\n",
      "Epoch [1/2], Step [3808/7464], Loss: 3.7863\n",
      "Epoch [1/2], Step [3809/7464], Loss: 3.2233\n",
      "Epoch [1/2], Step [3810/7464], Loss: 4.1791\n",
      "Epoch [1/2], Step [3811/7464], Loss: 3.4000\n",
      "Epoch [1/2], Step [3812/7464], Loss: 3.0848\n",
      "Epoch [1/2], Step [3813/7464], Loss: 4.1041\n",
      "Epoch [1/2], Step [3814/7464], Loss: 3.8568\n",
      "Epoch [1/2], Step [3815/7464], Loss: 3.1099\n",
      "Epoch [1/2], Step [3816/7464], Loss: 2.8742\n",
      "Epoch [1/2], Step [3817/7464], Loss: 3.2592\n",
      "Epoch [1/2], Step [3818/7464], Loss: 3.9012\n",
      "Epoch [1/2], Step [3819/7464], Loss: 3.0533\n",
      "Epoch [1/2], Step [3820/7464], Loss: 3.0631\n",
      "Epoch [1/2], Step [3821/7464], Loss: 2.4958\n",
      "Epoch [1/2], Step [3822/7464], Loss: 3.1604\n",
      "Epoch [1/2], Step [3823/7464], Loss: 2.9633\n",
      "Epoch [1/2], Step [3824/7464], Loss: 3.7011\n",
      "Epoch [1/2], Step [3825/7464], Loss: 2.8080\n",
      "Epoch [1/2], Step [3826/7464], Loss: 2.3079\n",
      "Epoch [1/2], Step [3827/7464], Loss: 2.2505\n",
      "Epoch [1/2], Step [3828/7464], Loss: 4.8394\n",
      "Epoch [1/2], Step [3829/7464], Loss: 2.6700\n",
      "Epoch [1/2], Step [3830/7464], Loss: 4.0785\n",
      "Epoch [1/2], Step [3831/7464], Loss: 3.6669\n",
      "Epoch [1/2], Step [3832/7464], Loss: 3.4908\n",
      "Epoch [1/2], Step [3833/7464], Loss: 3.5840\n",
      "Epoch [1/2], Step [3834/7464], Loss: 3.6161\n",
      "Epoch [1/2], Step [3835/7464], Loss: 4.0668\n",
      "Epoch [1/2], Step [3836/7464], Loss: 4.3121\n",
      "Epoch [1/2], Step [3837/7464], Loss: 3.2961\n",
      "Epoch [1/2], Step [3838/7464], Loss: 3.7080\n",
      "Epoch [1/2], Step [3839/7464], Loss: 3.8074\n",
      "Epoch [1/2], Step [3840/7464], Loss: 3.1512\n",
      "Epoch [1/2], Step [3841/7464], Loss: 3.8619\n",
      "Epoch [1/2], Step [3842/7464], Loss: 3.5270\n",
      "Epoch [1/2], Step [3843/7464], Loss: 3.2026\n",
      "Epoch [1/2], Step [3844/7464], Loss: 3.0411\n",
      "Epoch [1/2], Step [3845/7464], Loss: 3.9364\n",
      "Epoch [1/2], Step [3846/7464], Loss: 2.7927\n",
      "Epoch [1/2], Step [3847/7464], Loss: 3.0887\n",
      "Epoch [1/2], Step [3848/7464], Loss: 3.3624\n",
      "Epoch [1/2], Step [3849/7464], Loss: 3.7464\n",
      "Epoch [1/2], Step [3850/7464], Loss: 2.9744\n",
      "Epoch [1/2], Step [3851/7464], Loss: 3.8354\n",
      "Epoch [1/2], Step [3852/7464], Loss: 4.1767\n",
      "Epoch [1/2], Step [3853/7464], Loss: 3.0806\n",
      "Epoch [1/2], Step [3854/7464], Loss: 4.1348\n",
      "Epoch [1/2], Step [3855/7464], Loss: 3.1316\n",
      "Epoch [1/2], Step [3856/7464], Loss: 4.5843\n",
      "Epoch [1/2], Step [3857/7464], Loss: 3.2152\n",
      "Epoch [1/2], Step [3858/7464], Loss: 3.2773\n",
      "Epoch [1/2], Step [3859/7464], Loss: 3.8233\n",
      "Epoch [1/2], Step [3860/7464], Loss: 2.7420\n",
      "Epoch [1/2], Step [3861/7464], Loss: 3.0472\n",
      "Epoch [1/2], Step [3862/7464], Loss: 3.5484\n",
      "Epoch [1/2], Step [3863/7464], Loss: 3.3978\n",
      "Epoch [1/2], Step [3864/7464], Loss: 4.4928\n",
      "Epoch [1/2], Step [3865/7464], Loss: 2.6685\n",
      "Epoch [1/2], Step [3866/7464], Loss: 3.4577\n",
      "Epoch [1/2], Step [3867/7464], Loss: 3.2603\n",
      "Epoch [1/2], Step [3868/7464], Loss: 3.6505\n",
      "Epoch [1/2], Step [3869/7464], Loss: 2.7219\n",
      "Epoch [1/2], Step [3870/7464], Loss: 4.0438\n",
      "Epoch [1/2], Step [3871/7464], Loss: 3.9051\n",
      "Epoch [1/2], Step [3872/7464], Loss: 3.7168\n",
      "Epoch [1/2], Step [3873/7464], Loss: 3.0831\n",
      "Epoch [1/2], Step [3874/7464], Loss: 3.9411\n",
      "Epoch [1/2], Step [3875/7464], Loss: 3.2047\n",
      "Epoch [1/2], Step [3876/7464], Loss: 3.6332\n",
      "Epoch [1/2], Step [3877/7464], Loss: 2.4334\n",
      "Epoch [1/2], Step [3878/7464], Loss: 3.7392\n",
      "Epoch [1/2], Step [3879/7464], Loss: 4.3311\n",
      "Epoch [1/2], Step [3880/7464], Loss: 3.5685\n",
      "Epoch [1/2], Step [3881/7464], Loss: 3.3504\n",
      "Epoch [1/2], Step [3882/7464], Loss: 2.6326\n",
      "Epoch [1/2], Step [3883/7464], Loss: 3.5536\n",
      "Epoch [1/2], Step [3884/7464], Loss: 2.7505\n",
      "Epoch [1/2], Step [3885/7464], Loss: 3.5901\n",
      "Epoch [1/2], Step [3886/7464], Loss: 2.8263\n",
      "Epoch [1/2], Step [3887/7464], Loss: 3.9887\n",
      "Epoch [1/2], Step [3888/7464], Loss: 3.3700\n",
      "Epoch [1/2], Step [3889/7464], Loss: 4.0325\n",
      "Epoch [1/2], Step [3890/7464], Loss: 3.1480\n",
      "Epoch [1/2], Step [3891/7464], Loss: 2.6598\n",
      "Epoch [1/2], Step [3892/7464], Loss: 3.0184\n",
      "Epoch [1/2], Step [3893/7464], Loss: 3.7662\n",
      "Epoch [1/2], Step [3894/7464], Loss: 4.1384\n",
      "Epoch [1/2], Step [3895/7464], Loss: 3.4037\n",
      "Epoch [1/2], Step [3896/7464], Loss: 3.4991\n",
      "Epoch [1/2], Step [3897/7464], Loss: 3.0543\n",
      "Epoch [1/2], Step [3898/7464], Loss: 3.2644\n",
      "Epoch [1/2], Step [3899/7464], Loss: 3.2182\n",
      "Epoch [1/2], Step [3900/7464], Loss: 2.7038\n",
      "Epoch [1/2], Step [3901/7464], Loss: 3.8410\n",
      "Epoch [1/2], Step [3902/7464], Loss: 3.2631\n",
      "Epoch [1/2], Step [3903/7464], Loss: 3.4663\n",
      "Epoch [1/2], Step [3904/7464], Loss: 3.5818\n",
      "Epoch [1/2], Step [3905/7464], Loss: 2.4166\n",
      "Epoch [1/2], Step [3906/7464], Loss: 3.8718\n",
      "Epoch [1/2], Step [3907/7464], Loss: 4.0273\n",
      "Epoch [1/2], Step [3908/7464], Loss: 3.7476\n",
      "Epoch [1/2], Step [3909/7464], Loss: 2.9715\n",
      "Epoch [1/2], Step [3910/7464], Loss: 3.7630\n",
      "Epoch [1/2], Step [3911/7464], Loss: 4.2990\n",
      "Epoch [1/2], Step [3912/7464], Loss: 2.9675\n",
      "Epoch [1/2], Step [3913/7464], Loss: 3.3871\n",
      "Epoch [1/2], Step [3914/7464], Loss: 2.4687\n",
      "Epoch [1/2], Step [3915/7464], Loss: 3.9569\n",
      "Epoch [1/2], Step [3916/7464], Loss: 2.7632\n",
      "Epoch [1/2], Step [3917/7464], Loss: 3.5186\n",
      "Epoch [1/2], Step [3918/7464], Loss: 3.0655\n",
      "Epoch [1/2], Step [3919/7464], Loss: 2.9041\n",
      "Epoch [1/2], Step [3920/7464], Loss: 2.8095\n",
      "Epoch [1/2], Step [3921/7464], Loss: 3.4781\n",
      "Epoch [1/2], Step [3922/7464], Loss: 3.1950\n",
      "Epoch [1/2], Step [3923/7464], Loss: 3.7769\n",
      "Epoch [1/2], Step [3924/7464], Loss: 3.2769\n",
      "Epoch [1/2], Step [3925/7464], Loss: 2.9912\n",
      "Epoch [1/2], Step [3926/7464], Loss: 3.4057\n",
      "Epoch [1/2], Step [3927/7464], Loss: 3.5079\n",
      "Epoch [1/2], Step [3928/7464], Loss: 3.3052\n",
      "Epoch [1/2], Step [3929/7464], Loss: 3.9675\n",
      "Epoch [1/2], Step [3930/7464], Loss: 3.2976\n",
      "Epoch [1/2], Step [3931/7464], Loss: 3.9784\n",
      "Epoch [1/2], Step [3932/7464], Loss: 3.6934\n",
      "Epoch [1/2], Step [3933/7464], Loss: 3.2508\n",
      "Epoch [1/2], Step [3934/7464], Loss: 2.6258\n",
      "Epoch [1/2], Step [3935/7464], Loss: 3.4686\n",
      "Epoch [1/2], Step [3936/7464], Loss: 4.7235\n",
      "Epoch [1/2], Step [3937/7464], Loss: 3.8453\n",
      "Epoch [1/2], Step [3938/7464], Loss: 3.4451\n",
      "Epoch [1/2], Step [3939/7464], Loss: 3.8568\n",
      "Epoch [1/2], Step [3940/7464], Loss: 2.6723\n",
      "Epoch [1/2], Step [3941/7464], Loss: 2.7326\n",
      "Epoch [1/2], Step [3942/7464], Loss: 2.5403\n",
      "Epoch [1/2], Step [3943/7464], Loss: 3.6806\n",
      "Epoch [1/2], Step [3944/7464], Loss: 3.0440\n",
      "Epoch [1/2], Step [3945/7464], Loss: 3.6660\n",
      "Epoch [1/2], Step [3946/7464], Loss: 2.9311\n",
      "Epoch [1/2], Step [3947/7464], Loss: 3.1649\n",
      "Epoch [1/2], Step [3948/7464], Loss: 2.9308\n",
      "Epoch [1/2], Step [3949/7464], Loss: 3.8998\n",
      "Epoch [1/2], Step [3950/7464], Loss: 3.5380\n",
      "Epoch [1/2], Step [3951/7464], Loss: 3.5964\n",
      "Epoch [1/2], Step [3952/7464], Loss: 2.9798\n",
      "Epoch [1/2], Step [3953/7464], Loss: 3.4758\n",
      "Epoch [1/2], Step [3954/7464], Loss: 3.5945\n",
      "Epoch [1/2], Step [3955/7464], Loss: 3.4240\n",
      "Epoch [1/2], Step [3956/7464], Loss: 4.3991\n",
      "Epoch [1/2], Step [3957/7464], Loss: 4.1971\n",
      "Epoch [1/2], Step [3958/7464], Loss: 3.1188\n",
      "Epoch [1/2], Step [3959/7464], Loss: 2.7458\n",
      "Epoch [1/2], Step [3960/7464], Loss: 4.4172\n",
      "Epoch [1/2], Step [3961/7464], Loss: 2.1989\n",
      "Epoch [1/2], Step [3962/7464], Loss: 3.5756\n",
      "Epoch [1/2], Step [3963/7464], Loss: 2.3674\n",
      "Epoch [1/2], Step [3964/7464], Loss: 3.5827\n",
      "Epoch [1/2], Step [3965/7464], Loss: 3.6110\n",
      "Epoch [1/2], Step [3966/7464], Loss: 3.5027\n",
      "Epoch [1/2], Step [3967/7464], Loss: 3.9357\n",
      "Epoch [1/2], Step [3968/7464], Loss: 3.1395\n",
      "Epoch [1/2], Step [3969/7464], Loss: 4.5932\n",
      "Epoch [1/2], Step [3970/7464], Loss: 3.3808\n",
      "Epoch [1/2], Step [3971/7464], Loss: 3.3466\n",
      "Epoch [1/2], Step [3972/7464], Loss: 3.4322\n",
      "Epoch [1/2], Step [3973/7464], Loss: 2.8770\n",
      "Epoch [1/2], Step [3974/7464], Loss: 3.7076\n",
      "Epoch [1/2], Step [3975/7464], Loss: 3.4096\n",
      "Epoch [1/2], Step [3976/7464], Loss: 3.5165\n",
      "Epoch [1/2], Step [3977/7464], Loss: 3.3595\n",
      "Epoch [1/2], Step [3978/7464], Loss: 3.4644\n",
      "Epoch [1/2], Step [3979/7464], Loss: 4.1073\n",
      "Epoch [1/2], Step [3980/7464], Loss: 2.8081\n",
      "Epoch [1/2], Step [3981/7464], Loss: 2.8823\n",
      "Epoch [1/2], Step [3982/7464], Loss: 3.1258\n",
      "Epoch [1/2], Step [3983/7464], Loss: 3.0056\n",
      "Epoch [1/2], Step [3984/7464], Loss: 3.6521\n",
      "Epoch [1/2], Step [3985/7464], Loss: 3.5810\n",
      "Epoch [1/2], Step [3986/7464], Loss: 3.7284\n",
      "Epoch [1/2], Step [3987/7464], Loss: 3.6824\n",
      "Epoch [1/2], Step [3988/7464], Loss: 3.8164\n",
      "Epoch [1/2], Step [3989/7464], Loss: 2.7647\n",
      "Epoch [1/2], Step [3990/7464], Loss: 4.2201\n",
      "Epoch [1/2], Step [3991/7464], Loss: 3.9473\n",
      "Epoch [1/2], Step [3992/7464], Loss: 2.3223\n",
      "Epoch [1/2], Step [3993/7464], Loss: 3.7326\n",
      "Epoch [1/2], Step [3994/7464], Loss: 4.5209\n",
      "Epoch [1/2], Step [3995/7464], Loss: 3.2021\n",
      "Epoch [1/2], Step [3996/7464], Loss: 3.5146\n",
      "Epoch [1/2], Step [3997/7464], Loss: 3.6361\n",
      "Epoch [1/2], Step [3998/7464], Loss: 2.0232\n",
      "Epoch [1/2], Step [3999/7464], Loss: 3.3984\n",
      "Epoch [1/2], Step [4000/7464], Loss: 3.4548\n",
      "Epoch [1/2], Step [4001/7464], Loss: 3.1674\n",
      "Epoch [1/2], Step [4002/7464], Loss: 3.9635\n",
      "Epoch [1/2], Step [4003/7464], Loss: 2.0729\n",
      "Epoch [1/2], Step [4004/7464], Loss: 3.9292\n",
      "Epoch [1/2], Step [4005/7464], Loss: 4.0089\n",
      "Epoch [1/2], Step [4006/7464], Loss: 2.9696\n",
      "Epoch [1/2], Step [4007/7464], Loss: 4.2401\n",
      "Epoch [1/2], Step [4008/7464], Loss: 3.3531\n",
      "Epoch [1/2], Step [4009/7464], Loss: 3.9177\n",
      "Epoch [1/2], Step [4010/7464], Loss: 4.2496\n",
      "Epoch [1/2], Step [4011/7464], Loss: 4.1841\n",
      "Epoch [1/2], Step [4012/7464], Loss: 3.4862\n",
      "Epoch [1/2], Step [4013/7464], Loss: 3.0403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m i, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     33\u001b[0m     \u001b[39m# print(inputs[0].shape)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[39m# print(model(inputs).shape)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 36\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     37\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     38\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x\u001b[39m.\u001b[39mfloat())))\n\u001b[1;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool(torch\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)))\n\u001b[0;32m     17\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m64\u001b[39m \u001b[39m*\u001b[39m \u001b[39m56\u001b[39m \u001b[39m*\u001b[39m \u001b[39m56\u001b[39m)\n\u001b[0;32m     18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[0;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 512)\n",
    "        self.fc2 = nn.Linear(512, 50) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x.float())))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle= True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle= True)\n",
    "\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0495e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 25.00%\n",
      "Accuracy of the model on the test images: 12.50%\n",
      "Accuracy of the model on the test images: 8.33%\n",
      "Accuracy of the model on the test images: 12.50%\n",
      "Accuracy of the model on the test images: 20.00%\n",
      "Accuracy of the model on the test images: 16.67%\n",
      "Accuracy of the model on the test images: 17.86%\n",
      "Accuracy of the model on the test images: 18.75%\n",
      "Accuracy of the model on the test images: 16.67%\n",
      "Accuracy of the model on the test images: 15.00%\n",
      "Accuracy of the model on the test images: 15.91%\n",
      "Accuracy of the model on the test images: 14.58%\n",
      "Accuracy of the model on the test images: 15.38%\n",
      "Accuracy of the model on the test images: 14.29%\n",
      "Accuracy of the model on the test images: 15.00%\n",
      "Accuracy of the model on the test images: 15.62%\n",
      "Accuracy of the model on the test images: 16.18%\n",
      "Accuracy of the model on the test images: 16.67%\n",
      "Accuracy of the model on the test images: 15.79%\n",
      "Accuracy of the model on the test images: 16.25%\n",
      "Accuracy of the model on the test images: 16.67%\n",
      "Accuracy of the model on the test images: 15.91%\n",
      "Accuracy of the model on the test images: 15.22%\n",
      "Accuracy of the model on the test images: 14.58%\n",
      "Accuracy of the model on the test images: 14.00%\n",
      "Accuracy of the model on the test images: 14.42%\n",
      "Accuracy of the model on the test images: 15.74%\n",
      "Accuracy of the model on the test images: 15.18%\n",
      "Accuracy of the model on the test images: 14.66%\n",
      "Accuracy of the model on the test images: 15.00%\n",
      "Accuracy of the model on the test images: 15.32%\n",
      "Accuracy of the model on the test images: 14.84%\n",
      "Accuracy of the model on the test images: 14.39%\n",
      "Accuracy of the model on the test images: 14.71%\n",
      "Accuracy of the model on the test images: 14.29%\n",
      "Accuracy of the model on the test images: 13.89%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 12.80%\n",
      "Accuracy of the model on the test images: 12.50%\n",
      "Accuracy of the model on the test images: 12.79%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 13.04%\n",
      "Accuracy of the model on the test images: 12.77%\n",
      "Accuracy of the model on the test images: 13.02%\n",
      "Accuracy of the model on the test images: 12.76%\n",
      "Accuracy of the model on the test images: 13.00%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.50%\n",
      "Accuracy of the model on the test images: 12.26%\n",
      "Accuracy of the model on the test images: 12.04%\n",
      "Accuracy of the model on the test images: 11.82%\n",
      "Accuracy of the model on the test images: 11.61%\n",
      "Accuracy of the model on the test images: 11.84%\n",
      "Accuracy of the model on the test images: 12.07%\n",
      "Accuracy of the model on the test images: 11.86%\n",
      "Accuracy of the model on the test images: 11.67%\n",
      "Accuracy of the model on the test images: 11.48%\n",
      "Accuracy of the model on the test images: 11.29%\n",
      "Accuracy of the model on the test images: 11.90%\n",
      "Accuracy of the model on the test images: 11.72%\n",
      "Accuracy of the model on the test images: 11.92%\n",
      "Accuracy of the model on the test images: 11.74%\n",
      "Accuracy of the model on the test images: 12.31%\n",
      "Accuracy of the model on the test images: 12.13%\n",
      "Accuracy of the model on the test images: 11.96%\n",
      "Accuracy of the model on the test images: 12.14%\n",
      "Accuracy of the model on the test images: 11.97%\n",
      "Accuracy of the model on the test images: 12.15%\n",
      "Accuracy of the model on the test images: 11.99%\n",
      "Accuracy of the model on the test images: 11.82%\n",
      "Accuracy of the model on the test images: 12.00%\n",
      "Accuracy of the model on the test images: 12.17%\n",
      "Accuracy of the model on the test images: 12.01%\n",
      "Accuracy of the model on the test images: 11.86%\n",
      "Accuracy of the model on the test images: 12.03%\n",
      "Accuracy of the model on the test images: 11.88%\n",
      "Accuracy of the model on the test images: 12.04%\n",
      "Accuracy of the model on the test images: 11.89%\n",
      "Accuracy of the model on the test images: 11.75%\n",
      "Accuracy of the model on the test images: 11.90%\n",
      "Accuracy of the model on the test images: 12.06%\n",
      "Accuracy of the model on the test images: 11.92%\n",
      "Accuracy of the model on the test images: 12.07%\n",
      "Accuracy of the model on the test images: 11.93%\n",
      "Accuracy of the model on the test images: 12.08%\n",
      "Accuracy of the model on the test images: 11.94%\n",
      "Accuracy of the model on the test images: 12.09%\n",
      "Accuracy of the model on the test images: 12.23%\n",
      "Accuracy of the model on the test images: 12.10%\n",
      "Accuracy of the model on the test images: 11.97%\n",
      "Accuracy of the model on the test images: 12.11%\n",
      "Accuracy of the model on the test images: 12.24%\n",
      "Accuracy of the model on the test images: 12.37%\n",
      "Accuracy of the model on the test images: 12.24%\n",
      "Accuracy of the model on the test images: 12.12%\n",
      "Accuracy of the model on the test images: 12.25%\n",
      "Accuracy of the model on the test images: 12.13%\n",
      "Accuracy of the model on the test images: 12.01%\n",
      "Accuracy of the model on the test images: 12.38%\n",
      "Accuracy of the model on the test images: 12.50%\n",
      "Accuracy of the model on the test images: 12.38%\n",
      "Accuracy of the model on the test images: 12.26%\n",
      "Accuracy of the model on the test images: 12.15%\n",
      "Accuracy of the model on the test images: 12.27%\n",
      "Accuracy of the model on the test images: 12.16%\n",
      "Accuracy of the model on the test images: 12.05%\n",
      "Accuracy of the model on the test images: 11.94%\n",
      "Accuracy of the model on the test images: 12.05%\n",
      "Accuracy of the model on the test images: 11.95%\n",
      "Accuracy of the model on the test images: 11.84%\n",
      "Accuracy of the model on the test images: 11.96%\n",
      "Accuracy of the model on the test images: 12.07%\n",
      "Accuracy of the model on the test images: 11.97%\n",
      "Accuracy of the model on the test images: 12.08%\n",
      "Accuracy of the model on the test images: 12.18%\n",
      "Accuracy of the model on the test images: 12.29%\n",
      "Accuracy of the model on the test images: 12.19%\n",
      "Accuracy of the model on the test images: 12.30%\n",
      "Accuracy of the model on the test images: 12.40%\n",
      "Accuracy of the model on the test images: 12.30%\n",
      "Accuracy of the model on the test images: 12.20%\n",
      "Accuracy of the model on the test images: 12.10%\n",
      "Accuracy of the model on the test images: 12.01%\n",
      "Accuracy of the model on the test images: 12.11%\n",
      "Accuracy of the model on the test images: 12.21%\n",
      "Accuracy of the model on the test images: 12.12%\n",
      "Accuracy of the model on the test images: 12.02%\n",
      "Accuracy of the model on the test images: 11.93%\n",
      "Accuracy of the model on the test images: 12.03%\n",
      "Accuracy of the model on the test images: 11.94%\n",
      "Accuracy of the model on the test images: 12.04%\n",
      "Accuracy of the model on the test images: 12.13%\n",
      "Accuracy of the model on the test images: 12.04%\n",
      "Accuracy of the model on the test images: 12.14%\n",
      "Accuracy of the model on the test images: 12.23%\n",
      "Accuracy of the model on the test images: 12.14%\n",
      "Accuracy of the model on the test images: 12.23%\n",
      "Accuracy of the model on the test images: 12.32%\n",
      "Accuracy of the model on the test images: 12.24%\n",
      "Accuracy of the model on the test images: 12.15%\n",
      "Accuracy of the model on the test images: 12.24%\n",
      "Accuracy of the model on the test images: 12.16%\n",
      "Accuracy of the model on the test images: 12.24%\n",
      "Accuracy of the model on the test images: 12.16%\n",
      "Accuracy of the model on the test images: 12.08%\n",
      "Accuracy of the model on the test images: 12.00%\n",
      "Accuracy of the model on the test images: 11.92%\n",
      "Accuracy of the model on the test images: 12.01%\n",
      "Accuracy of the model on the test images: 12.09%\n",
      "Accuracy of the model on the test images: 12.01%\n",
      "Accuracy of the model on the test images: 12.10%\n",
      "Accuracy of the model on the test images: 12.18%\n",
      "Accuracy of the model on the test images: 12.26%\n",
      "Accuracy of the model on the test images: 12.18%\n",
      "Accuracy of the model on the test images: 12.11%\n",
      "Accuracy of the model on the test images: 12.19%\n",
      "Accuracy of the model on the test images: 12.27%\n",
      "Accuracy of the model on the test images: 12.19%\n",
      "Accuracy of the model on the test images: 12.27%\n",
      "Accuracy of the model on the test images: 12.20%\n",
      "Accuracy of the model on the test images: 12.27%\n",
      "Accuracy of the model on the test images: 12.20%\n",
      "Accuracy of the model on the test images: 12.13%\n",
      "Accuracy of the model on the test images: 12.05%\n",
      "Accuracy of the model on the test images: 11.98%\n",
      "Accuracy of the model on the test images: 12.06%\n",
      "Accuracy of the model on the test images: 11.99%\n",
      "Accuracy of the model on the test images: 11.92%\n",
      "Accuracy of the model on the test images: 11.99%\n",
      "Accuracy of the model on the test images: 11.93%\n",
      "Accuracy of the model on the test images: 12.00%\n",
      "Accuracy of the model on the test images: 11.93%\n",
      "Accuracy of the model on the test images: 12.01%\n",
      "Accuracy of the model on the test images: 12.36%\n",
      "Accuracy of the model on the test images: 12.29%\n",
      "Accuracy of the model on the test images: 12.22%\n",
      "Accuracy of the model on the test images: 12.29%\n",
      "Accuracy of the model on the test images: 12.23%\n",
      "Accuracy of the model on the test images: 12.16%\n",
      "Accuracy of the model on the test images: 12.23%\n",
      "Accuracy of the model on the test images: 12.16%\n",
      "Accuracy of the model on the test images: 12.10%\n",
      "Accuracy of the model on the test images: 12.30%\n",
      "Accuracy of the model on the test images: 12.23%\n",
      "Accuracy of the model on the test images: 12.17%\n",
      "Accuracy of the model on the test images: 12.11%\n",
      "Accuracy of the model on the test images: 12.30%\n",
      "Accuracy of the model on the test images: 12.50%\n",
      "Accuracy of the model on the test images: 12.44%\n",
      "Accuracy of the model on the test images: 12.37%\n",
      "Accuracy of the model on the test images: 12.69%\n",
      "Accuracy of the model on the test images: 12.63%\n",
      "Accuracy of the model on the test images: 12.56%\n",
      "Accuracy of the model on the test images: 12.50%\n",
      "Accuracy of the model on the test images: 12.56%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.69%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.74%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.79%\n",
      "Accuracy of the model on the test images: 12.73%\n",
      "Accuracy of the model on the test images: 12.79%\n",
      "Accuracy of the model on the test images: 12.84%\n",
      "Accuracy of the model on the test images: 13.01%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.01%\n",
      "Accuracy of the model on the test images: 12.95%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.06%\n",
      "Accuracy of the model on the test images: 13.00%\n",
      "Accuracy of the model on the test images: 13.05%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.05%\n",
      "Accuracy of the model on the test images: 12.99%\n",
      "Accuracy of the model on the test images: 13.04%\n",
      "Accuracy of the model on the test images: 12.99%\n",
      "Accuracy of the model on the test images: 13.04%\n",
      "Accuracy of the model on the test images: 13.09%\n",
      "Accuracy of the model on the test images: 13.03%\n",
      "Accuracy of the model on the test images: 12.98%\n",
      "Accuracy of the model on the test images: 12.92%\n",
      "Accuracy of the model on the test images: 12.87%\n",
      "Accuracy of the model on the test images: 12.82%\n",
      "Accuracy of the model on the test images: 12.97%\n",
      "Accuracy of the model on the test images: 12.92%\n",
      "Accuracy of the model on the test images: 12.97%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.01%\n",
      "Accuracy of the model on the test images: 12.96%\n",
      "Accuracy of the model on the test images: 13.01%\n",
      "Accuracy of the model on the test images: 12.96%\n",
      "Accuracy of the model on the test images: 13.00%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.09%\n",
      "Accuracy of the model on the test images: 13.04%\n",
      "Accuracy of the model on the test images: 12.98%\n",
      "Accuracy of the model on the test images: 12.93%\n",
      "Accuracy of the model on the test images: 12.88%\n",
      "Accuracy of the model on the test images: 12.84%\n",
      "Accuracy of the model on the test images: 12.98%\n",
      "Accuracy of the model on the test images: 12.93%\n",
      "Accuracy of the model on the test images: 12.97%\n",
      "Accuracy of the model on the test images: 12.92%\n",
      "Accuracy of the model on the test images: 12.88%\n",
      "Accuracy of the model on the test images: 12.83%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.73%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.73%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.73%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.73%\n",
      "Accuracy of the model on the test images: 12.77%\n",
      "Accuracy of the model on the test images: 12.73%\n",
      "Accuracy of the model on the test images: 12.77%\n",
      "Accuracy of the model on the test images: 12.90%\n",
      "Accuracy of the model on the test images: 12.95%\n",
      "Accuracy of the model on the test images: 12.99%\n",
      "Accuracy of the model on the test images: 12.94%\n",
      "Accuracy of the model on the test images: 12.99%\n",
      "Accuracy of the model on the test images: 13.03%\n",
      "Accuracy of the model on the test images: 12.98%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.10%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.10%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.09%\n",
      "Accuracy of the model on the test images: 13.05%\n",
      "Accuracy of the model on the test images: 13.00%\n",
      "Accuracy of the model on the test images: 12.96%\n",
      "Accuracy of the model on the test images: 12.92%\n",
      "Accuracy of the model on the test images: 12.96%\n",
      "Accuracy of the model on the test images: 12.91%\n",
      "Accuracy of the model on the test images: 12.95%\n",
      "Accuracy of the model on the test images: 12.91%\n",
      "Accuracy of the model on the test images: 12.95%\n",
      "Accuracy of the model on the test images: 12.91%\n",
      "Accuracy of the model on the test images: 12.87%\n",
      "Accuracy of the model on the test images: 12.82%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.74%\n",
      "Accuracy of the model on the test images: 12.70%\n",
      "Accuracy of the model on the test images: 12.74%\n",
      "Accuracy of the model on the test images: 12.70%\n",
      "Accuracy of the model on the test images: 12.66%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.74%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.74%\n",
      "Accuracy of the model on the test images: 12.77%\n",
      "Accuracy of the model on the test images: 12.81%\n",
      "Accuracy of the model on the test images: 12.93%\n",
      "Accuracy of the model on the test images: 12.97%\n",
      "Accuracy of the model on the test images: 13.00%\n",
      "Accuracy of the model on the test images: 12.96%\n",
      "Accuracy of the model on the test images: 12.92%\n",
      "Accuracy of the model on the test images: 12.88%\n",
      "Accuracy of the model on the test images: 12.84%\n",
      "Accuracy of the model on the test images: 12.88%\n",
      "Accuracy of the model on the test images: 12.84%\n",
      "Accuracy of the model on the test images: 12.95%\n",
      "Accuracy of the model on the test images: 12.99%\n",
      "Accuracy of the model on the test images: 13.03%\n",
      "Accuracy of the model on the test images: 12.99%\n",
      "Accuracy of the model on the test images: 13.02%\n",
      "Accuracy of the model on the test images: 12.99%\n",
      "Accuracy of the model on the test images: 13.02%\n",
      "Accuracy of the model on the test images: 12.98%\n",
      "Accuracy of the model on the test images: 13.02%\n",
      "Accuracy of the model on the test images: 12.98%\n",
      "Accuracy of the model on the test images: 13.01%\n",
      "Accuracy of the model on the test images: 13.05%\n",
      "Accuracy of the model on the test images: 13.08%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.08%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.22%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.22%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.10%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.03%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.03%\n",
      "Accuracy of the model on the test images: 13.00%\n",
      "Accuracy of the model on the test images: 12.96%\n",
      "Accuracy of the model on the test images: 12.93%\n",
      "Accuracy of the model on the test images: 12.96%\n",
      "Accuracy of the model on the test images: 12.93%\n",
      "Accuracy of the model on the test images: 12.89%\n",
      "Accuracy of the model on the test images: 12.92%\n",
      "Accuracy of the model on the test images: 12.89%\n",
      "Accuracy of the model on the test images: 12.92%\n",
      "Accuracy of the model on the test images: 12.89%\n",
      "Accuracy of the model on the test images: 12.92%\n",
      "Accuracy of the model on the test images: 13.08%\n",
      "Accuracy of the model on the test images: 13.05%\n",
      "Accuracy of the model on the test images: 13.08%\n",
      "Accuracy of the model on the test images: 13.04%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.04%\n",
      "Accuracy of the model on the test images: 13.01%\n",
      "Accuracy of the model on the test images: 12.97%\n",
      "Accuracy of the model on the test images: 12.94%\n",
      "Accuracy of the model on the test images: 12.91%\n",
      "Accuracy of the model on the test images: 12.88%\n",
      "Accuracy of the model on the test images: 12.84%\n",
      "Accuracy of the model on the test images: 12.81%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.72%\n",
      "Accuracy of the model on the test images: 12.69%\n",
      "Accuracy of the model on the test images: 12.72%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.65%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.65%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.65%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.65%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.71%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.65%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.65%\n",
      "Accuracy of the model on the test images: 12.68%\n",
      "Accuracy of the model on the test images: 12.65%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.59%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.59%\n",
      "Accuracy of the model on the test images: 12.56%\n",
      "Accuracy of the model on the test images: 12.59%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.59%\n",
      "Accuracy of the model on the test images: 12.56%\n",
      "Accuracy of the model on the test images: 12.59%\n",
      "Accuracy of the model on the test images: 12.56%\n",
      "Accuracy of the model on the test images: 12.59%\n",
      "Accuracy of the model on the test images: 12.62%\n",
      "Accuracy of the model on the test images: 12.59%\n",
      "Accuracy of the model on the test images: 12.61%\n",
      "Accuracy of the model on the test images: 12.59%\n",
      "Accuracy of the model on the test images: 12.61%\n",
      "Accuracy of the model on the test images: 12.59%\n",
      "Accuracy of the model on the test images: 12.61%\n",
      "Accuracy of the model on the test images: 12.64%\n",
      "Accuracy of the model on the test images: 12.67%\n",
      "Accuracy of the model on the test images: 12.70%\n",
      "Accuracy of the model on the test images: 12.73%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.81%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.72%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.78%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.72%\n",
      "Accuracy of the model on the test images: 12.75%\n",
      "Accuracy of the model on the test images: 12.88%\n",
      "Accuracy of the model on the test images: 12.85%\n",
      "Accuracy of the model on the test images: 12.83%\n",
      "Accuracy of the model on the test images: 12.80%\n",
      "Accuracy of the model on the test images: 12.77%\n",
      "Accuracy of the model on the test images: 12.74%\n",
      "Accuracy of the model on the test images: 12.72%\n",
      "Accuracy of the model on the test images: 12.69%\n",
      "Accuracy of the model on the test images: 12.77%\n",
      "Accuracy of the model on the test images: 12.74%\n",
      "Accuracy of the model on the test images: 12.77%\n",
      "Accuracy of the model on the test images: 12.74%\n",
      "Accuracy of the model on the test images: 12.71%\n",
      "Accuracy of the model on the test images: 12.79%\n",
      "Accuracy of the model on the test images: 12.82%\n",
      "Accuracy of the model on the test images: 12.79%\n",
      "Accuracy of the model on the test images: 12.87%\n",
      "Accuracy of the model on the test images: 12.89%\n",
      "Accuracy of the model on the test images: 12.87%\n",
      "Accuracy of the model on the test images: 12.84%\n",
      "Accuracy of the model on the test images: 12.81%\n",
      "Accuracy of the model on the test images: 12.84%\n",
      "Accuracy of the model on the test images: 12.86%\n",
      "Accuracy of the model on the test images: 12.94%\n",
      "Accuracy of the model on the test images: 13.02%\n",
      "Accuracy of the model on the test images: 13.04%\n",
      "Accuracy of the model on the test images: 13.02%\n",
      "Accuracy of the model on the test images: 13.09%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.04%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.22%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.22%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.08%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.08%\n",
      "Accuracy of the model on the test images: 13.10%\n",
      "Accuracy of the model on the test images: 13.08%\n",
      "Accuracy of the model on the test images: 13.10%\n",
      "Accuracy of the model on the test images: 13.08%\n",
      "Accuracy of the model on the test images: 13.06%\n",
      "Accuracy of the model on the test images: 13.08%\n",
      "Accuracy of the model on the test images: 13.05%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.05%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.05%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.09%\n",
      "Accuracy of the model on the test images: 13.07%\n",
      "Accuracy of the model on the test images: 13.09%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.52%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.52%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.55%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.55%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.52%\n",
      "Accuracy of the model on the test images: 13.61%\n",
      "Accuracy of the model on the test images: 13.59%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.59%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.59%\n",
      "Accuracy of the model on the test images: 13.60%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.60%\n",
      "Accuracy of the model on the test images: 13.61%\n",
      "Accuracy of the model on the test images: 13.59%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.66%\n",
      "Accuracy of the model on the test images: 13.71%\n",
      "Accuracy of the model on the test images: 13.69%\n",
      "Accuracy of the model on the test images: 13.68%\n",
      "Accuracy of the model on the test images: 13.69%\n",
      "Accuracy of the model on the test images: 13.67%\n",
      "Accuracy of the model on the test images: 13.69%\n",
      "Accuracy of the model on the test images: 13.74%\n",
      "Accuracy of the model on the test images: 13.72%\n",
      "Accuracy of the model on the test images: 13.74%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.71%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.81%\n",
      "Accuracy of the model on the test images: 13.80%\n",
      "Accuracy of the model on the test images: 13.81%\n",
      "Accuracy of the model on the test images: 13.83%\n",
      "Accuracy of the model on the test images: 13.81%\n",
      "Accuracy of the model on the test images: 13.82%\n",
      "Accuracy of the model on the test images: 13.84%\n",
      "Accuracy of the model on the test images: 13.82%\n",
      "Accuracy of the model on the test images: 13.83%\n",
      "Accuracy of the model on the test images: 13.82%\n",
      "Accuracy of the model on the test images: 13.80%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.81%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.85%\n",
      "Accuracy of the model on the test images: 13.90%\n",
      "Accuracy of the model on the test images: 13.91%\n",
      "Accuracy of the model on the test images: 13.89%\n",
      "Accuracy of the model on the test images: 13.87%\n",
      "Accuracy of the model on the test images: 13.86%\n",
      "Accuracy of the model on the test images: 13.87%\n",
      "Accuracy of the model on the test images: 13.95%\n",
      "Accuracy of the model on the test images: 13.97%\n",
      "Accuracy of the model on the test images: 13.95%\n",
      "Accuracy of the model on the test images: 13.93%\n",
      "Accuracy of the model on the test images: 13.94%\n",
      "Accuracy of the model on the test images: 13.93%\n",
      "Accuracy of the model on the test images: 13.91%\n",
      "Accuracy of the model on the test images: 13.92%\n",
      "Accuracy of the model on the test images: 13.90%\n",
      "Accuracy of the model on the test images: 13.89%\n",
      "Accuracy of the model on the test images: 13.87%\n",
      "Accuracy of the model on the test images: 13.85%\n",
      "Accuracy of the model on the test images: 13.83%\n",
      "Accuracy of the model on the test images: 13.84%\n",
      "Accuracy of the model on the test images: 13.86%\n",
      "Accuracy of the model on the test images: 13.84%\n",
      "Accuracy of the model on the test images: 13.82%\n",
      "Accuracy of the model on the test images: 13.80%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.71%\n",
      "Accuracy of the model on the test images: 13.70%\n",
      "Accuracy of the model on the test images: 13.68%\n",
      "Accuracy of the model on the test images: 13.66%\n",
      "Accuracy of the model on the test images: 13.64%\n",
      "Accuracy of the model on the test images: 13.66%\n",
      "Accuracy of the model on the test images: 13.67%\n",
      "Accuracy of the model on the test images: 13.65%\n",
      "Accuracy of the model on the test images: 13.67%\n",
      "Accuracy of the model on the test images: 13.65%\n",
      "Accuracy of the model on the test images: 13.67%\n",
      "Accuracy of the model on the test images: 13.65%\n",
      "Accuracy of the model on the test images: 13.66%\n",
      "Accuracy of the model on the test images: 13.68%\n",
      "Accuracy of the model on the test images: 13.69%\n",
      "Accuracy of the model on the test images: 13.67%\n",
      "Accuracy of the model on the test images: 13.66%\n",
      "Accuracy of the model on the test images: 13.67%\n",
      "Accuracy of the model on the test images: 13.65%\n",
      "Accuracy of the model on the test images: 13.64%\n",
      "Accuracy of the model on the test images: 13.62%\n",
      "Accuracy of the model on the test images: 13.60%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.60%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.60%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.62%\n",
      "Accuracy of the model on the test images: 13.64%\n",
      "Accuracy of the model on the test images: 13.69%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.71%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.74%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.80%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.88%\n",
      "Accuracy of the model on the test images: 13.86%\n",
      "Accuracy of the model on the test images: 13.84%\n",
      "Accuracy of the model on the test images: 13.82%\n",
      "Accuracy of the model on the test images: 13.81%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.80%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.74%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.80%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.74%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.74%\n",
      "Accuracy of the model on the test images: 13.72%\n",
      "Accuracy of the model on the test images: 13.74%\n",
      "Accuracy of the model on the test images: 13.72%\n",
      "Accuracy of the model on the test images: 13.70%\n",
      "Accuracy of the model on the test images: 13.72%\n",
      "Accuracy of the model on the test images: 13.70%\n",
      "Accuracy of the model on the test images: 13.68%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.71%\n",
      "Accuracy of the model on the test images: 13.69%\n",
      "Accuracy of the model on the test images: 13.71%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.74%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.80%\n",
      "Accuracy of the model on the test images: 13.84%\n",
      "Accuracy of the model on the test images: 13.85%\n",
      "Accuracy of the model on the test images: 13.83%\n",
      "Accuracy of the model on the test images: 13.82%\n",
      "Accuracy of the model on the test images: 13.83%\n",
      "Accuracy of the model on the test images: 13.84%\n",
      "Accuracy of the model on the test images: 13.86%\n",
      "Accuracy of the model on the test images: 13.87%\n",
      "Accuracy of the model on the test images: 13.85%\n",
      "Accuracy of the model on the test images: 13.84%\n",
      "Accuracy of the model on the test images: 13.82%\n",
      "Accuracy of the model on the test images: 13.81%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.80%\n",
      "Accuracy of the model on the test images: 13.79%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.78%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.77%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.74%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.71%\n",
      "Accuracy of the model on the test images: 13.72%\n",
      "Accuracy of the model on the test images: 13.71%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.76%\n",
      "Accuracy of the model on the test images: 13.75%\n",
      "Accuracy of the model on the test images: 13.73%\n",
      "Accuracy of the model on the test images: 13.72%\n",
      "Accuracy of the model on the test images: 13.70%\n",
      "Accuracy of the model on the test images: 13.69%\n",
      "Accuracy of the model on the test images: 13.70%\n",
      "Accuracy of the model on the test images: 13.71%\n",
      "Accuracy of the model on the test images: 13.70%\n",
      "Accuracy of the model on the test images: 13.68%\n",
      "Accuracy of the model on the test images: 13.67%\n",
      "Accuracy of the model on the test images: 13.65%\n",
      "Accuracy of the model on the test images: 13.64%\n",
      "Accuracy of the model on the test images: 13.62%\n",
      "Accuracy of the model on the test images: 13.61%\n",
      "Accuracy of the model on the test images: 13.65%\n",
      "Accuracy of the model on the test images: 13.63%\n",
      "Accuracy of the model on the test images: 13.64%\n",
      "Accuracy of the model on the test images: 13.63%\n",
      "Accuracy of the model on the test images: 13.67%\n",
      "Accuracy of the model on the test images: 13.68%\n",
      "Accuracy of the model on the test images: 13.69%\n",
      "Accuracy of the model on the test images: 13.68%\n",
      "Accuracy of the model on the test images: 13.66%\n",
      "Accuracy of the model on the test images: 13.65%\n",
      "Accuracy of the model on the test images: 13.63%\n",
      "Accuracy of the model on the test images: 13.62%\n",
      "Accuracy of the model on the test images: 13.60%\n",
      "Accuracy of the model on the test images: 13.59%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.60%\n",
      "Accuracy of the model on the test images: 13.61%\n",
      "Accuracy of the model on the test images: 13.59%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.55%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.55%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.55%\n",
      "Accuracy of the model on the test images: 13.59%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.55%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.55%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.55%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.58%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.57%\n",
      "Accuracy of the model on the test images: 13.56%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.52%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.52%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.52%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.52%\n",
      "Accuracy of the model on the test images: 13.53%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.52%\n",
      "Accuracy of the model on the test images: 13.54%\n",
      "Accuracy of the model on the test images: 13.52%\n",
      "Accuracy of the model on the test images: 13.51%\n",
      "Accuracy of the model on the test images: 13.50%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.49%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.48%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.22%\n",
      "Accuracy of the model on the test images: 13.22%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.10%\n",
      "Accuracy of the model on the test images: 13.09%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.10%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.23%\n",
      "Accuracy of the model on the test images: 13.22%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.18%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.10%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.10%\n",
      "Accuracy of the model on the test images: 13.11%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.12%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.13%\n",
      "Accuracy of the model on the test images: 13.14%\n",
      "Accuracy of the model on the test images: 13.15%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.16%\n",
      "Accuracy of the model on the test images: 13.17%\n",
      "Accuracy of the model on the test images: 13.19%\n",
      "Accuracy of the model on the test images: 13.20%\n",
      "Accuracy of the model on the test images: 13.21%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.24%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.25%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.26%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.47%\n",
      "Accuracy of the model on the test images: 13.46%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.45%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.44%\n",
      "Accuracy of the model on the test images: 13.43%\n",
      "Accuracy of the model on the test images: 13.42%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.41%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.40%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.39%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.38%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.37%\n",
      "Accuracy of the model on the test images: 13.36%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.35%\n",
      "Accuracy of the model on the test images: 13.34%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.33%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.32%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.31%\n",
      "Accuracy of the model on the test images: 13.30%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.29%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.27%\n",
      "Accuracy of the model on the test images: 13.28%\n",
      "Accuracy of the model on the test images: 13.28%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_dataloader:\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the model on the test images: {:.2f}%'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
