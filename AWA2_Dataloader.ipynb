{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d60b6b",
   "metadata": {},
   "source": [
    "Lien de téléchargement des données: https://cvml.ista.ac.at/AwA2/        \n",
    "\n",
    "13GB file : https://cvml.ista.ac.at/AwA2/AwA2-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d23f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cv2 # Pour utiliser open_cv, il faut la version de python est 3.7\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision \n",
    "from torchvision.io import read_image\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e8e4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant. Should be the path to the folder named JPEGImages, containing the 33K images in its subfolders.\n",
    "JPEGIMAGES_FOLDER_PATH = \"E:\\\\3710datas\\\\Animals_with_Attributes2\\\\JPEGImages\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b73113c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(764, 918, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick test\n",
    "test = JPEGIMAGES_FOLDER_PATH+\"fox\\\\fox_10001.jpg\"\n",
    "img = cv2.imread(test) \n",
    "print(img.shape) #ndarray\n",
    "print(type(img))\n",
    "cv2.imshow('Sample Image from AwA2 dataset',img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f192da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['antelope', 'bat', 'beaver', 'blue+whale', 'bobcat', 'buffalo', 'chihuahua', 'chimpanzee', 'collie', 'cow', 'dalmatian', 'deer', 'dolphin', 'elephant', 'fox', 'german+shepherd', 'giant+panda', 'giraffe', 'gorilla', 'grizzly+bear', 'hamster', 'hippopotamus', 'horse', 'humpback+whale', 'killer+whale', 'leopard', 'lion', 'mole', 'moose', 'mouse', 'otter', 'ox', 'persian+cat', 'pig', 'polar+bear', 'rabbit', 'raccoon', 'rat', 'rhinoceros', 'seal', 'sheep', 'siamese+cat', 'skunk', 'spider+monkey', 'squirrel', 'tiger', 'walrus', 'weasel', 'wolf', 'zebra']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dirs = os.listdir(JPEGIMAGES_FOLDER_PATH)\n",
    "print(labels_dirs)\n",
    "len(labels_dirs) # 50 labels / subdirectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60066a3e",
   "metadata": {},
   "source": [
    "# Note : Some labels have a low number of images. \n",
    "\n",
    "## Possible solutions to explore : \n",
    "    Data augmentation : creating new training data by applying random transformations to existing images, such as rotating, cropping, or flipping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ade44228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antelope': 1046, 'bat': 383, 'beaver': 193, 'blue+whale': 174, 'bobcat': 630, 'buffalo': 895, 'chihuahua': 567, 'chimpanzee': 728, 'collie': 1028, 'cow': 1338, 'dalmatian': 549, 'deer': 1344, 'dolphin': 946, 'elephant': 1038, 'fox': 664, 'german+shepherd': 1033, 'giant+panda': 874, 'giraffe': 1202, 'gorilla': 872, 'grizzly+bear': 852, 'hamster': 779, 'hippopotamus': 684, 'horse': 1645, 'humpback+whale': 709, 'killer+whale': 291, 'leopard': 720, 'lion': 1019, 'mole': 100, 'moose': 704, 'mouse': 185, 'otter': 758, 'ox': 728, 'persian+cat': 747, 'pig': 713, 'polar+bear': 868, 'rabbit': 1088, 'raccoon': 512, 'rat': 310, 'rhinoceros': 696, 'seal': 988, 'sheep': 1420, 'siamese+cat': 500, 'skunk': 188, 'spider+monkey': 291, 'squirrel': 1200, 'tiger': 877, 'walrus': 215, 'weasel': 272, 'wolf': 589, 'zebra': 1170}\n",
      "{'antelope': 0.028, 'bat': 0.0103, 'beaver': 0.0052, 'blue+whale': 0.0047, 'bobcat': 0.0169, 'buffalo': 0.024, 'chihuahua': 0.0152, 'chimpanzee': 0.0195, 'collie': 0.0275, 'cow': 0.0359, 'dalmatian': 0.0147, 'deer': 0.036, 'dolphin': 0.0253, 'elephant': 0.0278, 'fox': 0.0178, 'german+shepherd': 0.0277, 'giant+panda': 0.0234, 'giraffe': 0.0322, 'gorilla': 0.0234, 'grizzly+bear': 0.0228, 'hamster': 0.0209, 'hippopotamus': 0.0183, 'horse': 0.0441, 'humpback+whale': 0.019, 'killer+whale': 0.0078, 'leopard': 0.0193, 'lion': 0.0273, 'mole': 0.0027, 'moose': 0.0189, 'mouse': 0.005, 'otter': 0.0203, 'ox': 0.0195, 'persian+cat': 0.02, 'pig': 0.0191, 'polar+bear': 0.0233, 'rabbit': 0.0292, 'raccoon': 0.0137, 'rat': 0.0083, 'rhinoceros': 0.0186, 'seal': 0.0265, 'sheep': 0.038, 'siamese+cat': 0.0134, 'skunk': 0.005, 'spider+monkey': 0.0078, 'squirrel': 0.0322, 'tiger': 0.0235, 'walrus': 0.0058, 'weasel': 0.0073, 'wolf': 0.0158, 'zebra': 0.0313}\n"
     ]
    }
   ],
   "source": [
    "def find_num_images_per_label(img_dir = JPEGIMAGES_FOLDER_PATH) -> tuple[dict,dict]: \n",
    "    \"\"\" \n",
    "    USEFUL FOR SAMPLING.\n",
    "    Return a dict with keys as the 50 labels, and values being the number of images in each subdirectory corresponding to label\n",
    "    and a second dict with the relative numbers (proportion) for every label compared to the total number of images (useful for sampling)\"\"\"\n",
    "    labels_dirs = os.listdir(img_dir)\n",
    "    num_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    proportions_images_per_label = dict.fromkeys(labels_dirs)\n",
    "    total_num_images = 0\n",
    "\n",
    "    # Update absolute number of images per label\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        specific_label_path = os.path.join(img_dir, labels_dirs[i])\n",
    "        num_images_label = len(os.listdir(specific_label_path))\n",
    "        total_num_images += num_images_label\n",
    "        num_images_per_label[label] = num_images_label\n",
    "\n",
    "    # Update relative number of images per label (proportion)\n",
    "    for i, label in enumerate(labels_dirs) : \n",
    "        num_images_label = num_images_per_label[label]\n",
    "        proportion_label = round(num_images_label / total_num_images, 4)\n",
    "        proportions_images_per_label[label] = proportion_label\n",
    "\n",
    "    return num_images_per_label, proportions_images_per_label\n",
    "\n",
    "num_images_per_label, proportions_images_per_label = find_num_images_per_label()\n",
    "print(num_images_per_label)\n",
    "print(proportions_images_per_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc3b38ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully created annotations.csv file.\n"
     ]
    }
   ],
   "source": [
    "ANNOTATIONS_FILENAME = 'annotations.csv'\n",
    "\n",
    "def create_annotations_csv_file(annotations_filename = ANNOTATIONS_FILENAME, img_dir = JPEGIMAGES_FOLDER_PATH): \n",
    "    \"\"\" \n",
    "    Create a csv annotations_file, annotations.csv, with two columns, in the format : \n",
    "                        path/to/image, label\n",
    "    \n",
    "    The annotation csv is necessary for DataLoader.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels_dirs:list = os.listdir(img_dir)\n",
    "   \n",
    "    if os.path.exists(annotations_filename):\n",
    "        os.remove(annotations_filename)\n",
    "        print(f'Deleted existent {ANNOTATIONS_FILENAME} file.\\n ---------------------------')\n",
    "    \n",
    "    with open(annotations_filename, 'w', newline='') as file :\n",
    "        writer = csv.writer(file, dialect='excel', delimiter=',')\n",
    "\n",
    "        for i, label in enumerate(labels_dirs) : \n",
    "\n",
    "            specific_label_path = os.path.join(img_dir, label)\n",
    "            images_names = os.listdir(specific_label_path)\n",
    "\n",
    "            for j, image_name in enumerate(images_names):\n",
    "                full_path_to_img= os.path.join(specific_label_path, image_name)\n",
    "                full_path_to_img= os.path.join(label, image_name)\n",
    "\n",
    "                row = [full_path_to_img, label]\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(f'Sucessfully created {ANNOTATIONS_FILENAME} file.')\n",
    "\n",
    "#\n",
    "create_annotations_csv_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08979242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWA2Dataset(Dataset): # Dataset class to serve as input for the DataLoader.\n",
    "    \"\"\" \n",
    "    Dataset class to serve as input for the DataLoader.\n",
    "    Implements all the required methods and more. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, annotations_file=ANNOTATIONS_FILENAME, img_dir=JPEGIMAGES_FOLDER_PATH, \n",
    "                transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        numbers_infos_dicts: tuple[dict,dict] = find_num_images_per_label(img_dir=JPEGIMAGES_FOLDER_PATH)\n",
    "        self.num_images_per_label = numbers_infos_dicts[0]\n",
    "        self.proportions_images_per_label = numbers_infos_dicts[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # img_path = self.img_labels.iloc[idx, 0]\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        image = read_image(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a923daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buffalo\n",
      "tensor([[[ 90,  98, 110,  ..., 197, 200, 194],\n",
      "         [ 77,  73,  97,  ..., 197, 196, 111],\n",
      "         [ 89,  79, 103,  ..., 195, 172,  69],\n",
      "         ...,\n",
      "         [129, 199, 160,  ...,  99, 126,  87],\n",
      "         [145,  95, 175,  ..., 103,  77, 125],\n",
      "         [ 97, 117, 191,  ...,  65, 110,  98]],\n",
      "\n",
      "        [[130, 142, 138,  ..., 231, 229, 225],\n",
      "         [121, 100, 128,  ..., 229, 231, 125],\n",
      "         [118, 105, 135,  ..., 231, 191,  76],\n",
      "         ...,\n",
      "         [155, 208, 188,  ..., 131, 155, 122],\n",
      "         [143, 101, 193,  ..., 135, 104, 158],\n",
      "         [104, 114, 207,  ...,  82, 141, 126]],\n",
      "\n",
      "        [[ 63,  69, 120,  ..., 255, 252, 248],\n",
      "         [ 67,  67,  69,  ..., 254, 251, 178],\n",
      "         [ 82,  79, 101,  ..., 255, 241, 121],\n",
      "         ...,\n",
      "         [ 97, 166, 119,  ...,  81, 100,  36],\n",
      "         [104,  47, 136,  ...,  84,  34,  48],\n",
      "         [ 58,  66, 141,  ...,  43,  63,  58]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "dataset = AWA2Dataset()\n",
    "\n",
    "## TODO : Change transforms. Currently this is not useful.\n",
    "dataset.transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    # transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the reaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
    "                    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ])\n",
    "\n",
    "# Testing. All good\n",
    "random_index = np.random.randint(0, len(dataset))\n",
    "image, label = dataset[random_index]\n",
    "print(label)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64d291b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('german+shepherd', 'elephant', 'german+shepherd', 'rhinoceros') tensor([[[[252, 252, 252,  ..., 252, 252, 252],\n",
      "          [252, 252, 252,  ..., 252, 252, 252],\n",
      "          [252, 252, 252,  ..., 252, 252, 252],\n",
      "          ...,\n",
      "          [252, 252, 252,  ...,  12,  18,  26],\n",
      "          [252, 252, 252,  ...,  18,  19,  23],\n",
      "          [252, 252, 252,  ...,  18,  23,  19]],\n",
      "\n",
      "         [[252, 252, 252,  ..., 252, 252, 252],\n",
      "          [252, 252, 252,  ..., 252, 252, 252],\n",
      "          [252, 252, 252,  ..., 252, 252, 252],\n",
      "          ...,\n",
      "          [252, 252, 252,  ...,  13,  19,  31],\n",
      "          [252, 252, 252,  ...,  19,  20,  28],\n",
      "          [252, 252, 252,  ...,  19,  24,  24]],\n",
      "\n",
      "         [[252, 252, 252,  ..., 252, 252, 252],\n",
      "          [252, 252, 252,  ..., 252, 252, 252],\n",
      "          [252, 252, 252,  ..., 252, 252, 252],\n",
      "          ...,\n",
      "          [252, 252, 252,  ...,  18,  24,  35],\n",
      "          [252, 252, 252,  ...,  24,  25,  33],\n",
      "          [252, 252, 252,  ...,  24,  29,  30]]],\n",
      "\n",
      "\n",
      "        [[[160, 178, 197,  ..., 154, 159, 166],\n",
      "          [188, 200, 203,  ..., 165, 191, 196],\n",
      "          [207, 210, 207,  ..., 165, 156, 149],\n",
      "          ...,\n",
      "          [ 27,  55,  67,  ..., 216, 215, 218],\n",
      "          [ 67,  44,  36,  ..., 208, 202, 188],\n",
      "          [ 84,  50,  32,  ..., 210, 199, 153]],\n",
      "\n",
      "         [[132, 148, 167,  ..., 140, 149, 156],\n",
      "          [155, 166, 169,  ..., 151, 170, 171],\n",
      "          [173, 175, 172,  ..., 148, 140, 125],\n",
      "          ...,\n",
      "          [ 31,  53,  53,  ..., 191, 189, 191],\n",
      "          [ 70,  32,  27,  ..., 183, 183, 166],\n",
      "          [ 75,  37,  24,  ..., 189, 184, 138]],\n",
      "\n",
      "         [[102, 122, 140,  ..., 103, 113, 121],\n",
      "          [120, 138, 142,  ..., 109, 127, 131],\n",
      "          [136, 143, 142,  ..., 102,  95,  94],\n",
      "          ...,\n",
      "          [  9,  29,  32,  ..., 145, 151, 149],\n",
      "          [ 30,  20,  15,  ..., 126, 126, 109],\n",
      "          [ 20,  20,  12,  ..., 122, 121,  82]]],\n",
      "\n",
      "\n",
      "        [[[ 17,  19,  19,  ...,  68, 114, 134],\n",
      "          [ 21,  17,  13,  ...,  42, 125, 114],\n",
      "          [ 16,  12,  14,  ...,  34,  36, 104],\n",
      "          ...,\n",
      "          [ 21,  18,  20,  ...,  21,  22,  20],\n",
      "          [ 20,  17,  20,  ...,  20,  23,  22],\n",
      "          [ 21,  16,  23,  ...,  19,  22,  23]],\n",
      "\n",
      "         [[ 18,  23,  23,  ...,  62,  93,  97],\n",
      "          [ 22,  21,  16,  ...,  36, 115,  92],\n",
      "          [ 17,  16,  17,  ...,  30,  33,  91],\n",
      "          ...,\n",
      "          [ 13,  15,  15,  ...,  12,  13,  11],\n",
      "          [ 15,  12,  15,  ...,  13,  14,  13],\n",
      "          [ 16,  11,  15,  ...,  12,  13,  16]],\n",
      "\n",
      "         [[ 22,  24,  25,  ...,  38,  29,  28],\n",
      "          [ 26,  23,  21,  ...,  28,  14,  29],\n",
      "          [ 21,  19,  22,  ...,  28,  30,  69],\n",
      "          ...,\n",
      "          [ 11,  10,  11,  ...,   7,   8,   6],\n",
      "          [ 12,   9,  11,  ...,   7,   7,   8],\n",
      "          [ 13,   8,  12,  ...,   6,   6,  10]]],\n",
      "\n",
      "\n",
      "        [[[125, 128, 125,  ..., 208, 182, 182],\n",
      "          [127, 117,  92,  ..., 203, 192, 184],\n",
      "          [ 86,  98, 114,  ..., 169, 167, 186],\n",
      "          ...,\n",
      "          [241, 202, 179,  ..., 233, 230, 226],\n",
      "          [128, 194, 206,  ..., 237, 207, 230],\n",
      "          [214, 211, 199,  ..., 203, 224, 226]],\n",
      "\n",
      "         [[ 97, 104, 101,  ..., 191, 167, 171],\n",
      "          [105, 100,  94,  ..., 193, 180, 171],\n",
      "          [ 94, 100, 112,  ..., 163, 156, 172],\n",
      "          ...,\n",
      "          [226, 175, 149,  ..., 219, 212, 206],\n",
      "          [111, 167, 177,  ..., 221, 192, 211],\n",
      "          [190, 188, 181,  ..., 192, 211, 212]],\n",
      "\n",
      "         [[ 85,  91,  86,  ..., 143, 125, 123],\n",
      "          [ 93,  84,  62,  ..., 146, 130, 126],\n",
      "          [ 59,  72,  88,  ..., 115, 110, 133],\n",
      "          ...,\n",
      "          [204, 143, 129,  ..., 203, 196, 186],\n",
      "          [ 89, 137, 176,  ..., 199, 170, 192],\n",
      "          [172, 160, 173,  ..., 191, 193, 197]]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# Experiment with DataLoader. Everything works good\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=4, shuffle=True)\n",
    "dataiter = iter(dataloader)\n",
    "data = next(dataiter)\n",
    "\n",
    "images, labels = data \n",
    "print(labels, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8756690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37321 9331\n",
      "epoch 1 / 2, step, 5/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 1 / 2, step, 10/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 1 / 2, step, 15/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 1 / 2, step, 20/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "Completed\n",
      "epoch 2 / 2, step, 5/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 2 / 2, step, 10/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 2 / 2, step, 15/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "epoch 2 / 2, step, 20/9331, inputs torch.Size([4, 3, 224, 224])\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "# Training loop example\n",
    "num_epochs = 2 \n",
    "batch_size = 4\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "dataloader = DataLoader(dataset = dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs) : \n",
    "    # loop over trainloader \n",
    "    for i, (inputs, labels) in enumerate(dataloader) : \n",
    "        \n",
    "        # Do forward and backward pass, update the weights \n",
    "        if(i+1) % 5 == 0 :\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step, {i+1}/{n_iterations}, inputs {inputs.shape}')\n",
    "\n",
    "        if i==20 : \n",
    "            print('Completed')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f9705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
