{"cells":[{"cell_type":"markdown","metadata":{"id":"guNaUZaJhqcG"},"source":["Lien de téléchargement des données: https://cvml.ista.ac.at/AwA2/        \n","\n","13GB file : https://cvml.ista.ac.at/AwA2/AwA2-data.zip"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WbF2Cf05Nyx_","executionInfo":{"status":"ok","timestamp":1679621065430,"user_tz":240,"elapsed":20748,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"1bac509a-1d61-4236-bb9b-70bb61787a58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install wandb -qU"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gV68yhFicMdU","executionInfo":{"status":"ok","timestamp":1679624829447,"user_tz":240,"elapsed":9286,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"7b06b547-018d-4695-a5eb-951eed33a996"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/2.0 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["# Log in to your W&B account\n","import wandb\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hDf2Xbq1cj4Q","executionInfo":{"status":"ok","timestamp":1679626235206,"user_tz":240,"elapsed":186,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"f1f55bd8-992f-4a84-a385-3778ce3601df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAsS9gishqcJ","executionInfo":{"status":"ok","timestamp":1679621294773,"user_tz":240,"elapsed":436,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"930383fb-d9ab-4980-eb67-0e616218fe38"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.9.16 (main, Dec  7 2022, 01:11:51) \n","[GCC 9.4.0]\n"]}],"source":["import sys\n","print(sys.version)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEOxHANkhqcL"},"outputs":[],"source":["import sys\n","import cv2 # Pour utiliser opencv-python, il faut la version de python est 3.7\n","import os\n","import csv\n","\n","import numpy as np \n","import pandas as pd \n","import math\n","\n","import torch \n","from torch.utils.data import Dataset, DataLoader\n","import torchvision \n","from torchvision.io import read_image\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","\n","np.random.seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndd7-sX6hqcL"},"outputs":[],"source":["# Constant. Should be the path to the folder named JPEGImages, containing the 33K images in its subfolders.\n","DATA_FOLDER_PATH = '/content/drive/MyDrive/IFT3710/Animals_with_Attributes2/'\n","JPEGIMAGES_FOLDER_PATH = '/content/drive/MyDrive/IFT3710/Animals_with_Attributes2/JPEGImages/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82BKFdAQhqcL","executionInfo":{"status":"ok","timestamp":1679621592082,"user_tz":240,"elapsed":127,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"5234e014-8ee4-424b-9948-cf6de76c67c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["(764, 918, 3)\n","<class 'numpy.ndarray'>\n"]}],"source":["# quick test\n","test = JPEGIMAGES_FOLDER_PATH+\"fox/fox_10001.jpg\"\n","img = cv2.imread(test) \n","print(img.shape) #ndarray\n","print(type(img))\n","# cv2.imshow('Sample Image from AwA2 dataset',img)\n","# cv2.waitKey(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXGpnjCChqcM","outputId":"a13319c4-0cd2-4228-d1c8-726317d58c76","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679621596283,"user_tz":240,"elapsed":136,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['elephant', 'lion', 'deer', 'cow', 'squirrel', 'german+shepherd', 'skunk', 'horse', 'mole', 'walrus', 'weasel', 'grizzly+bear', 'rabbit', 'buffalo', 'beaver', 'chimpanzee', 'bobcat', 'wolf', 'bat', 'mouse', 'seal', 'collie', 'spider+monkey', 'otter', 'rat', 'tiger', 'blue+whale', 'hamster', 'ox', 'zebra', 'sheep', 'giraffe', 'chihuahua', 'leopard', 'polar+bear', 'dolphin', 'fox', 'siamese+cat', 'persian+cat', 'pig', 'moose', 'killer+whale', 'giant+panda', 'humpback+whale', 'antelope', 'raccoon', 'dalmatian', 'rhinoceros', 'gorilla', 'hippopotamus']\n"]},{"output_type":"execute_result","data":{"text/plain":["50"]},"metadata":{},"execution_count":15}],"source":["labels_dirs = os.listdir(JPEGIMAGES_FOLDER_PATH)\n","print(labels_dirs)\n","len(labels_dirs) # 50 labels / subdirectories"]},{"cell_type":"markdown","metadata":{"id":"siGBV5bUhqcM"},"source":["# Note : Some labels have a low number of images. \n","\n","## Possible solutions to explore : \n","    Data augmentation : creating new training data by applying random transformations to existing images, such as rotating, cropping, or flipping them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_PMi1FkRhqcM","outputId":"1ccb5eb7-fbaa-4a53-f1b7-fdcf655ecfd1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679621600738,"user_tz":240,"elapsed":1260,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'elephant': 1038, 'lion': 1019, 'deer': 1344, 'cow': 1338, 'squirrel': 1200, 'german+shepherd': 1033, 'skunk': 188, 'horse': 1645, 'mole': 100, 'walrus': 215, 'weasel': 272, 'grizzly+bear': 852, 'rabbit': 1088, 'buffalo': 895, 'beaver': 193, 'chimpanzee': 728, 'bobcat': 630, 'wolf': 589, 'bat': 383, 'mouse': 185, 'seal': 988, 'collie': 1028, 'spider+monkey': 291, 'otter': 758, 'rat': 310, 'tiger': 877, 'blue+whale': 174, 'hamster': 779, 'ox': 728, 'zebra': 1170, 'sheep': 1420, 'giraffe': 1202, 'chihuahua': 567, 'leopard': 720, 'polar+bear': 868, 'dolphin': 946, 'fox': 664, 'siamese+cat': 500, 'persian+cat': 747, 'pig': 713, 'moose': 704, 'killer+whale': 291, 'giant+panda': 874, 'humpback+whale': 709, 'antelope': 1046, 'raccoon': 512, 'dalmatian': 549, 'rhinoceros': 696, 'gorilla': 872, 'hippopotamus': 684}\n","{'elephant': 0.0278, 'lion': 0.0273, 'deer': 0.036, 'cow': 0.0359, 'squirrel': 0.0322, 'german+shepherd': 0.0277, 'skunk': 0.005, 'horse': 0.0441, 'mole': 0.0027, 'walrus': 0.0058, 'weasel': 0.0073, 'grizzly+bear': 0.0228, 'rabbit': 0.0292, 'buffalo': 0.024, 'beaver': 0.0052, 'chimpanzee': 0.0195, 'bobcat': 0.0169, 'wolf': 0.0158, 'bat': 0.0103, 'mouse': 0.005, 'seal': 0.0265, 'collie': 0.0275, 'spider+monkey': 0.0078, 'otter': 0.0203, 'rat': 0.0083, 'tiger': 0.0235, 'blue+whale': 0.0047, 'hamster': 0.0209, 'ox': 0.0195, 'zebra': 0.0313, 'sheep': 0.038, 'giraffe': 0.0322, 'chihuahua': 0.0152, 'leopard': 0.0193, 'polar+bear': 0.0233, 'dolphin': 0.0253, 'fox': 0.0178, 'siamese+cat': 0.0134, 'persian+cat': 0.02, 'pig': 0.0191, 'moose': 0.0189, 'killer+whale': 0.0078, 'giant+panda': 0.0234, 'humpback+whale': 0.019, 'antelope': 0.028, 'raccoon': 0.0137, 'dalmatian': 0.0147, 'rhinoceros': 0.0186, 'gorilla': 0.0234, 'hippopotamus': 0.0183}\n"]}],"source":["def find_num_images_per_label(img_dir = JPEGIMAGES_FOLDER_PATH): #-> tuple[dict,dict]: \n","    \"\"\" \n","    USEFUL FOR SAMPLING.\n","    Return a dict with keys as the 50 labels, and values being the number of images in each subdirectory corresponding to label\n","    and a second dict with the relative numbers (proportion) for every label compared to the total number of images (useful for sampling)\"\"\"\n","    labels_dirs = os.listdir(img_dir)\n","    num_images_per_label = dict.fromkeys(labels_dirs)\n","    proportions_images_per_label = dict.fromkeys(labels_dirs)\n","    total_num_images = 0\n","\n","    # Update absolute number of images per label\n","    for i, label in enumerate(labels_dirs) : \n","        specific_label_path = os.path.join(img_dir, labels_dirs[i])\n","        num_images_label = len(os.listdir(specific_label_path))\n","        total_num_images += num_images_label\n","        num_images_per_label[label] = num_images_label\n","\n","    # Update relative number of images per label (proportion)\n","    for i, label in enumerate(labels_dirs) : \n","        num_images_label = num_images_per_label[label]\n","        proportion_label = round(num_images_label / total_num_images, 4)\n","        proportions_images_per_label[label] = proportion_label\n","\n","    return num_images_per_label, proportions_images_per_label\n","\n","num_images_per_label, proportions_images_per_label = find_num_images_per_label()\n","print(num_images_per_label)\n","print(proportions_images_per_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7AovZgDhqcN","outputId":"7f27ba2e-78ce-47b8-9c2b-f5d20a99dea4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679621627079,"user_tz":240,"elapsed":842,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted existent annotations.csv file.\n"," ---------------------------\n","Sucessfully created annotations.csv file.\n"]}],"source":["ANNOTATIONS_FILENAME = 'annotations.csv'\n","\n","def create_annotations_csv_file(annotations_filename = ANNOTATIONS_FILENAME, img_dir = JPEGIMAGES_FOLDER_PATH): \n","    \"\"\" \n","    Create a csv annotations_file, annotations.csv, with two columns, in the format : \n","                        path/to/image, label\n","    \n","    The annotation csv is necessary for DataLoader.\n","    \"\"\"\n","    \n","    labels_dirs:list = os.listdir(img_dir)\n","   \n","    if os.path.exists(annotations_filename):\n","        os.remove(annotations_filename)\n","        print(f'Deleted existent {ANNOTATIONS_FILENAME} file.\\n ---------------------------')\n","    \n","    with open(annotations_filename, 'w', newline='') as file :\n","        writer = csv.writer(file, dialect='excel', delimiter=',')\n","\n","        for i, label in enumerate(labels_dirs) : \n","\n","            specific_label_path = os.path.join(img_dir, label)\n","            images_names = os.listdir(specific_label_path)\n","\n","            for j, image_name in enumerate(images_names):\n","                full_path_to_img= os.path.join(specific_label_path, image_name)\n","                full_path_to_img= os.path.join(label, image_name)\n","\n","                row = [full_path_to_img, label]\n","                writer.writerow(row)\n","\n","    print(f'Sucessfully created {ANNOTATIONS_FILENAME} file.')\n","\n","#\n","create_annotations_csv_file()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6kglDLKhqcN","outputId":"2d699d24-21e4-403d-a805-48c3be32b4da","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679621630788,"user_tz":240,"elapsed":359,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'antelope': 0, 'grizzly+bear': 1, 'killer+whale': 2, 'beaver': 3, 'dalmatian': 4, 'persian+cat': 5, 'horse': 6, 'german+shepherd': 7, 'blue+whale': 8, 'siamese+cat': 9, 'skunk': 10, 'mole': 11, 'tiger': 12, 'hippopotamus': 13, 'leopard': 14, 'moose': 15, 'spider+monkey': 16, 'humpback+whale': 17, 'elephant': 18, 'gorilla': 19, 'ox': 20, 'fox': 21, 'sheep': 22, 'seal': 23, 'chimpanzee': 24, 'hamster': 25, 'squirrel': 26, 'rhinoceros': 27, 'rabbit': 28, 'bat': 29, 'giraffe': 30, 'wolf': 31, 'chihuahua': 32, 'rat': 33, 'weasel': 34, 'otter': 35, 'buffalo': 36, 'zebra': 37, 'giant+panda': 38, 'deer': 39, 'bobcat': 40, 'pig': 41, 'lion': 42, 'mouse': 43, 'polar+bear': 44, 'collie': 45, 'walrus': 46, 'raccoon': 47, 'cow': 48, 'dolphin': 49}\n"]}],"source":["# labels_in_number = pd.read_csv(DATA_FOLDER_PATH+\"classes.txt\", delim_whitespace=True,header=None)\n","labels_dict = {}\n","with open(DATA_FOLDER_PATH+\"classes.txt\") as f:\n","    for line in f:\n","        # print(line.split())\n","        (key,val) = line.split()\n","        labels_dict[val] = int(key)-1\n","print(labels_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JlpINdEShqcO"},"outputs":[],"source":["class AWA2Dataset(Dataset): # Dataset class to serve as input for the DataLoader.\n","    \"\"\" \n","    Dataset class to serve as input for the DataLoader.\n","    Implements all the required methods and more. \n","    \"\"\"\n","\n","    def __init__(self, annotations_file=ANNOTATIONS_FILENAME, img_dir=JPEGIMAGES_FOLDER_PATH, \n","                transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","        numbers_infos_dicts: tuple[dict,dict] = find_num_images_per_label(img_dir=JPEGIMAGES_FOLDER_PATH)\n","        self.num_images_per_label = numbers_infos_dicts[0]\n","        self.proportions_images_per_label = numbers_infos_dicts[1]\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        # img_path = self.img_labels.iloc[idx, 0]\n","        key = self.img_labels.iloc[idx, 1]\n","\n","        # Mapping the labels from string to tensor\n","        label = labels_dict[key]\n","\n","        image = read_image(img_path)\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZUD0TFmhqcO"},"outputs":[],"source":["dataset = AWA2Dataset()\n","image,label = dataset[4125]\n","\n","\n","\n","## TODO : Change transforms. Currently this is not useful.\n","dataset.transform = transforms.Compose([\n","                        transforms.ToPILImage(),\n","                        transforms.Resize((224, 224)),\n","                        transforms.RandomHorizontalFlip(),\n","                        transforms.Grayscale(num_output_channels=3),\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.485, 0.456, 0.406), \n","                                             (0.229, 0.224, 0.225))])\n","\n","# Testing. All good\n","random_index = np.random.randint(0, len(dataset))\n","image, label = dataset[random_index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IeuQVvyjhqcO"},"outputs":[],"source":["# Experiment with DataLoader. Everything works good\n","dataloader = DataLoader(dataset = dataset, batch_size=4, shuffle=True)\n","dataiter = iter(dataloader)\n","data = next(dataiter)\n","\n","images, labels = data "]},{"cell_type":"markdown","metadata":{"id":"Pc-WOXn_hqcO"},"source":["### ViT ###"]},{"cell_type":"code","source":["pip install vit-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J5bsEjfsQNLN","executionInfo":{"status":"ok","timestamp":1679621695348,"user_tz":240,"elapsed":4725,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"ceba6904-0e57-4f39-f18e-df46a8b9f53f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting vit-pytorch\n","  Downloading vit_pytorch-1.2.0-py3-none-any.whl (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops>=0.6.0\n","  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from vit-pytorch) (0.14.1+cu116)\n","Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from vit-pytorch) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->vit-pytorch) (4.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->vit-pytorch) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->vit-pytorch) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->vit-pytorch) (8.4.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->vit-pytorch) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->vit-pytorch) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->vit-pytorch) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->vit-pytorch) (2.0.12)\n","Installing collected packages: einops, vit-pytorch\n","Successfully installed einops-0.6.0 vit-pytorch-1.2.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-aBw7hXhqcO","outputId":"3c094f8e-d7a9-4201-f318-aae4f46e7984","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679621699844,"user_tz":240,"elapsed":1655,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method Module.eval of SimpleViT(\n","  (to_patch_embedding): Sequential(\n","    (0): Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1=32, p2=32)\n","    (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n","    (2): Linear(in_features=3072, out_features=1024, bias=True)\n","    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (transformer): Transformer(\n","    (layers): ModuleList(\n","      (0): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (1): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (2): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (3): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (4): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (5): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (to_latent): Identity()\n","  (linear_head): Sequential(\n","    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (1): Linear(in_features=1024, out_features=1000, bias=True)\n","  )\n",")>"]},"metadata":{},"execution_count":26}],"source":["import torch.nn as nn\n","# import timm\n","from vit_pytorch import ViT\n","from vit_pytorch import SimpleViT\n","\n","vit_model = SimpleViT(\n","    image_size = 256,\n","    patch_size = 32,\n","    num_classes = 1000,\n","    dim = 1024,\n","    depth = 6,\n","    heads = 16,\n","    mlp_dim = 2048\n",")\n","\n","vit_model.eval"]},{"cell_type":"markdown","metadata":{"id":"ceJFyCPFhqcP"},"source":["### CUDA ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yTgQO3YIhqcP","executionInfo":{"status":"ok","timestamp":1679621703687,"user_tz":240,"elapsed":135,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"0ed032d0-65da-4613-f5e7-f91e5230917e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":27}],"source":["device = torch.device(\"cuda:0\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2N2GlS8ohqcP","executionInfo":{"status":"ok","timestamp":1679621705515,"user_tz":240,"elapsed":120,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"6a48e906-df20-434e-ee7a-be1bc1d16cb3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":28}],"source":["torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"_rdNS5LihqcP"},"source":["### Question: nombre de noeud ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smZioU3QhqcP"},"outputs":[],"source":["nb_noeud = 4096"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jptOvL68hqcP"},"outputs":[],"source":["# vit_model.classifier[4] = torch.nn.Linear(4096,nb_noeud)\n","# vit_model.classifier[6] = torch.nn.Linear(nb_noeud,50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQq_2Zf1hqcP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679621721054,"user_tz":240,"elapsed":140,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"2ea65e5a-74a4-4f75-fd84-e546cce626af"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.9.16 (main, Dec  7 2022, 01:11:51) \n","[GCC 9.4.0]\n"]}],"source":["import sys\n","print(sys.version)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIdfrm7zhqcP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679623217562,"user_tz":240,"elapsed":339,"user":{"displayName":"Kiron Rothschild","userId":"12191456841328262145"}},"outputId":"e5538d95-4da3-477a-9e85-466192acd4de"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SimpleViT(\n","  (to_patch_embedding): Sequential(\n","    (0): Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1=32, p2=32)\n","    (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n","    (2): Linear(in_features=3072, out_features=1024, bias=True)\n","    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (transformer): Transformer(\n","    (layers): ModuleList(\n","      (0): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (1): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (2): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (3): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (4): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","      (5): ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (to_qkv): Linear(in_features=1024, out_features=3072, bias=False)\n","          (to_out): Linear(in_features=1024, out_features=1024, bias=False)\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=2048, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Linear(in_features=2048, out_features=1024, bias=True)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (to_latent): Identity()\n","  (linear_head): Sequential(\n","    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (1): Linear(in_features=1024, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":49}],"source":["vit_model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"jjDdiEbEhqcP"},"source":["### Question: check dimension de dataloader   ###"]},{"cell_type":"markdown","metadata":{"id":"rxLnQq1AhqcP"},"source":["###  Split training data and test data ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xQY89R-hqcP"},"outputs":[],"source":["batch_size = 32"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkhb47LDhqcQ"},"outputs":[],"source":["dataloader = DataLoader(dataset = dataset, batch_size=batch_size, shuffle=True)\n","train_size = int(0.8*len(dataset))\n","test_size = len(dataset) - train_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6vV1Mq_hqcQ"},"outputs":[],"source":["train_dataset, test_dataset = torch.utils.data.random_split(dataset,[train_size,test_size])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1R26jPnZhqcQ"},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, num_workers=0)\n","test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, num_workers=0)"]},{"cell_type":"markdown","metadata":{"id":"KI861s-BhqcQ"},"source":["###   transfomer labels  ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJjvi-UlhqcQ"},"outputs":[],"source":["path_class = DATA_FOLDER_PATH +\"classes.txt\"\n","class_animal = pd.read_table(path_class,header= None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDsqr9_qhqcQ"},"outputs":[],"source":["animals = class_animal[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5j3p4Vj_hqcQ"},"outputs":[],"source":["dict_label_animal = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOi1R7-ShqcQ"},"outputs":[],"source":["n = 0\n","for i in range(0,len(animals)):\n","    dict_label_animal[animals[i]] = n\n","    n+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTPoi_EkhqcQ"},"outputs":[],"source":["def label_to_num(tuple_labels):\n","    list_labels =[]\n","    for tuple_label in tuple_labels:\n","        list_labels.append(dict_label_animal[tuple_label])\n","    return torch.tensor(list_labels) "]},{"cell_type":"markdown","metadata":{"id":"FCmkwOJohqcQ"},"source":["###   Loss function  ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tIOOdQhihqcQ"},"outputs":[],"source":["lr = 0.001"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NW3u29qjhqcQ"},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss()\n","optimizer = optim.SGD(vit_model.parameters(), lr= lr , momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"BStkQXXNhqcQ"},"source":["###  Training  ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lc5BGjjghqcQ","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5f414e25fb6c48d98485fc96f715df77"]},"outputId":"f13a767b-abe9-42ae-fc19-3393d4864004"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing last run (ID:69e4c8tr) before initializing another..."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f414e25fb6c48d98485fc96f715df77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▄▃▅▄▆▂▆▅▆▆█▄▆▇▆▄▃▂▆▅▆▅▄▂▅▄▅▄▆▅▅▇▄▃▄▁▄▇▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>3.70026</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">fresh-elevator-3</strong> at: <a href='https://wandb.ai/animal-detect-vit/my-awesome-project/runs/69e4c8tr' target=\"_blank\">https://wandb.ai/animal-detect-vit/my-awesome-project/runs/69e4c8tr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20230324_025120-69e4c8tr/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Successfully finished last run (ID:69e4c8tr). Initializing new run:<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.14.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20230324_031636-7o5mek2g</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/animal-detect-vit/vit-basic/runs/7o5mek2g' target=\"_blank\">neat-field-1</a></strong> to <a href='https://wandb.ai/animal-detect-vit/vit-basic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/animal-detect-vit/vit-basic' target=\"_blank\">https://wandb.ai/animal-detect-vit/vit-basic</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/animal-detect-vit/vit-basic/runs/7o5mek2g' target=\"_blank\">https://wandb.ai/animal-detect-vit/vit-basic/runs/7o5mek2g</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[1,     1] loss: 3.724\n","[1,     2] loss: 3.737\n","[1,     3] loss: 3.645\n","[1,     4] loss: 3.563\n","[1,     5] loss: 3.824\n","[1,     6] loss: 3.474\n","[1,     7] loss: 3.654\n","[1,     8] loss: 4.012\n","[1,     9] loss: 3.612\n","[1,    10] loss: 3.872\n","[1,    11] loss: 3.768\n","[1,    12] loss: 3.700\n","[1,    13] loss: 3.895\n","[1,    14] loss: 3.719\n","[1,    15] loss: 3.997\n","[1,    16] loss: 3.689\n","[1,    17] loss: 3.747\n","[1,    18] loss: 3.515\n","[1,    19] loss: 3.859\n","[1,    20] loss: 3.713\n","[1,    21] loss: 4.036\n","[1,    22] loss: 3.588\n","[1,    23] loss: 3.767\n","[1,    24] loss: 3.893\n","[1,    25] loss: 3.594\n","[1,    26] loss: 3.686\n","[1,    27] loss: 3.820\n","[1,    28] loss: 3.756\n","[1,    29] loss: 3.598\n","[1,    30] loss: 3.692\n","[1,    31] loss: 3.618\n","[1,    32] loss: 3.940\n","[1,    33] loss: 3.870\n","[1,    34] loss: 3.603\n","[1,    35] loss: 3.781\n","[1,    36] loss: 3.799\n","[1,    37] loss: 3.710\n","[1,    38] loss: 3.672\n","[1,    39] loss: 3.860\n","[1,    40] loss: 3.849\n","[1,    41] loss: 3.794\n","[1,    42] loss: 3.654\n","[1,    43] loss: 3.711\n","[1,    44] loss: 3.640\n","[1,    45] loss: 3.709\n","[1,    46] loss: 3.776\n","[1,    47] loss: 3.800\n","[1,    48] loss: 3.479\n","[1,    49] loss: 3.636\n","[1,    50] loss: 3.658\n","[1,    51] loss: 3.834\n","[1,    52] loss: 4.017\n","[1,    53] loss: 3.846\n","[1,    54] loss: 4.065\n","[1,    55] loss: 3.905\n","[1,    56] loss: 3.853\n","[1,    57] loss: 3.834\n","[1,    58] loss: 3.606\n","[1,    59] loss: 3.698\n","[1,    60] loss: 3.736\n","[1,    61] loss: 3.746\n","[1,    62] loss: 3.601\n","[1,    63] loss: 3.572\n","[1,    64] loss: 3.717\n","[1,    65] loss: 3.781\n","[1,    66] loss: 3.778\n","[1,    67] loss: 4.021\n","[1,    68] loss: 3.927\n","[1,    69] loss: 3.514\n","[1,    70] loss: 3.680\n","[1,    71] loss: 3.644\n","[1,    72] loss: 3.754\n","[1,    73] loss: 3.606\n","[1,    74] loss: 3.832\n","[1,    75] loss: 4.012\n","[1,    76] loss: 3.778\n","[1,    77] loss: 3.385\n","[1,    78] loss: 3.703\n","[1,    79] loss: 3.598\n","[1,    80] loss: 3.774\n","[1,    81] loss: 3.747\n","[1,    82] loss: 3.830\n","[1,    83] loss: 3.807\n","[1,    84] loss: 3.894\n","[1,    85] loss: 3.746\n","[1,    86] loss: 3.727\n","[1,    87] loss: 3.760\n","[1,    88] loss: 3.617\n","[1,    89] loss: 3.806\n","[1,    90] loss: 4.051\n","[1,    91] loss: 3.970\n","[1,    92] loss: 4.019\n","[1,    93] loss: 3.647\n","[1,    94] loss: 3.826\n","[1,    95] loss: 3.696\n","[1,    96] loss: 3.673\n","[1,    97] loss: 3.752\n","[1,    98] loss: 3.577\n","[1,    99] loss: 3.808\n","[1,   100] loss: 3.636\n","[1,   101] loss: 3.990\n","[1,   102] loss: 3.817\n","[1,   103] loss: 3.802\n","[1,   104] loss: 3.899\n","[1,   105] loss: 3.631\n","[1,   106] loss: 3.583\n","[1,   107] loss: 3.713\n","[1,   108] loss: 3.889\n","[1,   109] loss: 3.677\n","[1,   110] loss: 3.837\n","[1,   111] loss: 3.627\n","[1,   112] loss: 3.885\n","[1,   113] loss: 3.613\n","[1,   114] loss: 3.823\n","[1,   115] loss: 3.519\n","[1,   116] loss: 3.779\n","[1,   117] loss: 3.650\n","[1,   118] loss: 3.807\n","[1,   119] loss: 3.855\n","[1,   120] loss: 4.001\n","[1,   121] loss: 3.853\n","[1,   122] loss: 3.792\n","[1,   123] loss: 3.948\n","[1,   124] loss: 3.438\n","[1,   125] loss: 3.876\n","[1,   126] loss: 3.764\n","[1,   127] loss: 3.872\n","[1,   128] loss: 3.855\n","[1,   129] loss: 3.685\n","[1,   130] loss: 3.642\n","[1,   131] loss: 3.843\n","[1,   132] loss: 3.919\n","[1,   133] loss: 3.544\n","[1,   134] loss: 3.777\n","[1,   135] loss: 3.843\n","[1,   136] loss: 3.647\n","[1,   137] loss: 3.679\n","[1,   138] loss: 3.623\n","[1,   139] loss: 3.546\n","[1,   140] loss: 3.850\n","[1,   141] loss: 3.667\n","[1,   142] loss: 3.680\n","[1,   143] loss: 3.579\n","[1,   144] loss: 3.919\n","[1,   145] loss: 3.558\n","[1,   146] loss: 3.742\n","[1,   147] loss: 3.453\n","[1,   148] loss: 3.598\n","[1,   149] loss: 3.702\n","[1,   150] loss: 3.806\n","[1,   151] loss: 3.553\n","[1,   152] loss: 3.796\n","[1,   153] loss: 3.745\n","[1,   154] loss: 3.785\n","[1,   155] loss: 3.759\n","[1,   156] loss: 3.533\n","[1,   157] loss: 3.472\n","[1,   158] loss: 3.649\n","[1,   159] loss: 3.714\n","[1,   160] loss: 3.964\n","[1,   161] loss: 3.547\n","[1,   162] loss: 3.580\n","[1,   163] loss: 3.507\n","[1,   164] loss: 3.884\n","[1,   165] loss: 3.709\n","[1,   166] loss: 3.907\n","[1,   167] loss: 3.542\n","[1,   168] loss: 3.746\n","[1,   169] loss: 3.815\n","[1,   170] loss: 3.484\n","[1,   171] loss: 3.774\n","[1,   172] loss: 3.773\n","[1,   173] loss: 3.721\n","[1,   174] loss: 3.847\n","[1,   175] loss: 3.671\n","[1,   176] loss: 4.177\n","[1,   177] loss: 3.883\n","[1,   178] loss: 3.709\n","[1,   179] loss: 3.677\n","[1,   180] loss: 3.922\n","[1,   181] loss: 3.430\n","[1,   182] loss: 3.616\n","[1,   183] loss: 3.652\n","[1,   184] loss: 3.899\n","[1,   185] loss: 3.593\n","[1,   186] loss: 3.821\n","[1,   187] loss: 3.491\n","[1,   188] loss: 3.888\n","[1,   189] loss: 3.717\n","[1,   190] loss: 3.576\n","[1,   191] loss: 3.683\n","[1,   192] loss: 3.825\n","[1,   193] loss: 3.492\n","[1,   194] loss: 3.537\n","[1,   195] loss: 3.873\n","[1,   196] loss: 3.833\n","[1,   197] loss: 3.796\n","[1,   198] loss: 3.790\n","[1,   199] loss: 3.597\n","[1,   200] loss: 3.618\n","[1,   201] loss: 3.700\n","[1,   202] loss: 3.606\n","[1,   203] loss: 3.621\n","[1,   204] loss: 3.639\n","[1,   205] loss: 3.618\n","[1,   206] loss: 3.740\n","[1,   207] loss: 3.700\n","[1,   208] loss: 3.455\n","[1,   209] loss: 3.447\n","[1,   210] loss: 3.948\n","[1,   211] loss: 3.562\n","[1,   212] loss: 3.857\n","[1,   213] loss: 3.680\n","[1,   214] loss: 3.637\n","[1,   215] loss: 3.450\n","[1,   216] loss: 3.443\n","[1,   217] loss: 3.330\n","[1,   218] loss: 3.521\n","[1,   219] loss: 3.668\n","[1,   220] loss: 3.848\n","[1,   221] loss: 3.710\n","[1,   222] loss: 3.945\n","[1,   223] loss: 3.790\n","[1,   224] loss: 3.609\n","[1,   225] loss: 3.884\n","[1,   226] loss: 3.402\n","[1,   227] loss: 3.725\n","[1,   228] loss: 3.693\n","[1,   229] loss: 3.590\n","[1,   230] loss: 3.857\n","[1,   231] loss: 3.576\n","[1,   232] loss: 3.755\n","[1,   233] loss: 3.901\n","[1,   234] loss: 3.675\n","[1,   235] loss: 3.885\n","[1,   236] loss: 3.718\n","[1,   237] loss: 3.846\n","[1,   238] loss: 3.856\n","[1,   239] loss: 3.707\n","[1,   240] loss: 3.579\n","[1,   241] loss: 3.829\n","[1,   242] loss: 3.810\n","[1,   243] loss: 3.638\n","[1,   244] loss: 3.710\n","[1,   245] loss: 3.732\n","[1,   246] loss: 3.664\n","[1,   247] loss: 3.613\n","[1,   248] loss: 3.895\n","[1,   249] loss: 3.756\n","[1,   250] loss: 3.583\n","[1,   251] loss: 3.740\n","[1,   252] loss: 3.517\n","[1,   253] loss: 3.984\n","[1,   254] loss: 3.887\n","[1,   255] loss: 3.780\n","[1,   256] loss: 3.825\n","[1,   257] loss: 3.807\n","[1,   258] loss: 3.723\n","[1,   259] loss: 3.519\n","[1,   260] loss: 3.691\n","[1,   261] loss: 3.398\n","[1,   262] loss: 3.536\n","[1,   263] loss: 3.780\n","[1,   264] loss: 3.691\n","[1,   265] loss: 3.638\n","[1,   266] loss: 3.920\n","[1,   267] loss: 3.920\n","[1,   268] loss: 3.422\n","[1,   269] loss: 3.917\n","[1,   270] loss: 3.946\n","[1,   271] loss: 3.566\n","[1,   272] loss: 3.775\n","[1,   273] loss: 3.518\n","[1,   274] loss: 3.773\n","[1,   275] loss: 3.671\n","[1,   276] loss: 3.605\n","[1,   277] loss: 3.700\n","[1,   278] loss: 3.711\n","[1,   279] loss: 3.605\n","[1,   280] loss: 4.007\n","[1,   281] loss: 3.764\n","[1,   282] loss: 3.996\n","[1,   283] loss: 3.601\n","[1,   284] loss: 3.723\n","[1,   285] loss: 3.718\n","[1,   286] loss: 3.812\n","[1,   287] loss: 3.695\n","[1,   288] loss: 3.576\n","[1,   289] loss: 3.444\n","[1,   290] loss: 3.580\n","[1,   291] loss: 3.502\n","[1,   292] loss: 3.687\n","[1,   293] loss: 3.679\n","[1,   294] loss: 3.633\n","[1,   295] loss: 3.732\n","[1,   296] loss: 3.842\n","[1,   297] loss: 3.737\n","[1,   298] loss: 3.540\n","[1,   299] loss: 3.627\n","[1,   300] loss: 3.476\n","[1,   301] loss: 3.835\n","[1,   302] loss: 3.702\n","[1,   303] loss: 3.644\n","[1,   304] loss: 3.512\n","[1,   305] loss: 3.718\n","[1,   306] loss: 3.712\n","[1,   307] loss: 3.593\n","[1,   308] loss: 3.687\n","[1,   309] loss: 3.361\n","[1,   310] loss: 3.514\n","[1,   311] loss: 3.865\n","[1,   312] loss: 3.801\n","[1,   313] loss: 3.707\n","[1,   314] loss: 3.932\n","[1,   315] loss: 4.026\n","[1,   316] loss: 3.268\n","[1,   317] loss: 3.682\n","[1,   318] loss: 3.383\n","[1,   319] loss: 3.546\n","[1,   320] loss: 3.438\n","[1,   321] loss: 3.798\n","[1,   322] loss: 3.790\n","[1,   323] loss: 3.681\n","[1,   324] loss: 3.675\n","[1,   325] loss: 3.858\n","[1,   326] loss: 3.743\n","[1,   327] loss: 3.819\n","[1,   328] loss: 3.684\n","[1,   329] loss: 3.772\n","[1,   330] loss: 3.644\n","[1,   331] loss: 3.461\n","[1,   332] loss: 3.466\n","[1,   333] loss: 3.789\n","[1,   334] loss: 3.973\n","[1,   335] loss: 3.560\n","[1,   336] loss: 3.928\n","[1,   337] loss: 3.874\n","[1,   338] loss: 3.583\n","[1,   339] loss: 3.585\n","[1,   340] loss: 3.443\n","[1,   341] loss: 3.720\n","[1,   342] loss: 3.479\n","[1,   343] loss: 3.709\n","[1,   344] loss: 3.725\n","[1,   345] loss: 3.834\n","[1,   346] loss: 3.670\n","[1,   347] loss: 3.377\n","[1,   348] loss: 3.742\n","[1,   349] loss: 3.430\n","[1,   350] loss: 3.865\n","[1,   351] loss: 3.875\n","[1,   352] loss: 3.421\n","[1,   353] loss: 3.819\n","[1,   354] loss: 3.829\n","[1,   355] loss: 3.540\n","[1,   356] loss: 3.390\n","[1,   357] loss: 3.634\n","[1,   358] loss: 3.537\n","[1,   359] loss: 3.628\n","[1,   360] loss: 3.566\n","[1,   361] loss: 3.441\n","[1,   362] loss: 3.726\n","[1,   363] loss: 3.865\n","[1,   364] loss: 3.573\n","[1,   365] loss: 3.517\n","[1,   366] loss: 3.713\n","[1,   367] loss: 3.511\n","[1,   368] loss: 3.863\n","[1,   369] loss: 3.661\n","[1,   370] loss: 3.632\n","[1,   371] loss: 3.556\n","[1,   372] loss: 3.715\n","[1,   373] loss: 3.864\n","[1,   374] loss: 3.620\n","[1,   375] loss: 3.751\n","[1,   376] loss: 3.760\n","[1,   377] loss: 3.757\n","[1,   378] loss: 3.531\n","[1,   379] loss: 3.585\n","[1,   380] loss: 3.664\n","[1,   381] loss: 3.835\n","[1,   382] loss: 3.390\n","[1,   383] loss: 3.925\n","[1,   384] loss: 3.723\n","[1,   385] loss: 3.527\n","[1,   386] loss: 3.613\n","[1,   387] loss: 3.693\n","[1,   388] loss: 3.991\n","[1,   389] loss: 3.752\n","[1,   390] loss: 3.649\n","[1,   391] loss: 3.747\n","[1,   392] loss: 3.716\n","[1,   393] loss: 3.555\n","[1,   394] loss: 3.570\n","[1,   395] loss: 3.612\n","[1,   396] loss: 3.870\n","[1,   397] loss: 3.731\n","[1,   398] loss: 3.548\n","[1,   399] loss: 3.440\n","[1,   400] loss: 3.579\n","[1,   401] loss: 3.975\n","[1,   402] loss: 3.379\n","[1,   403] loss: 3.653\n","[1,   404] loss: 3.745\n","[1,   405] loss: 3.445\n","[1,   406] loss: 3.762\n","[1,   407] loss: 3.675\n","[1,   408] loss: 3.712\n","[1,   409] loss: 3.685\n","[1,   410] loss: 3.612\n","[1,   411] loss: 4.035\n","[1,   412] loss: 3.870\n","[1,   413] loss: 3.635\n","[1,   414] loss: 3.773\n","[1,   415] loss: 3.620\n","[1,   416] loss: 3.513\n","[1,   417] loss: 3.700\n","[1,   418] loss: 3.605\n","[1,   419] loss: 3.717\n","[1,   420] loss: 3.702\n","[1,   421] loss: 3.614\n","[1,   422] loss: 3.825\n","[1,   423] loss: 3.724\n","[1,   424] loss: 3.723\n","[1,   425] loss: 3.724\n","[1,   426] loss: 3.785\n","[1,   427] loss: 3.483\n","[1,   428] loss: 3.491\n","[1,   429] loss: 3.648\n","[1,   430] loss: 3.875\n","[1,   431] loss: 3.516\n","[1,   432] loss: 3.532\n","[1,   433] loss: 3.368\n","[1,   434] loss: 3.688\n","[1,   435] loss: 3.445\n","[1,   436] loss: 3.839\n","[1,   437] loss: 3.707\n","[1,   438] loss: 3.707\n","[1,   439] loss: 3.445\n","[1,   440] loss: 3.624\n","[1,   441] loss: 3.685\n","[1,   442] loss: 3.718\n","[1,   443] loss: 3.382\n","[1,   444] loss: 3.756\n","[1,   445] loss: 3.304\n","[1,   446] loss: 3.580\n","[1,   447] loss: 3.561\n","[1,   448] loss: 3.430\n","[1,   449] loss: 3.576\n","[1,   450] loss: 3.566\n","[1,   451] loss: 4.086\n","[1,   452] loss: 3.554\n","[1,   453] loss: 3.778\n","[1,   454] loss: 3.782\n","[1,   455] loss: 3.711\n","[1,   456] loss: 3.940\n","[1,   457] loss: 3.519\n","[1,   458] loss: 3.717\n","[1,   459] loss: 3.369\n","[1,   460] loss: 3.941\n","[1,   461] loss: 3.679\n","[1,   462] loss: 3.617\n","[1,   463] loss: 3.679\n","[1,   464] loss: 3.672\n","[1,   465] loss: 3.714\n","[1,   466] loss: 3.796\n","[1,   467] loss: 3.796\n","[1,   468] loss: 3.483\n","[1,   469] loss: 3.873\n","[1,   470] loss: 3.722\n","[1,   471] loss: 3.673\n","[1,   472] loss: 3.552\n","[1,   473] loss: 3.781\n","[1,   474] loss: 3.521\n","[1,   475] loss: 3.654\n","[1,   476] loss: 3.585\n","[1,   477] loss: 3.515\n","[1,   478] loss: 3.742\n","[1,   479] loss: 3.420\n","[1,   480] loss: 3.442\n","[1,   481] loss: 3.665\n","[1,   482] loss: 3.719\n","[1,   483] loss: 3.557\n","[1,   484] loss: 3.642\n","[1,   485] loss: 3.582\n","[1,   486] loss: 3.676\n","[1,   487] loss: 3.687\n","[1,   488] loss: 3.673\n","[1,   489] loss: 3.838\n","[1,   490] loss: 3.640\n","[1,   491] loss: 3.492\n","[1,   492] loss: 3.637\n","[1,   493] loss: 3.555\n","[1,   494] loss: 3.767\n","[1,   495] loss: 3.754\n","[1,   496] loss: 3.553\n","[1,   497] loss: 4.003\n","[1,   498] loss: 3.875\n","[1,   499] loss: 3.739\n","[1,   500] loss: 3.636\n","[1,   501] loss: 3.528\n","[1,   502] loss: 3.654\n","[1,   503] loss: 3.252\n","[1,   504] loss: 3.422\n","[1,   505] loss: 3.485\n","[1,   506] loss: 3.754\n","[1,   507] loss: 3.465\n","[1,   508] loss: 3.589\n","[1,   509] loss: 3.607\n","[1,   510] loss: 3.525\n","[1,   511] loss: 3.412\n","[1,   512] loss: 3.582\n","[1,   513] loss: 3.698\n","[1,   514] loss: 3.545\n","[1,   515] loss: 3.628\n","[1,   516] loss: 3.413\n","[1,   517] loss: 3.523\n","[1,   518] loss: 3.620\n","[1,   519] loss: 3.685\n","[1,   520] loss: 3.646\n","[1,   521] loss: 3.629\n","[1,   522] loss: 3.831\n","[1,   523] loss: 3.574\n","[1,   524] loss: 3.798\n","[1,   525] loss: 3.730\n","[1,   526] loss: 3.562\n","[1,   527] loss: 3.754\n","[1,   528] loss: 3.486\n","[1,   529] loss: 3.392\n","[1,   530] loss: 3.526\n","[1,   531] loss: 3.367\n","[1,   532] loss: 3.529\n","[1,   533] loss: 4.063\n","[1,   534] loss: 3.539\n","[1,   535] loss: 3.790\n","[1,   536] loss: 3.365\n","[1,   537] loss: 3.681\n","[1,   538] loss: 3.260\n","[1,   539] loss: 3.541\n","[1,   540] loss: 4.149\n","[1,   541] loss: 3.751\n","[1,   542] loss: 3.577\n","[1,   543] loss: 3.587\n","[1,   544] loss: 3.954\n","[1,   545] loss: 3.718\n","[1,   546] loss: 3.403\n","[1,   547] loss: 3.418\n","[1,   548] loss: 3.562\n","[1,   549] loss: 3.679\n","[1,   550] loss: 3.784\n","[1,   551] loss: 3.631\n","[1,   552] loss: 3.667\n","[1,   553] loss: 3.345\n","[1,   554] loss: 3.936\n","[1,   555] loss: 3.725\n","[1,   556] loss: 3.924\n","[1,   557] loss: 3.465\n","[1,   558] loss: 3.840\n","[1,   559] loss: 3.873\n","[1,   560] loss: 3.924\n","[1,   561] loss: 3.208\n","[1,   562] loss: 3.564\n","[1,   563] loss: 4.020\n","[1,   564] loss: 3.628\n","[1,   565] loss: 3.872\n","[1,   566] loss: 3.348\n","[1,   567] loss: 3.626\n","[1,   568] loss: 3.538\n","[1,   569] loss: 3.504\n","[1,   570] loss: 3.598\n","[1,   571] loss: 3.529\n","[1,   572] loss: 3.482\n","[1,   573] loss: 3.385\n","[1,   574] loss: 3.754\n","[1,   575] loss: 3.565\n","[1,   576] loss: 3.638\n","[1,   577] loss: 3.449\n","[1,   578] loss: 3.381\n","[1,   579] loss: 3.481\n","[1,   580] loss: 3.840\n","[1,   581] loss: 3.480\n","[1,   582] loss: 3.695\n","[1,   583] loss: 3.686\n","[1,   584] loss: 3.632\n","[1,   585] loss: 3.617\n","[1,   586] loss: 3.565\n","[1,   587] loss: 3.241\n","[1,   588] loss: 3.906\n","[1,   589] loss: 3.748\n","[1,   590] loss: 3.545\n","[1,   591] loss: 3.645\n","[1,   592] loss: 3.399\n","[1,   593] loss: 3.658\n","[1,   594] loss: 3.613\n","[1,   595] loss: 3.773\n","[1,   596] loss: 3.374\n","[1,   597] loss: 3.596\n","[1,   598] loss: 3.723\n","[1,   599] loss: 3.756\n","[1,   600] loss: 3.858\n","[1,   601] loss: 3.457\n","[1,   602] loss: 3.502\n","[1,   603] loss: 3.818\n","[1,   604] loss: 3.869\n","[1,   605] loss: 3.280\n","[1,   606] loss: 3.592\n","[1,   607] loss: 3.426\n","[1,   608] loss: 3.426\n","[1,   609] loss: 3.597\n","[1,   610] loss: 3.713\n","[1,   611] loss: 3.601\n","[1,   612] loss: 3.560\n","[1,   613] loss: 3.644\n","[1,   614] loss: 3.375\n","[1,   615] loss: 3.397\n","[1,   616] loss: 3.968\n","[1,   617] loss: 3.731\n","[1,   618] loss: 3.245\n","[1,   619] loss: 3.542\n","[1,   620] loss: 3.667\n","[1,   621] loss: 3.193\n","[1,   622] loss: 3.984\n","[1,   623] loss: 3.197\n","[1,   624] loss: 3.620\n","[1,   625] loss: 3.627\n","[1,   626] loss: 3.661\n","[1,   627] loss: 3.799\n","[1,   628] loss: 3.640\n","[1,   629] loss: 3.571\n","[1,   630] loss: 3.561\n","[1,   631] loss: 3.329\n","[1,   632] loss: 3.538\n","[1,   633] loss: 3.930\n","[1,   634] loss: 3.636\n","[1,   635] loss: 3.468\n","[1,   636] loss: 3.893\n","[1,   637] loss: 3.860\n","[1,   638] loss: 3.766\n","[1,   639] loss: 3.526\n","[1,   640] loss: 3.639\n","[1,   641] loss: 3.612\n","[1,   642] loss: 3.261\n","[1,   643] loss: 3.552\n","[1,   644] loss: 3.737\n","[1,   645] loss: 3.547\n","[1,   646] loss: 3.659\n","[1,   647] loss: 3.666\n","[1,   648] loss: 3.508\n","[1,   649] loss: 3.707\n","[1,   650] loss: 3.578\n","[1,   651] loss: 3.391\n","[1,   652] loss: 3.598\n","[1,   653] loss: 3.280\n","[1,   654] loss: 3.657\n","[1,   655] loss: 3.745\n","[1,   656] loss: 3.506\n","[1,   657] loss: 3.563\n","[1,   658] loss: 3.464\n","[1,   659] loss: 3.783\n","[1,   660] loss: 3.823\n","[1,   661] loss: 3.887\n","[1,   662] loss: 3.830\n","[1,   663] loss: 3.448\n","[1,   664] loss: 3.782\n","[1,   665] loss: 3.883\n","[1,   666] loss: 3.764\n","[1,   667] loss: 3.305\n","[1,   668] loss: 3.563\n","[1,   669] loss: 3.618\n","[1,   670] loss: 3.316\n","[1,   671] loss: 3.683\n","[1,   672] loss: 3.704\n","[1,   673] loss: 3.825\n","[1,   674] loss: 3.692\n","[1,   675] loss: 3.581\n","[1,   676] loss: 3.570\n","[1,   677] loss: 3.509\n","[1,   678] loss: 3.469\n","[1,   679] loss: 3.679\n","[1,   680] loss: 3.607\n","[1,   681] loss: 3.582\n","[1,   682] loss: 3.768\n","[1,   683] loss: 3.479\n","[1,   684] loss: 3.526\n","[1,   685] loss: 3.464\n","[1,   686] loss: 3.647\n","[1,   687] loss: 3.640\n","[1,   688] loss: 3.603\n","[1,   689] loss: 3.623\n","[1,   690] loss: 3.453\n","[1,   691] loss: 3.361\n","[1,   692] loss: 3.783\n","[1,   693] loss: 3.723\n","[1,   694] loss: 3.924\n","[1,   695] loss: 3.581\n","[1,   696] loss: 3.339\n","[1,   697] loss: 4.035\n","[1,   698] loss: 3.448\n","[1,   699] loss: 3.921\n","[1,   700] loss: 3.677\n","[1,   701] loss: 3.509\n","[1,   702] loss: 3.648\n","[1,   703] loss: 3.566\n","[1,   704] loss: 3.657\n","[1,   705] loss: 3.414\n","[1,   706] loss: 3.867\n","[1,   707] loss: 3.364\n","[1,   708] loss: 3.735\n","[1,   709] loss: 3.686\n","[1,   710] loss: 3.474\n","[1,   711] loss: 3.643\n","[1,   712] loss: 3.503\n","[1,   713] loss: 3.453\n","[1,   714] loss: 3.926\n","[1,   715] loss: 3.685\n","[1,   716] loss: 3.826\n","[1,   717] loss: 3.649\n","[1,   718] loss: 4.000\n","[1,   719] loss: 3.424\n","[1,   720] loss: 3.477\n","[1,   721] loss: 3.304\n","[1,   722] loss: 3.579\n","[1,   723] loss: 3.808\n","[1,   724] loss: 3.763\n","[1,   725] loss: 3.626\n","[1,   726] loss: 3.721\n","[1,   727] loss: 3.804\n","[1,   728] loss: 3.656\n","[1,   729] loss: 3.650\n","[1,   730] loss: 3.528\n","[1,   731] loss: 3.813\n","[1,   732] loss: 3.547\n","[1,   733] loss: 3.262\n","[1,   734] loss: 3.798\n","[1,   735] loss: 3.549\n","[1,   736] loss: 3.647\n","[1,   737] loss: 3.894\n","[1,   738] loss: 3.317\n","[1,   739] loss: 3.633\n","[1,   740] loss: 3.755\n","[1,   741] loss: 3.432\n","[1,   742] loss: 3.727\n","[1,   743] loss: 3.587\n","[1,   744] loss: 3.393\n","[1,   745] loss: 3.580\n","[1,   746] loss: 3.605\n","[1,   747] loss: 3.474\n","[1,   748] loss: 3.292\n","[1,   749] loss: 3.232\n","[1,   750] loss: 3.363\n","[1,   751] loss: 3.772\n","[1,   752] loss: 3.538\n","[1,   753] loss: 3.682\n","[1,   754] loss: 3.566\n","[1,   755] loss: 3.492\n","[1,   756] loss: 3.938\n","[1,   757] loss: 3.452\n","[1,   758] loss: 3.479\n","[1,   759] loss: 3.467\n","[1,   760] loss: 3.506\n","[1,   761] loss: 3.755\n","[1,   762] loss: 3.926\n","[1,   763] loss: 3.723\n","[1,   764] loss: 3.533\n","[1,   765] loss: 3.356\n","[1,   766] loss: 3.574\n","[1,   767] loss: 3.547\n","[1,   768] loss: 3.575\n","[1,   769] loss: 3.385\n"]}],"source":["wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"vit-basic\",\n","    \n","    # track hyperparameters and run metadata\n","    config={\n","    \"learning_rate\": 0.001,\n","    \"architecture\": \"ViT\",\n","    \"dataset\": \"AWA2\",\n","    \"epochs\": 50,\n","    }\n",")\n","\n","for epoch in range(50):  # loop over the dataset multiple times\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs = data[0].to(device)\n","        labels = data[1].to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        output = vit_model(inputs)\n","        loss = criterion(output, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","\n","        print('[%d, %5d] loss: %.3f' %\n","              (epoch + 1, i + 1, running_loss ))\n","        \n","        # log metrics to wandb\n","        wandb.log({\"loss\": running_loss})\n","\n","        running_loss = 0.0\n","\n","\n","# [optional] finish the wandb run, necessary in notebooks\n","wandb.finish()\n","\n","print('Finished Training of ViT')"]},{"cell_type":"markdown","metadata":{"id":"_6-rzUvMhqcR"},"source":["### Test ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6Pg8AjGhqcR"},"outputs":[],"source":["correct = 0\n","total = 0\n","wrong_pred =[]\n","right_label = []\n","with torch.no_grad():\n","    for data in test_loader:\n","        images = data[0].to(device)\n","        labels = data[1].to(device)\n","        outputs = vit_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        \n","        for i in range(0,len(predicted)):\n","              if predicted[i].item() != labels[i].item():\n","                    wrong_pred.append(predicted[i].item())\n","                    right_label.append(labels[i].item())\n","        \n","\n","        \n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the test images: %d %%' % (\n","    100 * correct / total))"]},{"cell_type":"markdown","metadata":{"id":"2jH5_zgghqcR"},"source":["###   Analyse  ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-t-t45uhqcR"},"outputs":[],"source":["nb_wrong_pred = []\n","for i in range(0,50):\n","    nb_wrong_pred.append(wrong_pred.count(i))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhBPJaG9hqcR"},"outputs":[],"source":["list_animal = list(dict_label_animal.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJl4FuYEhqcR"},"outputs":[],"source":["plt.bar(range(50), nb_wrong_pred)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Us_6dX14hqcR"},"outputs":[],"source":["good_classification = []\n","bad_classification = []\n","for i in range(50):\n","    if nb_wrong_pred[i]<=5:\n","        good_classification.append(i)\n","    if    nb_wrong_pred[i]>=60:\n","        bad_classification.append(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOd8y4F6hqcR"},"outputs":[],"source":["def find_right_animal(m):\n","    wrong_pred_m =[]\n","    for j in [i for i,x in enumerate(wrong_pred) if x == m]:\n","        wrong_pred_m.append(right_label[j])\n","    return list_animal[max(wrong_pred_m,key = wrong_pred_m.count)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ll3nppYnhqcR"},"outputs":[],"source":["for i in good_classification :\n","    print('ViT a bien classifie '+animals[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhDBUNZmhqcR"},"outputs":[],"source":["for i in bad_classification:\n","    print('ViT a mal classifie '+animals[i]+' , melange souvent avec '+find_right_animal(i))"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}