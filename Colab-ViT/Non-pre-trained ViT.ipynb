{"cells":[{"cell_type":"markdown","metadata":{"id":"guNaUZaJhqcG"},"source":["Lien de téléchargement des données: https://cvml.ista.ac.at/AwA2/        \n","\n","13GB file : https://cvml.ista.ac.at/AwA2/AwA2-data.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1434,"status":"ok","timestamp":1680667403880,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"},"user_tz":240},"id":"WbF2Cf05Nyx_","outputId":"0db41bd4-e82b-437c-dc0b-e0e28b954203"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gV68yhFicMdU"},"outputs":[],"source":["!pip install wandb -qU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3172,"status":"ok","timestamp":1680667412282,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"},"user_tz":240},"id":"hDf2Xbq1cj4Q","outputId":"54e7597f-3660-4e2f-ee8d-a16df2dc3afa"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk-kiron\u001b[0m (\u001b[33manimal-detect-vit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["# Log in to your W&B account\n","# 37c8a07c774e2e083984fb1eecd48091d723d40f\n","import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1680667412283,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"},"user_tz":240},"id":"cAsS9gishqcJ","outputId":"0344c124-fdce-4262-c1a9-8c1ae04fefdf"},"outputs":[{"output_type":"stream","name":"stdout","text":["3.9.16 (main, Dec  7 2022, 01:11:51) \n","[GCC 9.4.0]\n"]}],"source":["import sys\n","print(sys.version)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEOxHANkhqcL"},"outputs":[],"source":["import sys\n","import cv2 # Pour utiliser opencv-python, il faut la version de python est 3.7\n","import os\n","import csv\n","\n","import numpy as np \n","import pandas as pd \n","import math\n","\n","import torch \n","from torch.utils.data import Dataset, DataLoader\n","import torchvision \n","from torchvision.io import read_image\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","\n","np.random.seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndd7-sX6hqcL"},"outputs":[],"source":["# Constant. Should be the path to the folder named JPEGImages, containing the 33K images in its subfolders.\n","DATA_FOLDER_PATH = '/content/drive/MyDrive/IFT3710/Animals_with_Attributes2/'\n","JPEGIMAGES_FOLDER_PATH = '/content/drive/MyDrive/IFT3710/Animals_with_Attributes2/JPEGImages/'"]},{"cell_type":"markdown","metadata":{"id":"siGBV5bUhqcM"},"source":["# Note : Some labels have a low number of images. \n","\n","## Possible solutions to explore : \n","    Data augmentation : creating new training data by applying random transformations to existing images, such as rotating, cropping, or flipping them."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":587,"status":"ok","timestamp":1680667414748,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"},"user_tz":240},"id":"_PMi1FkRhqcM","outputId":"a1e1a56c-a238-4fb9-9cb5-5167a8cbe014"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'elephant': 1038, 'lion': 1019, 'deer': 1344, 'cow': 1338, 'squirrel': 1200, 'german+shepherd': 1033, 'skunk': 188, 'horse': 1645, 'mole': 100, 'walrus': 215, 'weasel': 282, 'mouse': 185, 'buffalo': 904, 'grizzly+bear': 852, 'bat': 383, 'chimpanzee': 728, 'beaver': 193, 'rabbit': 1088, 'wolf': 589, 'bobcat': 630, 'seal': 988, 'collie': 1028, 'spider+monkey': 291, 'otter': 758, 'rat': 310, 'leopard': 720, 'zebra': 1170, 'sheep': 1420, 'blue+whale': 174, 'ox': 728, 'chihuahua': 567, 'hamster': 779, 'tiger': 877, 'giraffe': 1202, 'polar+bear': 868, 'dolphin': 946, 'fox': 664, 'siamese+cat': 500, 'persian+cat': 747, 'raccoon': 512, 'antelope': 1046, 'pig': 713, 'giant+panda': 874, 'killer+whale': 291, 'moose': 704, 'dalmatian': 549, 'humpback+whale': 709, 'gorilla': 872, 'rhinoceros': 696, 'hippopotamus': 684}\n","{'elephant': 0.0278, 'lion': 0.0273, 'deer': 0.036, 'cow': 0.0358, 'squirrel': 0.0321, 'german+shepherd': 0.0277, 'skunk': 0.005, 'horse': 0.0441, 'mole': 0.0027, 'walrus': 0.0058, 'weasel': 0.0076, 'mouse': 0.005, 'buffalo': 0.0242, 'grizzly+bear': 0.0228, 'bat': 0.0103, 'chimpanzee': 0.0195, 'beaver': 0.0052, 'rabbit': 0.0291, 'wolf': 0.0158, 'bobcat': 0.0169, 'seal': 0.0265, 'collie': 0.0275, 'spider+monkey': 0.0078, 'otter': 0.0203, 'rat': 0.0083, 'leopard': 0.0193, 'zebra': 0.0313, 'sheep': 0.038, 'blue+whale': 0.0047, 'ox': 0.0195, 'chihuahua': 0.0152, 'hamster': 0.0209, 'tiger': 0.0235, 'giraffe': 0.0322, 'polar+bear': 0.0232, 'dolphin': 0.0253, 'fox': 0.0178, 'siamese+cat': 0.0134, 'persian+cat': 0.02, 'raccoon': 0.0137, 'antelope': 0.028, 'pig': 0.0191, 'giant+panda': 0.0234, 'killer+whale': 0.0078, 'moose': 0.0189, 'dalmatian': 0.0147, 'humpback+whale': 0.019, 'gorilla': 0.0234, 'rhinoceros': 0.0186, 'hippopotamus': 0.0183}\n"]}],"source":["def find_num_images_per_label(img_dir = JPEGIMAGES_FOLDER_PATH): #-> tuple[dict,dict]: \n","    \"\"\" \n","    USEFUL FOR SAMPLING.\n","    Return a dict with keys as the 50 labels, and values being the number of images in each subdirectory corresponding to label\n","    and a second dict with the relative numbers (proportion) for every label compared to the total number of images (useful for sampling)\"\"\"\n","    labels_dirs = os.listdir(img_dir)\n","    num_images_per_label = dict.fromkeys(labels_dirs)\n","    proportions_images_per_label = dict.fromkeys(labels_dirs)\n","    total_num_images = 0\n","\n","    # Update absolute number of images per label\n","    for i, label in enumerate(labels_dirs) : \n","        specific_label_path = os.path.join(img_dir, labels_dirs[i])\n","        num_images_label = len(os.listdir(specific_label_path))\n","        total_num_images += num_images_label\n","        num_images_per_label[label] = num_images_label\n","\n","    # Update relative number of images per label (proportion)\n","    for i, label in enumerate(labels_dirs) : \n","        num_images_label = num_images_per_label[label]\n","        proportion_label = round(num_images_label / total_num_images, 4)\n","        proportions_images_per_label[label] = proportion_label\n","\n","    return num_images_per_label, proportions_images_per_label\n","\n","num_images_per_label, proportions_images_per_label = find_num_images_per_label()\n","print(num_images_per_label)\n","print(proportions_images_per_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":674,"status":"ok","timestamp":1680667415416,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"},"user_tz":240},"id":"O7AovZgDhqcN","outputId":"fd018aec-7de6-44e4-84d5-8c12323d3ac0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted existent annotations.csv file.\n"," ---------------------------\n","Sucessfully created annotations.csv file.\n"]}],"source":["ANNOTATIONS_FILENAME = 'annotations.csv'\n","\n","def create_annotations_csv_file(annotations_filename = ANNOTATIONS_FILENAME, img_dir = JPEGIMAGES_FOLDER_PATH): \n","    \"\"\" \n","    Create a csv annotations_file, annotations.csv, with two columns, in the format : \n","                        path/to/image, label\n","    \n","    The annotation csv is necessary for DataLoader.\n","    \"\"\"\n","    \n","    labels_dirs:list = os.listdir(img_dir)\n","   \n","    if os.path.exists(annotations_filename):\n","        os.remove(annotations_filename)\n","        print(f'Deleted existent {ANNOTATIONS_FILENAME} file.\\n ---------------------------')\n","    \n","    with open(annotations_filename, 'w', newline='') as file :\n","        writer = csv.writer(file, dialect='excel', delimiter=',')\n","\n","        for i, label in enumerate(labels_dirs) : \n","\n","            specific_label_path = os.path.join(img_dir, label)\n","            images_names = os.listdir(specific_label_path)\n","\n","            for j, image_name in enumerate(images_names):\n","                full_path_to_img= os.path.join(specific_label_path, image_name)\n","                full_path_to_img= os.path.join(label, image_name)\n","\n","                row = [full_path_to_img, label]\n","                writer.writerow(row)\n","\n","    print(f'Sucessfully created {ANNOTATIONS_FILENAME} file.')\n","\n","#\n","create_annotations_csv_file()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1680667415416,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"},"user_tz":240},"id":"l6kglDLKhqcN","outputId":"1da1d9c7-a5ff-48e9-f5cf-e5dcbab67310"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'antelope': 0, 'grizzly+bear': 1, 'killer+whale': 2, 'beaver': 3, 'dalmatian': 4, 'persian+cat': 5, 'horse': 6, 'german+shepherd': 7, 'blue+whale': 8, 'siamese+cat': 9, 'skunk': 10, 'mole': 11, 'tiger': 12, 'hippopotamus': 13, 'leopard': 14, 'moose': 15, 'spider+monkey': 16, 'humpback+whale': 17, 'elephant': 18, 'gorilla': 19, 'ox': 20, 'fox': 21, 'sheep': 22, 'seal': 23, 'chimpanzee': 24, 'hamster': 25, 'squirrel': 26, 'rhinoceros': 27, 'rabbit': 28, 'bat': 29, 'giraffe': 30, 'wolf': 31, 'chihuahua': 32, 'rat': 33, 'weasel': 34, 'otter': 35, 'buffalo': 36, 'zebra': 37, 'giant+panda': 38, 'deer': 39, 'bobcat': 40, 'pig': 41, 'lion': 42, 'mouse': 43, 'polar+bear': 44, 'collie': 45, 'walrus': 46, 'raccoon': 47, 'cow': 48, 'dolphin': 49}\n"]}],"source":["# labels_in_number = pd.read_csv(DATA_FOLDER_PATH+\"classes.txt\", delim_whitespace=True,header=None)\n","labels_dict = {}\n","with open(DATA_FOLDER_PATH+\"classes.txt\") as f:\n","    for line in f:\n","        # print(line.split())\n","        (key,val) = line.split()\n","        labels_dict[val] = int(key)-1\n","print(labels_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JlpINdEShqcO"},"outputs":[],"source":["from torchvision.io import read_image, ImageReadMode\n","from PIL import Image\n","\n","\n","class AWA2Dataset(Dataset): # Dataset class to serve as input for the DataLoader.\n","    \"\"\" \n","    Dataset class to serve as input for the DataLoader.\n","    Implements all the required methods and more. \n","    \"\"\"\n","\n","    def __init__(self, annotations_file=ANNOTATIONS_FILENAME, img_dir=JPEGIMAGES_FOLDER_PATH, \n","                transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","        numbers_infos_dicts: tuple[dict,dict] = find_num_images_per_label(img_dir=JPEGIMAGES_FOLDER_PATH)\n","        self.num_images_per_label = numbers_infos_dicts[0]\n","        self.proportions_images_per_label = numbers_infos_dicts[1]\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        # img_path = self.img_labels.iloc[idx, 0]\n","        key = self.img_labels.iloc[idx, 1]\n","\n","        # Mapping the labels from string to tensor\n","        label = labels_dict[key]\n","\n","        image = read_image(path = img_path, mode = ImageReadMode.RGB)\n","        # with open(img_path, 'rb') as f:\n","        #     image = Image.open(f)\n","        #     image = image.convert('RGB')  # convert to RGB\n","\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label\n","\n","\n","class Subset_(AWA2Dataset) : \n","    def __init__(self, dataset, indices, transform=None):\n","        super().__init__()\n","        self.dataset = dataset\n","        self.indices = indices\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, index):\n","        original_index_in_AWA2Dataset = self.indices[index]\n","        image, label = self.dataset[original_index_in_AWA2Dataset]\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        return image, label\n","    "]},{"cell_type":"code","source":["transforms_pipeline_train = transforms.Compose([\n","                    ## Input size\n","                    transforms.ToPILImage(),\n","                    transforms.Resize((256,256)),\n","                    # transforms.Grayscale(num_output_channels=3),\n","                    \n","                    ## Data augmentation \n","                    transforms.RandomRotation(15),\n","                    transforms.RandomHorizontalFlip(p=0.4),\n","                    # transforms.RandomApply(transforms.RandAugment(), p=0.4), # 40% of the time, apply a random additional combo of transformations #https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html\n","                    transforms.ColorJitter(brightness=0.2,\n","                                            contrast=0.2,\n","                                            saturation=0.2,\n","                                            hue=0.1),\n","                    transforms.RandomCrop((224,224)),  # transforms.RandomResizedCrop(size=(224,224), scale=(0.6, 0.9), ratio=(0.5, 1.08,))\n","                    ## Normalize\n","                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the \n","                    transforms.Normalize(mean = [0.4643, 0.4640, 0.3985] , std=[0.2521, 0.2425, 0.2538]) # real mean and std of AwA2\n","                ])\n","\n","\n","transforms_pipeline_test = transforms.Compose([\n","                    ## Input size\n","                    transforms.ToPILImage(),\n","                    transforms.Resize((256,256)),\n","                    # transforms.Grayscale(num_output_channels=3),\n","                    transforms.CenterCrop((224,224)),   \n","                    ## Normalize\n","                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the \n","                    transforms.Normalize(mean = [0.4643, 0.4640, 0.3985] , std=[0.2521, 0.2425, 0.2538]) # real mean and std of AwA2\n","                ])\n"],"metadata":{"id":"8ByRcu1GQqyX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pc-WOXn_hqcO"},"source":["### ViT ###"]},{"cell_type":"markdown","metadata":{"id":"A2vaUwGe7wcO"},"source":["### NON Pre-trained ViT ###"]},{"cell_type":"code","source":["pip install timm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujmx0DxA7_X_","executionInfo":{"status":"ok","timestamp":1680667419223,"user_tz":240,"elapsed":3811,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"}},"outputId":"45660dad-89c6-4574-8be1-62e07ff5ea5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: timm in /usr/local/lib/python3.9/dist-packages (0.6.13)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm) (6.0)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from timm) (0.13.3)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm) (0.15.1+cu118)\n","Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.9/dist-packages (from timm) (2.0.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.10.7)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (2.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.1.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (1.11.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (4.5.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (23.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (2.27.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision.models import resnet50\n","from timm.models import create_model\n","from timm.data import resolve_data_config\n","from timm.data.transforms_factory import create_transform\n","\n","dataset = AWA2Dataset()\n","\n","vit_model = create_model('vit_base_patch16_224', pretrained=False, num_classes=50)\n","vit_model.eval()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DLB0nzf-75DV","executionInfo":{"status":"ok","timestamp":1680667421591,"user_tz":240,"elapsed":2380,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"}},"outputId":"0053cdca-eee4-470a-d6c9-62b8dd0427a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","    (norm): Identity()\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (norm_pre): Identity()\n","  (blocks): Sequential(\n","    (0): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (1): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (2): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (3): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (4): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (5): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (6): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (7): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (8): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (9): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (10): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (11): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","  )\n","  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  (fc_norm): Identity()\n","  (head): Linear(in_features=768, out_features=50, bias=True)\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"ceJFyCPFhqcP"},"source":["### CUDA ###"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1680667421591,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"},"user_tz":240},"id":"yTgQO3YIhqcP","outputId":"431ad771-0dc5-4dbb-fed2-407b9e2805b0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":15}],"source":["device = torch.device(\"cuda:0\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1680667421592,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"},"user_tz":240},"id":"2N2GlS8ohqcP","outputId":"9c7c21e0-cb4a-4952-ae78-22307052fc8e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":16}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBRHvNJo023K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680667427306,"user_tz":240,"elapsed":5720,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"}},"outputId":"7d5501fb-6ce2-4409-bd4a-e7a624df3c16"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[name: \"/device:CPU:0\"\n"," device_type: \"CPU\"\n"," memory_limit: 268435456\n"," locality {\n"," }\n"," incarnation: 9383145959073736394\n"," xla_global_id: -1,\n"," name: \"/device:GPU:0\"\n"," device_type: \"GPU\"\n"," memory_limit: 39552876544\n"," locality {\n","   bus_id: 1\n","   links {\n","   }\n"," }\n"," incarnation: 7264856288382495209\n"," physical_device_desc: \"device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\"\n"," xla_global_id: 416903419]"]},"metadata":{},"execution_count":17}],"source":["from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1680667427310,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"},"user_tz":240},"id":"bIdfrm7zhqcP","outputId":"725eba3e-632f-4f8e-a1dc-bc94878ac110"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["VisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","    (norm): Identity()\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (norm_pre): Identity()\n","  (blocks): Sequential(\n","    (0): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (1): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (2): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (3): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (4): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (5): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (6): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (7): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (8): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (9): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (10): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","    (11): Block(\n","      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=768, out_features=768, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls1): Identity()\n","      (drop_path1): Identity()\n","      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","        (act): GELU(approximate='none')\n","        (drop1): Dropout(p=0.0, inplace=False)\n","        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","        (drop2): Dropout(p=0.0, inplace=False)\n","      )\n","      (ls2): Identity()\n","      (drop_path2): Identity()\n","    )\n","  )\n","  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n","  (fc_norm): Identity()\n","  (head): Linear(in_features=768, out_features=50, bias=True)\n",")"]},"metadata":{},"execution_count":18}],"source":["vit_model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"rxLnQq1AhqcP"},"source":["###  Split training data and test data ###"]},{"cell_type":"code","source":["# Initialize dataset and train/valid/test split \n","from sklearn.model_selection import train_test_split\n","\n","dataset = AWA2Dataset()\n","n_images = len(dataset)\n","# Split all indices into training/testing sets\n","train_indices, test_indices = train_test_split(range(n_images), test_size=0.2, random_state=1) \n","# Split training indices into training/validation sets.\n","train_indices, valid_indices = train_test_split(train_indices, test_size=0.2, random_state=1) \n","\n","\n","# Initialize the 3 DataSet objects (as Subset_) and apply the relevant Transforms to each subset (train/test/valid)\n","train_data = Subset_(dataset, train_indices, transform = transforms_pipeline_train) \n","valid_data = Subset_(dataset, valid_indices, transform = transforms_pipeline_test) \n","test_data  = Subset_(dataset, test_indices, transform = transforms_pipeline_test) \n","\n","# Initalize DataLoaders\n","batch_size = 128\n","\n","train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True, num_workers=12, pin_memory=True)\n","valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)\n","test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=False, num_workers=12, pin_memory=True)"],"metadata":{"id":"ZZ0rIVbQTm9m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KI861s-BhqcQ"},"source":["###   transfomer labels  ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJjvi-UlhqcQ"},"outputs":[],"source":["path_class = DATA_FOLDER_PATH +\"classes.txt\"\n","class_animal = pd.read_table(path_class,header= None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTPoi_EkhqcQ"},"outputs":[],"source":["animals = class_animal[1]\n","dict_label_animal = {}\n","n = 0\n","for i in range(0,len(animals)):\n","    dict_label_animal[animals[i]] = n\n","    n+=1\n","def label_to_num(tuple_labels):\n","    list_labels =[]\n","    for tuple_label in tuple_labels:\n","        list_labels.append(dict_label_animal[tuple_label])\n","    return torch.tensor(list_labels) "]},{"cell_type":"markdown","metadata":{"id":"BStkQXXNhqcQ"},"source":["###  Training  ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3ChmbAkWytL"},"outputs":[],"source":["# pip install --upgrade timm"]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import StepLR\n","from tqdm.notebook import tqdm\n","\n","\n","\n","# loss function\n","# criterion = nn.CrossEntropyLoss()\n","# # optimizer\n","# optimizer = optim.Adam(vit_model.parameters(), lr=1e-4, weight_decay=1e-5) ### Added weight decay \n","# # scheduler\n","# scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(vit_model.head.parameters(), lr=1e-4)\n","\n","\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"Non_ViT-B_16_224\",\n","    \n","    # track hyperparameters and run metadata\n","    config={\n","    \"learning_rate\": 1e-4,\n","    \"architecture\": \"ViT\",\n","    \"dataset\": \"AWA2\",\n","    \"epochs\": 50,\n","    }\n",")\n","\n","for epoch in range(50):\n","    epoch_loss = 0\n","    epoch_accuracy = 0\n","\n","    for data, label in tqdm(train_loader):\n","        data = data.to(device)\n","        label = label.to(device)\n","\n","        output = vit_model(data)\n","        loss = criterion(output, label)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # scheduler.step() ### SHOULD BE HERE \n","\n","        acc = (output.argmax(dim=1) == label).float().mean()\n","        epoch_accuracy += acc / len(train_loader)\n","        epoch_loss += loss / len(train_loader)\n","\n","    with torch.no_grad():\n","        epoch_val_accuracy = 0\n","        epoch_val_loss = 0\n","        for data, label in valid_loader:\n","            data = data.to(device)\n","            label = label.to(device)\n","\n","            val_output = vit_model(data)\n","            val_loss = criterion(val_output, label)\n","\n","            acc = (val_output.argmax(dim=1) == label).float().mean()\n","            epoch_val_accuracy += acc / len(valid_loader)\n","            epoch_val_loss += val_loss / len(valid_loader)\n","\n","    print(\n","        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n","    )\n","\n","    # log metrics to wandb\n","    wandb.log({\"loss\" : epoch_loss , \"acc\": epoch_accuracy, \"val_loss\" : epoch_val_loss, \"val_acc\": epoch_val_accuracy})\n","\n","\n","# [optional] finish the wandb run, necessary in notebooks\n","wandb.finish()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495,"referenced_widgets":["604ceab2878944bea3461350b8ad3b66","10bad83e28994a01a4bc9f53a9bb94ee","0807c83e55954c9b98effffcec71cfa3","75a9ccf6a0d140af8dc0ce54aa5540c2","827f5fb6ffdb40a29b7c6d7192f44d82","3f18ebd5cef545b7a6b300efa137a7fc","067eb93117d146bdb6de8cfcd0f4e4fc","ac08af728b57424e8dfc8f673ce0cfd9","5a235b4d089146768e3a8ecbd5a45c86","c472f0072e634f5b8fee346635065590","5c14fd1df17b412bbae5d293bcd17525","080bf26cdf5946de8405ac889a1b9a9d","7d3091f44d9946e0b7b0340fb4374d50","7c9a8280b90f42c999325837e181f2a9","e8705be5da6343218fc924a9cec109ee","35360c0c61a04107857d32fe32753e0d","d64983270c054427ab0d6536b42da669","e0197ed5e7e04e498be1a089bb63f6d2","31d72485090e414a8a4867c1b9d72090","55a2aa1c77384838b533ba92e704e1cf","bdd633c5210a4fe58249f24dab7a993f","b1d5dbfd9dfc4446bdcb54a1551c9d37","fb698a1a53f6455095f413ac0ec34e6a","e791e01e6a4e4c51955d8afecedc75ae","a985706082d24985ae2c7945bd58c8c0","f390991000e34b37b6ffad1d554ed959","c2adc51f5df04ba69441633f1ea10651","417ed9e1b6514f168f9c07e48f35dc18","ea8c29baee5145e0b0add1ef567b7607","29947e952d4643b8a169b09a98015302","23fc66f22dfb4514a40ab5fb483c839d","a1c634932ff04795a9f1ea2f1c9d7bf8","2e218ca6424c4b7294eb00528772b99b"]},"id":"5s9bfwyKfzjl","outputId":"739ee8ec-be6e-45fe-9dbe-fdf740949f51","executionInfo":{"status":"error","timestamp":1680667871406,"user_tz":240,"elapsed":441898,"user":{"displayName":"Mila Quebec","userId":"10679210924075687823"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.14.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20230405_040348-3sl4g16z</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/animal-detect-vit/Non_ViT-B_16_224/runs/3sl4g16z' target=\"_blank\">jem-hadar-phage-5</a></strong> to <a href='https://wandb.ai/animal-detect-vit/Non_ViT-B_16_224' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/animal-detect-vit/Non_ViT-B_16_224' target=\"_blank\">https://wandb.ai/animal-detect-vit/Non_ViT-B_16_224</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/animal-detect-vit/Non_ViT-B_16_224/runs/3sl4g16z' target=\"_blank\">https://wandb.ai/animal-detect-vit/Non_ViT-B_16_224/runs/3sl4g16z</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/187 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604ceab2878944bea3461350b8ad3b66"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch : 1 - loss : 3.6352 - acc: 0.0793 - val_loss : 3.4983 - val_acc: 0.1051\n","\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/187 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"080bf26cdf5946de8405ac889a1b9a9d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch : 2 - loss : 3.5012 - acc: 0.1038 - val_loss : 3.4373 - val_acc: 0.1197\n","\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/187 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb698a1a53f6455095f413ac0ec34e6a"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-ae065a11cb89>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# save model to file in Google Drive\n","model_path = '/content/drive/MyDrive/IFT3710/ViT/pre-vit-b_16_224.pth'\n","torch.save(vit_model.state_dict(), model_path)"],"metadata":{"id":"awwakYQ4wTFa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_6-rzUvMhqcR"},"source":["### Test ###"]},{"cell_type":"code","source":["!pip install -Uqq ipdb\n","import ipdb\n","%pdb off"],"metadata":{"id":"aHXt-jyTFP5S"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6Pg8AjGhqcR"},"outputs":[],"source":["train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True)#suggested_max_workers_for_colab_env = 6\n","valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True)\n","test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True)\n","\n","correct = 0\n","total = 0\n","y_true = []\n","y_pred = []\n","wrong_pred =[]\n","right_label = []\n","with torch.no_grad():\n","    # for data in test_loader:\n","    for data in tqdm(test_loader):\n","    # for data, labels in tqdm(test_loader) :\n","        # data = data.to(device)\n","        # labels = label.to(device)\n","        # output = vit_model(data)\n","        \n","        images = data[0].to(device)\n","        labels = data[1].to(device)\n","        outputs = vit_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        # ipdb.set_trace(context=6)\n","        y_pred.extend(predicted.tolist())\n","        y_true.extend(labels.tolist())\n","        # ipdb.set_trace(context=6)\n","        total += labels.size(0)\n","        \n","        for i in range(0,len(predicted)):\n","              if predicted[i].item() != labels[i].item():\n","                    wrong_pred.append(predicted[i].item())\n","                    right_label.append(labels[i].item())\n","        \n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the test images: %d %%' % (\n","    100 * correct / total))"]},{"cell_type":"markdown","metadata":{"id":"2jH5_zgghqcR"},"source":["###   Analyse  ###"]},{"cell_type":"code","source":["# pip install pandas"],"metadata":{"id":"C4lugsIDae0R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np \n","import pandas as pd \n","import math\n","import matplotlib.pyplot as plt"],"metadata":{"id":"2bdkp-MRbBt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-t-t45uhqcR"},"outputs":[],"source":["nb_wrong_pred = []\n","for i in range(0,50):\n","    nb_wrong_pred.append(wrong_pred.count(i))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhBPJaG9hqcR"},"outputs":[],"source":["list_animal = list(dict_label_animal.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJl4FuYEhqcR"},"outputs":[],"source":["plt.bar(range(50), nb_wrong_pred)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Us_6dX14hqcR"},"outputs":[],"source":["good_classification = []\n","bad_classification = []\n","for i in range(50):\n","    if nb_wrong_pred[i]<=5:\n","        good_classification.append(i)\n","    if    nb_wrong_pred[i]>=60:\n","        bad_classification.append(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOd8y4F6hqcR"},"outputs":[],"source":["def find_right_animal(m):\n","    wrong_pred_m =[]\n","    for j in [i for i,x in enumerate(wrong_pred) if x == m]:\n","        wrong_pred_m.append(right_label[j])\n","    return list_animal[max(wrong_pred_m,key = wrong_pred_m.count)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ll3nppYnhqcR"},"outputs":[],"source":["for i in good_classification :\n","    print('ViT a bien classifie '+animals[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhDBUNZmhqcR"},"outputs":[],"source":["for i in bad_classification:\n","    print('ViT a mal classifie '+animals[i]+' , melange souvent avec '+find_right_animal(i))"]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","confusion_mat = confusion_matrix(y_true, y_pred)\n","\n","# Seaborn heatmap\n","fig, ax = plt.subplots(figsize=(20,12))  \n","sns.heatmap(confusion_mat, annot=True, cmap='rocket_r')\n","\n","# set plot labels\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","\n","# show plot\n","plt.show()"],"metadata":{"id":"FLDIpp3fFVfB"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"13d0Qd8bQ--7B4RVTxcOkF4NQEcOjI-Ay","timestamp":1680662046179},{"file_id":"1J1ory4j_zNbOWhvGsgeF66wdVXGCUMEn","timestamp":1680639505005}],"collapsed_sections":["ceJFyCPFhqcP"]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"},"widgets":{"application/vnd.jupyter.widget-state+json":{"604ceab2878944bea3461350b8ad3b66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10bad83e28994a01a4bc9f53a9bb94ee","IPY_MODEL_0807c83e55954c9b98effffcec71cfa3","IPY_MODEL_75a9ccf6a0d140af8dc0ce54aa5540c2"],"layout":"IPY_MODEL_827f5fb6ffdb40a29b7c6d7192f44d82"}},"10bad83e28994a01a4bc9f53a9bb94ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f18ebd5cef545b7a6b300efa137a7fc","placeholder":"​","style":"IPY_MODEL_067eb93117d146bdb6de8cfcd0f4e4fc","value":"100%"}},"0807c83e55954c9b98effffcec71cfa3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac08af728b57424e8dfc8f673ce0cfd9","max":187,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a235b4d089146768e3a8ecbd5a45c86","value":187}},"75a9ccf6a0d140af8dc0ce54aa5540c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c472f0072e634f5b8fee346635065590","placeholder":"​","style":"IPY_MODEL_5c14fd1df17b412bbae5d293bcd17525","value":" 187/187 [02:50&lt;00:00,  1.17it/s]"}},"827f5fb6ffdb40a29b7c6d7192f44d82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f18ebd5cef545b7a6b300efa137a7fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"067eb93117d146bdb6de8cfcd0f4e4fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac08af728b57424e8dfc8f673ce0cfd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a235b4d089146768e3a8ecbd5a45c86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c472f0072e634f5b8fee346635065590":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c14fd1df17b412bbae5d293bcd17525":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"080bf26cdf5946de8405ac889a1b9a9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d3091f44d9946e0b7b0340fb4374d50","IPY_MODEL_7c9a8280b90f42c999325837e181f2a9","IPY_MODEL_e8705be5da6343218fc924a9cec109ee"],"layout":"IPY_MODEL_35360c0c61a04107857d32fe32753e0d"}},"7d3091f44d9946e0b7b0340fb4374d50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d64983270c054427ab0d6536b42da669","placeholder":"​","style":"IPY_MODEL_e0197ed5e7e04e498be1a089bb63f6d2","value":"100%"}},"7c9a8280b90f42c999325837e181f2a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31d72485090e414a8a4867c1b9d72090","max":187,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55a2aa1c77384838b533ba92e704e1cf","value":187}},"e8705be5da6343218fc924a9cec109ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdd633c5210a4fe58249f24dab7a993f","placeholder":"​","style":"IPY_MODEL_b1d5dbfd9dfc4446bdcb54a1551c9d37","value":" 187/187 [02:44&lt;00:00,  1.17it/s]"}},"35360c0c61a04107857d32fe32753e0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d64983270c054427ab0d6536b42da669":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0197ed5e7e04e498be1a089bb63f6d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31d72485090e414a8a4867c1b9d72090":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55a2aa1c77384838b533ba92e704e1cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bdd633c5210a4fe58249f24dab7a993f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1d5dbfd9dfc4446bdcb54a1551c9d37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb698a1a53f6455095f413ac0ec34e6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e791e01e6a4e4c51955d8afecedc75ae","IPY_MODEL_a985706082d24985ae2c7945bd58c8c0","IPY_MODEL_f390991000e34b37b6ffad1d554ed959"],"layout":"IPY_MODEL_c2adc51f5df04ba69441633f1ea10651"}},"e791e01e6a4e4c51955d8afecedc75ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_417ed9e1b6514f168f9c07e48f35dc18","placeholder":"​","style":"IPY_MODEL_ea8c29baee5145e0b0add1ef567b7607","value":" 38%"}},"a985706082d24985ae2c7945bd58c8c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_29947e952d4643b8a169b09a98015302","max":187,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23fc66f22dfb4514a40ab5fb483c839d","value":71}},"f390991000e34b37b6ffad1d554ed959":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1c634932ff04795a9f1ea2f1c9d7bf8","placeholder":"​","style":"IPY_MODEL_2e218ca6424c4b7294eb00528772b99b","value":" 71/187 [01:06&lt;01:39,  1.17it/s]"}},"c2adc51f5df04ba69441633f1ea10651":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"417ed9e1b6514f168f9c07e48f35dc18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea8c29baee5145e0b0add1ef567b7607":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"29947e952d4643b8a169b09a98015302":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23fc66f22dfb4514a40ab5fb483c839d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a1c634932ff04795a9f1ea2f1c9d7bf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e218ca6424c4b7294eb00528772b99b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}