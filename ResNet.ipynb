{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4d60b6b"
      },
      "source": [
        "Lien de téléchargement des données: https://cvml.ista.ac.at/AwA2/        \n",
        "\n",
        "13GB file : https://cvml.ista.ac.at/AwA2/AwA2-data.zip"
      ],
      "id": "c4d60b6b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxxp8BZhv_c5"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ],
      "id": "Yxxp8BZhv_c5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-1U0EBIj5YB"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ],
      "id": "u-1U0EBIj5YB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jny1ySOrhNIL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "jny1ySOrhNIL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxjyjl4YheyU"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.copytree('/content/drive/MyDrive/Animals_with_Attributes2/','AWA2')"
      ],
      "id": "vxjyjl4YheyU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEDq89HkpDkC"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"ResNet18-Aug-PreTrain-50epoches\",\n",
        "    \n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"architecture\": \"ResNet\",\n",
        "    \"dataset\": \"AWA2\",\n",
        "    \"epochs\": 10,\n",
        "    }\n",
        ")"
      ],
      "id": "XEDq89HkpDkC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4d23f77"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import cv2 # Pour utiliser open_cv, il faut la version de python est 3.7\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import math\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision \n",
        "from torchvision.io import read_image\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "np.random.seed(0)"
      ],
      "id": "f4d23f77"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e8e4d92"
      },
      "outputs": [],
      "source": [
        "# Constant. Should be the path to the folder named JPEGImages, containing the 33K images in its subfolders.\n",
        "DATA_FOLDER_PATH = '/content/AWA2/'\n",
        "JPEGIMAGES_FOLDER_PATH = '/content/AWA2/JPEGImages/'"
      ],
      "id": "2e8e4d92"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60066a3e"
      },
      "source": [
        "# Note : Some labels have a low number of images. \n",
        "\n",
        "## Possible solutions to explore : \n",
        "    Data augmentation : creating new training data by applying random transformations to existing images, such as rotating, cropping, or flipping them."
      ],
      "id": "60066a3e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ade44228"
      },
      "outputs": [],
      "source": [
        "def find_num_images_per_label(img_dir = JPEGIMAGES_FOLDER_PATH): #-> tuple[dict,dict]: \n",
        "    \"\"\" \n",
        "    USEFUL FOR SAMPLING.\n",
        "    Return a dict with keys as the 50 labels, and values being the number of images in each subdirectory corresponding to label\n",
        "    and a second dict with the relative numbers (proportion) for every label compared to the total number of images (useful for sampling)\"\"\"\n",
        "    labels_dirs = os.listdir(img_dir)\n",
        "    labels_dirs = [f for f in labels_dirs if not f.startswith('.')] # remove hidden files\n",
        "    num_images_per_label = dict.fromkeys(labels_dirs)\n",
        "    proportions_images_per_label = dict.fromkeys(labels_dirs)\n",
        "    total_num_images = 0\n",
        "\n",
        "    # Update absolute number of images per label\n",
        "    for i, label in enumerate(labels_dirs) : \n",
        "        specific_label_path = os.path.join(img_dir, labels_dirs[i])\n",
        "        num_images_label = len(os.listdir(specific_label_path))\n",
        "        total_num_images += num_images_label\n",
        "        num_images_per_label[label] = num_images_label\n",
        "\n",
        "    # Update relative number of images per label (proportion)\n",
        "    for i, label in enumerate(labels_dirs) : \n",
        "        num_images_label = num_images_per_label[label]\n",
        "        proportion_label = round(num_images_label / total_num_images, 4)\n",
        "        proportions_images_per_label[label] = proportion_label\n",
        "\n",
        "    return num_images_per_label, proportions_images_per_label\n",
        "\n",
        "num_images_per_label, proportions_images_per_label = find_num_images_per_label()"
      ],
      "id": "ade44228"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc3b38ba"
      },
      "outputs": [],
      "source": [
        "ANNOTATIONS_FILENAME = 'annotations.csv'\n",
        "\n",
        "def create_annotations_csv_file(annotations_filename = ANNOTATIONS_FILENAME, img_dir = JPEGIMAGES_FOLDER_PATH): \n",
        "    \"\"\" \n",
        "    Create a csv annotations_file, annotations.csv, with two columns, in the format : \n",
        "                        path/to/image, label\n",
        "    \n",
        "    The annotation csv is necessary for DataLoader.\n",
        "    \"\"\"\n",
        "    \n",
        "    labels_dirs:list = os.listdir(img_dir)\n",
        "   \n",
        "    if os.path.exists(annotations_filename):\n",
        "        os.remove(annotations_filename)\n",
        "        print(f'Deleted existent {ANNOTATIONS_FILENAME} file.\\n ---------------------------')\n",
        "    \n",
        "    with open(annotations_filename, 'w', newline='') as file :\n",
        "        writer = csv.writer(file, dialect='excel', delimiter=',')\n",
        "\n",
        "        for i, label in enumerate(labels_dirs) : \n",
        "\n",
        "            specific_label_path = os.path.join(img_dir, label)\n",
        "            images_names = os.listdir(specific_label_path)\n",
        "\n",
        "            for j, image_name in enumerate(images_names):\n",
        "                full_path_to_img= os.path.join(specific_label_path, image_name)\n",
        "                full_path_to_img= os.path.join(label, image_name)\n",
        "\n",
        "                row = [full_path_to_img, label]\n",
        "                writer.writerow(row)\n",
        "\n",
        "    print(f'Sucessfully created {ANNOTATIONS_FILENAME} file.')\n",
        "\n",
        "create_annotations_csv_file()"
      ],
      "id": "cc3b38ba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24dc7a00"
      },
      "outputs": [],
      "source": [
        "labels_dict = {}\n",
        "with open(DATA_FOLDER_PATH+\"classes.txt\") as f:\n",
        "    for line in f:\n",
        "        (key,val) = line.split()\n",
        "        labels_dict[val] = int(key)-1\n",
        "print(labels_dict)"
      ],
      "id": "24dc7a00"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08979242"
      },
      "outputs": [],
      "source": [
        "from torchvision.io import read_image, ImageReadMode\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class AWA2Dataset(Dataset): # Dataset class to serve as input for the DataLoader.\n",
        "    \"\"\" \n",
        "    Dataset class to serve as input for the DataLoader.\n",
        "    Implements all the required methods and more. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, annotations_file=ANNOTATIONS_FILENAME, img_dir=JPEGIMAGES_FOLDER_PATH, \n",
        "                transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        numbers_infos_dicts: tuple[dict,dict] = find_num_images_per_label(img_dir=JPEGIMAGES_FOLDER_PATH)\n",
        "        self.num_images_per_label = numbers_infos_dicts[0]\n",
        "        self.proportions_images_per_label = numbers_infos_dicts[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        key = self.img_labels.iloc[idx, 1]\n",
        "\n",
        "        # Mapping the labels from string to tensor\n",
        "        label = labels_dict[key]\n",
        "\n",
        "        image = read_image(path = img_path, mode = ImageReadMode.RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "class Subset_(AWA2Dataset) : \n",
        "    def __init__(self, dataset, indices, transform=None):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        original_index_in_AWA2Dataset = self.indices[index]\n",
        "        image, label = self.dataset[original_index_in_AWA2Dataset]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "    "
      ],
      "id": "08979242"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-Zqfn6AI_4v"
      },
      "outputs": [],
      "source": [
        "transforms_pipeline_train = transforms.Compose([\n",
        "                    ## Input size\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.Resize((256,256)),\n",
        "                    # transforms.Grayscale(num_output_channels=3),\n",
        "                    \n",
        "                    ## Data augmentation \n",
        "                    transforms.RandomRotation(15),\n",
        "                    transforms.RandomHorizontalFlip(p=0.4),\n",
        "                    # transforms.RandomApply(transforms.RandAugment(), p=0.4), # 40% of the time, apply a random additional combo of transformations #https://sebastianraschka.com/blog/2023/data-augmentation-pytorch.html\n",
        "                    transforms.ColorJitter(brightness=0.2,\n",
        "                                            contrast=0.2,\n",
        "                                            saturation=0.2,\n",
        "                                            hue=0.1),\n",
        "                    transforms.RandomCrop((224,224)),  # transforms.RandomResizedCrop(size=(224,224), scale=(0.6, 0.9), ratio=(0.5, 1.08,))\n",
        "                    ## Normalize\n",
        "                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the \n",
        "                    transforms.Normalize(mean = [0.4643, 0.4640, 0.3985] , std=[0.2521, 0.2425, 0.2538]) # real mean and std of AwA2\n",
        "                ])\n",
        "\n",
        "\n",
        "transforms_pipeline_test = transforms.Compose([\n",
        "                    ## Input size\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.Resize((256,256)),\n",
        "                    # transforms.Grayscale(num_output_channels=3),\n",
        "                    transforms.CenterCrop((224,224)),   \n",
        "                    ## Normalize\n",
        "                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the \n",
        "                    transforms.Normalize(mean = [0.4643, 0.4640, 0.3985] , std=[0.2521, 0.2425, 0.2538]) # real mean and std of AwA2\n",
        "                ])\n",
        "\n",
        "\n",
        "transforms_resize = transforms.Compose([\n",
        "                    ## Input size\n",
        "                    transforms.ToPILImage(),\n",
        "                    transforms.Resize((224,224)),\n",
        "\n",
        "                    ## Normalize\n",
        "                    transforms.ToTensor(), # Already a tensor as implemented in Dataset class with the \n",
        "                    transforms.Normalize(mean = [0.4643, 0.4640, 0.3985] , std=[0.2521, 0.2425, 0.2538]) # real mean and std of AwA2\n",
        "                ])\n",
        "\n",
        "\n",
        "# Initialize dataset and train/valid/test split \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = AWA2Dataset()\n",
        "n_images = len(dataset)\n",
        "# Split all indices into training/testing sets\n",
        "train_indices, test_indices = train_test_split(range(n_images), test_size=0.2, random_state=1)\n",
        "# Split training indices into training/validation sets.\n",
        "train_indices, valid_indices = train_test_split(train_indices, test_size=0.2, random_state=1)\n",
        "\n",
        "\n",
        "# Initialize the 3 DataSet objects (as Subset_) and apply the relevant Transforms to each subset (train/test/valid)\n",
        "train_data = Subset_(dataset, train_indices, transform = transforms_pipeline_train)\n",
        "valid_data = Subset_(dataset, valid_indices, transform = transforms_pipeline_test)\n",
        "test_data  = Subset_(dataset, test_indices, transform = transforms_pipeline_test) \n",
        "\n",
        "# W/O data augmentation\n",
        "# train_data = Subset_(dataset, train_indices, transform = transforms_resize)\n",
        "# valid_data = Subset_(dataset, valid_indices, transform = transforms_resize)\n",
        "# test_data  = Subset_(dataset, test_indices, transform = transforms_resize) \n",
        "\n",
        "# Initalize DataLoaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True)\n",
        "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True)"
      ],
      "id": "M-Zqfn6AI_4v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEfyXgU3Nq18"
      },
      "source": [
        "## Importation de ResNet et Entraînement:"
      ],
      "id": "LEfyXgU3Nq18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "727a669e"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "# resnet = models.resnet18(pretrained=False)  "
      ],
      "id": "727a669e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzn3A6e_xivQ"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, using CPU\")"
      ],
      "id": "wzn3A6e_xivQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOm5mnBrkOV5"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm"
      ],
      "id": "BOm5mnBrkOV5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "802f9705"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "model = resnet\n",
        "\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(50):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for data, label in tqdm(train_loader):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == label).float().mean()\n",
        "        epoch_accuracy += acc / len(train_loader)\n",
        "        epoch_loss += loss / len(train_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        epoch_val_accuracy = 0\n",
        "        epoch_val_loss = 0\n",
        "        for data, label in valid_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            val_output = model(data)\n",
        "            val_loss = criterion(val_output, label)\n",
        "\n",
        "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
        "            epoch_val_accuracy += acc / len(valid_loader)\n",
        "            epoch_val_loss += val_loss / len(valid_loader)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    # log metrics to wandb\n",
        "    wandb.log({\"loss\" : epoch_loss , \"acc\": epoch_accuracy, \"val_loss\" : epoch_val_loss, \"val_acc\": epoch_val_accuracy})\n"
      ],
      "id": "802f9705"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spmCXsjTgzbb"
      },
      "outputs": [],
      "source": [
        "# save model to file in Google Drive\n",
        "model_path = '/content/drive/MyDrive/resnet/ResNet_saved/ResNet_Aug_pretrain.pth'\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "id": "spmCXsjTgzbb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fC878tTg1G4"
      },
      "outputs": [],
      "source": [
        "# Load model \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "model = models.resnet18(pretrained=False)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# load saved model state dict\n",
        "model_path = '/content/drive/MyDrive/resnet/ResNet_saved/ResNet_aug_WOpretrain.pth'\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "id": "4fC878tTg1G4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test de ResNet sur \"test dataset\":"
      ],
      "metadata": {
        "id": "k_oWwRR7YzhJ"
      },
      "id": "k_oWwRR7YzhJ"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0495e340"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    wrong_pred =[]\n",
        "    right_label = []\n",
        "\n",
        "    for image, label in test_loader:\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = model(image)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        y_pred.extend(predicted.tolist())\n",
        "        y_true.extend(label.tolist())\n",
        "        total += label.size(0)\n",
        "\n",
        "        for i in range(0,len(predicted)):\n",
        "              if predicted[i].item() != label[i].item():\n",
        "                    wrong_pred.append(predicted[i].item())\n",
        "                    right_label.append(label[i].item())\n",
        "\n",
        "        correct += (predicted == label).sum().item()\n",
        "\n",
        "print('Accuracy of the model on the test images: {:.2f}%'.format(100 * correct / total))"
      ],
      "id": "0495e340"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyse du résultat du test:"
      ],
      "metadata": {
        "id": "_7NR6Dg9ZGrq"
      },
      "id": "_7NR6Dg9ZGrq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Rapport de classification:"
      ],
      "metadata": {
        "id": "ODpprFY4ZcNd"
      },
      "id": "ODpprFY4ZcNd"
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred)) "
      ],
      "metadata": {
        "id": "n5a7xikkZFyY"
      },
      "id": "n5a7xikkZFyY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Matrice de confusion:"
      ],
      "metadata": {
        "id": "MgzH7EElZlRQ"
      },
      "id": "MgzH7EElZlRQ"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FcOi8z4FDuui"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Seaborn heatmap\n",
        "fig, ax = plt.subplots(figsize=(20,12))\n",
        "sns.heatmap(confusion_mat, annot=True, cmap='rocket_r')\n",
        "\n",
        "# set plot labels\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "\n",
        "# show plot\n",
        "plt.show()"
      ],
      "id": "FcOi8z4FDuui"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Graphique histogramme de la distribution des classes mal prédites:"
      ],
      "metadata": {
        "id": "kumnJ7msZvlO"
      },
      "id": "kumnJ7msZvlO"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "63nz5kSFjE0U"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "path_class = DATA_FOLDER_PATH +\"classes.txt\"\n",
        "class_animal = pd.read_table(path_class,header= None)\n",
        "# class_animal\n",
        "animals = class_animal[1]\n",
        "dict_label_animal = {}\n",
        "n = 0\n",
        "for i in range(0,len(animals)):\n",
        "    dict_label_animal[animals[i]] = n\n",
        "    n+=1\n",
        "def label_to_num(tuple_labels):\n",
        "    list_labels =[]\n",
        "    for tuple_label in tuple_labels:\n",
        "        list_labels.append(dict_label_animal[tuple_label])\n",
        "    return torch.tensor(list_labels) \n",
        "\n",
        "nb_wrong_pred = []\n",
        "for i in range(0,50):\n",
        "    nb_wrong_pred.append(wrong_pred.count(i))\n",
        "\n",
        "list_animal = list(dict_label_animal.keys())\n",
        "\n",
        "plt.bar(range(50), nb_wrong_pred)\n",
        "plt.show()"
      ],
      "id": "63nz5kSFjE0U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Classes correctement prédites:"
      ],
      "metadata": {
        "id": "XgWv6WCHaD25"
      },
      "id": "XgWv6WCHaD25"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2B7luNW6jHp_"
      },
      "outputs": [],
      "source": [
        "good_classification = []\n",
        "bad_classification = []\n",
        "for i in range(50):\n",
        "    if nb_wrong_pred[i]<=75:\n",
        "        good_classification.append(i)\n",
        "    if nb_wrong_pred[i]>=100:\n",
        "        bad_classification.append(i)\n",
        "\n",
        "def find_right_animal(m):\n",
        "    wrong_pred_m =[]\n",
        "    for j in [i for i,x in enumerate(wrong_pred) if x == m]:\n",
        "        wrong_pred_m.append(right_label[j])\n",
        "    return list_animal[max(wrong_pred_m,key = wrong_pred_m.count)]\n",
        "\n",
        "for i in good_classification :\n",
        "    print('Model a bien classifie '+animals[i])"
      ],
      "id": "2B7luNW6jHp_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Classes faussement prédites:"
      ],
      "metadata": {
        "id": "SJtJgF9UaPiR"
      },
      "id": "SJtJgF9UaPiR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32yI9N6hjKHj"
      },
      "outputs": [],
      "source": [
        "for i in bad_classification:\n",
        "  print('Model a mal classifie '+animals[i]+' , melange souvent avec '+find_right_animal(i))"
      ],
      "id": "32yI9N6hjKHj"
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}